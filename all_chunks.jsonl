{"text": "foreign [Music] Delta which avoids setting an initial learning rate completely right so let's see what it does so I.\nhave some n epochs or n iterations so I have Computing the gradient then I keep exponentially moving history VT.\nokay then I do compute the update right so this is what my update is going to be right and.\nthat update is a ratio of two variables one is UT I have not defined what UT is yet but.\nit's ratio of u t and VT of course the square root is there but there is no ETA naught.\nhere right so we still need to see what is UT and how does this ratio take care of the.\nchallenges that I was talking about yeah so let's see so UT then I update WT because I've computed the.\nDelta which needs to be there right so that's uh this is simply uh yeah so this part is simply.\nequivalent to I mean the equivalent of minus ETA naught divided by square root of beta into Del WT which.\nI had in the previous algorithm and then my update tool was WT minus this right so this is my.", "metadata": {"video_title": "AdaDelta"}}
{"text": "I had in the previous algorithm and then my update tool was WT minus this right so this is my.\nDelta and then my update tool is just going to be WT plus Delta so that negative sign has been.\ntaken care of inside and I don't have the initial learning rate instead I have this in the numerator right.\nso all of this is making sense and this is what my UT is right so my UT is an.\naccumulation of the updates that I have made so remember this is nabla WT Square I am representing the gradient.\nas nabla WT right so derivative is knabla WT and the update is Delta WT right so this is what.\nthis is keeping track of the Deltas and my current derivatives are lab loss right so I have like two.\nhistories being tracked and then I'm taking a ratio of these histories and then adjusting the learning rate how does.\nit solve my problem is still not clear but right now we are just trying to understand what the equations.", "metadata": {"video_title": "AdaDelta"}}
{"text": "it solve my problem is still not clear but right now we are just trying to understand what the equations.\nare right so instead of one history which was VT I have two histories now it's also clear that one.\nhistory is kind of ahead of the other history right so VT was tracking everything that is happening up to.\nthis time step but UT is also tracking what has happened at this time step because it was also taking.\ninto account the current update which has been made okay uh so these two changes are these two ratios are.\nbeing uh these two histories are being tracked and then I'm taking a ratio of these two histories and that.\nis my effective learning rate so how does this help is the question okay foreign that's what I meant by.\nthat one history is running ahead the other St VT is already used in the current iteration but the UT.\nthat I have computed now will be used in the next iteration as UT minus 1 right so the numerator.", "metadata": {"video_title": "AdaDelta"}}
{"text": "that I have computed now will be used in the next iteration as UT minus 1 right so the numerator.\nis one step behind the denominator okay so now the numerator uh is a function of the history as opposed.\nto being a constant in RMS prop and add a grid so we have made it a function of the.\nhistory we have got rid of the initial learning rate but does it really solve our problem is the question.\nright and the other thing to note which will be important in understanding that it indeed solves our problem is.\nthat whatever uh derivative we have computed here whatever derivative we had computed we are already taking only a fraction.\nof that in VT right and even the update that we have right we are only taking a fraction of.\nthat when we are looking at UT okay this is something important to note why and how this makes a.\ndifference is something that we'll see but the main thing to note is that we are only taking a fraction.", "metadata": {"video_title": "AdaDelta"}}
{"text": "difference is something that we'll see but the main thing to note is that we are only taking a fraction.\nof the update that we have made and this update that we had made was uh a function of this.\nDelta WT that we had right the gradient that we had okay so uh don't worry I will just clear.\nthese things in the next couple of slides yeah so now suppose you are in a high curvature region right.\nso you are in a steep region and what do you want in the Steep region you want the effective.\nlearning rate to be small okay so at time step 0 your V 0 so your you can refer to.\nthe equations here the equations for VT uh UT and all of that I'll just not bother too much about.\nreferring to them again and again but these computations are according to these equations right so at any point you're.\nconfused you can just pause and verify that we have used the same equations right so V 0 is going.\nto be point one into Delta W 0 squares then you will have the sorry nabla W zero square and.", "metadata": {"video_title": "AdaDelta"}}
{"text": "confused you can just pause and verify that we have used the same equations right so V 0 is going.\nto be point one into Delta W 0 squares then you will have the sorry nabla W zero square and.\nnabla W 0 is the derivative then you have the update which is the Delta W naught the change that.\nyou're going to make in W naught and you're currently your U minus 1 is 0 right so then you.\nonly have Epsilon in the numerator U minus 1 is going to be 0. so we make that update then.\nyou do W 1 is equal to W naught plus Delta W naught then you compute U naught right and.\nthis is what I was emphasizing on right that this U naught is just taking a fraction of the derivative.\nthat you had right and now uh so yeah you U naught was actually uh beta times the old history.\nwhich is 0 in this case and 1 minus beta time the current update that we have made but the.\ncurrent update which is Delta is in turn a function of this nabla so I've just substituted the value of.", "metadata": {"video_title": "AdaDelta"}}
{"text": "which is 0 in this case and 1 minus beta time the current update that we have made but the.\ncurrent update which is Delta is in turn a function of this nabla so I've just substituted the value of.\ndelta uh W naught and in terms of uh sorry uh yeah Delta W naught in terms of navbla w.\nnaught right so this is nabla just in case you're confused and this is Delta okay so that's done okay.\nnow let's see what happens next I'll just keep continuing for a while so fraction of the history is being.\nstored for the next iteration now at t equal to 1 again you will just apply this B1 is equal.\nto 0.9 into the previous history plus point one into the current gradient and if I substitute the value of.\nthe prevent history then I get the entire equation in terms of the nav loss so nabla w 0 square.\nand abla W1 square right now I again uh do the update and now I'm ignoring the Epsilon because I.\nhave reached a state where I have non-zero U naught and Epsilon is very small so U naught plus Epsilon.", "metadata": {"video_title": "AdaDelta"}}
{"text": "have reached a state where I have non-zero U naught and Epsilon is very small so U naught plus Epsilon.\nis going to be very close to U naught anyways right so this is what the update is I make.\nthis update and then I compute U1 right and again U1 is a function of Deltas remember but the Deltas.\nin terms are functions of the nablas so I just put those values appropriately and I have computed this right.\nand now if you look at this right so or maybe let me just go to one more time step.\nt equal to 2 again I am doing the same thing as I'm just mechanically substituting values into the formula.\nand just making sure that I express everything in terms of Delta W I's right that's the only idea so.\nthat all the formulas become comparable right all the values become comparative so this is where I am now I'm.\njust going to focus on the U's and the v's okay so now if you look at this at this.\ntime step right yeah so when I was here at t equal to 2 time step I will use U1.", "metadata": {"video_title": "AdaDelta"}}
{"text": "just going to focus on the U's and the v's okay so now if you look at this at this.\ntime step right yeah so when I was here at t equal to 2 time step I will use U1.\ndivided by V 2 right so this is my U1 and this is my V2 and you can compare these.\ntwo equations and you will know that U1 is actually smaller than V2 right so if U1 is smaller than.\nV2 then the numerator is smaller than the denominator and hence the effective learning rate would be small and hence.\nmy problem has been taken care of right in the Steep region I am having a smaller denominator a smaller.\neffective learning rate right that's exactly what I wanted and the entire picture will become clear also when I talk.\nabout what happens in the flat regions right so that's also something which is coming so let's wait for that.\nto happen so both VT and UT are increasing right because both are accumulating gradients but because of this one.", "metadata": {"video_title": "AdaDelta"}}
{"text": "to happen so both VT and UT are increasing right because both are accumulating gradients but because of this one.\ntime step delay between them right one is U T minus one the other is VT so in the Steep.\nregions while both are growing UT is growing slightly slower than VT and hence the effective learning rate is going.\nto be smaller right now yeah that's always same here whereas in the RMS prop the effective learning rate would.\njust have been ETA by VT and hence there is no adoption Happening Here Right okay so this is all.\nthat I've already said you could just read what there's on the slide now let's see what happens in the.\nflat regions okay so now I'm in the flat region so this is some Delta w i right at the.\nith time step I am trying to take the gradient now what will happen here right so now if I.\nlook at uh U T minus 1 and VT okay now U T minus 1 remember is one time step.\nbehind a VT so then one way of looking at it is that in ut minus 1 these all are.", "metadata": {"video_title": "AdaDelta"}}
{"text": "look at uh U T minus 1 and VT okay now U T minus 1 remember is one time step.\nbehind a VT so then one way of looking at it is that in ut minus 1 these all are.\nparticipating right and in VT minus V T these all are participating because by this point the beta raised to.\nK is so small that that guy is effectively not participating right because there's this one time step difference so.\nthis is what the situation is now what is happening if you look at this curve right so since all.\nthe gradients were high high and then they started decreasing my UT since it's looking at more High guys than.\nthe low guys right so because of this one time so so v v t has this current guy also.\nwhich is again very small right and UT does not have this guy it has some other guy which was.\nactually high right because it's one time step behind so now UT is going to be larger than VT does.\nthat make sense because VT is one step higher so once you enter the Steep regions after a while because.", "metadata": {"video_title": "AdaDelta"}}
{"text": "that make sense because VT is one step higher so once you enter the Steep regions after a while because.\nthis new guy is getting more weightage in VT that sum would stop start becoming smaller as compared to UT.\nbecause UT still has to come to that step it's still in the older history and the older history was.\nall high gradients so it's still taking more benefit from the older history whereas VT is now relying more on.\nthe current guy and this current guy will affect U T minus 1 only in the next time step right.\nso now the history the U T minus 1 is going to be larger than VT and hence the ratio.\nis going to be larger and hence the effective learning rate is increasing as you are entering the uh flat.\nregions right and then you can continue arguing that once again if you enter the Steep region again because of.\nthis one time step delay what will happen is VT will start looking at a much bigger gradient than what.", "metadata": {"video_title": "AdaDelta"}}
{"text": "this one time step delay what will happen is VT will start looking at a much bigger gradient than what.\nUT is looking at right so UT might be looking at these guys whereas V T minus v u t.\nminus 1 would be looking at sorry yeah U T minus 1 might be looking at these guys whereas VT.\nwill start looking at the current gradient also and this is a very large value so it has more benefit.\nfrom the current large value and hence it would be larger than UT minus 1 and hence the effective learning.\nrate will again decrease right so that's what is happening here okay so that's that's the explanation of why having.\nthis two histories one delayed by one time step is helping because it allows you to catch up with what.\nis happening in the history and then update the gradients according right so these values are not important but as.\nlong as you have understood the pictorial explanation you will be able to relate it to what is happening with.", "metadata": {"video_title": "AdaDelta"}}
{"text": "long as you have understood the pictorial explanation you will be able to relate it to what is happening with.\nthese values right so therefore Delta allows the numerator to increase or decrease based on the current and past gradients.\nand that exactly what we wanted now let's see what happens if we run Ada Delta so as usual RMS.\nprop is oscillating Ada grad is taking more time to reach the Minima because it aggressively kills the learning rate.\nwhereas adaptive Delta let's just look at it again right so yeah so in the Steep regions as it became.\nsteeper it's decreased and then it again it did whatever was required to reach the Minima quickly right well the.\nother two algorithms are struggling one is oscillating while the other one is slow right yeah okay yeah now let's.\nlook at uh we we are also plotting now the VT and the UT right so uh because they are.\ndelayed by one time step their curvature would be the same right they would have the same shape so both.", "metadata": {"video_title": "AdaDelta"}}
{"text": "look at uh we we are also plotting now the VT and the UT right so uh because they are.\ndelayed by one time step their curvature would be the same right they would have the same shape so both.\nof them have the same shape but their magnitudes are different right because of this delay one is relying on.\nmore on the past history like one more time step from the past history whereas the other one is relying.\nmore on the current history so what is happening is that when you are entering this uh uh let's see.\nso what is happening is in this curve so what happened was initially you were in a high loss region.\nright or a high loss region but also relatively flatter region right here the difference between the Constable contour lines.\nis higher than what happens as you keep going down towards the valley right so that means initially your gradients.\nwere small right so the guy who is going to be behind which is U T minus 1 is going.", "metadata": {"video_title": "AdaDelta"}}
{"text": "were small right so the guy who is going to be behind which is U T minus 1 is going.\nto look at smaller gradients whereas the guy who is ahead is started looking at faster gradients higher gradients right.\nso that's why what is happening is that the magnitude of VT is large here and the magnitude of u.\nt is small here because UT is still looking at the flat history whereas BT is one step ahead so.\nit has looked at more uh steeper history than What U T minus 1 is looking at right therefore V.\nT is larger than UT and since we were going in a steep Direction all the way around this is.\na favorable situation to have right we want VT to be larger than UT so that effectively the ratio is.\nsmall and the learning rate is accordingly smaller right so that's exactly what we wanted and that is what is.\nhappening now I can show you uh this more clearly on the next slide where uh if you look at.", "metadata": {"video_title": "AdaDelta"}}
{"text": "happening now I can show you uh this more clearly on the next slide where uh if you look at.\nthe learning rates for Ada Delta and RMS prop then RMS prop it constantly decreases right because it does not.\nhave any adaptive nature as it uh kind of uh it has to keep uh decreasing because it's one over.\nthe accumulated uh history in this case and since the history was always steep whereas in other Delta you see.\nsome kind of adoption happening adaptation happening and overall Adar Delta is able to converge faster on the plot that.\nwe had see wow yeah so let's look at some of these values also so here we had set the.\ninitial learning rate to 0.013 so this curve is 0.013 divided by the square root of whatever and this both.\nstarted at Point 20 right that's the uh what the initial value was but this only decreased up to 0.12.\nand hence it was able to kind of converge faster whereas this was 0.05 and then it became uh uh.", "metadata": {"video_title": "AdaDelta"}}
{"text": "started at Point 20 right that's the uh what the initial value was but this only decreased up to 0.12.\nand hence it was able to kind of converge faster whereas this was 0.05 and then it became uh uh.\nso it decayed more as compared to what Ada Delta DK right so this is what has happened here and.\nthis is especially important because in the uh flat regions we did see a slight bump in the learning rate.\nright so it's not like monotonically decreasing but in the events it reached a bit closer to the uh I.\nknow it's still decreasing only right yeah sorry that's not correct yeah so that's that's how the two decays happened.\nokay so this is what the effective learning rates are looking like and now let's see how the algorithm proceeds.\nso as you can say that ADA Delta has already converged because it was not aggressively decaying the learning rate.\nit had a reasonably High Learning rate which allowed it to converge RMS prop is slowly getting there because it.", "metadata": {"video_title": "AdaDelta"}}
{"text": "it had a reasonably High Learning rate which allowed it to converge RMS prop is slowly getting there because it.\nhas aggressively decreased the learning rate right and now the maybe the usual problem of oscillations may be there or.\nmaybe not I think the figure has ended so I don't know the animation has ended so I don't know.\nbut the main thing is the difference between these two plots which is explained by these two products these are.\nthe same two plots drawn on the same scale right so now I've drawn both the plots on the same.\nscale here and in the case of R Delta it was not decaying aggressively whereas in this case it was.\nleaking aggressively right okay yeah so that's what we have already gone through now let's put all the algorithms together.\non one plot and see how they behave so the same loss function same starting point uh and we have.\nthese different algorithms right from gradient descent all the way up to Ida Delta and other Delta is the only.", "metadata": {"video_title": "AdaDelta"}}
{"text": "these different algorithms right from gradient descent all the way up to Ida Delta and other Delta is the only.\none which has this adaptive learning rate in the sense of not having an initial learning rate right so let's.\nsee and you see the expected behaviors right uh okay let's see let it stabilize and we'll comment on it.\nso you see all of it all the behaviors that we expected the black curve is gradient descent which is.\nslowly moving we know that that's does not have any momentum does not have any adaptation the uh the the.\nmomentum and the nestrov have the usual effect of oscillations and next of you can again see slightly better at.\ndealing with the oscillations but all three of them have a problem in moving in the W Direction because W.\nwas relatively sparse so they first try to move in the B Direction and then come back whereas other grid.\nRMS prop in Ada Delta they all start moving in W and B Direction and of these other Delta converts.", "metadata": {"video_title": "AdaDelta"}}
{"text": "RMS prop in Ada Delta they all start moving in W and B Direction and of these other Delta converts.\nthe faster because it was able to adjust the learning rate RMS prop as usual had the oscillation problem right.\nso all of these effects you may again wish to watch this a few times uh just play the video.\nwhen you go to the slides and just make sure you note notice all the behaviors that you are familiar.\nwith okay yeah so we'll end this here.", "metadata": {"video_title": "AdaDelta"}}
{"text": "foreign [Music] so let's see how do we go about that so the intuition is that we want to Decay.\nThe Learning rate in proportion to the update history right so for the features which are getting a lot of.\nupdates so I have a very dense feature so the derivatives are large and I updated it in the first.\nstep by a large value Second Step by a large value third step by a large value so I am.\ndoing reasonable updates for this so maybe now I need to be a bit conservative and Decay The Learning rate.\nbut if there was Speech feature which is very sparse got a very small update in the first iteration because.\nthe total gradient that I computed was very small again a small update in the second pass again a small.\nupdate in the third pass then I need to let the learning rate be high right because I'm getting small.\nupdates so let me at least move a bit aggressively in those update directions right so that's what uh kind.", "metadata": {"video_title": "AdaGrad"}}
{"text": "updates so let me at least move a bit aggressively in those update directions right so that's what uh kind.\nof is the intuition behind the update rule for Ada grad what I'm going to do is I'm going to.\nmaintain the history of the updates that I am making right okay so now let me try to explain what.\nis happening here so now I am taking a running some of the history of the update so it's this.\nDelta WT tells me uh what was the update that I've made or what was my gradients right and this.\nis a running sum at every time step I am adding the current uh derivative and the square of it.\nright so I'm just taking the uh squared derivative and keeping it as the history so it tells me the.\nmagnitude of the updates that I have done so far and then what I'm doing is I am taking the.\nlearning rate and I'm dividing it by this history so what will happen is if they have a feature which.\nis very dense and which has got a lot of updates then this history keeps increasing right because you are.", "metadata": {"video_title": "AdaGrad"}}
{"text": "is very dense and which has got a lot of updates then this history keeps increasing right because you are.\nmaking large updates at every time Step at time step 0 you made an update time step one you made.\nan update time step two you made an update and all of this is getting added here and all of.\nthese were large quantities because this this was say a spa a dense feature right then your VT after a.\nfew iterations will grow and the learning rate would appropriately shrink right but if this was a sparse feature then.\nacross iterations maybe even when I reach t equal to 100 nothing much has been accumulated here right because all.\nthese updates were very very small because this was a sparse feature so hence my VT has not grown much.\nand hence my learning rate will not decrease aggressively right so now the learning rate has become uh proportional or.\nrather inversely proportional to my update history if I made large updates then I degrees the learning rate aggressively if.", "metadata": {"video_title": "AdaGrad"}}
{"text": "rather inversely proportional to my update history if I made large updates then I degrees the learning rate aggressively if.\nI made only a few updates I don't decrease it aggressive right so that's what is happening here so just.\nas have you have the update equations for w you can also have the update equations for uh B right.\nokay so this is the algorithm from now on I'll not spend too much time on the code right I'll.\njust uh uh modification of the codes that you have seen so far so in this place here this is.\nwhere I'm accumulating the history right I'm just taking the previous update plus the current update square right that's what.\nthe equation said and then my learning rate here is going to be divided by this update right so now.\nI think these are small code Snippets which you are comfortable with and you can take the equation and write.\nthe code for that right so to now to see this in action right we need to First create some.", "metadata": {"video_title": "AdaGrad"}}
{"text": "the code for that right so to now to see this in action right we need to First create some.\ndata where one of the features is sparse right and I also remember that I can only show you a.\n2d plot that means I can have one feature on the air one axis and another feature on the other.\naxis so I'm just going to be able to work with two features one of them is going to be.\nW and the other one is going to be B right and remember the feature b or the feature corresponding.\nto B is dense because it's always on right the corresponding input is always one so I can't do much.\nabout that so what I'm going to do is that I am going to create a data where my X.\nwhich corresponds to W is going to be very sparse right so that's how I'm going to create this data.\nso let's see this is what I'm going to do so this is what I'm saying right the third bullet.\npoint here says that we just have two parameters W and B of these the input feature correspond to B.", "metadata": {"video_title": "AdaGrad"}}
{"text": "so let's see this is what I'm going to do so this is what I'm saying right the third bullet.\npoint here says that we just have two parameters W and B of these the input feature correspond to B.\nis always on so we can't really make its path so the only option is to make X pass so.\nthe way I've created the data for this is that I created some random 500 random X comma y Pairs.\nand then for roughly 80 of these pairs I set X to 0 right and thereby making the feature uh.\nX very sparse because 80 percent of the times it is 0 and that's the definition of a sparse feature.\nthat a majority of times are significantly more than a majority of times it's zero so that's how I created.\nthis training data and then I uh so I took that data and I ran the gradient descent momentum based.\ngradient descent and nestro accelerated gradient descent algorithm on that using that data and uh the law surface was something.", "metadata": {"video_title": "AdaGrad"}}
{"text": "gradient descent and nestro accelerated gradient descent algorithm on that using that data and uh the law surface was something.\nlike this and I am showing the 2D loss surface but let's try to understand uh let's try to make.\na commentary on it and try to see what it actually is right so as you know this red means.\nhigh so these are the regions here where the loss is a bit high and also in those regions you.\nsee that there's a slightly larger difference between consecutive contour lines that means they are a bit flat region right.\nand this region is blue here okay so that means in that region the loss is very low and and.\nalso from going from here to here as we go down let me just change the color going from here.\nto here as we go down the slope is very Steep and that is obvious because the difference between two.\nconsecutive contour lines is now very small right so then this is essentially a surface like this which is peaking.", "metadata": {"video_title": "AdaGrad"}}
{"text": "consecutive contour lines is now very small right so then this is essentially a surface like this which is peaking.\nat the two red portions and then it's going into a valley which is the Blue Valley and it's going.\nsteeply into the valley right that's why you see this steep slopes here where the difference between two consecutive contour.\nlines is very small right so that's what the loss surface looks like now I ran gradient descent I started.\nfrom some point I started all the three algorithms from the same point right and I started gradient descent ah.\nmomentum based gradient is a menstrual right and the as I would like to say let me just go back.\nto the slides so there's something interesting that is these three algorithms are doing right and can you spot it.\nis the question right so what exactly is happening here and remember that uh I should have mentioned that my.\nhorizontal axis is the W axis and my vertical axis is the B axis as has been appropriately labeled on.", "metadata": {"video_title": "AdaGrad"}}
{"text": "horizontal axis is the W axis and my vertical axis is the B axis as has been appropriately labeled on.\nthe plot right now with that information can you tell me what peculiar behavior is happening here okay good so.\nwhat what you're saying is this right that uh I mean as I say it it looks like these algorithms.\nwent to a school where no one taught them greed in descent or no one taught them Pythagoras Theorem right.\nso what what I mean by that is uh if you look here uh initially all the three algorithms are.\nmoving in the direction of B right they are not they have moved only till this point right till this.\nyear they've moved only little bit in the direction of w right so till this point is where they have.\nreached so they have not much move from the initial value of w right but where they have moved a.\nlot in the direction of B right and that is expected because B was the dense feature hence its gradients.", "metadata": {"video_title": "AdaGrad"}}
{"text": "lot in the direction of B right and that is expected because B was the dense feature hence its gradients.\nwere larger and hence you were making larger movements in the direction of B whereas W was the sparse feature.\nintentionally made sparse as explained on the previous slide so the gradients for w are smaller and hence you are.\nmaking smaller movements in the direction of w right and after you reach the valley then the algorithms realize that.\nnow there is no more uh value in moving in the direction of B right anyways the gradients that are.\nbecoming smaller now because you have reached the Steep region and then it slowly starts moving very very slow steps.\nin the direction of w because there again the gradients are small what can you do but then it still.\nmoves in that direction and then it reaches the minimum all right so it takes a right angled path right.\nwhereas ideally you would have wanted something which just directly travels like this which would mean that it is making.", "metadata": {"video_title": "AdaGrad"}}
{"text": "whereas ideally you would have wanted something which just directly travels like this which would mean that it is making.\nproportionate movements in both the direction even though the derivatives in the direction of w are small you are still.\nable to somehow jack up the learning rate so that you are able to make proportionate movements there and then.\nreach the Minima faster or at least like a more appropriate root right so that's that's what is going wrong.\nwith these algorithms the algorithms that we've seen in the last lecture foreign are moving in the vertical Lexus and.\nthen they start moving along the W axis and we know the reason for this we already explained I just.\nexplained what the reason was and such sparsity right it is very common in large neural networks right you have.\nthousands of input features and uh you many of them might be sparse right so there's not something like a.", "metadata": {"video_title": "AdaGrad"}}
{"text": "thousands of input features and uh you many of them might be sparse right so there's not something like a.\ntoy example that I've created this is going to happen in many neural networks and hence you need to address.\nthis issue right that if you have sparse features can you still make faster movements in those directions right so.\nnow let's see what Ada grad does for this case Okay uh okay let me just play this so as.\nyou can see right aragrad is moving in the W direction also right why is that happening because the learning.\nrate for w because our denominator is going to be small our square root of VT is going to be.\nsmall right and if it's a quantity Which is less than 1 then the denominator is uh you're dividing ETA.\nby a quantity Which is less than 1 so hence ETA increases and hence your updates in that direction increase.\nand hence unlike the other algorithms you're proportionately moving in the direction of w Oz right of course here still.", "metadata": {"video_title": "AdaGrad"}}
{"text": "and hence unlike the other algorithms you're proportionately moving in the direction of w Oz right of course here still.\nthe momentum wage algorithm was faster because momentum of course has its Advantage right it makes mistakes but it still.\nreaches the Minima faster so let me just play that again you can see that the momentum based algorithms both.\nnestrum and all this reached faster right but compared to gradient descent adagrad was able to move faster right and.\nthere is still scope to improve Ada guide because it does not have a momentum term if I had added.\na momentum term term then I could combine the advantages of momentum as well as this adaptive learning rate and.\nwe'll probably get in that direction at some point right we'll get in that direction meaning we'll get towards such.\nan algorithm at some point right but for now you see the advantage that I'm able to move uh proportionately.", "metadata": {"video_title": "AdaGrad"}}
{"text": "an algorithm at some point right but for now you see the advantage that I'm able to move uh proportionately.\neven though my derivatives are small I have adaptedly changed the learning rate ETA divided by the history of the.\nupdates and that is allowing me to do that right so remember my effective learning rate oh is ETA divided.\nby square root of VT plus Epsilon and let me just ignore Epsilon for now so it's divided by the.\nsquare root of VT and for w this VT is going to be very small because I am not getting.\nlarge derivatives there hence ETA is getting divided by a very small quantity and if that quantity is less than.\n1 then effectively this learning rate is increasing right and that's what is pushing uh the derivative there right and.\nI'll just explain this in more detail on the next slide foreign but one disadvantage that not disadvantaged but one.\nthing I want you to notice and I'll just play this again if required is that as Ada grid starts.", "metadata": {"video_title": "AdaGrad"}}
{"text": "thing I want you to notice and I'll just play this again if required is that as Ada grid starts.\nreaching the Minima just observe from now on yeah so it's slowing down quite a bit right and what is.\nhappening is that by this time despite the uh smaller derivatives of in the direction of w by this time.\nyou have accumulated some history and now this effective learning rate is starting to slow down because it is getting.\ndivided by this accumulated history and by that time that history has increased right so as it comes close to.\nthe convergence not necessarily goes to conversions after a certain number of iterations when the history is becoming large then.\nadagrad is slowing down it so that's one observation I'm making on this slide and we'll come back to see.\nif we can somehow solve that problem okay so now let's examine this a bit more uh closely yeah so.\nyou remember that we are accumulating this history now V 0 is Delta W 0 squared V1 is Delta W.", "metadata": {"video_title": "AdaGrad"}}
{"text": "if we can somehow solve that problem okay so now let's examine this a bit more uh closely yeah so.\nyou remember that we are accumulating this history now V 0 is Delta W 0 squared V1 is Delta W.\n0 squared plus Delta W1 square and so on right so this is constantly growing VT is constantly growing uninhibitedly.\nbecause you are adding those uh derivatives right and uh our derivative is proportional to X that means for the.\nuh feature which is pass these derivatives would be small so then our history will accumulate slowly hence this effective.\nlearning rate will Decay slowly and it's also possible that initially it increases because us denominator might be less than.\none so the effective learning rate increases and even when it decreases it is slow to decrease right because your.\naccumulations may happen only after a large number of iterations such that the denominator becomes large and then the effect.", "metadata": {"video_title": "AdaGrad"}}
{"text": "accumulations may happen only after a large number of iterations such that the denominator becomes large and then the effect.\nof slowing down kicks in right but as opposed to this uh for a dense feature what would happen is.\nyou had the B feature which was a dense feature now this history is accumulating faster right because now the.\nderivatives are going to be non-zero uh quite always right because this is like an always on feature so your.\nhistory is accumulating fast uh it will not be zero for most of the time steps and therefore what will.\nhappen is uh VT will start growing rapidly and hence this will start decaying rapidly right so now effectively what.\nwill happen is this advantage that you had that you quickly move in the direction of B and came to.\nwhere you want it to be that will not happen right because now this directions the in the increments in.\nthe vertical direction will happen a bit slowly as opposed to earlier where it was quickly moving in the vertical.", "metadata": {"video_title": "AdaGrad"}}
{"text": "the vertical direction will happen a bit slowly as opposed to earlier where it was quickly moving in the vertical.\nDirection now you are moving a bit slowly in the vertical Direction because of this effective learning rate decaying as.\nyou keep making updates right so that's what is happening in this algorithm okay and we can now see that.\nuh visually uh so here uh d w let me just annotate the plot a bit so here DW is.\nessentially this Delta W that I have uh and BT is the history that is getting accumulated for the weight.\nw and ETA T is the learning rate for the weight w right so now what is happening let me.\njust uh show you things here right so as I had said right the VT will keep increasing right because.\nit's accumulating the gradients of course at some point when you reach close to the Minima and your Delta W.\nbecomes zero then there's no more additions happening right so that's why this will saturate here in this region right.", "metadata": {"video_title": "AdaGrad"}}
{"text": "becomes zero then there's no more additions happening right so that's why this will saturate here in this region right.\nand that you can see here if you look at the derivative right in that region the derivatives have now.\nalmost become zero and hence no additions are happening to the history right no large additions are happening to the.\nhistory it is very close to zero so some small small additions are happening but it's not reflecting in the.\nuh plot right and uh looking at this also so initially remember that the way your plot was and this.\nthis itself is interesting and we should try to look at it in the context of the uh now the.\nlast plot that we had right how is the derivative uh changing so I'll make some comments I'll go back.\nto the loss plot and I'll make some comments there and then come back here yes yes and I'm on.\nthe access x-axis you have the number of iterations right yeah so if I look at the loss plot right.", "metadata": {"video_title": "AdaGrad"}}
{"text": "to the loss plot and I'll make some comments there and then come back here yes yes and I'm on.\nthe access x-axis you have the number of iterations right yeah so if I look at the loss plot right.\nand let me just clear this and then make some comments here yeah so remember we had started from here.\nokay so initially the uh you are in a slightly it's t steep right from the beginning it's steep right.\nbut the steepness is increasing that means as I keep going down my derivatives will become larger and larger right.\nin magnitude the direction may be positive or negative but the magnitude will keep increasing then when I enter this.\nValley region my derivatives will start becoming smaller again right until they reach a point where the derivatives will become.\nclose to zero right so remember initially they'll increase because I'm going down the slope and the slope is increasing.", "metadata": {"video_title": "AdaGrad"}}
{"text": "close to zero right so remember initially they'll increase because I'm going down the slope and the slope is increasing.\nso the derivatives will increase and I'm showing it this way because they are increasing in the negative direction right.\nso the slope here is negative uh so this is what the curve is like so here the slope at.\nthis point it's negative right because you're going down as you are going down or as you are moving in.\nthe X Direction the value of y is decreasing right or as so that's why the slope is going to.\nbe negative so it's going to increase in the negative Direction then again it's going to start decreasing and then.\nit will come to zero right and that's exactly what is happening in the derivative plot that we just saw.\nnow let me go back to the plot yeah so this is what is happening right so initially uh here.\nyour derivatives as you keep going down it keeps increasing in magnitude right I mean remember that you are increasing.", "metadata": {"video_title": "AdaGrad"}}
{"text": "your derivatives as you keep going down it keeps increasing in magnitude right I mean remember that you are increasing.\non the negative side which is fine its magnitude is increasing and then at some point because here I'm going.\nto take the square that's why I'm talking about the magnitude right and then when it comes close to the.\nMinima it again starts decreasing in magnitude and then it comes to zero right and that's because the loss surface.\nwas like that right so this is how the history is getting accumulated this is how the derivatives are changing.\nand this is what is happening the red curve tells you what is happening to the effective learning rate as.\nyour history is accumulating the learning rate keeps decreasing exponentially right so if uh either to just plot let's just.\nsee this is what the learning rate curve looks like right I'm the same plot I've plotted separately so the.\nred curve I plotted in a new plot so that I can have more range on the y-axis so the.", "metadata": {"video_title": "AdaGrad"}}
{"text": "see this is what the learning rate curve looks like right I'm the same plot I've plotted separately so the.\nred curve I plotted in a new plot so that I can have more range on the y-axis so the.\ny-axis tells you the learning rate and the x axis is the number of iterations as the number of iterations.\nkeeps increasing the learning rate keeps decreasing exponentially right so here it's continuously decreasing okay and here for example in.\nthe first step the learning rate was 0.72 because I had initialized ETA to 0.1 and then the denominator turned.\nout to be 0.01 because V 0 turned out to be uh something which led to this point zero one.\nnine and then my learning rate initially was 0.72 but as I kept going down it decreased exponentially right so.\nthe main thing here to show is that my I had kept my learning rate initially learning rate is 0.1.\nbut it became 0.72 right because initially my history was not large and hence the learning rate increased and then.", "metadata": {"video_title": "AdaGrad"}}
{"text": "but it became 0.72 right because initially my history was not large and hence the learning rate increased and then.\nit started decreasing again and that's exactly what we wanted initially it could be high and then it starts decreasing.\nokay and the same argument for B also uh so here for yeah so the same argument for uh B.\nuh so the derivatives so here the scale is a bit different right so derivatives are on the x axis.\nyou have two thousand four thousand sorry twenty thousand forty thousand sixty thousand that's why you're not able to see.\nthe DWS here carefully uh so that again the same behavior happens your gradients change uh uh as you go.\nalong and the history keeps getting accumulated the only thing I need you to focus on in this plot is.\nthat your history is constantly increasing and as your history is increasing here your learning rate is decaying even more.\nfaster right so here notice that the learning rate starts off itself with a very small value which is point.", "metadata": {"video_title": "AdaGrad"}}
{"text": "faster right so here notice that the learning rate starts off itself with a very small value which is point.\nzero one zero right and again I'd initialize the learning rate to the same value right so here again so.\nhere see remember the learning rate was 0.1 but your initial gradients were high because B is a very Spa.\ndense feature so V 0 itself was 84.45 right and since that was high your initial learning rate itself was.\nsmall and hence you are moving less aggressive in the direction of b as compared to gradient descent and uh.\nnestro and moment right and that's why right these gradients are also getting accumulated very quickly it's already 84 this.\nhistory right the blue curve that you see already starts off at 84 and it keeps growing from there on.\nright that's why it reaches very high digits right it treats reaches five digit numbers and that's why your learning.", "metadata": {"video_title": "AdaGrad"}}
{"text": "right that's why it reaches very high digits right it treats reaches five digit numbers and that's why your learning.\nrate gets killed off aggressively right because your denominator has now become five digit number right the square root of.\na five digit number okay and that's again in line with what the intuition that we had right so these.\nplots are again an explanation of everything that we saw right we're just seeing how the learning rate is changing.\nhow the gradients are accumulating aggressively in the case of B and hence the learning rate is decreasing aggressively right.\nand the main main point is that in B the learning rate started off itself with 0.010 and then kept.\ndecreasing from there whereas for w the learning rate started off with 0.72 which was higher than the ETA which.\nI had said and then started out decreasing from there and that's exactly what we wanted because W is a.", "metadata": {"video_title": "AdaGrad"}}
{"text": "I had said and then started out decreasing from there and that's exactly what we wanted because W is a.\nsparse feature so let the learning rate be higher and then let it decay as the gradients accumulate B is.\na dense feature so let the learning rate be small because anyways which are going to get large gradients there.\nright so that adaptive Behavior has been seen here right okay yeah now one more thing to note is that.\neven though as you come close to the Minima right as you Class come close to the Minima your derivatives.\nhave become zero you here you can't see them also because the scale here is very uh large uh so.\nthe derivatives have become zero that means these these quantities right that means these quantities have become zero right but.\nyour history does not become zero and that makes sense right because it's a running sum you're continuously summing it.\nup so even if the new terms that are getting added are zero you still have the previous terms which.", "metadata": {"video_title": "AdaGrad"}}
{"text": "up so even if the new terms that are getting added are zero you still have the previous terms which.\nyou had accumulated in the sum so your history will going to be remain large only right so that's what.\nis happening here ah and that could cause problems right and that exactly what is causing problems near the Minima.\nwhen your gradients are becoming small right but your history is still large and hence your effective learning rate is.\nsmall so now the gradients are small the effective learning rate is small and hence when you're close to the.\nMinima I'll close in these flat regions you're not able to move fast right because you are not getting rid.\nof this accumulative history that you have and we need to see if we can do something about that right.\nso let's just uh look at that is it clear what I just said okay so now by using a.\nparameter specific learning rate uh we have ensured that despite W being sparse we are still being able to move.", "metadata": {"video_title": "AdaGrad"}}
{"text": "so let's just uh look at that is it clear what I just said okay so now by using a.\nparameter specific learning rate uh we have ensured that despite W being sparse we are still being able to move.\nin that direction and also we have ensured that uh B is going undergoing a lot of updates but its.\neffective learning rate is decreasing uh and there is this was something which is to ponder about which is not.\ntheoretically Justified now you could imagine that in the denominator I could have had ETA divided by VT as opposed.\nto square root of VT right I would have got the same effect that my learning rate is inversely proportional.\nthe effective learning rate is inversely proportional to the accumulated history right but if you replace square root by just.\nV T then this algorithm does not work so well and at least my guess is one of the reasons.\nfor that is that if you don't take the square root then these quantities become very large very quickly right.", "metadata": {"video_title": "AdaGrad"}}
{"text": "for that is that if you don't take the square root then these quantities become very large very quickly right.\nand maybe that kills of the learning rate in an unfavorable way right but it's there's no clear theoretical justification.\nthat I have seen what's the flip side while we got an Adaptive learning rate for W and B the.\nflip side is that over time the effective learning rate for B became so small that it was having a.\ndifficulty in moving in the W Direction in the B direction right so you see that now I was able.\nto reach the right value of w right I was able to reach the right value of w that's the.\nright value of w because my Minima is also here right so I'm in line with the right value of.\nw but I took longer time to find the right value of b as opposed to the other algorithms which.\nfound the right value of B first and then took a longer time to find the right value of w.\nright and this is happening because the history for B has accumulated and the updates in B are now very.", "metadata": {"video_title": "AdaGrad"}}
{"text": "found the right value of B first and then took a longer time to find the right value of w.\nright and this is happening because the history for B has accumulated and the updates in B are now very.\nvery small and we saw that in the plots where on the x axis on the sorry on the y-axis.\nyou had very large numbers which indicated a large amount of History has been accumulated so the question now is.\ncan we avoid this.", "metadata": {"video_title": "AdaGrad"}}
{"text": "foreign [Music] move to a different algorithm and before doing that we are going to revisit the lp Norm right.\nso LP Norm is Gen the general formula for LP Norm is as given here right and now if uh.\nyour p is equal to 2 then you get the L2 Norm right so then you get take the squares.\nof the values and then take the square root right so that's the L2 norm and remember when we had.\nintroduced Adam I had said that actually we were using uh L to Norm they're the exponentially weighted uh L2.\nNorm there and that's why I'm revisiting this because now we are going to see the possibility of using something.\nother than the L2 Norm okay uh all of this will become clear soon so this is the uh yeah.\nso we can visualize this so let's let's fix the norm let's take vectors which have Norm one right and.\nnow if you try to visualize it now for L one the shape looks like this right so what does.\nthat mean that for any point on this surface the norm is going to be 1 right and you can.", "metadata": {"video_title": "AdaMax and MaxProp"}}
{"text": "now if you try to visualize it now for L one the shape looks like this right so what does.\nthat mean that for any point on this surface the norm is going to be 1 right and you can.\nsee that so this point for example here is minus 0.5 sorry 0.5 comma minus 0.5 foreign with this right.\nand now if I substitute that in this formula I'll get the answer as 1. right so all the points.\nwhich lie on this uh uh Square they all are going to have the norm as uh one right and.\nnow if you look at L2 Norm you would have seen this oops yeah so if you look at LD.\nNorm now all the points lying on this circle would have are the enormous one and it makes sense right.\nbecause the equation that you get is the following right so I'm saying that X1 I'll not use the mod.\nhere because anyways I'm squaring okay let me just write the mod plus X2 Square equal to 1 right you.\nare looking for All Points which satisfy this equation and of course they will lie on a circle right so.", "metadata": {"video_title": "AdaMax and MaxProp"}}
{"text": "are looking for All Points which satisfy this equation and of course they will lie on a circle right so.\nthat's that's the how the norm works now what happens is that as uh you take larger and larger Norms.\nright greater than 2 3 4 10 hundred then if you have small values of x 1 x 2 right.\nthen these computations become numerically unstable right because any small value raised to a large value will become very very.\nsmall or the other way around right if they are greater than one again if you do 1.1 raise to.\n100 it will just blow up right not maybe 100 but a slightly higher power than that it might blow.\nup right and then you don't want that right so that's why L2 Norm is the most preferred uh Norm.\nuh but as P keeps increasing and it tends to Infinity then something interesting happens so it's as P becomes.\nInfinity then the norm just boils down to the max right so if you look at L Infinity Norm then.", "metadata": {"video_title": "AdaMax and MaxProp"}}
{"text": "Infinity then the norm just boils down to the max right so if you look at L Infinity Norm then.\nit's just taking the max of the values that you have right so in the vector you have the values.\nx 1 x 2 up to X N and L Infinity Norm is just the max of this and you.\ncan quickly prove why that is the case so if if I had uh if I wanted to uh yeah.\nso let's see so I'm talking about x 1 raised to P plus X2 raised to p all the way.\nup to X n raised to P right and then the 1 by pH root of that okay and I.\nwant to make a case for y this would be Infinity when P why would this be equal to the.\nmax of the values x 1 x 2 x n if P tends to Infinity rate so if I take.\nlimit P tends to Infinity I'll just give you a rough fight outline of the proof I will not do.\nthe whole thing I'll just do the necessary part now what I can do is I can take this X1.\nby P outside right and let I'm assuming that X1 is the maximum value let's assume without loss of generality.", "metadata": {"video_title": "AdaMax and MaxProp"}}
{"text": "the whole thing I'll just do the necessary part now what I can do is I can take this X1.\nby P outside right and let I'm assuming that X1 is the maximum value let's assume without loss of generality.\nwe can assume that X1 is the maximum value so this would become X2 by X1 raised to p and.\nso on and you will have x n by X1 raised to p and then the 1 by P at.\nthe root of the whole thing right now as P tends to Infinity what will happen is as each of.\nthese quantities is uh each of these quantities is actually less than one right because X1 is the largest value.\nso all the other quantities divided by X1 is going to be a quantity less than one so now if.\nyou're going to raise this to the power of P all of NP tends to Infinity then all of these.\nterms will disappear right so then you'll just have X1 raised to p and then the pth root of that.\nwhich will just be X1 so what you get back is just the maximum element that you had in the.", "metadata": {"video_title": "AdaMax and MaxProp"}}
{"text": "terms will disappear right so then you'll just have X1 raised to p and then the pth root of that.\nwhich will just be X1 so what you get back is just the maximum element that you had in the.\nvector right so that's why LP Norm the L Infinity Norm is just like taking is equivalent to taking the.\nmaximum value and that's all we need to do right so that's that's easy to compute there's no numerical instability.\nthere right so that's uh that's an interesting result okay so now we understand about LP Norms in general and.\nwe understand about L Infinity which is just taking the maximum value right now what is the point that we're.\ntrying to make here right so recall the equation of VT this was the equation of VT that we had.\nand this was as we had seen equivalent to doing right uh beta so what if I just expand this.\nright then what we had seen was we are doing uh beta 2 uh raised to at time step T.\nright so T minus 1 into Delta W 0 squared Plus beta 2 into T minus 2 Delta W 1.", "metadata": {"video_title": "AdaMax and MaxProp"}}
{"text": "right then what we had seen was we are doing uh beta 2 uh raised to at time step T.\nright so T minus 1 into Delta W 0 squared Plus beta 2 into T minus 2 Delta W 1.\nsquare Plus all the way up to 1 minus beta Delta WT square right so this is what we had.\nseen so your history VT is actually if I just ignore all the beta right let me just ignore all.\nthe betas then this is just like the L2 now right you're just taking the keeping the history of gradients.\nand then you are taking the L2 Norm of that is just exponentially weighted average L2 Norm that you are.\ntaking right now the point is instead of doing that can we replace this by L Infinity norm and does.\nit have any parameters that's the thing that we want to check okay uh yeah so that's where we are.\nheaded so now if we place it by Max then what it boils down to is that we're just going.\nto take the max of these quantities right and that in turn will just boil down to whatever we had.", "metadata": {"video_title": "AdaMax and MaxProp"}}
{"text": "headed so now if we place it by Max then what it boils down to is that we're just going.\nto take the max of these quantities right and that in turn will just boil down to whatever we had.\nat the previous time step because you're taking the max let's say you're already taking the max of these two.\nguys uh when you are at time step two you have taken the max of these two guys and retained.\none of those then again whatever was the max that you took at time step 3 and retain one of.\nthose right so then it just becomes like taking a pairwise Max whatever you had at time step T minus.\n1 the history that multiplied by meter and the max with the next guy right so that's all we are.\ndoing notice that here we are not scaling uh the gradient by 1 minus beta right so that's a change.\nthat we have made right so then essentially now this becomes the L Infinity Norm of the history of the.", "metadata": {"video_title": "AdaMax and MaxProp"}}
{"text": "that we have made right so then essentially now this becomes the L Infinity Norm of the history of the.\ngradients the exponentially weighted L Infinity Norm that you are taking here and if you're doing that uh let's see.\nwhat happens so that's a change we are making right we are saying that instead of using the L2 Norm.\nfor VT we can use the uh L Infinity Norm which is just like taking the max it's also computationally.\nvery simple and let's see if that leads to some benefits right and now we don't need to take the.\nsquare root because in the L Infinity Norm the max value comes out by P raised to 1 by P.\nright so I have already taken the p through it and then you have got the max value so there.\nis no more square root here square root is only associated associated with the L2 Norm but the proof that.\nwe saw on the previous slide said that the lp Norm is just the max right this after that there.", "metadata": {"video_title": "AdaMax and MaxProp"}}
{"text": "we saw on the previous slide said that the lp Norm is just the max right this after that there.\nis no squaring square root of any of those things right so we do not need the square root here.\nso this is what the WT plus 1 would be and we did not uh do any bias correction for.\nVT right we just took we are just going to take VT as it is because it turns out that.\nthe max Norm is not susceptible to the initial zero bias as opposed to the exponentially weighted average norm that.\nwe have right so let's see why that is the case so this is what happens right so now if.\nyou have the max Norm so again what will happen is that your m 0 is sorry M minus 1.\nat initialization is 0 and now in this case my at time step 0 I had the value 0.5 so.\nmy Max Norm is going to be the max of these two quantities right and then it will end up.\nbeing 0.5 so I'll start from here right and so max is of course more uh zigzag it's not as.", "metadata": {"video_title": "AdaMax and MaxProp"}}
{"text": "my Max Norm is going to be the max of these two quantities right and then it will end up.\nbeing 0.5 so I'll start from here right and so max is of course more uh zigzag it's not as.\nsmooth as you had in the L2 Norm it's understandable because you're just moving to the maximum values and say.\nthis is so so that's why it's not susceptible to the initial zero bias so we don't need bias correction.\nwhen we are using the max knob right and we are taking the max between beta V time uh beta.\ntimes the history uh that means beta times the max up till time step T minus 1 and then the.\ncurrent value right so this history is again decaying exponentially right because every time beta gets multiplied by the history.\nbut the current gradient you are not multiplying by anything okay okay so that's the justification for not having bias.\ncorrection okay now let's see now suppose we initialize w0 so that's that so now we are trying to see.", "metadata": {"video_title": "AdaMax and MaxProp"}}
{"text": "correction okay now let's see now suppose we initialize w0 so that's that so now we are trying to see.\nso we have proposed that we'll use the max Norm of for VT instead of using the L2 Norm now.\nwe want to see is there any benefit of doing that right so suppose that we initialize W such that.\nthe gradient at w0 is higher we just did a random initialization and it turns out that your initial gradient.\nwas high right and now suppose further that the gradients for the subsequent iterations are all zero right for a.\nfew iterations they are zero so this is what is pectorially depicted here right so you have the gradient along.\nthis axis your initial gradient was high and then for the next few time steps your gradient is zero and.\nthis could be possible because X is part so it's a x is passed so you in the next time.\nsteps when you are seeing the input that x is 0 so the gradient is going to be zero okay.", "metadata": {"video_title": "AdaMax and MaxProp"}}
{"text": "this could be possible because X is part so it's a x is passed so you in the next time.\nsteps when you are seeing the input that x is 0 so the gradient is going to be zero okay.\nso this is again a typical situation that you encounter in fact this is the situation that we have been.\narguing about ever since we started the uh discussion on adaptive learning race right so now ideally we don't want.\nthe learning rate to change right uh when the gradient is zero so now you have looked at uh an.\nupdate and your gradient was Zero you are in this time step and your gradient is zero you don't want.\nto do anything to the learning rate right why do you want to change the learning rate at this time.\nstep right because you have not doing anything you did not get any update but now in the case of.\nuh in the case of the max value this is what will happen right it will not change right because.", "metadata": {"video_title": "AdaMax and MaxProp"}}
{"text": "step right because you have not doing anything you did not get any update but now in the case of.\nuh in the case of the max value this is what will happen right it will not change right because.\nyou're taking uh sorry please since you're using the max your current gradient is 0 right so then your goal.\nis just going to go by the max value so far and hence the the history is not going to.\nchange right because this is going to be uh zero so that is uh uh what happens in the case.\nof Max now let's see what happens in the case if you are using the L2 Norm right as we.\nwere using earlier let's see so let's look at the example now I've just changed the example a bit suppose.\nthat 50 of the inputs that you see are zero right so after every uh input here every time step.\nyou are seeing a every alternate time step you are seeing a zero input and hence the gradient is also.\nzero right and let's say the other time steps you are seeing a high gradient it's just an artificial example.", "metadata": {"video_title": "AdaMax and MaxProp"}}
{"text": "zero right and let's say the other time steps you are seeing a high gradient it's just an artificial example.\nconstructed to prove a point so let's just see what the point is so ideally we don't want the learning.\nrate to change as I said when the gradient is 0 or when the input is zero so in the.\ncase of Max initially you had say V 0 at time step V 0 you had Max of 0 comma.\nthe current gradient so your V 0 was 1 and your ETA was one over one so it was one.\nright so ETA was uh I assume that the initial learning rate ETA naught is also set to 1 so.\nit will be ETA naught divided by VT and since V T is one it would also be one right.\nthis is this clear yeah it's just a minute I'll just write it down remember that ETA T is ETA.\nnaught divided by VT right and I assumed that ETA naught is equal to 1 just for the sake of.\nSimplicity okay so now ETA T remains 1 in this case uh becomes one in this case now let's see.", "metadata": {"video_title": "AdaMax and MaxProp"}}
{"text": "naught divided by VT right and I assumed that ETA naught is equal to 1 just for the sake of.\nSimplicity okay so now ETA T remains 1 in this case uh becomes one in this case now let's see.\nwhat happens at the next time Step at the next time step my derivative was 0 right so my V1.\nis just uh 0.999 times my history right so that's almost one so my effective learning rate again becomes 1.\nby V T which is 1.001 so my effective learning rate has not changed much right and same for time.\nStep 2 now the gradient is one right oops yeah now the derivative is one and my history was 0.999.\nsorry this should have been Max of 0.999 uh multiplied by 0.99 because it is beta times VT minus 1.\nbut it doesn't matter the max would anyways have been one so my V2 is 1 and my learning rate.\nagain becomes uh 1 right so my learning rate is not changing and the same thing will keep repeating right.\nthe next input is 0 so again it will be 1.001 and so on right so this is what my.", "metadata": {"video_title": "AdaMax and MaxProp"}}
{"text": "again becomes uh 1 right so my learning rate is not changing and the same thing will keep repeating right.\nthe next input is 0 so again it will be 1.001 and so on right so this is what my.\nlearning rate is looking like so remember on the x axis I have uh so this plus one you need.\nto do right so uh it was easier to draw it like this so this is a plus one so.\nthis is 1.001 1.002 and so on right so my learning rate only changes from 1 to 1.001 which means.\nit almost remains constant as I am moving across and that's what I want right I don't want the learning.\nrate to change too much for the zero inputs right that's and that this is ensuring that right now let's.\nsee what would have happened in the case of I'll just do the full stuff and then we start using.\nthe pay point yeah so now when I'm using this L2 knob okay at time step 0 I get point.\nzero zero one and then I use the bias corrected value and then my initial learning rate at time sorry.", "metadata": {"video_title": "AdaMax and MaxProp"}}
{"text": "the pay point yeah so now when I'm using this L2 knob okay at time step 0 I get point.\nzero zero one and then I use the bias corrected value and then my initial learning rate at time sorry.\nmy learning rate at time step uh 0 is going to be 1 okay that is same as what I.\nhad earlier now at time step 0 when I got the 0 input now you see that my learning rate.\nis still changing right which should not have happened because I did not do anything at this time step so.\nwhy should I change my learning rate but my learning rate is changing because of this exponentially moving uh L2.\nNorm average that I'm taking right so now uh it increases the learning rate even though the gradient was zero.\nand this is what the learning rate profile looks like it was one then at time step two for no.\nreason my input was Zero still my learning rate became 1.4 right and then time step 3 it again decreased.\nbut again for a zero input it increased and so on right so it's more uh it it's still making.", "metadata": {"video_title": "AdaMax and MaxProp"}}
{"text": "reason my input was Zero still my learning rate became 1.4 right and then time step 3 it again decreased.\nbut again for a zero input it increased and so on right so it's more uh it it's still making.\nchanges when I have a zero input and I don't want that to happen right so any have many sparse.\nfeatures and if you want the learning rate to kind of not get affected by these parts features then Adam.\nX is useful because it's not changing the learning rate too much for such sparse features right whereas atom is.\nchanging okay okay so now suppose this is what your gradient profile looks like so at time step 0 suppose.\nthe derivative is this at time step one it's this and so on and so there are 100 time steps.\nhere and I've just smoothed in the curve so there should have been 100 points and I have just smoothened.\nthose curve drawn a curve through those hundred points right so this is what it looks like and now if.", "metadata": {"video_title": "AdaMax and MaxProp"}}
{"text": "those curve drawn a curve through those hundred points right so this is what it looks like and now if.\nI um foreign so this is what happens for RMS property so sorry earlier uh yeah when we're looking at.\nthe case of uh VT using the average norm in even in RMS prop we use our average normal so.\nI'm comparing RMS prop with a version of RMS prop which uses L Infinity Norm instead of the L2 Norm.\nright so in the case of RMS prop the learning rate is continuously increasing in this case okay and now.\nlet's see what is happening in the case of uh the max version of RMS Pro right so the learning.\nrate is increasing more smoothly in this case as opposed to uh in the case of RMS okay so that's.\nthat brings us to the update rule of this Max prop right which is the version of RMS prop with.\nthe using the max Norm for VT as opposed to the L2 norm and that's it right so this is.\nas simple as it gets you just replace the uh L to num by the max norm and we can.", "metadata": {"video_title": "AdaMax and MaxProp"}}
{"text": "the using the max Norm for VT as opposed to the L2 norm and that's it right so this is.\nas simple as it gets you just replace the uh L to num by the max norm and we can.\nalso extend this to Adam when calling Adam access atom X is actually an algorithm that is used in practice.\nright so the key idea here was to use the maximum instead of the editor right okay so this is.\nwhat the update rule for Adam Max would be so this is Adam you had the momentum term you had.\nthe bias correction that will remain the same then you had VT which was uh L to Norm and then.\nyou had the bias correction but here you will have me T which is going to be the max knob.\nand since its Max Norm you don't need the bias correction and then you will have that update tool for.\nWT right so everything Remains the Same it's just the equation of VT that changes and VT does not need.\nbias correction in atom X right so that's all there is.", "metadata": {"video_title": "AdaMax and MaxProp"}}
{"text": "foreign [Music] okay so we have seen three adaptive algorithms so far one the first one was at a grad.\nwhich had this problem of rapidly killing the learning rate which was solved by RMS prop by making using an.\nexponentially averaged history but still it is the problem of dependency on the initial learning rate so then we saw.\nadd a Delta which got rid of this initial learning rate and made the effective learning rate a ratio of.\ntwo histories right now continuing in our algorithms on in continuing our journey towards the algorithms for with adaptive learning.\nrates we'll now look at Adam which is expands to Adaptive moments and the intuition here is that do everything.\nthat RMS prop does right to solve the DK problem of ADA grad which is to use an exponentially weighted.\nhistory in addition use a cumulative history of the gradients as you used in a kind of a momentum based.", "metadata": {"video_title": "Adam"}}
{"text": "history in addition use a cumulative history of the gradients as you used in a kind of a momentum based.\nalgorithm side where you use uh exponentially weighted history of the gradients as opposed to only relying on the current.\ngradient right so this is what my uh uh so this is what my current gradient is but instead of.\nonly relying on the current gradient I'm looking at the history of the gradients right this is similar to what.\nwe have seen in the momentum based gradient descent and yeah this is classical momentum then you do something known.\nas bias correction I'll just tell you the formula for now and not comment much on it as of now.\nright so it's at time step T once you have computed empty then you compute the bias corrected value of.\nempty which is empty hat which is just empty divided by 1 minus beta 1 raised to T then we.\nhave VT which was the usual VT which acted as a denominator for the effective learning rate this is the.", "metadata": {"video_title": "Adam"}}
{"text": "empty which is empty hat which is just empty divided by 1 minus beta 1 raised to T then we.\nhave VT which was the usual VT which acted as a denominator for the effective learning rate this is the.\nsame as what we had in RMS prop right so as I said do everything that RMS prop does plus.\njust add this classical momentum term right that's what Atom does so again you have a bias correction for VT.\nalso so you'll compute a VT hat which is VT divided by 1 minus beta 2 raised to T right.\nand then your update is WT minus the effective learning rate into this uh moving average right as you have.\nin momentum which is the exponentially weighted average right so this is what uh the update rules update equations for.\natom look like and here this is looking at the L2 Norm what we mean by that is that if.\nyou have or rather exponentially weighted average L2 numerate so you have uh V 0 it is simply Delta W.", "metadata": {"video_title": "Adam"}}
{"text": "atom look like and here this is looking at the L2 Norm what we mean by that is that if.\nyou have or rather exponentially weighted average L2 numerate so you have uh V 0 it is simply Delta W.\n0 square right because initially you don't have any other term or rather 1 minus into one minus beta right.\nand then if you look at VT it will have Delta W 0 square appropriately multiplied by some Decay Factor.\nthen Delta W 1 square again multiplied by some Decay factor and all the way up to the current gradient.\nright multiplied by 1 minus beta right so if I just ignore the Decay factors and if I treat this.\nas a collection of values as a vector then what I'm looking at is the L2 Norm of that Vector.\nright so that's what I uh what is the meaning here and it's just not an L2 Norm it's an.\nexponentially weighted L2 Norm that you are taking right so that's what you need to remember if you look at.\nVT it has all the terms from Delta W 0 square all the way up to Delta w t square.", "metadata": {"video_title": "Adam"}}
{"text": "exponentially weighted L2 Norm that you are taking right so that's what you need to remember if you look at.\nVT it has all the terms from Delta W 0 square all the way up to Delta w t square.\nand they are exponentially weighted and you are taking the square so that's the same as taking like an exponentially.\nweighted L2 Norm of a certain Vector right so that's what I wanted to indicate here uh and why am.\nI saying that is something that will become clear I think a little bit later in the discussion and typical.\nvalues for beta1 and beta 2 are beta 1 is 0.9 and beta2 is 0.999 so since beta 2 is.\n0.999 uh so this 1 minus beta 2 is essentially a very small quantity right so you're taking a very.\nsmall fraction of the current uh gradient okay so now let's learn Adam and try to compare it with the.\nother algorithms that we have yeah so let me just see again yeah so in this case again Ada Delta.", "metadata": {"video_title": "Adam"}}
{"text": "other algorithms that we have yeah so let me just see again yeah so in this case again Ada Delta.\nreached the conversions fast but atom is also uh reached faster than the other algorithms and RMS prop as usual.\nhas the uh convergence problem right so this is not to show that okay Adam is always going to be.\nbetter than all the other algorithms but I'm just showing how Adam is behaving in this case right and there's.\na difference between atom and Delta uh atom is more in the line of RMS prop uh with the momentum.\nwhereas Adder Delta also and this uh adaptive effective learning rate whereas here you still have the initial learning rate.\nright so you still have ETA here okay yeah so you saw what happens there and it seems to behave.\nuh reasonably um okay so now I had mentioned that this Adam had this bias correction term right so uh.\nthe other Concepts in the equations of Adam you already know right you already know what VT does it kind.", "metadata": {"video_title": "Adam"}}
{"text": "the other Concepts in the equations of Adam you already know right you already know what VT does it kind.\nof decays the learning rate you also know what Mt does right Mt is the classical momentum term the only.\nnew thing which has come up in Adam is this bias correction right which is not uh clear to us.\nwhy we are doing that bias correction right so let's see why we are doing that bias correction right so.\nthese are the update equations for uh atom so note that we are taking like a running average of empty.\nright we are in in Mt we are taking a running average of the gradients right so what do I.\nmean by taking a running average of the gradients like I have the current gradient I'm giving it some weightage.\nand I'm also accumulating the history so the reason we are doing this is that we don't want to rely.\ntoo much on the current gradient right because you might get a sudden High gradient and then you might take.", "metadata": {"video_title": "Adam"}}
{"text": "too much on the current gradient right because you might get a sudden High gradient and then you might take.\na very large step and you might not want to rely too much on the current gradient this is the.\nsame explanation as we have in momentum where you want to keep looking at the history if I am constantly.\nmoving in a direction then I want to keep some weightage for the history also and then not make sudden.\nmovements based on just the current derivative right so that's what that's why you have this exponentially weighted average so.\nthat you have more weightage for the history because history has accumulated over time and less weightage for the current.\npoint in time gradient right so so one way of looking at this is that you are actually right what.\nyou want in empty is that you want to look at the expected value of the gradient as opposed to.\nthis current value of the gradient right so instead of just looking at the current gradient I would have ideally.", "metadata": {"video_title": "Adam"}}
{"text": "this current value of the gradient right so instead of just looking at the current gradient I would have ideally.\nwanted to look at the expected value right so remember that our true gradient is actually a sum over all.\nthe data points that we have right but here we are going to do mini batch or some such version.\nand say we are doing stochastic in the worst case right we are just looking at one data point then.\nI don't want to make decisions based on this one data point but I would like to be closer to.\nthe mean of the gradients or the expected value of the gradients right and this exponentially weighted average is also.\ntrying to capture some kind of an average right now the question is whether this exponentially weighted average is close.\nto the expected value of the gradient right because I am taking a average and exponentially moving average is that.\nclose to the expected value because ideally I would wanted the expected value because that would tell me over all.", "metadata": {"video_title": "Adam"}}
{"text": "close to the expected value because ideally I would wanted the expected value because that would tell me over all.\nthe data points if I had taken all of them what would my gradient be right but I'm not able.\nto take the expected value for because there are large number of data points right so what that means is.\ninstead of expected value of the derivative we are Computing empty as some kind of an average right so expectation.\nyou could think of expectation as the mean or the average but instead of computing the true expectation we are.\nComputing some empty which is an exponentially moving average ideally we would want that the expected value of empty is.\nthe same as the expected value of the derivative itself of the gradient itself right this is what we would.\nwant ideally now let us see if that is really the case that is that happening if that's happening then.\nperhaps we don't need to do anything right so remember the momentum equations that we had where we had u.", "metadata": {"video_title": "Adam"}}
{"text": "perhaps we don't need to do anything right so remember the momentum equations that we had where we had u.\nt is equal to Beta U T minus 1 plus Delta WT now in and when we had expanded out.\nthose equations that you can go back and check those videos uh we had so we had written it as.\nu0 is equal to uh Delta W 0 U1 is equal to Beta times Delta W 0 plus Delta W.\n1 then U2 is equal to Beta Square Times Delta W 0 plus beta times Delta W1 plus Delta W.\n2 and so on right and this entire thing is essentially captured by this formula and so you can go.\nback and look at those videos but now I've in fact already explained it here itself so you know what.\nwhat I am talking about okay we now have in Adam we have a slight modification that we have uh.\nbeta1 and then 1 minus beta here right so here we only had the beta term we only had the.\nbeta term here but in atom we have beta and then this is also 1 minus beta right so we.", "metadata": {"video_title": "Adam"}}
{"text": "beta1 and then 1 minus beta here right so here we only had the beta term we only had the.\nbeta term here but in atom we have beta and then this is also 1 minus beta right so we.\nwill arrive at a similar formula but just note that this 1 minus beta is there so let's see at.\nwhat formula do we arrive at in the case of Adam so we'll have Mt is beta into Mt minus.\n1 so this is the 1 minus beta so m0 is 0 M1 is going to be this M2 is.\ngoing to be this and again if I substitute uh the value of so what I did here is that.\nI started off with uh can I move this [Music] this yeah I started off with M 0 equal to.\n0 then my M1 was 1 minus beta times Delta W 1 m 2 was beta into M1 plus 1.\nminus beta into Delta W 2 but then I have substituted the value of M1 as 1 minus beta into.\nDelta W 1 and I'll keep doing this right so as I keep doing this let's see where I reach.\nI'll again reach at some kind of a formula and so M3 is going to be this and if I.", "metadata": {"video_title": "Adam"}}
{"text": "Delta W 1 and I'll keep doing this right so as I keep doing this let's see where I reach.\nI'll again reach at some kind of a formula and so M3 is going to be this and if I.\nexpand it I get this and so now if I look at M3 it's actually 1 minus beta the 1.\nminus beta factor I can take out and it is time Step 1 2 3 beta raised to T minus.\nTau into the derivative at that time step right so that's what exactly what is happening here so I have.\nthe 1 minus beta term which I have taken out common then I have for t equal to 1 I.\nhave beta square into derivative of letter one right then I have beta into derivative of Delta W 2 and.\nthen I just have Delta W3 right so for 3 this would be 3 minus 3 which would become 0.\nso the beta Factor would disappear and you'll just have Delta W3 for 2 it would be uh 3 minus.\n2 so beta raised to 3 minus 2 which is the same as beta and then you will have the.\nDelta W 2 here so this is how the formula has been arrived at so in general for the tth.", "metadata": {"video_title": "Adam"}}
{"text": "2 so beta raised to 3 minus 2 which is the same as beta and then you will have the.\nDelta W 2 here so this is how the formula has been arrived at so in general for the tth.\ntime step I am now ready to write the formula the formula would just be 1 minus beta of factor.\nof 1 minus beta being common right and then beta raised to T minus Tau Delta W2 and it's very.\nsimilar to this formula except that you had this 1 minus beta term which has shown up on this formula.\nalso right so nothing great here don't to worry too much if you have not understood this right away you.\ncan go back and look at the steps and you can derive this formula on your own right so I've.\nderived the formula for Mt and this is important for the rest of the discussion that we'll have okay so.\nnow let's look at okay now let's take the expectation on both sides that I have empty on one side.\nand I have uh the Delta WT quantity on the other side so I'm going to take expectation on both.", "metadata": {"video_title": "Adam"}}
{"text": "now let's look at okay now let's take the expectation on both sides that I have empty on one side.\nand I have uh the Delta WT quantity on the other side so I'm going to take expectation on both.\nsides so expectation of m t is equal to the expected value of this expression now I'll just take the.\nconstants out and move the expectation inside so the expectation so 1 minus beta here will come out because it's.\na constant then I have the expectation of a sum which is the same as the sum of an expectation.\nokay foreign and now again the beta term will come out and the expectation goes inside and now what I'm.\ngoing to assume is that all the WTS they come from the same distribution right because I have what does.\nthat mean right so what does the distribution mean in this case so you could think of hey the gradient.\nitself could have different values right and that those values could come from some distribution right so these are your.", "metadata": {"video_title": "Adam"}}
{"text": "itself could have different values right and that those values could come from some distribution right so these are your.\nderivative values on the x axis and you have a distribution that means certain values here have a higher density.\nas opposed to other values right and now at every time step because your updates are changing and your weights.\nare changing and you're moving along uh the loss surface this distribution might be different right at some time steps.\nyou might expect more smaller gradients at terms time trips you might expect larger gradients but we are making a.\nsimplifying assumption here that every time step this distribution looks the same right so whatever what does this distribution come.\nfrom where does the randomness come from so it comes from the data point that I'm picking up right I'm.\npicking up random data points at every time step so depending on the time data point that I have picked.", "metadata": {"video_title": "Adam"}}
{"text": "picking up random data points at every time step so depending on the time data point that I have picked.\nup my derivatives might be different and this is the distribution that I expect them to follow right and this.\ndistribution might change at different time steps but I am making this assumption that all at all time steps the.\ndistribution Remains the Same that means the expected value at any time step is equal to the same way so.\nI am clocking that expected value of is the same for all the time steps that's one simplifying assumption that.\nI have made and under that assumption now let's see where we reach so now we have made this assumption.\nthat all the derivatives were the same so then we have this expected value of delta w t now I.\ncan just it's it does not depend on the time safety anymore right so I can take it outside the.\nsummation so then I have 1 minus beta into expected value of delta W into that summation that I had.", "metadata": {"video_title": "Adam"}}
{"text": "can just it's it does not depend on the time safety anymore right so I can take it outside the.\nsummation so then I have 1 minus beta into expected value of delta W into that summation that I had.\nnow this summation looks familiar it's a summation of a series and this is beta raised to T minus 1.\nplus beta raised to T minus 2 all the way up to Beta 0 and the sum of that series.\nis just going to be 1 minus beta T divided by 1 minus beta raised to T divided by 1.\nminus beta of course so now I get this equation so the expected value of Mt is actually the expected.\nvalue of the derivative into 1 minus beta T so now if I had taken the expected value of m.\nt by 1 minus beta T then I would have got the expected value of the grade right hence I.\nneed this bias correction I am just dividing the value of m t why this term 1 minus beta T.\nbecause if I do that now this is actually my empty hat so now what this is telling me is.", "metadata": {"video_title": "Adam"}}
{"text": "need this bias correction I am just dividing the value of m t why this term 1 minus beta T.\nbecause if I do that now this is actually my empty hat so now what this is telling me is.\nthat the expected value of empty hat is this same as the expected value of the derivative right so that's.\nwhat I am doing this bias correction and you can have the similar derivation for the value of VT also.\nright so that's why we have to use bias correction in Adam so that's the main point of discussion when.\nwe are talking about Adam right why do you need a bias correction all the other things in the equations.\nare familiar one is related to momentum the other VT is related to decaying the learning rate in the denominator.\nand again the vth self is an exponentially moving average as we had justified in the case of RMS prop.\nright so there are no other unknown or New Concepts in the equation of atom but this bias correction is.", "metadata": {"video_title": "Adam"}}
{"text": "right so there are no other unknown or New Concepts in the equation of atom but this bias correction is.\nthe main New Concept which we have just understood why to do that okay so now let's see what would.\nhappen if we don't do bias correction right so we'll again look at the equations put in some values and.\ntry to see what happens right so this is the equation for VT okay and now suppose Delta W naught.\nis equal to 0.1 okay uh so now V naught is going to be beta 2 times 0 because VT.\nminus 1 was 0 and then 0.001 times 0.1 Square so it's going to be 0.0001 okay and now ETA.\nT the effective learning rate is going to be 1 divided by the square root of this which is going.\nto be 316.22 right so it already looks a very high learning rate to me right now let's see what.\nV2 would be V2 would be 0.999 into V naught plus 0.01 into 0 square and that would give me.\nthis quantity okay and that is again a reasonably High Learning array right so what we are seeing is that.", "metadata": {"video_title": "Adam"}}
{"text": "V2 would be V2 would be 0.999 into V naught plus 0.01 into 0 square and that would give me.\nthis quantity okay and that is again a reasonably High Learning array right so what we are seeing is that.\nuh in the absence of bias correction this is not bias corrected I just took VT I did not take.\nVT hat right please note that so in the absence of bias correction as the paper says uh your initial.\nsteps could be very large because your initial learning rate would be very high but now if I had done.\nbias correction okay and I had the same situation my w naught was 0.1 so now V naught is this.\nthe bias collected value is 0.01 okay and now if I substitute this bias corrected value in ETA my effective.\nlearning rate is 10 as opposed to 316 here right similarly V2 gives me a certain value if I bias.\ncorrected it I get certain value and now my learning rate is 13.8 as opposed to 316 here right so.", "metadata": {"video_title": "Adam"}}
{"text": "learning rate is 10 as opposed to 316 here right similarly V2 gives me a certain value if I bias.\ncorrected it I get certain value and now my learning rate is 13.8 as opposed to 316 here right so.\nin the bias correction when I'm doing bias correction my initial learning rates are not becoming very high right so.\nthat's why you need a bias correction to make sure that initially you are not doing some very rapid movements.\nbecause that could be faulty and we'll see also through an example what happens right so let's uh look at.\nthis plot here so here I'm going to compare what happens when you have Adam with bias correction and Adam.\nwithout bias correction and remember our understanding is that if you don't use bias correction the initial updates would be.\nvery high that means the algorithm would move very fast right so let's see if that is happening so notice.\nthat this blue curve corresponding to atom without bias correction would make very large initial updates that's what we expect.", "metadata": {"video_title": "Adam"}}
{"text": "that this blue curve corresponding to atom without bias correction would make very large initial updates that's what we expect.\nso let's see if that is happening and that is indeed the case right it has like gone very fast.\nahead and then it had to come back right it still converts probably uh easily in this example right because.\nthis is a toy example and it had come came back from the valley but you could imagine situations where.\nafter making an initial update it would enter some region of the Lost surface from which a recovery might be.\ndifficult right it would have reached a region where in that region whatever Minima you could get would perhaps be.\nmuch higher than what you could have got otherwise right so hence this initially the learning rate is uh very.\nhigh uh in the case when you do not use bias connection right and that's what you're seeing here in.\nthese plots also so let's understand these plots so when we don't have bias correction you see that your learning.", "metadata": {"video_title": "Adam"}}
{"text": "these plots also so let's understand these plots so when we don't have bias correction you see that your learning.\nrates are higher right there on this scale you have values which are 2 2.5 and so on whereas when.\nyou use bias correction your learning rates initially are smaller and then initially the learning rate was uh large uh.\nand then it starts decaying and in the case of I battum with bias correction so remember this brown or.\nthe red curve actually corresponds to the learning rate for w and W was the direction which was sparse so.\nas it uh came close to the Minima and it went into the valley it still had to make some.\nupdates for w so the learning rate was still increasing which is what we would want right so this atom.\nwith bias correction was able to adjust easier whereas this guy had some initial very high learning rate and then.\nit had to follow some trajectory of course both of them converged that is not a problem but this is.", "metadata": {"video_title": "Adam"}}
{"text": "it had to follow some trajectory of course both of them converged that is not a problem but this is.\njust to show that with without bias correction your initial learning rate could be very high so I'll stop here.\nuh with the discussion on Adam and now uh we'll look at uh in the next half an hour or.\nso we'll look at some variants of atom or maybe just one or two variants of atom okay thank you.\nso welcome back uh and we were talking about Adam in the last lecture and I would just like to.\nsay a few more things about the bias kind Direction in Adam which may again relate to things that we.\nare going to do today so I'll start with that and then I'll go to the next algorithm which is.\ngoing to be Max prop right so let's start with uh let's just quickly talk about bias correction again right.\nso one way of looking at uh what is happening uh in Adam is that or in many of these.\ngradient descent based algorithms is that so you have these uh on the x-axis I have the number of iterations.", "metadata": {"video_title": "Adam"}}
{"text": "so one way of looking at uh what is happening uh in Adam is that or in many of these.\ngradient descent based algorithms is that so you have these uh on the x-axis I have the number of iterations.\nare epochs right and at every iteration maybe from a batch or in a stochastic from a mini batch or.\nin a stochastic manner I am Computing the gradients right so let's uh say that and there is some Randomness.\nin this there's some noise in this because it depends on the batch that I have picked up right so.\nnow the batch is not the mini batch is not a representation of the entire data so depending on how.\ngood this many batch is a representation of that entire data the gradients that I compute from this mini batch.\nmay vary as compared to what I would have gotten from the full match right so here what I'm trying.\nto illustrate is that suppose I had computed the derivatives from the full batch right suppose the red function here.", "metadata": {"video_title": "Adam"}}
{"text": "to illustrate is that suppose I had computed the derivatives from the full batch right suppose the red function here.\nis what my true derivative function would have looked like from time step 0 to time step 100 right but.\nsince I'm doing this mini batch version of the algorithms I'm Computing the derivatives from a smaller batch and hence.\nI might get noisy estimates so you can see that every time step so this is the gradient that I.\ncomputed at time step 0 this is at one this is a two and so on it's every Point here.\ncorresponds to one time step right so it's 0 200 so there are 100 points here right uh now uh.\nas you can see that it's like kind of varying right it's uh not it's close to the true derivatives.\nbut also there is some noise right you see some up and down a movement there right and one way.\nof dealing with this is to kind of say that instead of just relying on this gradient at this time.", "metadata": {"video_title": "Adam"}}
{"text": "but also there is some noise right you see some up and down a movement there right and one way.\nof dealing with this is to kind of say that instead of just relying on this gradient at this time.\nstep I will take the average of all the gradients that I have taken so far right or I would.\ntake the moving average of all the gradients so instead of just relying on this point for example this should.\nbe clear right suppose instead of relying on this value here alone let me just use a different color if.\nI take like an average of things which fall in this window right and this is the same as saying.\nthat I mean an exponential average is almost similar like to taking an average in a small window because anyways.\noutside this window your beta powers are so small that those points won't have much value right so here is.\nyou are kind of re-estimating uh the average in the case of empty you're writing it as beta into Mt.", "metadata": {"video_title": "Adam"}}
{"text": "you are kind of re-estimating uh the average in the case of empty you're writing it as beta into Mt.\nminus 1 plus 1 minus beta into the current uh derivative right so the current derivative is this one blue.\npoint that you have instead of relying on that blue point you are also taking an exponentially weighted average of.\nthe history so this is your way of kind of uh accounting for the fact that the current batch could.\nbe uh noisy so hence let me look at the history also and kind of re-estimate this point right now.\nif you don't do bias correction so now what I'm going to do is that for every black point I.\nam going to compute the new value of that Black Point using this equation that means I'm going to compute.\nthis weighted history right so at time step 40 I have 39 Blue Points below it and I'm going to.\ntake the value of each of those 39 blue points and I will say that beta raised to 39 into.", "metadata": {"video_title": "Adam"}}
{"text": "this weighted history right so at time step 40 I have 39 Blue Points below it and I'm going to.\ntake the value of each of those 39 blue points and I will say that beta raised to 39 into.\nthe value at time step or beta raised to 40 I guess right maybe 39 or 40 whatever that power.\ncomes to into the value at time step 0 right then beta raised to 38 into the value of time.\nstep T1 all the way up to 1 minus beta into the value at this t48 time step right so.\nmy new value is going to be uh this exponentially weighted average right that's what I'm going to do so.\nlet's see if I plot that so now for each of these Blue Points I could compute the new uh.\nexponentially weighted uh value and then I can plot that right so let me see what happens if I plot.\nthat so this is what happens if I plot that without the bias correction right so what happens is that.\nI get you can see that my black curve now is a bit farther away from the red curve at.", "metadata": {"video_title": "Adam"}}
{"text": "that so this is what happens if I plot that without the bias correction right so what happens is that.\nI get you can see that my black curve now is a bit farther away from the red curve at.\nleast during the initial time steps right and yesterday also we had seen that during initial times test we have.\na problem when we use bias correction so why does that happen let's try to understand that so so our.\nformula is beta into Mt minus 1 plus 1 minus beta into the current time step value right so that's.\nit the Delta w t right now at time step 0 my Delta w t was around 1 right but.\nmy Beta is 0.999 and this is 1 minus beta so this is going to be 0.001 right and my.\nm t minus 1 is 0 because I initialize at like so m t minus 1 since T is equal.\nto 0 this is going to be M minus 1 and that I have initialized to 0 right so this.\nwhole sum moves towards zero it becomes very small and then the same thing keeps repeating right because in the.", "metadata": {"video_title": "Adam"}}
{"text": "to 0 this is going to be M minus 1 and that I have initialized to 0 right so this.\nwhole sum moves towards zero it becomes very small and then the same thing keeps repeating right because in the.\nnext time step again now at time step uh one again my value was around say 0.8 right so then.\nI have 1 minus beta into 0.8 right and plus beta into my history so far which was around say.\npoint zero zero one right and now again this 1 minus beta is a very small value so that's why.\nit will stay towards close to zero and it'll take a while to then start coming close to the two.\nvalues because after a while now this is your history and all these initial parts are getting very very small.\nweightage and these new guys are getting higher weightage so then slowly it kind of gets rid of its history.\nbecause of the large power of beta right and then it starts relying more on the recent guys and those.", "metadata": {"video_title": "Adam"}}
{"text": "because of the large power of beta right and then it starts relying more on the recent guys and those.\nrecent guys are all farther away from zero so then it also starts coming close to the true curve right.\nso that's why what happens if you don't use bias correction now the same thing if I had used bias.\ncorrection this is what will happen right so now if I use bias correction then it gets divided by that.\none minus beta factor and then I don't have a problem right so in particular what happens right so I'll.\nshow you so my M 0 is minus 1 okay and my Mt is going to be beta into oh.\nsorry m z sorry what am I seeing M minus 1 is 0 because I initialize it as 0 my.\nhistory uh is initialized as a 0 then plus 1 minus beta into m t now at time step 0.\nuh uh my value is 0.5 right so this uh yeah so this value is 0.5 and this term is.\ngoing to disappear right so I'll get 1 minus beta uh into 0.5 that's what my Mt is going to.", "metadata": {"video_title": "Adam"}}
{"text": "uh uh my value is 0.5 right so this uh yeah so this value is 0.5 and this term is.\ngoing to disappear right so I'll get 1 minus beta uh into 0.5 that's what my Mt is going to.\nbe okay the pen is not working well that's what my Mt is going to be and my M hat.\nT which is the bias corrected guy is going to be Mt divided by 1 minus beta raised to 1.\nin this case and so now these two betas will cancel and I get 0.5 and hence my approximation is.\nclose to the True Value and now it stays like that right because it does not have this weight of.\n0 pulling it down at earlier it it started off at zero then it got stuck there and it took.\na while to get the shape of the function but now it has started from the right point and it's.\nnow closer to the uh function right so that's that's what happens in the case of uh exponentially moving average.\nand that's why we use this bias correction in the case of atom okay uh as we are done with.", "metadata": {"video_title": "Adam"}}
{"text": "and that's why we use this bias correction in the case of atom okay uh as we are done with.\nthis so I'd skipped this part yesterday now the rest of the stuff of Adam we have already done and.\nwe already did this we looked at multiple explanations for why the initial uh updates are going to be larger.\nright so we saw this these derivations then we saw this plot all of that is done right.", "metadata": {"video_title": "Adam"}}
{"text": "foreign [Music] so that brings us to the next body which is on this some manual work on attention is.\nall you need or the Transformers right so the Transformer architecture which was introduced around 2017 maybe yeah uh so.\nthat's that's what you're going to focus on while keeping in mind the limitations that we saw about in the.\ncase of a recurrent neural networks right so this is how we'll transition to Transformers right so we have the.\nbasic encoder decoder RNN based model where at first you have recurrent connections and that's causes a problem then you.\nalso have the attention-based encoder decoder model where we said we'll do the uh encoder computations then get rid of.\nthem and then we have the uh attention we just take the outputs of the encoder right um we just.\ntake the outputs of the encoder and then we have the attention function on top of that at every step.\nwe compute a new contextual Vector right so that's the attention-based model but here again uh we were able to.", "metadata": {"video_title": "Attention is all you need"}}
{"text": "we compute a new contextual Vector right so that's the attention-based model but here again uh we were able to.\ncompute the attention weights for one time Step In Parallel but across time steps T1 T2 and so on we.\nhad to wait for the previous computations to finish right that's where we were so from here we'll transition to.\nthe Transformer Network which kind of again has an encoder decoder architecture but there are some other blocks that I.\nam naming here as is something known as a self-attention block then something known as a feed forward Network then.\nin the decoder you have self-attention encoder decoder attention again feed forward Network so it's all of this we need.\nto understand the words look familiar you have attention your feed forward Network so that doesn't look too problematic but.\nuh how these different blocks interact what exactly is there in each of these blocks is something that we'll have.", "metadata": {"video_title": "Attention is all you need"}}
{"text": "uh how these different blocks interact what exactly is there in each of these blocks is something that we'll have.\nto study and that's what the focus of the next uh half an hour to one hour would be right.\nso before we focus on the differences right this clearly looks at least in the diagram a bit different from.\nthe recurrent neural network architecture or The Recoil neural network with attention architecture that we were used to uh there's.\nstill some similarities which come out right and I'll tell you the few easy ones right the input is still.\ngiven to me fully right and the output again will be produced one word at a time right the other.\nuh similarity is that in RN and also I had said that at the input you could just feed in.\nyour uh favorite uh word Vector so for word embedding so you have again your feeding word embeddings and this.\nblock right if I were to look at it as an encoder as same as what I had in the.", "metadata": {"video_title": "Attention is all you need"}}
{"text": "your uh favorite uh word Vector so for word embedding so you have again your feeding word embeddings and this.\nblock right if I were to look at it as an encoder as same as what I had in the.\nrnns so the N output of the encoder was these representations right and don't worry about the change in notation.\nbecause there are more uh intermediate outputs here so I need to use no more variables but in the case.\nof rnns you had X1 X2 up to X Phi as input and the output was H1 h2h5 right so.\nin a sense what you are doing is that you are taking the inward embeddings as input right and then.\nyou were Computing a contextual representation how are you Computing a conflict contextual representation using the bi-directional lstm right so.\nH2 dependent on H1 so hence you had all seen all the information up to or rather HD dependent on.\nHT minus one so when you are Computing HT you had information of all the T minus one Birds before.", "metadata": {"video_title": "Attention is all you need"}}
{"text": "H2 dependent on H1 so hence you had all seen all the information up to or rather HD dependent on.\nHT minus one so when you are Computing HT you had information of all the T minus one Birds before.\nit that's where the contextual representation was coming out but at each step what you are doing is this right.\nso you are Computing a new representation for that word which was a context of a representation here looks like.\nmore computations are happening but at the output you just have these Z1 Z2 Z5 so again you could think.\nof it as you have the excise as input and you are producing these zis as an output and these.\nz-is are again some kind of a contextual representation of the input that you have given that means zis are.\nnot only aware of X I's they are also aware of all the other words in the uh sentence right.\nso that's that's what is happening here so the similarity is that RNN takes X1 to x y and gives.", "metadata": {"video_title": "Attention is all you need"}}
{"text": "not only aware of X I's they are also aware of all the other words in the uh sentence right.\nso that's that's what is happening here so the similarity is that RNN takes X1 to x y and gives.\nyou H1 to H5 Transformers also takes X1 to X5 and gives you Z1 to Z5 the naming of the.\nvariables may be different but just both have the same semantics in the sense that they are some kind of.\na contextual representation of the input how this contextual representation is computed is different and the main difference is going.\nto be that we are going to do something which does not require the current connections or something which can.\nbe computed in parallel so that's the main change how do we effect that change is something that we will.\ndiscuss as we go along okay so let's start yeah so let's look at each of these layers in detail.\nlet's say my goal would be to kind of go over each of these yellow boxes that you see here.", "metadata": {"video_title": "Attention is all you need"}}
{"text": "discuss as we go along okay so let's start yeah so let's look at each of these layers in detail.\nlet's say my goal would be to kind of go over each of these yellow boxes that you see here.\nword embedding of course you know so there's nothing to say much over there but self-attention feed forward networks then.\nagain self-attention the case of decoder encoder decoder attention and feed forward networks again in the case of decoder so.\nwhat do each of these blocks have right and of course there will be some overlap in the discussion so.\nif you understand this this should be fairly straightforward if you understand this then I'll not even say anything about.\nthis right so mainly three components that we need to understand self-attention and good or decoder attention and the feed.\nforward neural networks so we'll look at these in the coming slides okay starting with self attention right so this.", "metadata": {"video_title": "Attention is all you need"}}
{"text": "forward neural networks so we'll look at these in the coming slides okay starting with self attention right so this.\nis the self-attention block so this is what is happening I have the word I okay and I have the.\nword embedding for I here I have just used some random initialization but you can either do random initialization that's.\nthe practice you don't really rely on any word embeddings you just do a random initialization and then these are.\npassing through something known as self attention and then producing a new representation for every word right that's what is.\nhappening here so very similar to what is happening in the case of RNN you have the excise and then.\nyou get some Edge Ice right so the same thing is happening here of course the advantage the only reason.\nI would be interested in discussing this is that if this overcomes the disadvantage of rnns which is sequential computation.\ncan I do this in part right so that's the idea uh that's the main uh idea here right so.", "metadata": {"video_title": "Attention is all you need"}}
{"text": "can I do this in part right so that's the idea uh that's the main uh idea here right so.\nthe self attention mechanism once you understand it fully we'll realize that it can be computed in parallel and still.\ngive the same flavor that means it still computes a contextual representation that means it's aware of all the other.\nwords in the input right so that's the main thing that we'll be discussing so now this is called self.\nattention and what does the attention how do you compute attention right so the attention just requires two inputs so.\nin the attention that we have seen so far we had St minus 1 comma h i right which was.\nused to compute the attention at time step t for the input I right but here there is no decoder.\nwe're just talking about encoder and now the word self should tell you what is going to happen here right.\nwhat I'm going to be interested in is that I have all the word representations now if I want to.", "metadata": {"video_title": "Attention is all you need"}}
{"text": "what I'm going to be interested in is that I have all the word representations now if I want to.\ncompute a new representation for the word movie right I want to Output a new representation of the word movie.\nthen can I compute this representation as a attention weighted sum of all the other representations in my sentence right.\nand if I were to do that all I need to do is I need attention function right and which.\ntakes in my current representations h i and H J right so these are what I'm going to call edges.\nthis change of notation is unavoidable right because I have more outputs being produced here or more intermediate outputs being.\nproduced here so I'll have to use more variables earlier H was the output of the encoder itself now that.\nI am calling as Z so H is now the word embeddings that you have computed it so this is.\nH1 H2 H3 up to H5 now I want to compute a new representation for movie and I'm going to.", "metadata": {"video_title": "Attention is all you need"}}
{"text": "I am calling as Z so H is now the word embeddings that you have computed it so this is.\nH1 H2 H3 up to H5 now I want to compute a new representation for movie and I'm going to.\ncall it as S4 so I want to compute S4 as an attention weighted sum of all the Phi inputs.\nthat I have that means first I need to compute the attention weights and what how am I going to.\ncompute the attention weights everything is only on the encoder side so I want to compute the attention weight between.\nthe words I and J and that would be a function of just h i and H J now what.\nfunction do I choose how do I use that all that we'll see as we go along but this is.\nthe basic idea right so once I have these Alpha ijs I'm just going to compute S4 as the attention.\nweighted sum so Alpha 4 J J equal to 1 to capital T into h j right so now I.\nam just taking a weighted aggregate of all these representations right and these are the alphas for the time step.", "metadata": {"video_title": "Attention is all you need"}}
{"text": "weighted sum so Alpha 4 J J equal to 1 to capital T into h j right so now I.\nam just taking a weighted aggregate of all these representations right and these are the alphas for the time step.\n4 and using that I'm going to compute S4 similarly now I should better clear some things here similarly if.\nI want to compute S2 I am again going to take an attention weighted sum and these are going to.\nbe Alpha 2 so there will be Alpha 2 1 Alpha 2 2 all the way up to Alpha 2.\nPhi in this case and general Alpha 2T and once I have the alphas I'm going to compute S2 as.\nthe attention weighted sum so Alpha 2 J h j j equal to 1 to capital T or Phi in.\nthis case right and these are just to remind you my word embeddings are what I'm calling as the edges.\nright so it's just going to take a weight at some of the edges now you already see the advantage.\nI already have the edges with me right I just got the entire sequence of words so I just looked.", "metadata": {"video_title": "Attention is all you need"}}
{"text": "right so it's just going to take a weight at some of the edges now you already see the advantage.\nI already have the edges with me right I just got the entire sequence of words so I just looked.\nup the word embeddings so I already have all the edges with me now there is no recurrence here right.\nso I can compute the alpha twos at the same time as alpha 1 at the same time as Alpha.\n3 because now I am not depending on one step to another right I don't need for computing Alpha once.\nI don't need to know rather for computing Alpha threes I don't need to know what happened at time step.\ntwo right because I am just looking at the contextual representation for this word by taking uh a vote from.\nall the other words in the sentence and I'm only looking at H's I'm not relying on S2 here right.\nso this attention equation unlike the earlier equation which was St minus 1 and H I now this is just.\nh i comma h j right so there is no dependence here I already know all the edges so I.", "metadata": {"video_title": "Attention is all you need"}}
{"text": "so this attention equation unlike the earlier equation which was St minus 1 and H I now this is just.\nh i comma h j right so there is no dependence here I already know all the edges so I.\ncan just compute this all at one go right all of this will become a bit more clear I'm just.\ngiving you a trailer of what is coming right so two takeaways from this slide one is that I have.\nH's as the input and I'm going to compute this intermediate representation s and the way I'm going to compute.\nthat is by taking a weighted sum of all the edges so this makes sure that my s's are contextual.\nright because they are looking at all the words in the neighborhood and while doing this I'm going to rely.\non the attention equation this is called self-attention because I'm looking at self this is not earlier when I had.\nattention that was between decoder and encoder now I have attention within the encoder right I'm looking at all the.", "metadata": {"video_title": "Attention is all you need"}}
{"text": "attention that was between decoder and encoder now I have attention within the encoder right I'm looking at all the.\nwords in the input itself this is not as opposed to earlier where I had the attention between the encoder.\nand the decoder so I was looking at the decoder to State at time step T and then Computing the.\nattention at attention of all the inputs that's not what is happening here this is self-attention this is attention within.\nthe input itself right and this can already you have a feeling that this can be paralyzed we'll concretize that.\nfeeling further as we go along okay yeah so this is what uh now now what I want to do.\nis try to First motivate right that what we are doing here and why do we need to do this.\nright why do we need to compute this attention or why do we need to rely on all the other.\ninputs because that is not what we were doing earlier we are of course computer contextual representation but earlier the.", "metadata": {"video_title": "Attention is all you need"}}
{"text": "inputs because that is not what we were doing earlier we are of course computer contextual representation but earlier the.\nattention was only between encoder and decoder so why do you need this self-attention why does it make sense right.\nso now if you have the sentence the elephant the animal didn't cross the street because it was too tired.\nhere the word it is referring to animal and not street right so when I'm Computing a representation for it.\nit should pay more attention to animal it as opposed to stream that's why I need this self-attention because I.\nneed to capture the importance of the words in context as with respect to the current word right and this.\nwould change if the sentence was different if the sentence was the animal didn't cross the street because it was.\ncongested so now the word it here actually represents refers to street so now when I'm Computing a contextual representation.", "metadata": {"video_title": "Attention is all you need"}}
{"text": "congested so now the word it here actually represents refers to street so now when I'm Computing a contextual representation.\nfor it it should pay more attention to stream right so the same word but paying attention to different uh.\ninput different other contextual words right that's why I cannot use like a static embedding of it which is just.\ntaking the word embedding I need this contextual embedding and that's why I need to learn these Alphas here the.\nalphas should learn to focus more on street here the alpha should learn to focus more on anyway that's why.\nI need this self-attention so that's what is being said here and this is what we call it as self.\nattention and we'll distinguish this from concentration which I have already said that cross attention is between the encoder and.\nthe decoder but here the attention is within the encoder inputs itself right so our goal would be that for.", "metadata": {"video_title": "Attention is all you need"}}
{"text": "the decoder but here the attention is within the encoder inputs itself right so our goal would be that for.\na given word I want to be able to compute the similarity score or the attention score with all the.\nother words in the sentence right so this Matrix is what I want to fill that I want to compute.\na representation for street and I'm going to compute it as a weighted sum of all these representations so in.\nmy weighted sum what should the alphas be right that's what I want to compute okay that is what my.\ngoal is so this entire Matrix which is a t cross T Matrix is what I want to compute I.\nwant to compute all these Alphas if I have all these Alphas then I compute the SS by just taking.\na weighted Alpha weighted sum of the edges right so that's what my goal is okay and I want to.\nbe able to do this in parallel which you already have a feeling that you can do this in parallel.\nbecause in particular right when could you do that in parallel so let me call this Alpha Street right or.", "metadata": {"video_title": "Attention is all you need"}}
{"text": "because in particular right when could you do that in parallel so let me call this Alpha Street right or.\nI could have used the timestamp so one two three four five six right so this is Alpha six Alpha.\nsix one alpha six two three all the way up to Alpha six capital T and say this is Alpha.\ntwo one alpha two two all the way up to Alpha to T right now when can I compute these.\nin parallel earlier remember we had spoken that uh when we are looking at encoder decoder I could not compute.\ntwo rows of alpha in parallel because this row of alpha depended on everything that happened before right but now.\nI don't have that dependency to compute this Alpha I just need H2 and H1 which I already have to.\ncompute this I need H6 and H3 which I already have I don't need to see what is happening here.\nto be able to compute this function right so you already have a feeling that all these rows can be.", "metadata": {"video_title": "Attention is all you need"}}
{"text": "compute this I need H6 and H3 which I already have I don't need to see what is happening here.\nto be able to compute this function right so you already have a feeling that all these rows can be.\ncomputed in parallel as compared to the previous case where each row dependent on the previous row right because of.\nthis St minus 1 that you have right so this Matrix is what I want to compute and I should.\nbe able to compute this Matrix in parallel is what my wish list would be so essentially what we want.\nis a table like this which has some numbers and those numbers are meaningful in the sense that here it.\nand animal are related so the contextual similarity between them should be higher right in particular when I am Computing.\nthe contextual representation for it I should have more weight on animal as opposed to the other words in the.\nsentence right so that's what we want so we want to learn these Alphas in that manner okay so how.", "metadata": {"video_title": "Attention is all you need"}}
{"text": "sentence right so that's what we want so we want to learn these Alphas in that manner okay so how.\ndo we do that is the question what kind of a architecture do we use so to start that discussion.\nand just to be able to relate it to what we have already seen right so in the earlier case.\nwe were talking about the attention function as s t minus 1 and hi right so just to kind of.\nkeep the convention or the variables similar for now for for some time right and we will get rid of.\nit soon we'll just think of the rows as H I's sorry the rows as s I's and the columns.\nas at J and now what you are interested in is to compute an attention function between the s i.\nand the AJ but unlike earlier now as I said the Si's and the edges are available to you at.\none go so you can compute all of this at one go as opposed to earlier when your sis which.\nare essentially the decoder states were getting available only one step at a time right so just to draw a.", "metadata": {"video_title": "Attention is all you need"}}
{"text": "one go so you can compute all of this at one go as opposed to earlier when your sis which.\nare essentially the decoder states were getting available only one step at a time right so just to draw a.\nclear variable level analogy between what we have seen so far and what we are going to see earlier uh.\nI should say that this is actually just h i right because these are both the same representations but I.\njust wanted to connect it to the earlier discussion where we had the S and the H and the difference.\nis that now the s's are also available in at one go as opposed to the earlier case right so.\nnow what is the attention function that we should choose so earlier we had this attention function right so when.\nwe're discussing about rnns this was the attention function where you had St minus 1 here at J here both.\nwere undergoing some linear transformation then the resulting output was going under a non-linearity so you have this tannage and.", "metadata": {"video_title": "Attention is all you need"}}
{"text": "were undergoing some linear transformation then the resulting output was going under a non-linearity so you have this tannage and.\nthen you had a vector which was the same size as this vector and then you take the dot product.\nbetween these two vectors so you get just get a scalar quantity in fact we are calling it A's and.\nthen the A's get normalized to give you the alphas right so that's the function that we were choosing earlier.\nnow for the this work right the Transformers paper introduce a new attention function and that is what we'll uh.\ntry to arrive at now right so if you notice this equation right there are three vectors involved here there.\nis s there is H and then there is B it remember this is also a vector right because the.\noutput of this is a vector and that gets multiplied by our DOT product with another Vector let's have three.\nvectors and then you have uh two linear Transformations happening so this is the first linear transformation which is at.", "metadata": {"video_title": "Attention is all you need"}}
{"text": "vectors and then you have uh two linear Transformations happening so this is the first linear transformation which is at.\nhere and then here's the second linear transformation that is happening right uh and then you have this one non-linearity.\nin the form of the tannage right uh and then what you have is that that internal non-linearity multiplied by.\nthose two linear Transformations their entire equation gives you one vector and then you're taking a DOT product between that.\nvector and your V right so just to summarize there are three vectors two linear Transformations one nonlinearity and then.\nfinally a DOT product right so the final answer that you get right if I were to simplify this what.\nyou get at the end is just the dot product between two vectors this Vector in turn was computed using.\ntwo other vectors which in turn had gone through a linear transformation right so that is what is happening here.", "metadata": {"video_title": "Attention is all you need"}}
{"text": "two other vectors which in turn had gone through a linear transformation right so that is what is happening here.\nthat is what the equation is and now I could do this in other ways and that's where the Transformer.\nset of equations for uh attention will drop out from right but this is what is happening here the final.\ncomputation is a DOT product and within that green Vector you had these two linear Transformations which were happening followed.\nby a non-linearity right so now let's look at but in the earlier case you had this St minus 1.\nwhich was being generated at every time step right but here now you have h i and h j which.\nis they just have one word embedding for hi and one word embedding for a j whereas here you had.\nthese three vectors which were participating right so now from this h i n h j how do you get.\nthese three vectors to participate in this equation right so you have these three vectors participating in the equation but.", "metadata": {"video_title": "Attention is all you need"}}
{"text": "these three vectors to participate in this equation right so you have these three vectors participating in the equation but.\nnow you just have h i and H J so where do you get these three vectors from right how.\ndo you generate these three vectors that's the question and that's what uh one of the uh Innovations are one.\nof the uh equations proposed in the case of Transformer based models how do you get these three vectors uh.\nfrom these two uh word embeddings so that's the idea so what we'll do is we'll use this Matrix transformation.\nand that's not surprising because anyways in the case of the original equation also you had this linear transformation right.\nand now from each Vector I want to be able to generate these three uh vectors so I'll use three.\nlinear Transformations right and they call them specifically as key query and value vectors and the corresponding matrices as the.\nkey Matrix the query Matrix and the value Matrix right so that's what we'll do on the next slide.", "metadata": {"video_title": "Attention is all you need"}}
{"text": "foreign [Music] welcome back so we we cover are we currently right so we started with a simple Network which.\nwas only one neuron and he had two weights W and B and today for the first five ten minutes.\nI'll actually revisit that and we'll you learn about the gradient descent algorithm for learning the parameters of this simple.\nNetwork right and then we took this idea and went to a more complex setup when instead of a single.\none neurons with just two weights wnb we had a very deep neural network with many arbitrary number of hidden.\nlayers and arbitrary number of neurons in each layer and we saw that the same gradient descent algorithm we could.\nextend it for this complex case also using what is known as back propagation and black propagation is essentially about.\nComputing the gradients or the partial derivatives or of the loss function with respect to the parameters in an efficient.\nmanner right and we saw that we just move one layer at a time in the backward Direction in so.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "manner right and we saw that we just move one layer at a time in the backward Direction in so.\nthat at every point we already have whatever we need to compute the derivative of the loss function with respect.\nto that element in the chain right so first of all we had said that the derivative of the loss.\nfunction with respect to any parameter can be expressed as a chain Rule and then you just keep Computing every.\nelement in that chain and the idea was that any single element is this change is easy to compute and.\nyou just keep doing that right and we came up with a very small algorithm right which like four or.\nfive steps and you just compute the gradients of the entire weight Vector entire bias Vector at one go for.\nall the weights and biases in the network right now while doing that we had used the gradient descent algorithm.\nright now today what we are going to do is we are going to look at various variants of the.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "right now today what we are going to do is we are going to look at various variants of the.\ngradient descent algorithm and so to do that we'll first start with quickly revising the gradient descent algorithm that we.\nhad done and then we'll point out some peculiarities that we see there and then we'll keep trying to fix.\nthose peculiarities right we'll say okay this looks bad let's try to work around it we'll propose something new then.\nwe'll say okay this is fine but this has some other problems let's try to fix that fix that and.\nit'll keep going to a series of uh algorithms which are all variants of the basic gradient descent algorithm okay.\nso that's the agenda so with that the first 5-10 minutes is going to be like at a Brisk Pace.\nwhere I have these around 30 slides on gradient descent which we have already covered uh in the lecture three.\nuh like lecture three meaning the slide which had the title lecture three uh so those I'm going to rush.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "uh like lecture three meaning the slide which had the title lecture three uh so those I'm going to rush.\nthrough right is just to give you the context of what was gradient descent and then I'll start with identifying.\nsome problems in gradient descent and then move on to the other algorithms okay so don't uh be worried if.\nyou're thinking I'm going to first that's the intention because we've already done it in the past okay um yeah.\nI should mention these acknowledgments I got the ideas for some of the visualizations uh from uh the videos by.\nRyan Harris this is long back uh six seven years I had seen that and then uh those were there.\ninitially in whatever visualizations they had the idea of this sort of uh the new version of this course is.\nto make some of those visualizations even uh better so that's why I hope you have been able to do.\nthat and some of the content is also inspired by uh the course which Henry carpathy uh used to teach.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "that and some of the content is also inspired by uh the course which Henry carpathy uh used to teach.\nin cs231n right so I just wanted to make sure I mentioned those acknowledgments okay so let's uh start so.\nthis part as I said module 5.1 and 5.2 is just a reputation so we had looked at the guesswork.\nalgorithm so this is what our setup was we had a simple network with two ways W and B and.\nwe were interested in learning these uh weights right and what was uh the intention that we wanted to find.\na sigmoid uh function right or rather we wanted the weights to be such that that when I plot the.\nsigmoid function using those weights then these two points should lie on that sigmoid function right that's what uh training.\nuh meant that's what we meant by uh training and to do that what we decided is okay let's start.\nwith some uh random guesswork right so I was trying to take these W comma B values that you see.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "uh meant that's what we meant by uh training and to do that what we decided is okay let's start.\nwith some uh random guesswork right so I was trying to take these W comma B values that you see.\nhere right and I was trying to move them this slider so that the values of w and B change.\nand as every time I do that this blue sigmoid curve that you see was changing and ultimately I was.\nable to come up with some curve which was passing through both the point so this was just random guess.\nI used to move the w a bit okay it's changing like this I don't like it maybe I want.\nit to come a bit lower so maybe let's move it in the other direction and so on right and.\nuh then we were trying to connect this intuition right any one of you who takes this plot and tries.\nto adjust it will definitely be able to come up with a sigmoid function of the logistic function which passes.\nto the two points but what is it that you are doing right intuition is clear but what's the connection.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "to the two points but what is it that you are doing right intuition is clear but what's the connection.\nto the math path so the connection was the loss function right what you're trying to do subconsciously is that.\nevery time you are adjusting the W and the B you're trying to make that sure that the loss that.\nI have which is the difference between the true value and the predicted value right is you're trying to make.\nsure that that decreases that's what you are trying to do and you are able to actually compute that loss.\nright so we knew what the loss function is we are using the squared error loss you knew the True.\nValue you knew the predicted value and you could just actually compute that physical quantity and in this case with.\nthe current values of w and B this is how bad my uh approximation was right this is how bad.\nmy current sigmoid function is it gives me a loss of 0.099 now once I had handled for this loss.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "the current values of w and B this is how bad my uh approximation was right this is how bad.\nmy current sigmoid function is it gives me a loss of 0.099 now once I had handled for this loss.\nI knew that I should keep trying to adjust the weights so that this loss decreases and I did that.\nby some doing a guesswork at every Point slide guesswork slight smart guesswork I was seeing okay this brought it.\ncloser so maybe further move W in that direction this took it far so maybe let's not move W in.\nthat direction but maybe uh make it in the other direction right and here the two directions are like either.\nincrease W or decrease W right move it positive or move it negative and we are able to come up.\nwith the sigmoid function right now what we wanted is something better than the guesswork algorithm right so then we.\nsaid that what you're trying to do is take every stage you're doing this random movements and trying to make.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "said that what you're trying to do is take every stage you're doing this random movements and trying to make.\nsure that the loss decreases but what you know is there's some true loss for for a surface which exists.\nright so this is the loss surface right which exists and if I had directly plotted this loss function and.\nhow do I plot this logs function for all values of w comma B all possible values of w comma.\nB and in this case I have only com considered values which are ranging from say minus 6 to plus.\nsix right for both W and B I just computed the loss I plugged it into the formula of loss.\nand whatever I got I just kept plotting here and that way I got this entire surface right but this.\nand now in this surface I can see that the loss is very high here if I choose the values.\nof w and B to be um say minus Phi and Phi then it's very high if I choose it.\nto be 4 and minus 4 close to 4 and minus 4 then the loss seems to be low right.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "of w and B to be um say minus Phi and Phi then it's very high if I choose it.\nto be 4 and minus 4 close to 4 and minus 4 then the loss seems to be low right.\nI can do all this but this is not again feasible right so I just had two parameters and I.\njust uh did this cheating of restricting the weights between minus six to six but actually the values are wnb.\ncan take our minus infinity to infinity and if you have like a large number of parameters that we had.\nin that complex neural network then you can't really draw this plot that means Brute Force compute all the values.\nfor the loss and find that value which is the smallest and then go and find the W and B.\nwhich will corresponding so you can't do this right so this guesswork or just Computing a Brute Force both are.\ninfeasible the guesswork can lead you to uh false Direction so when I was guessed trying to guess the values.\nare W and B at some points I was making a guess such that my loss was actually more than.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "infeasible the guesswork can lead you to uh false Direction so when I was guessed trying to guess the values.\nare W and B at some points I was making a guess such that my loss was actually more than.\nwhat it was at my previous case so that doesn't work and this is the brute force method which also.\ndoesn't work right you cannot do this Brute Force search when you have a large number of parameters even for.\ntwo parameters we cannot do right so you want something which is better than the uh Brute Force as well.\nas the uh guesswork algorithm and now I just shown you the visualization for the guesswork algorithm right so what.\nI was doing is that as I keep picking values from this table here uh my loss changes and what.\nhappens is actually I'm moving on this lost surface right so this yellow Point keeps changing depending on my current.\nwnb and subconsciously I was actually moving on this lost surface without actually knowing what the loss surface was that's.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "wnb and subconsciously I was actually moving on this lost surface without actually knowing what the loss surface was that's.\nwhat I was doing in the uh in the in the guesswork algorithm and Computing this entire loss surface is.\ndifficult because you have values from minus infinity to Infinity right okay so then uh we said we want a.\nmore principled way such that I don't need to do Brute Force I don't need to compute all values of.\nthe loss function for or rather the value of the loss function for all values of w and B nor.\ndo I want to do this random guessing so that's where we went to gradient descent and the key I.\nidea here was to rely on Taylor series right and I'll just skip all this part which was setting up.\nthe context for the Taylor series right so we all understand what Taylor series is allows us to compute the.\nvalue of a function in a small neighborhood around the current point so if you know the value of the.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "value of a function in a small neighborhood around the current point so if you know the value of the.\nfunction around the current point you could compute its value in a small neighborhood around it right so that's what.\nTaylor series was allowing us to do and the key thing for us why we needed Taylor series was this.\nright so we started off with some a random value for the loss function right and we wanted to find.\na sorry sorry sorry sorry we started off with some random value for the parameter and we wanted a new.\nvalue for the parameter right so we have started somewhere we know what the loss there is now from here.\nI want to move somewhere else and what is my guiding principle that wherever I move my loss should decrease.\nright that means this is the condition that I was aiming for that my new loss should be less than.\nthe current loss right and when we solved for this uh inequality uh we made some arguments and what we.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "the current loss right and when we solved for this uh inequality uh we made some arguments and what we.\nrealized is that if you want that then this U which was the change that you were making uh which.\nis a vector should be in the direction opposite to the gradient right so that's what we arrived at so.\nwe arrived at the gradient descent update rule that wherever you are okay you just move in the direction oppose.\nit hence minus take a small step in the direction oppose it to the gradient and this is the gradient.\nand what is this quantity what was the definition this is important because we will be using this quantity throughout.\nthis lecture was that you compute the partial derivative of the loss function with respect to w and then evaluate.\nit at w equal to WT and be equal to b or BT right so suppose your loss function is.\nx square so you have only one parameter X or let me just instead of x square let me just.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "it at w equal to WT and be equal to b or BT right so suppose your loss function is.\nx square so you have only one parameter X or let me just instead of x square let me just.\nsay w Square then you have only one parameter W and the derivative of the loss function with respect to.\nW is 2W and this derivative evaluated the current value of w right which might be say w current is.\nequal to 1 would just be 2 right because I'll substitute the value W equal to 1 in this equation.\nright so that's what this means it's the derivative or the partial derivative of the loss function with respect to.\nthis variable evaluated at the current value of the variable right so this is what we had and then we.\nalso went ahead and computed that gradient right so we got the algorithm we this is what our algorithm is.\nwe'll initialize it randomly we'll keep doing these updates so at every Point wherever I'm I am I'll move to.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "we'll initialize it randomly we'll keep doing these updates so at every Point wherever I'm I am I'll move to.\na new Point such that from the Taylor series I know that the loss at the new point is going.\nto be less than the loss at the current point right so that's what the gradient descent update rule was.\nbut the only thing we don't know here is how to compute this gradient so we actually went ahead and.\ncomputed that right so we did this derivation which I'll not go over but we knew how to compute the.\nderivative and uh once we knew how to compute that derivative right it was this quantity we then wrote the.\ngradient descent algorithm and what were we doing there for at every point we're Computing this gradient and then moving.\nthe direction opposite to the gradient and when we did that this is what happened right so this is yeah.\nso this is what was happening in our gradient when we when we ran this algorithm right so let me.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "so this is what was happening in our gradient when we when we ran this algorithm right so let me.\njust give some uh context here ignore this for now ignore the of yeah so ignore uh this for now.\nright just focus on the uh top graph and I'll tell you what we are trying to do there okay.\nyeah so I was just trying to run the gradient descent algorithm this was my the the Flying Carpet kind.\nof a structure that you see it was what my loss function looked alike and I started with some random.\ninitial value right which is the black point where I have started this is the loss and these this is.\nthe point corresponding to W comma B right so my w was somewhere around -5 and my B was also.\nsomewhere around minus 5 right so that's the point from which I started and as you can see my loss.\nis quite high it's around 0.4 or 0.5 something like that okay so now I run the gradient descent algorithm.\nso at every step I am going to change my w and B so this point that you see on.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "is quite high it's around 0.4 or 0.5 something like that okay so now I run the gradient descent algorithm.\nso at every step I am going to change my w and B so this point that you see on.\nthe bottom is going to keep moving in the WB plane right so what you see here is the WB.\nplane right so at every point I'm Computing a new WB so I'm just going to keep plotting that right.\nand this is how it is moving right so I'll make as this I'll just play it once and then.\nmake a few observations or I'll keep making the observation so right now you can see that I'm on a.\nfairly flat surface right and because of that I'm moving very slowly right my updates are very slow they're so.\nslow that when you see on the bottom you don't are not even able to distinguish between two points W.\nand B right because they're so close to each other they're just looking like a snake's body right but when.\nI come here okay it's still running okay so when I came here let's see okay now let me just.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "I come here okay it's still running okay so when I came here let's see okay now let me just.\nannotate this right so in this region I was moving very very slowly and you can uh okay so in.\nthis region I was moving very slowly and you can see here the the change in wnb was so small.\nthat the two consecutive points were so close to each other that they're almost seeming connected right that's what is.\nhappening at the bottom and then when here in this region when I went inside the valley right so I.\nwas at the plateau at the top you can see that now my w and B are changing fast right.\nso maybe it's not visible clearly now let me just uh so you know which region I want you to.\nlook at so now let me just try to expand this or rather make maximize this right so this region.\nhere is what I want you to look at and you can see that now my w and B are.\nactually I mean the every new W comma B point is actually farther away from my previous W comma B.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "here is what I want you to look at and you can see that now my w and B are.\nactually I mean the every new W comma B point is actually farther away from my previous W comma B.\npoint right what does that mean that my updates are getting bigger now because my new W is my old.\nW plus sum quantity or rather minus some quantity so if that quantity is Big then I will start moving.\na bit uh farther away from where I was right so in this part where the slope was high right.\nwhere I was entering uh into the valley at that point I'm seeing that uh I am uh the distance.\nis increasing right and then again when I enter into the valley at that time again my updates become very.\nsmall right so this is what my observation is so let's uh keep that and let's move to move ahead.\nright so now what we have done so far is quickly revised what the gradient descent algorithm was and now.\nwe have made some observations about the gradient descent algorithm right and from here I want to move forward and.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "we have made some observations about the gradient descent algorithm right and from here I want to move forward and.\ntalk about why are we making those observations right what's the Intuition or are we yeah what's the intuition behind.\nthe observations that we have made okay so let me go to the next slide yeah so now so let's.\nignore this figure for now right so what we observed in the previous slide was that when the slope when.\nthe curve was gentle right those ah yeah at the the red portion at the top or the pink portion.\nat the top the slope was very gentle my movements were very small when the slope became steep My movement.\nwas a bit fast and then again when I came to this gentle slope my movement became slow right I'm.\ntrying to now and get a reasoning for this right why is this happening so what exactly is happening there.\nright so what is our movement right what do I mean by the movement so let me just make that.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "trying to now and get a reasoning for this right why is this happening so what exactly is happening there.\nright so what is our movement right what do I mean by the movement so let me just make that.\nclear first right so my update rule is W is equal to W minus ETA into Delta W okay so.\nnow if my this quantity is very small let's say it's 0 right then my new W would be the.\nsame as the old W right I'll not be moving right hence when I was looking at this uh WB.\nplane at the bottom right now I'm trying to update the values of w as well as uh B now.\nthese updates are very small right then what will happen is I was at some WB and I'll now again.\nremain very close to that right and not move very far but if these updates are large then my w.\nwill change by a large value so say w was uh here it will change by a large value and.\ngo somewhere here and my B will also change by a large value and go somewhere else so hence now.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "will change by a large value so say w was uh here it will change by a large value and.\ngo somewhere here and my B will also change by a large value and go somewhere else so hence now.\nif I look at the original point where I was which I'll now Mark as red right this is where.\nI was and if my Delta W and Delta B were large then I'll move to a point which is.\nfarther away from it right so that is what will happen that means these Delta quantities that you see here.\nare controlling how much fast or or by La the amount by which I move right the magnitude by which.\nI move and what are these quantities these are actually the partial derivatives of the loss function with respect to.\nthose parameters right so now how does this collect connect to the smooth and the gentle surfaces what is happening.\nthen on the smooth surfaces you are moving slowly that means your Delta W is small that's why you're moving.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "then on the smooth surfaces you are moving slowly that means your Delta W is small that's why you're moving.\nslowly why should that be the case we will see that on the Steep surfaces you are moving fast because.\nyour Delta W is larger why should that be larger on the Steep surfaces that is something that we'll see.\nnow okay so that's the context and with that I'll just go back and play the video again right I'll.\njust want you to observe that indeed when when you are on the smooth surfaces the movement is very very.\nslow but when you come to the yeah so when you're on the smooth surfaces the movement is indeed very.\nslow but when you come to the uh steep surfaces the movement becomes fast right so I'm going to to.\nplay the video again and observe that what happens when I transition from this red region here to jumping into.\nthe valley right you will see that there's a certain fast movement there just make sure you observe that okay.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "the valley right you will see that there's a certain fast movement there just make sure you observe that okay.\nsee now I'm moving very very slowly very very slowly it's almost I mean I was for a minute worried.\nwhether the video is paused right that's how slow the movement is and here I can see it's flat and.\nsomehow on the flat region uh the Delta W is small why is that the case we'll see that soon.\nbut now let's see what happens right fast right it went very fast that means my updates were very happening.\nwere very large in that region where my slope was steep now why is it that the updates are large.\nin the Steep areas and why is it that the updates are small in the gentle areas is what I'll.\nnow what we need to see in the next slide okay so that's where we'll go foreign and the video.\nand all is a bit chaotic in terms of the interface okay so now let's see right so what is.\nthe derivative the derivative is the change in so I have written change in the function right so it's a.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "and all is a bit chaotic in terms of the interface okay so now let's see right so what is.\nthe derivative the derivative is the change in so I have written change in the function right so it's a.\nchange in the function by the change in the value right so what does the derivative capture as we had.\nsaid it captures the sensitivity of the function to a change in the variable so if I change X by.\na small amount how much does the function change right so if you have x square then you know that.\nif you change X from 2 to 2.5 still the square changes a bit quite a bit right it changes.\nfrom 4 to 6.25 so although you have made only a 0.5 change in the value of x the square.\nhas changed by 2.25 right uh so that's that's what happens right so now let's look at what the so.\nthis is how I would note the gradient as so it is the movement in the y direction divided by.\nthe moment in the X direction right so in those regions now let's see this this here is a region.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "this is how I would note the gradient as so it is the movement in the y direction divided by.\nthe moment in the X direction right so in those regions now let's see this this here is a region.\nwhere the store slope is steep right and this here is a region where the slope is gentle so now.\nlet's see what does the derivative look like in these two regions okay so I'll just try to move the.\npoint so in this region uh you can look at the value being computed here right which is d y.\nby DX that's quite high right the derivative is minus 3.54 now and all of this are very steep regions.\nright now slowly the slope is becoming gentle and when I come here to this point right you can see.\nthat the derivative is very small right why is that happening it's obvious because here for a small change in.\nX you are getting a large change in y and that's what the derivative captures but here when you're closer.\nto 1 right let me come this side so when you're closer to 1 when you're changing X by a.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "X you are getting a large change in y and that's what the derivative captures but here when you're closer.\nto 1 right let me come this side so when you're closer to 1 when you're changing X by a.\nsmall amount and the change in X is constant right I'm just changing X by 0.5 right so when I'm.\nchanging X by 0.5 my uh y would uh is uh the the change in y divided by the change.\nin X is 1.17 in this region but as I go up right now when I'm around two as I.\nwas saying right so so when I change the gradient for example when I change from 1 . yeah so.\nwhen I'm in this region around one okay so when I change to 1 say X from 1 to 1.5.\nright my uh my x square changes from 1 to 2.25 okay so that's the change that happens in X.\nand Y but now when I'm around 2 right or maybe say around yeah let's look at around 2 here.\nhere I see that the slope is much more steeper what is happening there when I change X from 2.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "and Y but now when I'm around 2 right or maybe say around yeah let's look at around 2 here.\nhere I see that the slope is much more steeper what is happening there when I change X from 2.\nto 2.5 my y changes much more rapidly right so y changes from 4 to 6.25 so the difference here.\nis 2.25 as opposed to 1.25 here so I did the same change in X my change in X was.\nconstant which was 0.5 but in one uh neighborhood my y change is very a lot the change in y.\nis very high right that means d y by D X would be high in that region that means the.\ngradient would be high and that gradient is essentially the slope uh of the function uh at that point right.\nso in those regions the slope is very high and the slope is essentially the derivative so wherever I have.\nsteep slopes there my d y by DX or the derivative would be high and hence this quantity would be.\nhigh so if this quantity is high my update or the change in W would be high and when I'm.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "steep slopes there my d y by DX or the derivative would be high and hence this quantity would be.\nhigh so if this quantity is high my update or the change in W would be high and when I'm.\nin gentle regions my derivative would be small and hence the change in W would be small because this quantity.\nis small so whatever am I updating by is going to be small okay so that's the intuition and you.\ncould just play around with this curve a bit so I have kept the change to be constant at 0.5.\nyou could also try with changing one and you could also try with different function right so currently the function.\nis f x equal to 0.6 X square if I make it 2x square right then you can see that.\nnow the function has a even bigger slope in these regions and it does make sense right because the derivative.\nof 2x square is foreign X where is the derivative of x square is 2x right so it does make.\nsense that this is more uh steeper slope than the uh slope of the function x square right so the.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "of 2x square is foreign X where is the derivative of x square is 2x right so it does make.\nsense that this is more uh steeper slope than the uh slope of the function x square right so the.\ntakeaway from here is that when you have a flat surfaces the gradient would be slow so the movement would.\nbe slow when you have steep surfaces the gradient would be large hence the movement or the change in W.\nwould be large right now what are the repercussions of this so the repercussion of this is what we saw.\nin the previous slide that if you start off from a point as we are unfortunately done in the previous.\nslide where the surface is largely flat right then your gradients are going to be very very very very very.\nvery small and you'll be making these tiny tiny updates W and take a long time to move away from.\nthat flat surface right so imagine like a very flat surface which is quite big right in terms of the.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "very small and you'll be making these tiny tiny updates W and take a long time to move away from.\nthat flat surface right so imagine like a very flat surface which is quite big right in terms of the.\nW comma B neighborhood that it spans then you'll be stuck there for a very very very very long time.\nright before you move to some favorable region where you see a slope and then you start going towards the.\nMinima right and it could happen that you run so we remember we had sent said this Max iterations as.\nthousand so it could happen that you are running your algorithm for thousand iterations but you are not seeing any.\ndramatic change in the loss because indeed you are on this flat surface where the loss is more or less.\nconstant or very gentle slope right it's decreasing from say maybe 0.9 to 0.8999999 over a very long distance right.\nand now if you're just stuck there and making these small changes to W you'd run the code for thousand.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "and now if you're just stuck there and making these small changes to W you'd run the code for thousand.\niterations and you will see oh nothing is happening my loss is not changing my I'm not going towards a.\nMinima and so on it will just be stuck there so you need to be able to come out of.\nthese situations right so this is one problem that you have with gradient descent that if it's stuck in regions.\nwhich have a flat surface or have a very gentle slope then it'll take a lot of time if you.\nperhaps run it for 10 000 iterations then it will come out and go to some slope and then take.\ngo towards the Minima right but then you might not have the patience to run it for 10 000 iteration.\nso can we do something so that in these gentle regions the movement of gradient descent is fast right so.\nthat's the drawback that we want to solve okay so that's where we are headed and just to convince you.\nthat is not like uh I mean this is not a characteristic of where I started with and so on.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "that's the drawback that we want to solve okay so that's where we are headed and just to convince you.\nthat is not like uh I mean this is not a characteristic of where I started with and so on.\nand just to give you a different starting point right and if you see a convenient starting point so this.\nhere was an example of me getting stuck right on I mean I just happened to start with this yellow.\nPoint here which was on a flat surface right so I'll take a while to come out of that flat.\nsurface now let me just try some different uh initialization right and there was some suggestion on the uh slides.\nI'll just follow that suggestion oops yeah so the suggestion was to uh try W as minus 2 and B.\nas 2 right so I'll just start that I'll just try that okay so I'll set this to -2 I'll.\nset this to 2. okay so this is where I'm going to start now and now you can see that.\nit's a slightly favorable point because I'm on a slope I'm not in a flat surface I am on a.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "set this to 2. okay so this is where I'm going to start now and now you can see that.\nit's a slightly favorable point because I'm on a slope I'm not in a flat surface I am on a.\nflow so on a on a steep surface so let's see what happens if I run now right see it's.\nmoving faster now it's immediately gone into the valley and I don't see now you can also see that these.\nyellow dots are actually farther from each other because the updates are larger right so there is a corresponding plot.\nbeing drawn on the WB plane also which you could perhaps uh see offline where you can see how the.\nmovement on the WB plane is happening and now again it has reached a region of uh of a flat.\nsurface out of a gentle slope and now you can't do anything right it'll just keep moving very very slowly.\nand it is a lot of distance to cover uh to reach the Minima right and you can see that.\nI've already exhausted my 100 iterations it's done I've done with 100 iterations but I have not been able to.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "and it is a lot of distance to cover uh to reach the Minima right and you can see that.\nI've already exhausted my 100 iterations it's done I've done with 100 iterations but I have not been able to.\nreach the Minima which would have been somewhere here right so so that did not happen maybe if I ran.\nfor 200 300 400 so you can go back and experiment and see how many iterations it takes and this.\nlooks a bit frustrating right because you know that you're almost there you just need to run faster and get.\nthere right but you're not able to run faster because the derivatives become small right and this will happen this.\nwill always happen when you're close to the convergence so when you're close to convergence your surface does not have.\na very high slope and now we are very very very slowly moving towards the Minima which you don't want.\nto happen right so the problem that you want to solve is that in regions where you have a gentle.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "to happen right so the problem that you want to solve is that in regions where you have a gentle.\nslope you should be able to move faster right so we'll try to solve that problem but before that we'll.\nperhaps take a D2 let me see what we have planned yeah so we uh okay so then what I'll.\ndo is I'll end this video here it's already been a long video and when we come back we'll first.\ntake a D2 try to understand what contos are uh then look at gradient distance in the context of Contours.\nand then try to solve the problem that we just described which is on flat surfaces gradient descent moves very.\nslowly okay thank you.", "metadata": {"video_title": "A quick recap on gradient descent and derivative"}}
{"text": "[Music] hi uh so welcome back uh so we'll now talk about a typical supervised machine learning setup and this.\nin my opinion is the most important module of the course of course as we go along i'll say this.\nfor many modules but i think this is the most important one right so please pay attention uh so this.\nis what we have right so we looked at the sigmoid neuron and we looked at one particular function within.\nthis family which is the logistic function now what next right so we have seen this right so what what.\nwas our pattern when we had introduced uh perceptrons we saw the perceptron model and then what did we do.\nwhere did it go from there we first looked at the algorithm for learning the weights right so we want.\nthe same thing now i've defined this but now how do i learn the weights that's not clear right so.\ni want to have an algorithm which can learn these weights but before i had done the perceptron learning algorithm.", "metadata": {"video_title": "A typical supervised machine learning setup"}}
{"text": "i want to have an algorithm which can learn these weights but before i had done the perceptron learning algorithm.\ni had taken a slight detour and spoken about errors right because that was important to understand the error because.\nthe whole point of the algorithm was that whenever you make an error you do some correction right and that's.\nwhy understanding error was important here also there is something similar so i'm going to revisit the concept of error.\nagain and then from there start working towards the algorithm right so but uh in this video we'll not do.\nthe algorithm we'll just do the machine learning setup right so let's look at that so coming going back to.\nthe perceptron algorithms and seen these kind of data distributions there and i had said that this is not linearly.\nseparable of course it is not and we had said that a single perceptron cannot deal with this data right.", "metadata": {"video_title": "A typical supervised machine learning setup"}}
{"text": "separable of course it is not and we had said that a single perceptron cannot deal with this data right.\nor cannot deal with linearly separable not linearly separable data but what do i mean by cannot deal right what.\nwould happen if i apply perceptron algorithm suppose i blindly go and say okay i want to just apply the.\nperceptron algorithm here what would it do can you tell me what would happen in this case it'll make errors.\nfor some points what's the point of a perceptron algorithm the idea of a perceptron algorithm is that you'll find.\nsome separation boundary right now we know that if the data is not linearly separable this it will not converge.\nright because we'll keep toggling some points will be here and there but if i just do okay i'll just.\ndo it for some thousand iterations for a fixed k i'll do it so what do you expect would happen.\nat the end it could draw some line right now there many lines possible you could draw this draw this.", "metadata": {"video_title": "A typical supervised machine learning setup"}}
{"text": "do it for some thousand iterations for a fixed k i'll do it so what do you expect would happen.\nat the end it could draw some line right now there many lines possible you could draw this draw this.\nor this many things right so what do you think is a line a possible line here that it would.\nend up with if i just run the perceptron algorithm just think about it okay i don't i mean it's.\nhard for you to tell me what you're thinking but just think about it and then when i show you.\nthe answer just see whether you are in agreement with that sorry somewhere in the middle of the data and.\nuh we all understand what he means by middle of the data right so let's see so this is what.\nit would do in the upper case right where most points are i mean some there are errors i mean.\nthere's that's a hard problem given it cannot really separate by a linear boundary but it'll draw some boundary right.\nin the second case also it will draw some boundary and here the situation is better as you can see.", "metadata": {"video_title": "A typical supervised machine learning setup"}}
{"text": "in the second case also it will draw some boundary and here the situation is better as you can see.\nit's making an error on some three orange points which are getting classified as blue and maybe a couple of.\nblue points which are getting classified as orange right and in most real-world applications we could deal with this kind.\nof an error right so there are like maybe uh 10 uh blue points here of its two are wrong.\nand 10 orange points of which two or three are wrong so 20 error we could deal with unless this.\nis like a mission critical uh application in which case it might be difficult but in most real world applications.\nwe might be okay with it right for example if you're predicting whether this person is likely to vote for.\none party or the other and if you make some error in that prediction like 20 of the people you.\npredict wrongly it won't make much of a difference right so it would still give you some rough idea of.", "metadata": {"video_title": "A typical supervised machine learning setup"}}
{"text": "one party or the other and if you make some error in that prediction like 20 of the people you.\npredict wrongly it won't make much of a difference right so it would still give you some rough idea of.\nhow the election is going right and in many cases of course unless it's a very closely fought election but.\nin most cases if the error is around fighting person you can live with it right so from now on.\nwe are going to live with this idea of error that error may not always go to zero and our.\ngoal would be to just minimize the error as much as possible right so if i were to actually have.\na zero error here i would need something very complex right so i would want a decision boundary uh which.\nperhaps i don't know looks something like this right and now even that is not enough uh yeah i just.\nwant an island here right so it's very difficult right i mean like a very complex boundary so that all.", "metadata": {"video_title": "A typical supervised machine learning setup"}}
{"text": "want an island here right so it's very difficult right i mean like a very complex boundary so that all.\nthe blue points are separated from the uh orange points and we'll be okay if at least i can find.\nsome boundary right where the error is as small as it can be and i can live with that little.\nerror right so that's what i'm going to do i'm going to now start using the term minimize the error.\ninstead of saying that the error should be zero right so that's one switch i'm going to make right and.\nnow with this background we are ready to discuss what a typical supervised machine learning setup looks like not machine.\nlearning but supervised machine learning right so always given some data okay what is this data this is sum r.\nn right these are vectors belonging to rn okay and this is some let's for the simplicity assume that it's.\njust a real value it could again be a vector right it could be the case that given the data.", "metadata": {"video_title": "A typical supervised machine learning setup"}}
{"text": "n right these are vectors belonging to rn okay and this is some let's for the simplicity assume that it's.\njust a real value it could again be a vector right it could be the case that given the data.\nabout uh movie i am trying to predict the uh box office collection of the movie i am also trying.\nto predict the imdb rating of the move right so i could be predicting two things here right but for.\nnow i'll just assume that we're just dealing with a real valued output and not a vector okay now we.\nhave already seen what this data also could look like right so uh and this is where the most important.\npart comes in let me just quickly go ahead okay this is not what i want to do right so.\nthis we have seen application right so let me just define all of these right so these are my x's.\nokay this is a vector which belongs to r let me just call it m right because i'm using n.\nfor something else and i have n such data points right so in my uh just i have n such.", "metadata": {"video_title": "A typical supervised machine learning setup"}}
{"text": "okay this is a vector which belongs to r let me just call it m right because i'm using n.\nfor something else and i have n such data points right so in my uh just i have n such.\ndata points so in my oil mining case uh i'm assuming that there are already n drilling stations set up.\nsomewhere and i have all the data about them right what was the pressure what was the salinity these end.\npoints by which i am defining a location or characterizing a location and i also know the y for them.\nright so i have a m dimensional input i have a single dimensional output which is the amount of oil.\nin mind for them and i have given some n such training points right so this is what a typical.\nmachine learning setup looks like similarly if i look at the other example where i had the credit card or.\nmaybe i had the bank interest rate that i wanted to decide right so i have the past history of.\nthe person and this history is being characterized by various things what is the salary of the person what is.", "metadata": {"video_title": "A typical supervised machine learning setup"}}
{"text": "the person and this history is being characterized by various things what is the salary of the person what is.\nthe education level of the person what is the family size of the person does he have any past loans.\nwhat was the interest rate on the past loans how soon did he dispose of that loan and so on.\nright so all of those are the past history and based on that for my existing customers i know what.\nwas the rate at which it was suitable to give them right and now this training data is given to.\nme and there are uh n such training points each of which is m dimensional and i'm trying to predict.\na real valued output right and now you can think of on and on of so many such problems right.\nso all the machine learning supervised machine learning problems have this form i'll give one more which is the movie.\nprotection problem or the box office collection problem right so i have want to predict the box office collection of.", "metadata": {"video_title": "A typical supervised machine learning setup"}}
{"text": "protection problem or the box office collection problem right so i have want to predict the box office collection of.\na movie i have data about the movie right what was the budget of the movie how many superstars are.\nthere in the movie right how good who is the music director of the movie who is the cinematographer all.\nof this i will have and based on past movies i know what what is the box office collection what.\nis the genre of the movie all of this is there right and i know this so the form i've.\ngiven such many such movies from past and such movies i'm characterizing each movie by an m-dimensional vector which is.\nsome inputs about the movie and i want to predict the box connection right so this is what the data.\npart looks like and the main point here is that across all supervised machine learning setups this is what it.\nlooks like every machine learning problem you can cast it into this matrix where you are given the x's and.", "metadata": {"video_title": "A typical supervised machine learning setup"}}
{"text": "looks like every machine learning problem you can cast it into this matrix where you are given the x's and.\nthe y's and you are given many such x comma y pairs okay now this is where now uh thinks.\nuh the challenge is right this is what the crux of machine learning is so we have a y right.\nand we have a reason to believe that this y depends on the input x right so what is the.\ny here the box office collection and it depends on the characteristics of the movie right this depends this equation.\nis still not complete y is equal to x is not what i want to write i want to write.\nsomething else but i'll come to that right i'm saying that y depends on x then similarly in the interest.\nrate y depends on that all the inputs that i'm taking and even in the first example of oil it.\ndepends so what do you what's the mathematical way of saying that something depends on something y is equal to.\na function of x right so i would say that y is equal to f of x right now if.", "metadata": {"video_title": "A typical supervised machine learning setup"}}
{"text": "depends so what do you what's the mathematical way of saying that something depends on something y is equal to.\na function of x right so i would say that y is equal to f of x right now if.\ni knew this f there's no problem in the world right if i knew what is the function such that.\ni plug in the values of salinity pressure density all of these factors and just tells me what the y.\nwould be if i had such a function all of us would be billionaires right we will just look at.\ndifferent locations plug in those values and wherever we get the highest mining we'll go and set up drilling stations.\nthere right but that function is not known right so this is the beauty of the problem we know that.\na function exists the output has to depend on certain inputs right these are the inputs and you could argue.\nabout oh but maybe you have not considered all the x's i would argue by saying okay consider all the.", "metadata": {"video_title": "A typical supervised machine learning setup"}}
{"text": "about oh but maybe you have not considered all the x's i would argue by saying okay consider all the.\nx's if you have forgotten to consider the parent salary okay put it in you might say that it would.\ndepend on the parent salary you would say that you depend on the children's education okay put all of that.\nin right and construct as comprehensive and x as you can and then i would say that the y depends.\non this x so y is a function of x so that is known that y is a function of.\nx but what is not known is what is this function so now what do you do in this situation.\nright so you want to now your goal is to come up with this function so that for new x's.\nso for a new location which i say the shell company has hired you and said okay this is the.\nlocation i'm considering can you tell me what is the oil that i could mine from there so you want.\nto plug in that here and tell the company that this is what the quantity of oil i estimate right.", "metadata": {"video_title": "A typical supervised machine learning setup"}}
{"text": "location i'm considering can you tell me what is the oil that i could mine from there so you want.\nto plug in that here and tell the company that this is what the quantity of oil i estimate right.\nso you want to do that so what do you do in this case and this is the crux of.\nmachine learning that you assume some function okay you assume there is a function x parameterized by some parameters w.\nor more generically we call it theta also at times so i'll just use theta maybe okay so you assume.\nsome form and that assumption is up to you right you assume that there is a function f hat which.\nrelates the inputs to the outputs it has certain parameters and now your job is to learn these parameters from.\nthe given data okay now let's look at a simple k so suppose you assume that y is equal to.\nw x plus b this is a function okay and you have assumed that y depends on x it's capturing.\nthat and you have some parameters now how would you estimate the values of w and b if i give.", "metadata": {"video_title": "A typical supervised machine learning setup"}}
{"text": "w x plus b this is a function okay and you have assumed that y depends on x it's capturing.\nthat and you have some parameters now how would you estimate the values of w and b if i give.\nyou two points maybe i should use a different form which is y is equal to mx plus c and.\nnow you would remember from my school that how do you estimate m and c if you're given two points.\nyou can get these two simultaneous equations and then you can find the values of m and c right so.\nthat is how we could do now in real-world applications it won't be like i mean i've taken a very.\nsimplistic example but what would happen is that you'll be given many such data points the key takeaway from the.\nexample is that if you're given data points which was two x comma y pairs right i have n x.\ncomma y pairs here if i just had 2 x comma y plus then i could predict the values of.\nw and p right i could find out the values right and from that i just want you to extrapolate.", "metadata": {"video_title": "A typical supervised machine learning setup"}}
{"text": "comma y pairs here if i just had 2 x comma y plus then i could predict the values of.\nw and p right i could find out the values right and from that i just want you to extrapolate.\nyour belief and say that if you have many data points and many parameters you could come up with algorithms.\nto find the values of these parameters okay in this case our algorithm was simple let's just construct a simultaneous.\nequation and solve that simultaneous equation right but you could extrapolate this idea and you could say that if you.\nhave many data points given to you where the values of y and x are known and now you have.\nsome parameters to find you could estimate these parameters from the data okay so that's the game in machine learning.\nthere are few more details i'll now erase this whatever i have here and go into those details okay okay.\nso our approximation of the relation between x and y is called the model right so when i say that.", "metadata": {"video_title": "A typical supervised machine learning setup"}}
{"text": "so our approximation of the relation between x and y is called the model right so when i say that.\ni have a machine learning model what does a model mean model means your belief of what is the relation.\nbetween y and x right and it could be anything it could be this function we just saw this that.\nwhy it does it satisfy the criteria that i have y is a function of x it also has some.\nparameters okay what is this have you seen this before in machine learning in machine learning what is this called.\nit's called logistic regression right this is the logistic regression model what is this linear regression you are assuming there.\nis a linear relationship between y and x this is you are assuming there is a quadratic relation right any.\nof this assumption is equally good or bad right now in deep learning you are going to we'll see that.\nlater on again you assume some f right and as the name suggests this f is going to be very.", "metadata": {"video_title": "A typical supervised machine learning setup"}}
{"text": "of this assumption is equally good or bad right now in deep learning you are going to we'll see that.\nlater on again you assume some f right and as the name suggests this f is going to be very.\ndeep what it means in mathematical terms is going to be a very complex composite function right and we'll get.\nthere but again in deep learning there's nothing different it's just that as you had certain forms of f hat.\nhere deep learning has a certain form and that form we will see later as we go on okay now.\none question so why are parameters important here why can't i just assume that y hat is equal to 1.\nover 1 plus e raised to minus x why do i need the parameter this is also a function it.\ny hat depends on x because this is a function of x why can't i just use this why am.\ni complicating it by adding parameters different different weightages to different features okay but something more fundamental than that right.", "metadata": {"video_title": "A typical supervised machine learning setup"}}
{"text": "i complicating it by adding parameters different different weightages to different features okay but something more fundamental than that right.\nlet me okay so this doing this is akin to saying that y is equal to 3x plus 2. i.\nhave fixed my parameters right here i'm just saying that the parameter is 1 right what is the problem with.\nthis i'm not learning this i just said okay y is equal to 3x plus 2 i just cooked it.\nup now who tells me whether 3 and 2 are the right values i have to learn that from the.\ndata right so if i don't have any parameters i have nothing to learn i just told you y hat.\nis equal to 1 plus e raised to minus x now throw away all the data that you have for.\nany new x that you get just plug in here and you'll get the value and live with whatever value.\nand that's not what you want to do right you want to have certain parameters so that those parameters can.\nbe learned right so what would happen in the sigmoid case for example right this is what a sigmoid function.", "metadata": {"video_title": "A typical supervised machine learning setup"}}
{"text": "be learned right so what would happen in the sigmoid case for example right this is what a sigmoid function.\nlooks like right and it will look like this s-shaped but depending on the values of w and b it.\nwill decide whether this is something like this something like this or is it something like this right so all.\nof this will depend on your values of them so you can have a function form but you can adjust.\nthat form of the function by or adjust the shape of the function by using these parameters right so that's.\nwhy if you don't have parameters and all your data is useless right if you just say my equation is.\nthis you have not even looked at the two points how do you know that those two points will satisfy.\nthis equation right so to look at the points and then come up with this equation right then you have.\nto find the values of w and b so that's the same idea here that's why you need these parameters.", "metadata": {"video_title": "A typical supervised machine learning setup"}}
{"text": "this equation right so to look at the points and then come up with this equation right then you have.\nto find the values of w and b so that's the same idea here that's why you need these parameters.\nokay now learning algorithm right so once you have set up the model you have the data now you need.\nan algorithm so that you can learn these parameters template in the case of y is equal to mx plus.\nc our algorithm was simple just have two points plug them in get two simultaneous equations solve that right so.\nthe algorithm is the algorithm for solving the simultaneous equation and we are done right but in large parameters large.\ndata we need something better something more principled and we have already seen one example which is the perceptron learning.\nalgorithm and now we are going to see gradient descent in some time right so that's what we need now.\nwhat should the goal of this learning algorithm be so when would you say that the parameters that i have.", "metadata": {"video_title": "A typical supervised machine learning setup"}}
{"text": "what should the goal of this learning algorithm be so when would you say that the parameters that i have.\nlearnt are good right so in the case of perceptron algorithm we had this loop which said while not convergence.\nto this right and there we had assumed that the data is linearly separable so i will keep running the.\nalgorithm till no point moves right or still the w the new value of w does not move because all.\nthe points are on the correct side of the line or the play right that was fine but that was.\nthat means you were going for zero error right but now there is no idea of zero error as i.\nsaid in most available applications we'll have to deal with minimum error right so then what should your learning algorithm.\ndo it should have a certain objective right in the case of perceptron the objective was to separate the data.\nthat means no point misclassified that means zero error now your objective function which is going to guide the learning.", "metadata": {"video_title": "A typical supervised machine learning setup"}}
{"text": "that means no point misclassified that means zero error now your objective function which is going to guide the learning.\nalgorithm has to be something about minimizing the error right how to do that what is error all of that.\nwe'll see in the subsequent slides right but that should what the goal be right and let's see yeah so.\nthe learning algorithm should aim to minimize the loss function right there is more to be said here but i'll.\nnot say those things in this slide we are going to come back to this slide again in a different.\nform and then i'll say a bit more right so right now i think what should be clear is the.\ndata set up those three examples that i showed the model which is our approximation of the relation between x.\nand y the learning algorithm which whole point is to learn w any learning has to be driven by an.\nobjective right so that the objective is to minimize the error function and that's what the objective is now how.", "metadata": {"video_title": "A typical supervised machine learning setup"}}
{"text": "objective right so that the objective is to minimize the error function and that's what the objective is now how.\ndo you come up with the learning algorithm how do you define the objective function right so up till this.\npoint i have defined everything mathematically right but now objective function have objective function i have not defined so i.\nwill have to tell you what the objective function is but we will come back to that okay so now.\nconsider our movie example so this is the movie data given to me right x i is equal to movie.\ni'm just using a shortcut when i say x is equal to movie i i mean that entire vector right.\nall the actor information budget information number of superstars blah blah and y i is equal to 0 comma 1.\nright so that's what it is our approximation of the relation between x and y so our approximation was that.\ny hat is equal to the sigmoid function of x right with the parameters being w okay then learning algorithm.", "metadata": {"video_title": "A typical supervised machine learning setup"}}
{"text": "y hat is equal to the sigmoid function of x right with the parameters being w okay then learning algorithm.\nis gradient decent we don't know that but the idea of gradient descent is to find the w right and.\nwhat is the objective function what should my objective function be right we can let's try to arrive at an.\nobjective function so i am given some data okay x comma y and now i have this approximation at the.\nend of learning what would you want you would want that this y hat should be as close to y.\nright that means the difference between y hat and y should be as small as possible okay for all the.\ntraining examples that i have maybe n right if i sum up this difference okay and i'll not i'll just.\ncall it as difference right so difference between y hat and y and i can now define the difference the.\nfunction the way i want to right so the difference between y hat and y sum across all the training.", "metadata": {"video_title": "A typical supervised machine learning setup"}}
{"text": "function the way i want to right so the difference between y hat and y sum across all the training.\nexamples should be as small as possible right that's the same as saying that minimize this difference right so this.\nis my loss function now okay and i want to minimize this okay and i could choose different ways of.\ndefining this distance or the d function right and i could choose the squared error loss for example so what.\nis this saying that this is my loss function which is i look at all the training examples i look.\nat the difference between y hat and y and i take the square of the difference why do we take.\nthe square of the difference so that the positive errors don't cancel the negative errors right so now suppose i.\nwanted the value to be 1 okay and it predicted it as 0.5 then my error is minus 0.5 okay.\nyeah this was y hat this was y so y hat minus y is minus 0.5 but there was another.", "metadata": {"video_title": "A typical supervised machine learning setup"}}
{"text": "wanted the value to be 1 okay and it predicted it as 0.5 then my error is minus 0.5 okay.\nyeah this was y hat this was y so y hat minus y is minus 0.5 but there was another.\nexample where i wanted y and y hat was predicted as 1.5 so now my difference would be 0.5 plus.\n0.5 so if i just take the sum of the differences then this minus 0.5 will cancel this 0.5 and.\nit will look like i have zero error whereas actually i have made two errors and two horrible errors right.\nboth by off by 0.5 so the moment you square it that sign disappears right so you could take the.\nsquare and the other you could have argued that why not just take the norm right it's just the absolute.\nvalue right but the problem with the absolute value is that it's not a smooth differentiable function whereas a square.\nfunction is a smooth differentiable function and we want uh smooth differentiable functions because later on the hero of the.", "metadata": {"video_title": "A typical supervised machine learning setup"}}
{"text": "function is a smooth differentiable function and we want uh smooth differentiable functions because later on the hero of the.\ncourse is going to be calculus where differentiability would be important right so that's the area so now i have.\ndefined an objective function and the learning algorithm should find w's such that when i plug in these values of.\nw this loss value that i get should be as small as possible what does that mean there are infinite.\npossible values of w right so w is an rn vector right it's an n dimensional real vector right m.\nokay sorry m dimensional real vector and these are real values ah that's okay m plus one right that's okay.\nnot a problem uh so order m we can say right so m plus one because w naught is also.\nthere so this is uh m plus one dimensional vector which can take real values each value can go from.\nminus infinity to infinity right so there are infinite possibilities here of those i want one such w which if.", "metadata": {"video_title": "A typical supervised machine learning setup"}}
{"text": "minus infinity to infinity right so there are infinite possibilities here of those i want one such w which if.\ni plug in if i were able to plug in all these infinite values of w into this equation right.\nthis is the equation that i care about and i want that w which will give me the smallest value.\nfor the loss function that's what my goal is right so that's the objective of training now the other thing.\nthat i want to say here right i just want to talk a bit more about this learning algorithm and.\nthe loss function right so any kind of learning as i said is always driven by some objective right and.\ni like to give this analogy let's suppose in high school you're learning trigonometry right so when you're learning the.\nchapter what are you doing you're reading the chapter because you're allowed to read this is not a test right.\nyou're allowed to read the textbook and your goal would be to ensure that all the formulae which are given.", "metadata": {"video_title": "A typical supervised machine learning setup"}}
{"text": "you're allowed to read the textbook and your goal would be to ensure that all the formulae which are given.\nin the chapter you don't make any errors on that is that correct that's what your goal would be that.\nis akin to saying that for all the training points that i am given i should not make any error.\nor i should make minimum errors the minimum possible errors right so that's what training is then at the end.\nof the chapter you have exercises right that's your validation set okay because now you you you have not seen.\nthis formula you have just learned the formula in the training data but using that you should be able to.\nsolve this exercise and you might make some mistakes here so you're again allowed to go back and relearn the.\nchapter right you could say oh i'm not able to figure this out maybe go back and look at this.\nright so that's your validation set and then you have a test set which is your exam so now you.", "metadata": {"video_title": "A typical supervised machine learning setup"}}
{"text": "chapter right you could say oh i'm not able to figure this out maybe go back and look at this.\nright so that's your validation set and then you have a test set which is your exam so now you.\nhave done the training you have done the validation set again toggle between training and validation and maybe you have.\nsome confidence now once you go to the test set once you go to write the exam you're not again.\nallowed to do the training right so you have a training error you have a validation error and you have.\na test error during training what we are talking about is the training error which means the error on the.\ntraining data right which is the error on the chapter that you have read right the contents of the chapter.\nyou should know all everything well or you have minimum errors right so that's the analogy you should keep in.\nmind okay uh yeah so i'll end this uh module here and we'll come back and talk about and ways.\nof learning these parameters right so let's let's do that.", "metadata": {"video_title": "A typical supervised machine learning setup"}}
{"text": "[Music] okay so now let's build an intuition for the back propagation algorithm so so far we have answered the.\nfirst part of the question which was how to choose the loss function and we have taken two popular problems.\nclassification and regression and motivated the choice for the loss function for both of them right now what we want.\nto do is that once you know the loss function now we can start talking about the derivative of the.\nloss function with respect to parameters right because that's the quantity that we are interested in if we know the.\nderivative of the loss function with respect to the parameters for all the parameters on our network then we can.\nsimply plug it that back into the gradient descent algorithm and we are done right so that's all that we.\nneed to do but now in the case of a deep neural network this is i wouldn't say complicated but.\nthere is slightly more work involved than what we had in that simple network that we had right so here's.", "metadata": {"video_title": "Backpropagation Intuition"}}
{"text": "there is slightly more work involved than what we had in that simple network that we had right so here's.\nour gradient descent algorithm where we had this and we were just updating the weights using the gradients and we.\nwant to apply the same algorithm by replacing these by all the weights that i have in the network right.\nthe large number of weights and then be able to compute the partial derivatives of all those weights the same.\nalgorithm goes through right so now let's focus on one of these weights w112 and to learn this weight using.\nthe gradient descent algorithm we need a formula for the derivative of the loss function with respect to this weight.\nright and that needs some work as i said right now what we want to do is first build an.\nintuition for how what would that formula look like and then do some hard work but then come back to.\na state where we can then say okay once i know how to compute this this red guy can i.", "metadata": {"video_title": "Backpropagation Intuition"}}
{"text": "intuition for how what would that formula look like and then do some hard work but then come back to.\na state where we can then say okay once i know how to compute this this red guy can i.\ncompute all the weights in that network in one go right can i compute the derivatives uh the partial derivatives.\nfor the loss function with respect to all the weights in that layer once i know how to do it.\nfor one weight in that layer right then that would not not require us to compute these many formulae but.\njust one formula and then we could generalize it and also not just that once i know it for one.\nlayer can i do apply a similar formula for all the layers right then we are coming up with a.\nvery generalized formula and the computations will become easier but that's for later for now i want to understand how.\nto compute this right so we'll see how to calculate that but first let us look at a very simple.", "metadata": {"video_title": "Backpropagation Intuition"}}
{"text": "to compute this right so we'll see how to calculate that but first let us look at a very simple.\ndeep neural network which is a very thin network but it's a deep network right that means that every layer.\ni just have one neuron and i have just one weight which connects my input to that and so on.\nright it just keeps computing it's a very simple computation so you can actually write y hat as a function.\nof x very easily in this case right now it is easy to find the derivative by the chain rule.\nso what is the derivative that are interested in i'm interested in the derivative of the loss function with respect.\nto this weight right and this weight is very far away from the loss function right because it's not uh.\nvery close right i would say this weight is closer to the loss function and the weight that i am.\ninterested is much farther right but i know how to use the chain rule right so i can compute the.", "metadata": {"video_title": "Backpropagation Intuition"}}
{"text": "very close right i would say this weight is closer to the loss function and the weight that i am.\ninterested is much farther right but i know how to use the chain rule right so i can compute the.\nderivative of the loss function with this guy which is the green guy dark green guy then the derivative of.\nthe dark green guy with respect to al1 and then i keep going down right and this is something that.\nyou have done right so if i could write y hat is equal to say sine of cosine of square.\nof say tan of e of log of something right then you know how to compute this derivative right and.\nthat's that's the chain this is a very long chain and you see a very long chain here too but.\nyou know how to compute that right you just go about it one by one you compute the last guy.\nthe derivative of y hat with the last guy that you had then the guy previous to that and so.\non it just keep going uh in a chain that's a chain rule of derivative that all of you know.", "metadata": {"video_title": "Backpropagation Intuition"}}
{"text": "the derivative of y hat with the last guy that you had then the guy previous to that and so.\non it just keep going uh in a chain that's a chain rule of derivative that all of you know.\nand now i could even compress this chain right so i could even write it as this this is just.\na compressed form of this chain rule and why i'm doing this i'm just trying to do this to build.\nthe intuition that somehow if i know all of this right then to compute this guy i don't need to.\ncompute all of these again right i have suppose i have already computed this right suppose i have already computed.\nthis which corresponds to that entire box here right so this corresponds to that entire box there so i can.\njust reuse that i don't need to compute the whole of it again and then i just need to compute.\nthis red quantity right so that's the idea that we are going to use in back propagation that we'll have.", "metadata": {"video_title": "Backpropagation Intuition"}}
{"text": "just reuse that i don't need to compute the whole of it again and then i just need to compute.\nthis red quantity right so that's the idea that we are going to use in back propagation that we'll have.\nthese long chains and that would make the task daunting but then we'll argue that some portions of this chain.\nwe have already computed and we're just going to recompute reuse them right so it's not as daunting as as.\nit seems right and now some of you could realize that if there were like multiple weights here and if.\nthis part of the chain was already computed then maybe for all those weights you could reuse that computation right.\nif it's not clear at this point don't worry this all will become clear as we keep discussing the back.\npropagation algorithm but that's the intuition that i want at least on this slide whatever is there this you should.\nunderstand that i had this big blue part but if i had already computed that then i can just reuse.", "metadata": {"video_title": "Backpropagation Intuition"}}
{"text": "understand that i had this big blue part but if i had already computed that then i can just reuse.\nthat when i'm computing this left-hand side here okay okay similarly for w two one one similarly for wl one.\none okay uh now let us see an intuitive explanation of the back propagation algorithm before we go into the.\nmathematical details right so i've already told you some intuition that there's this large network of many weights and i.\nwant to compute the derivative of the loss function with respect to all these weights i took a weight which.\nwas like very far away from the loss function and i made a case that if you could have a.\nchain from the loss function to that weight and you have a chain from the loss function to that weight.\nthen you could just apply the chain rule right so let's that's that intuition is already there now let's just.\nstrengthen it further right so this is what is happening right you had this network okay i gave you an.", "metadata": {"video_title": "Backpropagation Intuition"}}
{"text": "strengthen it further right so this is what is happening right you had this network okay i gave you an.\nexample x as input you did all this computation because you had some current values of w b w 1.\nw 2 w 3 b 1 b 2 b 3 these are not the final values these are not the.\nfinal learned values you're somewhere in the training and based on your current understanding current values of w1 w2 w3.\nyou computed y-hat l not just that you computed some loss right after having y hat l you computed the.\nloss function also and you got a non-zero loss so now you're trying to find out what went wrong i.\ngave the network an x okay it produced a certain output i computed the loss based on that output and.\nmy loss is non-zero so who is responsible for this so this loss was sitting here so i will ask.\nthese guys first these are the dark green guys that i'll ask first right i'll ask them sorry hey you.", "metadata": {"video_title": "Backpropagation Intuition"}}
{"text": "my loss is non-zero so who is responsible for this so this loss was sitting here so i will ask.\nthese guys first these are the dark green guys that i'll ask first right i'll ask them sorry hey you.\nare not producing the desired output right because if you guys were perfect then my loss would have been zero.\nso why are you not producing the desired output you should take responsibility you should do give me better output.\nbut these dark the shaded green guys and they'll say hey what can i do right i what responsibility can.\ni take but i am only as good as the hidden layer before me right because how did i get.\nthe dark green guys these are the dark green guys how did i get them i had certain values in.\nthe previous layer i multiplied them by the weights and the biases and then i get so this y hat.\nwill tell me i can't do anything right whatever these guys gave me i just computed soft max on that.", "metadata": {"video_title": "Backpropagation Intuition"}}
{"text": "the previous layer i multiplied them by the weights and the biases and then i get so this y hat.\nwill tell me i can't do anything right whatever these guys gave me i just computed soft max on that.\nand gave you some values so if these guys had given me perfect values then i would also have been.\nperfect so please go and ask them you say okay this is fine that sounds interesting sounds correct that if.\nthese guys are not doing the job properly then how will the shaded guide do its problem joint property so.\nyou go and talk to these three guys who are these guys w's h and b right so the w.\nand uh b so you ask them what is wrong with you right so the w and b say yeah.\nwe understand we have made a mistake because we are the weights we are the only things that can be.\nadjusted in the network and maybe we have not really been adjusted very well so far and hence the output.\nis bad right but then hl says that okay i am also part of this computation but i can't do.", "metadata": {"video_title": "Backpropagation Intuition"}}
{"text": "adjusted in the network and maybe we have not really been adjusted very well so far and hence the output.\nis bad right but then hl says that okay i am also part of this computation but i can't do.\nmuch right because i am again as good as the previous activation layer after all how do you compute hl.\nhl is again a function of the previous guys right so it's again a function of what these weights were.\nand what were the inputs that got passed to those weights right so then again you go and talk to.\nthese guys so again the weights will say that hey we are fine right okay we made a mistake maybe.\nwe need to get adjusted we take responsibilities but these shaded red guys they'll again say hey what can i.\ndo i am a function of the previous guys so maybe you should go and talk to them right and.\nthen of course you cannot pass the responsibility to the input input is whatever it is right you cannot say.", "metadata": {"video_title": "Backpropagation Intuition"}}
{"text": "do i am a function of the previous guys so maybe you should go and talk to them right and.\nthen of course you cannot pass the responsibility to the input input is whatever it is right you cannot say.\noh change your input if you want the right output right no i have given you a certain input i.\nexpect you to uh give me the output so the responsibility never goes to the input but what you realize.\nis that in the entire network the responsibility lies with all the weights all the yellow things that i have.\nshown here right all the weights and all the biases these guys although they are being computed they are just.\na function of the weights and the biases so their weights and biases are wrong then these guys are going.\nto be wrong right and this argument flows all through the network and then you find out that the responsibility.\nlies between the weights and the biases right and this is what i am trying to do right so i.", "metadata": {"video_title": "Backpropagation Intuition"}}
{"text": "lies between the weights and the biases right and this is what i am trying to do right so i.\nwas interested in finding the responsibility of this weight right the first weight here but instead of talking to the.\nweight directly i first spoke to the output layer then i spoke to the previous hidden layer then the previous.\nhidden layer and now i am talking to the weights right so i have constructed this chain rule because directly.\ntalking to the weight of interest is hard right because i've given you that function remember sine of cosine of.\ne of log of something something a long chain if i directly try to compute the derivative it's harder but.\ninstead if i break it down into this chain rule it's easier right so that's what i'm trying to do.\nhere you are interested in the law derivative of the loss function with respect to some weight you talk to.\nthese intermediate guys find out each of their responsibility and then find using those computations find the responsibility of the.", "metadata": {"video_title": "Backpropagation Intuition"}}
{"text": "these intermediate guys find out each of their responsibility and then find using those computations find the responsibility of the.\nweight that you're interested in right now where did this jump happen right so it's still this point i was.\nonly talking english and then suddenly i introduced this math part i suddenly came to partial derivatives right so how.\ndid i make that jump why i want to know what is the responsibility of these guys so how did.\nthat responsibility became derivative what does the derivative tell us the derivative tells us is that if i change w.\na bit how much does the loss change right so if changing w a very tiny bit changes the loss.\nof makes a large change to the loss that means w has a very strong influence on the loss w.\nis more responsible for the loss if i change the w a small amount maybe the loss will decrease a.\nlot right hence derivative or partial derivatives is a good way of assigning responsibility to the weights for the loss.", "metadata": {"video_title": "Backpropagation Intuition"}}
{"text": "lot right hence derivative or partial derivatives is a good way of assigning responsibility to the weights for the loss.\nbecause it tells me if i change this weight a bit how much will the loss change and that's what.\ni want to know right how much is this guy responsible can i change its value a bit and drastically.\nreduce the loss function then let me do that and that is what the partial derivative tells right so from.\nthat english discussion that we had of talking to every guy we came to this mathematical uh kind of realization.\nof that which is that you just need to compute the partial derivatives and if you want to compute the.\npartial derivative then just as we had this uh detective work that we did we went and spoke to every.\nlayer and then came to the last guy we just have to do the same thing which just gives us.\nthe chain rule of property right so this is what we are going to do in the remaining part of.", "metadata": {"video_title": "Backpropagation Intuition"}}
{"text": "layer and then came to the last guy we just have to do the same thing which just gives us.\nthe chain rule of property right so this is what we are going to do in the remaining part of.\nthis lecture the quantities of interest that we have is the gradient with respect to the output layer okay the.\ngradients with respect to the hidden units sorry and there can be multiple hidden units and then the gradients with.\nrespect to oh sorry i wanted to change the color but yeah the weights and the bias right so these.\nare the three parts to the remaining of to the remainder of this lecture right we'll first see how to.\ncompute this then this and then this right and we want to do this in a manner that once i.\ndo it for w11 i should somehow be able to do for all the w's all the weights in the.\nnetwork right this formula should not be painfully computed for every weight in the network right so that's what we're.", "metadata": {"video_title": "Backpropagation Intuition"}}
{"text": "network right this formula should not be painfully computed for every weight in the network right so that's what we're.\ngoing to do in the remaining part of this lecture and our focus is going to be on cross entropy.\nso our loss function is going to be cross entropy which means we are going to deal with classification problems.\nwhich means we are going to have the output function as soft marks right so i'll end here and we'll.\ncome back and do the entire back propagation in its in the gory details of the mathematical details of it.\nin the subsequent lectures thank you.", "metadata": {"video_title": "Backpropagation Intuition"}}
{"text": "foreign [Music] okay so now we are ready to wrap up this discussion on back propagation we'll take everything that.\nwe have done so far and put it together into a nice algorithm so why we have all the pieces.\nof the puzzle so we have the derivative of the loss function with respect to the output layer we have.\nthe derivative of the loss function with respect to any hidden layer activation and pre-activation we have the derivative of.\nthe loss function with respect to the weights and the biases right now we can write all of this into.\na full learning algorithm so this is what it looks like I'm going to start with the gradient descent algorithm.\nso you had start at time step 0 you run it for some thousand iterations you initialize all the weights.\nin the network at every stage what will you do you will first compute all the activations and all the.\nactually should have been the other way around all the pre-activations and the activations and the output using the forward.", "metadata": {"video_title": "BackPropagation Pseudocode"}}
{"text": "actually should have been the other way around all the pre-activations and the activations and the output using the forward.\npass right and you know the formula for this right you start with X you compute A1 as W1 X.\nplus b and you compute H1 as G of A1 then you compute A2 as W 2 H1 plus v.\nand so on right so this all is simple Matrix Vector multiplication there are no gradients involved this is just.\ntaking the input and passing it through a series of Transformations and all of this is coming from a formula.\nthat you can Implement right you know how to implement these functions right you know how to implement this you.\nknow how to compute the element wise uh logistic for example if G is equal to the logistic function okay.\nso this is straightforward you'll just do a forward propagation on the input right this should have been comma X8.\nbecause you're taking the inputs now once you have done the forward propagation you do the backward propagation so once.", "metadata": {"video_title": "BackPropagation Pseudocode"}}
{"text": "because you're taking the inputs now once you have done the forward propagation you do the backward propagation so once.\nyou have done the forward propagation you compute y hat you also know y right so using that you can.\ncompute the loss function okay loss function depends on y hat and Y and you will need all of these.\nthings right they were showing up in the back propagation formula that you had seen right so we will see.\nthat again so all of these quantities you will need right so everything that you have computed in the forward.\npropagation you will need it in the backward propagation also and what is the output of the backward propagation it's.\nthe derivative of the loss function with respect to all the weights in the network and I'm just collectively calling.\nit as a derivative of the loss function with respect to Theta whereas the Theta is a large collection of.\nWeights once you have that you can just update the weights using the gradient descent update right so now let's.", "metadata": {"video_title": "BackPropagation Pseudocode"}}
{"text": "Weights once you have that you can just update the weights using the gradient descent update right so now let's.\nzoom into the forward propagation and the backward propagation right so this is the forward propagation for k equal to.\n1 to L minus 1 this is what you will do you will compute a k as so A1 is.\nequal to B1 plus w k w 1 into H 0 and H 0 as I had said is going.\nto be equal to X right and then once you have that you can compute h k as the uh.\nby applying the activation function on the AK Vector this is all you just need to run this Loop l.\nminus 1 times and what happens to the lth layer there you will first compute uh Al okay I've just.\ncomputed I've just put the output layer outside because for the output layer you need to use a spatial function.\nyou don't use the same G function right that's why I put it out so now we have computed everything.\nyou have computed the activations for all the layers including the output layer activations and free activations and then you.", "metadata": {"video_title": "BackPropagation Pseudocode"}}
{"text": "you have computed the activations for all the layers including the output layer activations and free activations and then you.\nhave computed the output also and this is all you need to compute the loss so if you have y.\nhat you can also compute the loss right so once you do the forward propagation you have all the edges.\nall the A's and the Y hat now you start doing the backward propagation so first what will you do.\nyou will compute the gradient with respect to the output layer and this is what our formula was now this.\nyou already know because you have computed in the forward propagation this is just the one hot Vector where there.\nwill be a one in the correct class and this you know from the training data right you know for.\nthis example what is the correct class right so this entire algorithm is run for one example for now okay.\none input X so that input you know what the Y Vector is and that's why you can compute the.", "metadata": {"video_title": "BackPropagation Pseudocode"}}
{"text": "this example what is the correct class right so this entire algorithm is run for one example for now okay.\none input X so that input you know what the Y Vector is and that's why you can compute the.\none hot Vector okay to this you know now from k equal to this is actually wrong this should have.\nbeen L minus 1 2 1 right because you always start from the last layer and keep going on to.\nthe first layer so you compute the gradients with respect to the parameters I want to compute the derivative of.\nthe loss function with respect to the parameter in the last layer so this is k equal to l minus.\n1 to 1 is what you are doing right so I want to compute the derivative of the loss function.\nyeah so now you want to compute the derivative of the loss function with respect to the weights in the.\nlast rear which is W3 so this will be a from L uh going from L to 1 right so.\nw 3 uh which will depend on the derivative of the loss function with respect to A3 and uh H2.", "metadata": {"video_title": "BackPropagation Pseudocode"}}
{"text": "last rear which is W3 so this will be a from L uh going from L to 1 right so.\nw 3 uh which will depend on the derivative of the loss function with respect to A3 and uh H2.\nright so this you have already computed in the forward pass this you have already computed this is just computed.\noutside the loop so you have all the elements that you require to compute this right similarly you can compute.\nthe derivative of the loss function with respect to the weights in the layer three so this also you can.\ndo because you just need this quantity which you have already computed because K is equal to l right now.\nwe are running the loop from L to 1 okay so this is done now you compute the ingredients with.\nrespect to the layer below so now you can compute the derivative of the loss function with respect to K.\nuh K minus 1 so you had started with k equal to l to 1 right so now at this.\npoint uh K is equal to l so K minus 1 would be L minus 1 so which would be.", "metadata": {"video_title": "BackPropagation Pseudocode"}}
{"text": "uh K minus 1 so you had started with k equal to l to 1 right so now at this.\npoint uh K is equal to l so K minus 1 would be L minus 1 so which would be.\nH2 so you're Computing the derivative of the loss function with respect to H2 and for that you need the.\nweights W3 which you already have and you need the derivative of the loss function with respect to A3 which.\nagain you already have right so this I already explained this when I was saying that you're just going step.\nby step and then you compute the gradients with respect to the uh pre-activation layer below so this is what.\nyou want to compute and for that you just need this quantity which you had just computed and this quantity.\nwhich you have already argued is easy to come right so you just this Loop just keeps going on and.\non till the first layer and you just keep Computing all the uh the the gradients with respect to all.", "metadata": {"video_title": "BackPropagation Pseudocode"}}
{"text": "on till the first layer and you just keep Computing all the uh the the gradients with respect to all.\nthe weights all the activations all the pre-activations all the biases in the network so this entire Loop you could.\nwrite in Python you first do the forward propagation then do the backward propagation so we have the formula for.\nall the weights it does not matter it's w 1 W 2 W 3 the same formula applies similarly we.\nhave the formula for all the preactivations all the activations and all the preactivations so we just keep applying this.\nformula inside a loop right so I don't have to do this painful computation where I'm trying to compute the.\nderivative of the loss function with respect to every weight w k i j or W3 1 comma 2 W.\n3 2 comma two and so on right I just have a generic formula I'm just doing Matrix operations and.\nI get the derivatives with respect to all the you can say so that is what is the entire back.", "metadata": {"video_title": "BackPropagation Pseudocode"}}
{"text": "3 2 comma two and so on right I just have a generic formula I'm just doing Matrix operations and.\nI get the derivatives with respect to all the you can say so that is what is the entire back.\npropagation algorithm is coded in just this these many steps at a very small Loop and all of these are.\nMatrix Vector multiplications so we are almost done one last thing that was left was uh the derivatives of the.\nuh the G primes right which I did not covers I I already told you it's easy to do so.\nthis is our gz so if it's a logistic function then this is what it is and this is how.\nyou can compute G Prime so this is what you will do right so you can again write a function.\nto compute G prime it takes any value as input uh foreign H is equal to G of a right.\nso you just pass that a and you substitute in this formula so you get G Dash right that's all.\nthat this says and in fact it can be written even more uh simply it's just G of Z into.", "metadata": {"video_title": "BackPropagation Pseudocode"}}
{"text": "so you just pass that a and you substitute in this formula so you get G Dash right that's all.\nthat this says and in fact it can be written even more uh simply it's just G of Z into.\n1 minus G of Z you can derive this this is not and similarly for this you can derive and.\nit's just 1 minus U of Z square right so those G primes are easy to compute so that's all.\nI had so if I got this formula as saying right so if I had H already right and if.\nI want to compute G then I already have G Prime then I already have everything that I wanted to.\ncompute right so this is all we are done with the entire back propagation algorithm uh we have seen it.\nin quite gory details uh you have to watch these videos a few times to get a complete grasp on.\nit but everything that you need to understand it is there in the videos and the slides so please look.\nat it so I'll end here and the next class we'll go back to gradient descent and look at a.\nfew variants of your data so thank you.", "metadata": {"video_title": "BackPropagation Pseudocode"}}
{"text": "foreign [Music] so this is where we were when we left off yesterday we were talking about how deep learning.\nhas evolved and there are these four axes that we were considering and we have already talked in detail about.\nthe first two and today we'll start a discussion on better activation functions right so that's where we are so.\nwith that let me just go to the next module uh yeah so so before we look at these activation.\nfunctions right so let's try to uh first motivate why do we need to talk about activation functions and the.\nmotivation comes if we try to introspect this question what makes deep neural Nets powerful right and we already saw.\nthis uh briefly when we were talking about the universal not briefly actually quite in detail when we are talking.\nabout the universal approximation theorems I just want to revisit that and emphasize on uh some a component of the.\ndeep neural network which is important in granting it its power right so let's see what that is so now.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "deep neural network which is important in granting it its power right so let's see what that is so now.\nsuppose I have this deep neural network right it's a thin network but doesn't matter it's still a deep neural.\nnetwork and suppose all my sigmars which are the sigmoid or the logistic function I just replace them by a.\nsimple linear transformation it's earlier I had a any of the A's is equal to W into h plus b.\nand then H was sigmoid of a so I am just saying instead of H equal to sigmoid of a.\nif I just make it h equal to a right so there's only a linear layer and there is no.\nnon-linear layer so that's the situation that I am considering uh so this is what it would look like right.\nso Y is a function of X so X first gets transformed linearly by W1 so that is what a.\none is a one is W one into X but then H1 is just a one so it is again.\nW one into X then this x is the input to the next layer and then you get W 2.\nis into W1 into X right so w 2 into H1 which is W 2 into W 1 into X.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "W one into X then this x is the input to the next layer and then you get W 2.\nis into W1 into X right so w 2 into H1 which is W 2 into W 1 into X.\nwhich is the same as H2 right because H2 is equal to a two and so on you continue and.\nyour output is just a linear uh it's just a multiplication of Weights uh by the input X right and.\nnow here this is as good as just having a single linear transformation right so this is just as good.\nas saying that Y is equal to sum w x where W is a product of these four W's right.\nso this is just a linear transformation this is not even though it's a deep Network it does not add.\nany value to me beyond what a linear transformation would act right so the depth of the network is not.\nwhat ah solely gives the Deep neural network a power of course it plays a role because then you have.\nthese composite non-linearities but if you remove the non-linearities then you just have a linear transformation of X and then.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "these composite non-linearities but if you remove the non-linearities then you just have a linear transformation of X and then.\nyou are just going to learn linear decision boundaries right because you have just said that Y is equal to.\nw x so your y you are just assuming that the relationship between Y and X is given by a.\nline or a hyperplane in higher Dimensions right so then in particular if you have these complex decision boundaries like.\nthat we had here where you really need something long linear right so here you are not learning a line.\nyou need something like a circular either a decision boundary which separates the positive points which are inside the brown.\npoints from the negative points which are the blue points right I cannot draw any line which suitably separates my.\nuh the brown points from the blue points right without giving me a high error so that is not possible.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "uh the brown points from the blue points right without giving me a high error so that is not possible.\nright so uh the reason uh deep neural network can learn such arbitrary boundaries and hence act as a universal.\napproximator right because now we have a very complex function between Y and X which is looking like this right.\nthis is the function that we are interested in and as we had seen while looking at the universal approximation.\ntheorem and the illustrative proof of it where we are drawn these towers we could do that because we had.\na lean a non-linear layer it what the theorem said is that if you have even a single sigmoid layer.\nthen you can then the then the network can act as a universe Universal approximator so sigmoid was important there.\nright so these non-linearities are what give deep neural networks their power right so there's nothing new that I have.\nsaid we have already discussed this but I just wanted to quickly refresh it because it's relevant for the discussion.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "said we have already discussed this but I just wanted to quickly refresh it because it's relevant for the discussion.\nso now if the power of deep neural networks comes from such non-linearities where do the nonlinearities come from the.\nnon-linearities essentially come from the activation function and hence we need to discuss about these activation functions and what do.\nthese bring and are there better activation functions possible is is any non-linear function okay or we want certain better.\nand better properties should be satisfied by these nonlinear functions so that's what we are going to focus on today.\nright so that's the reason why our discussion on activation functions is important right now we look at some of.\nthe popular activation functions the quite a bit of this material at least the initial part is taken from the.\nlecture like a few years back uh lecture slides from Andre carpathy's course I'm not sure if that those videos.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "lecture like a few years back uh lecture slides from Andre carpathy's course I'm not sure if that those videos.\nare still available but they were available a few years back and I had taken material from there so Let's.\nuh let's start with the sigmoid function so sigmoid is given by this Sigma of X is equal to 1.\nover 1 plus e raised to minus X and this is what it looks like it is between 0 and.\n1. so whatever input you pass it in our case uh the input that we passed to uh the sigmoid.\nfunction is sum a which in turn is some linear transformation of the previous layer right we pass some Ai.\nand it is some linear transformation of the previous layer right that's what our input is going to be and.\nwhatever is the input that applies element wise on this Vector a and it just squishes it or compresses it.\nbetween 0 to 1 right so we already have seen this in the past uh so now since we are.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "whatever is the input that applies element wise on this Vector a and it just squishes it or compresses it.\nbetween 0 to 1 right so we already have seen this in the past uh so now since we are.\nalways interested in gradients right because as I said that training deep neural networks is largely about Computing gradients and.\nthen based on the gradients either the training would go fast low and so on right so let's see what.\nis the gradient of the sigmoid function right and this again we have computed if you have not you can.\njust take this as an exercise and try to find the gradient of in fact we have but you can.\njust revisit it and try to compute the gradient of this function and you will end up with this formula.\nright which is Sigma X into 1 minus Sigma X right and now let's focus on this formula so what.\nwhat could happen if we use sigmoid in a deep neural network given that the gradient of the sigmoid is.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "right which is Sigma X into 1 minus Sigma X right and now let's focus on this formula so what.\nwhat could happen if we use sigmoid in a deep neural network given that the gradient of the sigmoid is.\ngiven by this formula right in particular note that at the extremes ah when the sigmoid neuron saturates what do.\nI mean by saturate it takes its maximum value which is 1 or if it takes its minimum value which.\nis 0 right or close to 0 in those cases the gradient vanishes right so if I put Sigma x.\nequal to 1 then I'll get 1 into 1 minus 1 so the gradient will vanish that is in this.\nregion or if I put Sigma x equal to 0 which is corresponding to this region then again 0 into.\nanything is 0 right so the gradient actually vanishes and we know that if the gradient vanishes or if it's.\nvery smaller so in these regions starting from here the gradient is very small and that is a problem in.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "very smaller so in these regions starting from here the gradient is very small and that is a problem in.\ngradient based learning methods because the updates will not be high right so that's where this discussion is headed ah.\nso let's let's try to go there now okay so this is a deep neural network ah and if I.\nhave used a sigmoid non-linearity then in some point in this chain rule right suppose I am trying to find.\nthe derivative of this weight with respect to the loss function at some point I am going to encounter this.\nin the chain rule I will encounter derivative of H3 with respect to A3 and you can convince yourself right.\nI mean any uh any even if W1 or W2 or W3 if I'm trying to find the derivative of.\nthis with respect to the loss function then this guy is sitting in the path so you will encounter that.\nand that is nothing but the sigmoid of the derivative of the sigmoid function with respect to A3 and that.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "and that is nothing but the sigmoid of the derivative of the sigmoid function with respect to A3 and that.\nis given by this formula right so that's something that we'll always encounter whenever we are doing a back propagation.\nwith and using any gradient based method so now what is the consequence of this right uh so as I.\nsaid there is this concept of saturated neuron so a neuron is said to be saturated if it is at.\nits peak or lowest value rate minimum or maximum value in the case of sigmoid neurons it's 0 or 1..\nso whenever it is saturated the gradient is going to be 0 right and if it is 0 then your.\ntraining will not progress because once the neuron is saturated your gradients are zero now the weights are not getting.\nupdated and then it's likely that because of these weights right especially if the weights are high so let's see.\nwhy would the neurons get saturated right so let's try to understand that why is it that it would get.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "why would the neurons get saturated right so let's try to understand that why is it that it would get.\nsaturated and it's not given right because the function can take values between 0 to 1. so why is it.\nthat it would suddenly become one or suddenly become zero right so let's let's try to answer that so this.\nif I were to consider any such neuron in the network right so suppose this is one of the neurons.\nthis is not the output neuron but say some neuron in the network and this is what the output of.\nthis neuron would be right it would be the sigmoid of the linear combination of the inputs connected to it.\nright so this is what the output would be now suppose I have initialized the weights to a very high.\nvalue right suppose my weights are say suppose 100 200 and so on and now my inputs are all standardized.\nit'll be anyway standardize the inputs that means we divide by the subtract the mean and then divide by the.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "it'll be anyway standardize the inputs that means we divide by the subtract the mean and then divide by the.\nvariance so all our inputs are between 0 to 1. so the inputs are typically standardized that's anyways recommended for.\nusing gradient based methods so the inputs will not cause a problem there will be anyway small between zero to.\none but if the weights are initialized to high value then this sub w i Sigma I is w i.\nx is going to be high and remember see in the plot even for values like 2.5 the sigmoid neuron.\nhas already saturated right on the x axis you have 2.5 and the y axis you have almost one right.\nso if your weights are initialized even reasonably High then your neurons your uh the sigma this the quantity pass.\nthrough the sigmoid neuron is going to be high quantity and then for that the sigma is going to be.\nclose to 1 right uh similarly if the weights are initiates to high negative value the same would happen you.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "close to 1 right uh similarly if the weights are initiates to high negative value the same would happen you.\nhave a high negative value and the neuron would saturate to zero now of course you could argue that why.\nwould the wage be highered I would maybe initialize them to low values but now if there are like a.\nthousand neurons in one layer right then this is a sum of thousand terms and even with very small weights.\na sum of thousand terms reaching a value of 2 or 3 is not really surprising right so you have.\nto very carefully initialize the weights and that's why this part would be connected to the second part of this.\nlecture where we will talk about weight initialization methods and we will try to see how to initialize the weights.\ncorrectly so that we don't end up with saturated neurons right so the main point here is that neurons can.\nsaturate and if they saturate then the training kind of becomes problematic because the gradients are not flowing through right.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "saturate and if they saturate then the training kind of becomes problematic because the gradients are not flowing through right.\nand if if the neuron is saturating with a value one that means actually these weights were like uh strongly.\nparticipating in the output right and hence they would have had some impact on the loss function but that signal.\nwill not flow back to these weights because the derivatives have died right so that's a serious problem so this.\ncould lead to challenges during training since this is not a good the next problem is that sigmoids are not.\na zero Center that's again obvious from the plot because the plot is from zero to one zero centered would.\nbe something like say from example minus one to one so there is a zero in between so it's equally.\ndistributed around zero whereas this itself starts from zero and goes to one so the center is around 0.5 right.\nso what if it's not zero center right what could the possible what could the problem be right so now.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "so what if it's not zero center right what could the possible what could the problem be right so now.\nlet us try to again see that through an example suppose I have this output layer and before that I.\nhave this one sigmoid neuron right and it is connected to two inputs h21 and H22 so I'll have the.\nA3 would be this and then I'll passing it through the sigmoid neuron right now again if I were to.\nupdate the weights W1 or W2 let's see what chain rule I will use there so this is what my.\nderivative would be right so it would be derivative of the loss function which is sitting here with respect to.\nY right then the derivative of y with respect to H3 so the output is H3 then the derivative of.\nH3 with respect to A3 right and then the uh oh sorry this and then the derivative of uh A3.\nwith respect to W1 which I have already written as h21 so this is what A3 is if I take.\nthe derivative of this with respect to W 1 this quantity disappears because it's a constant and the derivative of.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "the derivative of this with respect to W 1 this quantity disappears because it's a constant and the derivative of.\nW1 h21 is just h21 similarly the derivative of W2 or rather eighth of the loss function with respect to.\nW 2 would have an H22 here right again because derivative of A3 with respect to W2 this quantity would.\ndisappear and you will get an H22 right so this is fine this formula you understand now what's with the.\ncolor coding right what's the red part and what's the blue part so the red part is common for both.\nthe derivatives right so this part is actually the same in both the derivatives okay and this part is changing.\nokay this is h21 and H22 now what is h21 h21 is the output of the previous sigmoid neuron right.\nso I know that this is going to be positive right both the blue quantities I know are going to.\nbe positive right now if both the blue quantities are going to be positive then the sign of these two.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "so I know that this is going to be positive right both the blue quantities I know are going to.\nbe positive right now if both the blue quantities are going to be positive then the sign of these two.\ngradients is decided by the red quantity right and the red quantity is the same for both the gradients so.\nnow both the partial derivatives so now ah either the red quantity is negative in which case both these derivatives.\nare negative or the red quantity is positive in which case again both these derivatives are positive right so I.\ncannot have a situation where I have my gradient Vector which contains all these ah derivatives and either it will.\nbe all positives or it will be all negatives right and this would be true even if I had like.\nn neurons here the same argument I could have extended to it so if I had computed those n partial.\nderivatives I would have had this red term common and then this blue term the blue term is always positive.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "derivatives I would have had this red term common and then this blue term the blue term is always positive.\nbecause it is the output of a sigmoid neuron so it has no say on the sign and then the.\nsign is then determined by or other I mean it contributes equally I mean whatever it is all of it.\nis positive the sign will just depend on the red part and the red part is same for all these.\nguys so if the red part is negative all of them would be negative it's positive all of them would.\nbe positive right so a gradient Vector is either consisting all positive values or all negative values right so that's.\nwhat is happening and the why am I discussing this in the case of sigmoid Duron because this happened because.\nyou had used sigmoid neurons hence the blue parts were always positive right if you had some use some other.\nneuron which had a range between minus one to one then it would have been possible some of these Blues.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "neuron which had a range between minus one to one then it would have been possible some of these Blues.\nwere positive some of these Blues were negative and then accordingly the the vectors the elements of the vector some.\nof them would be positive some of them would be negative but because you are using a sigmoid neuron all.\nW's are positive and hence all the elements of the gradient Vector would have the same sign now what is.\nthe problem with that that is I've just told you what the what is happening but we don't know what.\nthe problem with that is right so this is just in words whatever I explained so let's see what the.\nproblem is right so this is the uh uh say derivative of the partial derivative with respect to W1 partial.\nderivative with respect to W2 so now if I consider the plane right which has uh all possible values of.\npartial derivative of W1 and W2 so now which are the quadrants which are possible either the quadrant where both.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "partial derivative of W1 and W2 so now which are the quadrants which are possible either the quadrant where both.\nthe values are positive or the quadrant where both the values are negative right so you can never have a.\nvector in this direction your gradient Vector cannot be in this direction nor can be it in any of these.\ndirections it can only be in these directions or these directions right that is what this previous explanation is telling.\nyou so you can already see that when you're doing gradient descent you're moving in directions now half the directions.\nhave been thrown away because of using the sigmoid neuron these directions are not possible at all right so you.\ncan only move in certain directions and that already looks like a problem and here's a toy example to show.\nthat it is indeed a problem right so suppose you have initialized suppose this this Arrow here this is the.\noptimal value of w right this is where you want to go and suppose this is where you had initialized.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "optimal value of w right this is where you want to go and suppose this is where you had initialized.\nyour uh weight right so now I would want to have a possible movement like this right and reach the.\noptimum but this movement is not possible because it has one value positive one value negative and we know that.\nthat is not possible we already saw that all these kind of movements are not possible right so this is.\nnot going to be possible so now how will I have to reach the optimum I'll have to just take.\nthe movements which are allowed so I will have to perhaps take this movement this is uh okay this is.\nthe node yeah this is allowed right so this is where like 1 is 0 and the other is uh.\nnegative so that is allowed this this and so on right so I'll have to move like this in a.\nzigzag pattern to kind of reach the optimum right so I'll take more steps to converge and and keeping this.\ntoy example aside right the simple thing is this right so I'm asking you to go from place a to.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "toy example aside right the simple thing is this right so I'm asking you to go from place a to.\nplace B but I am saying that hey certain types of turns are not allowed if you're not allowed to.\ntake certain times of turns and you are restricted only to some types of turns then you are going to.\ntake more time right so that's the main idea and that is happening because sigmoids are not zero centered hence.\nwe get all values as positive hence this negative positive combination is not possible in the uh directions that you.\nchoose to move in right and lastly of course sigmoids are computationally expensive because you have the exponent to compute.\nbecause there are faster ways of computing it but still it's a computation that you need to do right so.\nthat's the other reason so that is all about the sigmoid neurons and why they are not the most convenient.\nactivation function so then ah because of these limitations right so at some point uh so now because of these.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "activation function so then ah because of these limitations right so at some point uh so now because of these.\nlimitations the tan H function is popular because one of the things that you immediately see is that it's zero.\ncentered right and then whatever explanation I gave at the end that is kind of taken care of of course.\ntanh is not a new activation function it has been used since 1990s but we are just going over all.\nthe activation functions so sigmoid is the default that we start explanations with but now I have shown you certain.\nproblems with it and then tanh overcomes some of those problems it is zero centered uh but let's see what.\nis the derivative of this so it's a derivative of this is 1 minus tan H Square X and again.\nat saturation the gradients will vanish right and that you don't need to look at the formula the same problem.\nas the graph itself shows you that if the function value lies here then the derivative is very small or.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "as the graph itself shows you that if the function value lies here then the derivative is very small or.\nzero same thing here right and if you look at the formula so when Tan x square is 1 your.\nderivative would be uh zero right or when it would be minus 1 whenever it is plus or minus 1.\nyour derivative is going to be 0 right so that's again clear so the problem of the saturating neuron and.\nhence the gradient Vanishing still exist the problem of limited directions to move in has been overcome and also the.\nproblem of computational expensive Still Remains it but still it has the E component in it right so tanets the.\nformula is contains the uh Eep part of it right so yeah okay so the next that will look next.\nfor Activation function that we look at is the rectified linear unit and this is a very one of the.\nmost popular activation functions as we will see at the end of the discussion we'll show you what are the.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "most popular activation functions as we will see at the end of the discussion we'll show you what are the.\npopular activation functions and this is proposed almost a decade back in the context of convolutional neural networks and it's.\nstill one of the de facto activation functions to be used across a wide variety of networks right the first.\nquestion that I have this is what the value relu function looks like right so it is simply Max of.\n0 comma X right so what does that mean that again in the context of a deep neural network so.\nsay this is one of the new translate so this is suppose my H1 this is say x now I.\nhave computed the A's right so this guy here lets let me call it say a 1 2 right that.\nis going to be again summation uh w i x i that we know right now that is this a.\n1 2 is going to be the input to the relu function that is the non-linearity setting here and it.\nwill simply look at whether a 1 2 is greater than 0 if so it will just return a 1.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "1 2 is going to be the input to the relu function that is the non-linearity setting here and it.\nwill simply look at whether a 1 2 is greater than 0 if so it will just return a 1.\n2 otherwise it will return 0 right so if you have any negative value being passed to it it clamps.\nit to 0 and if it's a positive value it just lets it let it be as it is right.\nso let's see what are the implications of that right so the first question is that is this really a.\nnon-linear function so indeed it is a longer function the max function the way it is shown here is a.\nnon-linear function a linear function would just uh behave uh linearly on both sides but this is on the negative.\nside you have uh clamped it and in fact you can show that if we combine two relu units right.\nso these are two relu units that have combined right so this is a relu function and this is another.\nrelu function and I have just taken the difference between these two right so you could think of it in.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "relu function and I have just taken the difference between these two right so you could think of it in.\nterms of neurons I had two neurons both of these were relu neurons right and then I'm subtracting one from.\nthe other and I'm getting some output and that output looks almost like the sigmoid function it looks like a.\nlinear approximation of the sigmoid function so it is indeed a non-linear function right it's as close to almost like.\nthe step function that you have and if you adjust these parameters a bit more you will get even better.\nkind of a step function and so on right so indeed a relu is a non-linear function some of you.\nhave not seen it before but might just look at the line and think that it's a linear function but.\nit is not okay uh this specific function that I've drawn is called relu 6 because 6 denotes the maximum.\nvalue of the relu that you have here okay okay so what are the advantages of relu so it does.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "it is not okay uh this specific function that I've drawn is called relu 6 because 6 denotes the maximum.\nvalue of the relu that you have here okay okay so what are the advantages of relu so it does.\nnot saturate in the positive region right what do I mean by that so as long as your A3 right.\nwas or a was positive the output would remain positive and it will not saturated there is no saturation here.\nthere is no clamping on this side as opposed to what you had in the sigmoid or any of the.\nsigmoid shaped functions right so that is what I mean here but in the negative region it can still saturate.\nright so in the if your contribution is negative right if the a is negative then it will be clamped.\nto zero and of course the derivative would again then be zero right so it does saturate in the negative.\nregion so roughly half the problem solved but computational is of course very efficient right there is no exponent here.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "region so roughly half the problem solved but computational is of course very efficient right there is no exponent here.\nyou just look at the value and the simple implementation is if greater than zero pass it right so it's.\na very simple uh uh function to compute and in practice at least that's what this paper and as I.\nsaid it's a decade old paper had shown that it converges much faster than sigmoid and tannage and this was.\nin the context of deep convolutional neural networks then but thereafter it has been used in all sorts of networks.\nas I said great feed power networks recurrent networks Transformers also right all the now in Transformers there are there's.\nthe Gedo activation function which is perhaps the default but yeah this has been used in multiple contexts right so.\nnow uh although this all sounds good right so what all sounds good one is it does not saturate at.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "now uh although this all sounds good right so what all sounds good one is it does not saturate at.\nleast half the time so it does not saturate as opposed to sigmoid and tanets which have two saturation regions.\nuh it's of computationally efficient so these two things are taken care of but still there is some challenges here.\nright and we'll see what those are so if you look at the derivative of relu then it is going.\nto be 0 if x is less than 0 and it is going to be 1 if x is greater.\nthan 0 right that simply follows from the fact that relu is equal to 0 or X so when it's.\nX the derivative of this with respect to X is just going to be 1 right so this is what.\nthe derivative looks like and now if this is what the derivative looks like uh let's see what's the implication.\nthat we have right so I've given you some toy example here a toy Network now at some point Suppose.\nthere is a large gradient which flows through the network right there is a large gradient which flows and it.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "there is a large gradient which flows through the network right there is a large gradient which flows and it.\nmakes B's value uh a very negative right suppose B takes on a large negative value okay that is what.\nhappens and this is not like cooked up this is conceivable that this could happen you have some training example.\nwhich causes B to update and then B becomes a very large negative value right now if that happens what.\nwould the consequence of it be so now your uh A1 right which is the input to the value function.\nright the value function is sitting here would is just W and X1 plus W 2 x 2 plus b.\nnow if B is very negative irrespective of what W1 X1 and W 2 x 2 are right this b.\na very large negative quantity would get added so this is going to be 0. so this is going to.\nbe 0 then the derivative is going to be 0. so if the derivative is 0 again you have this.\nproblem that when you are trying to update W1 at some point in the chain you have this derivative of.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "be 0 then the derivative is going to be 0. so if the derivative is 0 again you have this.\nproblem that when you are trying to update W1 at some point in the chain you have this derivative of.\nH1 with respect to A1 and that derivative is going to be 0 right which means the derivative of the.\nloss function with respect to W 1 W 2 is going to be 0 right now if and also with.\nrespect to B is going to be 0. so now if that happens the neuron of course output at zeros.\nor the neurons output was Zero which means it was a dead neuron which is fine if it's dead for.\nthis training example I can live with it right but now it's not got updated neither has B got an.\nupdate right and now from here on what will happen is that the neuron will remain dead for the rest.\nof the training it will never become alive again right so why is that the case so this is all.\njust the explanation that I gave in uh just I was speaking through it this is all on the slides.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "of the training it will never become alive again right so why is that the case so this is all.\njust the explanation that I gave in uh just I was speaking through it this is all on the slides.\nalso and this is the main point that I'm talking about now right that the neuron will stay Dead Forever.\nright so why would that happen yeah so weights are never getting updated right so once it became dead W1.\nW2 and B all three will not get updated right so B Still Remains this large negative value now the.\nnext input comes in again the same will happen the B is large negative value so this sum W 1.\nx 1 plus W 2 x 2 plus b would again be less than zero again the neuron is dead.\nand then again the gradient does not flow through again W and W2 B don't get updated again the next.\ninput comes in it's the same story right so once it becomes dead it stays Dead Forever right and in.\npractice people have observed that uh as high as greater than 50 percent of the relu units can die if.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "practice people have observed that uh as high as greater than 50 percent of the relu units can die if.\nthe learning rate is set to high right why is this related to the running rate if the learning rate.\nis very high then this problem of a large gradient flowing to B and then getting multiplied by a reasonable.\nlearning rate can cause B to be very negative right so that's where the learning rate has to be carefully.\nuh satisf carefully set and also when you are using uh relu it is advised to initialize bias to a.\npositive value right and in context of deep learning this is a fairly large positive value 0.01 because you have.\nmany elements contributing to the sum so one of them is point zero one right so you should set it.\nproperly otherwise many of your neurons would die or you could use some of the other variants of relu that.\nwe will see soon so this is one variant which is the Leaky relu right and as the name suggests.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "we will see soon so this is one variant which is the Leaky relu right and as the name suggests.\nit's allowed some leakage on the negative side so earlier whenever you had a negative value you were kind of.\nmaking it zero now you make it small you multiply by a very small constant so it becomes small right.\nso it's close to zero but it does not die completely so it allows some gradients to flow even when.\nyour X is negative right so it takes care of neurons not saturating and ah yeah and it will not.\ndie because it will ensure that some gradient always flows through and it remains computationally efficient right just making it.\n0.1 x does not make it any more expensive than the value function right maybe slightly more and also it.\ngives you these close to zero centered outputs because now you have outputs which are negative also and your outputs.\nwhich are positive also so this problem of gradients only being in One Direction is also taken care of it.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "which are positive also so this problem of gradients only being in One Direction is also taken care of it.\nso multiple advantages of leaky relu and then there are a few such other variants right so one of them.\nis parametric relu so then the question is right that y set this to point One X right so let.\nit be some Alpha and this Alpha could also be learned and it will also get updated during back preparation.\nright so one simple way of thinking is that you set Alpha equal to 0.1 to begin with right and.\nthen just as all the weights are getting updated compute this derivative of the loss function with respect to Alpha.\nalso and then update Alpha also as Alpha minus ETA times this gradient or whatever gradient doesn't update rule you.\nare using right ah so this is it kind of again makes it uh more careful that why have you.\nselected a certain slope here maybe that slope needs to be adjusted as the training is going right so it.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "are using right ah so this is it kind of again makes it uh more careful that why have you.\nselected a certain slope here maybe that slope needs to be adjusted as the training is going right so it.\ngives me more flexibility and again takes care of all the problems that you had with relu of neurons dying.\nand it not being zero center right while these variants have been proposed they are not like so popular I.\nthink relu the default version is still the most popular among these variants and one more variant of relu is.\nthe exponential linear unit right so yeah so far everything was linear on both sides it was like this but.\nnow on the whenever the input is less than zero we are making it exponentially Decay so there's still some.\ngradient flowing on the negative side also it has all the benefits of relu because largely it has a linear.\nuh on the positive side and it's also now again close to zero centered right and some gradient always flows.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "uh on the positive side and it's also now again close to zero centered right and some gradient always flows.\nthrough but it again becomes exponentially uh it becomes expensive right because you have to compute this exponent right so.\nnow all of these they were proposed in certain contexts and in those contexts they were like found to be.\nbetter than relu also some of them theoretically showed that why they are better than relu but at least in.\npractice many of these are not very popular right you just use the default relu and largely that works or.\nyou use some of the more recent ones which is angelu and swish right so they are more popular now.\nright so that ends the discussion on three of the most popular or so of the initial activation functions which.\nwere there around 2012 2014 which is the logistic activation function then the tanh and then the relu right so.\nI'll end this video here and when you come back we'll discuss a few more uh later activation functions that.\ncame out.", "metadata": {"video_title": "Better Activation functions"}}
{"text": "[Music] hi everyone uh so let's start with uh lecture one of this course uh where we'll be talking about.\na brief and maybe a bit selective partial history of uh deep learning right so you talk about uh deep.\nlearning right so most of this material uh the early material that is that at least there in these slides.\nuh is taken by from this article on deep learning in neural networks and overview by schmidtober there might be.\nsome errors in my accounting of the history and if they are then i apologize for them and also feel.\nfree to contact me if you think certain portions need to be corrected or there are more things which have.\nhappened and you would like to uh me to add them right so i first did this history lecture almost.\nlike six years back uh and it was more easier to manage but in the past four five years i.\nthink there's been a much rapid explosion than what it was earlier even at that time there was quite a.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "like six years back uh and it was more easier to manage but in the past four five years i.\nthink there's been a much rapid explosion than what it was earlier even at that time there was quite a.\nbit of work happening but it has exponentially grown since then and it's often hard to keep track of uh.\ndifferent things so maybe i've missed quite a few things i understand for example uh speech progress in speech is.\nnot very appropriately captured in these slides but the idea is just to give you an overall flavor from where.\nwe are and where we have reached and maybe a few things here and there would have been missed but.\nit should still give you a fairly reasonable idea of what are the latest developments and how they have evolved.\nover the years okay so with that uh primer let me just start so as i was trying to say.\nthat when we talk about deep learning uh we're talking about neural networks and at least we hear that keep.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "over the years okay so with that uh primer let me just start so as i was trying to say.\nthat when we talk about deep learning uh we're talking about neural networks and at least we hear that keep.\nhearing that a lot of inspiration comes from the biological neurons or at least we are also currently uh still.\nstriving right to try to understand how the brain does things and maybe come up with models of that right.\nso let's start that history uh from biology actually and we go back uh quite a bit of quite a.\nfew years 150 years in time uh to around 1871 right and this was a time when people are trying.\nto understand what does our nervous system look like right and joseph von gerlach who was one of the researchers.\nor scientists in this field he came up with this idea that our nervous system is a continuous network right.\nas opposed to being composed of various discrete cells which are connected to each other right and this view of.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "as opposed to being composed of various discrete cells which are connected to each other right and this view of.\nthe brain or the nervous system was called the reticular theory and this will be important in our discussions on.\ndeep learning as we uh keep seeing through these uh slides right so and again one of the things i.\nwant to emphasize through this history is that there are various discoveries happen not necessarily in deep learning or in.\ncomputer science but in different fields right which have over the years influenced where we are today so one of.\nthese was again very early in the 1870s the staining technique was developed and it allowed it was a basically.\na chemical reaction which allowed you to examine nervous tissues better right and camillio golgi came up with this and.\nusing that chemical reaction he analyzed a slice of the nervous tissue and he came up with the same conclusion.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "using that chemical reaction he analyzed a slice of the nervous tissue and he came up with the same conclusion.\nthat actually this nervous system is like one continuous network and it's not composed of discrete units so he was.\nalso a proponent of reticular theory right and he his evidence for that was through the staining technique but quite.\ninterestingly around the same time uh santiago i won't pronounce the full name because i can't use the same technique.\nthe same straining technique which uh golgi had come up with right and by making similar observations using the same.\ntechnique but maybe seeing things a bit differently he came up with the conclusion around 88 and 1888 1891 time.\nframe that no actually the nervous system is made up of discrete individual cells forming a network right so two.\ndifferent uh prominent theories at that time one says that it's a single uh network the other says that it's.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "different uh prominent theories at that time one says that it's a single uh network the other says that it's.\na single continuous network the other says it's a network of discrete elements right that means there are many individual.\ncomponents which are connected together right and one is the neural neuron doctrine and the other is the reticular theory.\nand then around 1891 this gentleman i'll again not pronounce the full name he coined the term neuron right this.\nterm was coined by him and today when we talk about artificial neurons the word neuron originally is attributed to.\nthis gentleman and he further consolidated the neuron doctrine that means he found further evidence and consolidated different views and.\nsaid that indeed it seems that the neuron doctrine is the right way of explaining what the nervous system is.\nas opposed to the reticular theory right and just uh trivia here he was also the person who coined the.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "as opposed to the reticular theory right and just uh trivia here he was also the person who coined the.\nterm chromosome it's the two very important terms that we hear about today both are coined by this gentleman here.\nright now here's a question right so there are these computing theories one proposed by um golgi and the one.\nby the other gentleman uh reticular theory and neuron doctrine right so now around 1905 when the nobel prize in.\nmedicine was given who do you think it went to right the person who propagated the reticular theory or the.\nneuron doctrine what's your take on that so i hear various answers but it turns out that both of them.\ngot it right so by that time again i mean we have been like 1871 to 1906 quite a few.\nyears that in terms of the way uh research progresses today at least in deep learning like several generations right.\n35 years but still there was not uh any conclusion of these and both these schools of thoughts uh were.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "35 years but still there was not uh any conclusion of these and both these schools of thoughts uh were.\nin existence with their own champions right and again given that the nobel prize was given to both of them.\nthis resulted in lasting conflicting ideas and between the two groups of scientists who believed in this these two different.\ntheories right and this took quite a bit of time to resolve and not by developments in biology but developments.\nin a very different field so around 1950s when electron microscopy was became useful at that time using the electron.\nmicroscopy technology it was finally confirmed that the neuron doctrine is the right one that means that the brain or.\nthe nervous system has many small cells called neurons and they are all interconnected to each other through synapses right.\nand this structure where you have neurons and they have connections between them is what kind of forms the foundation.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "and this structure where you have neurons and they have connections between them is what kind of forms the foundation.\nof modern deep neural networks also it took over quite a bit of time and progress in different fields to.\ncome to this conclusion which then motivated work uh in deep neural networks or neural networks at that time which.\nhas now evolved to where it has right so quite a bit of history and quite a bit of developments.\nfrom different fields which have led to this place right so now moving on or in fact there is one.\nmore before we move on right there's one more thing about the brains which there was a great debate about.\nwhich was uh whether the processing in brain is localized or is it distributed so what it means by means.\nthat there was one group which felt that if you're talking about speech or vision or any specific uh activities.\nthat the brain is responsible for they are localized there are certain portions of the brain which are responsible for.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "that the brain is responsible for they are localized there are certain portions of the brain which are responsible for.\nspeech certain for vision and so on and there was this another group which thought it's distributed that means different.\nparts of the brain connect in different ways to do different activities it's not that one part of speech and.\nthe other part is so right and you can go back and look at this video i'll not play it.\nwhich is about this great brain debate about localized versus distributed processing and that again took a while to settle.\nright and now this is again important because again in modern deep networks we believe that it's more distributed different.\nparts interact with each other to do different things whereas there's also push for having what is known as localized.\nprocessing especially in modern multilingual models we want certain portions of the network to only adhere to computations related to.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "processing especially in modern multilingual models we want certain portions of the network to only adhere to computations related to.\na certain language or even now certain inputs right and the rest of the network may be doing other things.\nand so on right so you can just look at this video also which is uh again another kind of.\na debate about brains the first one was whether it's single or distribu or disconnected now this is more about.\nwhether it's localized or distributed right and both of these are relevant even today in terms of modern deep neural.\nnetworks right so from these uh biological neurons where the inspiration for neural networks came now let's move to the.\nactual artificial neuron set and that's where this period of time that we'll cover which is called the spring to.\nthe winter of ai and i'll tell you why these two seasons have cropped up in us in our discussion.\nright so around 1943 while we were still trying to understand uh what uh the brain is and we are.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "the winter of ai and i'll tell you why these two seasons have cropped up in us in our discussion.\nright so around 1943 while we were still trying to understand uh what uh the brain is and we are.\nin fact i mean still today trying to understand it uh uh mcculloch and pits right to neuroscientists and logistics.\nright and again people coming from different fields who were contributing to this area right of course at that time.\ncomputer science was not so evolved it was still a field which was in formation right so uh or just.\ngetting formed so so these two uh uh one neuroscientist and logician they proposed a simplified model of the neuron.\nand this is something that we'll do in the course in uh detail in the next lecture itself where they.\njust said that a neuron a model of the brain would be that there are multiple inputs coming to it.\nright and these could be inputs from a sensory organs and based on that it takes a decision and a.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "just said that a neuron a model of the brain would be that there are multiple inputs coming to it.\nright and these could be inputs from a sensory organs and based on that it takes a decision and a.\nvery simplified model is where all these inputs are binary and the decision is also binary so i take inputs.\nlike uh is it raining outside do i have money do i have time and if so maybe i'll go.\nout to watch a movie right so that's how the decision uh making processes modeled with a very simplified model.\nright and then this again got modified and this perceptron model was proposed by frank rosenblatt and this is what.\nhe had to say about it when he proposed it and if you look at the diagram of course the.\nperceptron model is again something that we'll do in detail in the course uh you'll see that it's very similar.\nto the earlier diagram except you see some weights here right so now these different decision factors have some weight.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "to the earlier diagram except you see some weights here right so now these different decision factors have some weight.\ni would give more emphasis to the input about me having money or not as opposed to whether i'm in.\nthe mood to go out or not because if i don't have money maybe i can't go out and really.\nspend anything or do anything right so that's the basic idea here and this is what he had to say.\nthat the perceptron may eventually be able to learn make decisions and translate languages right now when i read this.\ni i find something very strange about this statement right learn make decisions and translate languages so what is the.\noddity here right what what seems a bit odd here [Music] yeah so the phrase translate language i can understand.\nabout learning and making decisions because that's generally what we associate the brain with but why something so specific which.\nis about translate languages right so this as you can see this is a period 1957 to 58 which is.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "is about translate languages right so this as you can see this is a period 1957 to 58 which is.\nafter the war and even during the war right i mean there was a lot of emphasis on being able.\nto translate messages from enemies which spoke different languages german russian english and so on right and so translation was.\nan important problem to be solved in that era and hence this was said that this will also be able.\nto learn how to translate languages and today are almost like 70 years later right we are still trying to.\nsolve that problem very recently facebook has released a paper on which can do translation between 40 000 pairs involving.\n200 languages but there's still a very long tale of languages that we still need to enable translation for and.\nof course this is just initial flag planting in the sense that by no means are these 40 000 directions.\nuh the translations adequate they're still at various levels of quality and much more is desired to make them really.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "uh the translations adequate they're still at various levels of quality and much more is desired to make them really.\nuseful so we are still trying to solve that problem after so many years right while this statement was made.\nmany years back and the reason i'm saying that is that as has been a characteristic of machine learning deep.\nlearning a lot of tall claims get made but it takes time for those claims to uh realize right and.\nthat often leads to certain dissatisfaction in terms of what we expect these systems to do versus what they can.\ndo of late of course we are making uh rapid progress where we're trying to meet some of these expectations.\nbut still i would say a lot needs a lot still a lot of still desire in terms of really.\nunderstanding the way humans do and this statement right again very interesting the am this is the embryo of an.\nelectronic computer that the navy expects will be able to walk talk see write reproduce itself and be conscious of.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "electronic computer that the navy expects will be able to walk talk see write reproduce itself and be conscious of.\nits existence right even today 70 years later this is the stuff of sci-fi movies right there's a stuff of.\nfuturistic movies where we see that okay ai can now become conscious it can reproduce itself it can take over.\nthe world and so on if you're still it's still the subject of sci-fi movies it's still not something that.\nhas happened in reality right so and this is an article from way back 1957-58 even today we see these.\nsuch similar articles right so much not much has changed in terms of the hype that is generally there around.\nai machine learning deep learning then this all was for a single perceptron and what we know today as deep.\nlearnings which is like a multi-layer network of neurons right so this idea is also not new right this was.\nthere way back in 1965-68 by by scientist called evac nenco and his group who proposed what is looking like.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "there way back in 1965-68 by by scientist called evac nenco and his group who proposed what is looking like.\na very modern uh deep neural network right so this idea also existed quite bad a lot of these ideas.\nhave had their generations right they were proposed initially maybe the conditions at that time were not so conducive for.\nthese ideas maybe they were a bit ahead of their times and then 30 years back again people picked up.\nthose ideas and then maybe the conditions were right so there's again like a repeating theme in this area but.\naround the same time what happened right in 1969 so 1957 when you saw those statements being made about perceptron.\nand then the new york times articles and many such similar articles the next 12 to 13 years was what.\ni would call as the spring time of ei that was a lot of curiosity around this area a lot.\nof government funding available for this right at least in the u.s a lot of the funding was on science.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "i would call as the spring time of ei that was a lot of curiosity around this area a lot.\nof government funding available for this right at least in the u.s a lot of the funding was on science.\nand technology was driven by government bodies at that time and there was a lot of interest in making really.\nai work given the promise made of what it could do if allowed to flourish right but then around 1969.\nin their now famous book uh minsky and pepper uh outlined the limits of what perceptrons could do right and.\nwhat they said in very simple terms is that uh while we are thinking that uh perceptron can model any.\nreal world phenomenon what that means is that suppose i have a complex decision to make right which depends on.\nsay various inputs right and i'll just start using some terminology say rn x belongs to rn which oh sorry.\nright which means there are n inputs right and you have a function and you want to take a decision.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "right which means there are n inputs right and you have a function and you want to take a decision.\nand this function is of course complex that you don't know what it is right so what claims were being.\nmade is that if you have this relation where you have an x you have some function which gives you.\nout y right and if you give me enough instances of this which means okay yeah if you give me.\nx 1 y 1 x 2 y 2 x 3 y 3 enough input output pairs of this function right.\nso you have decisions that you have taken in the past for certain inputs then i could train a model.\nright which could take again an input x and give out an output y which would be very close to.\nthe true y that should have been there right that is the claim that was being made right and what.\nuh and that's why right i mean translation languages could be one such example you have an input which is.\na sentence and you produce an output and what was being claimed is i can make a model which can.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "a sentence and you produce an output and what was being claimed is i can make a model which can.\ntake an english sentence and give you a russian sentence as output which would be very close to the true.\nrussian sentence that you would expect so those are the claims being made right any you could take a bunch.\nof inputs and give the output which would be very close to the human output that you would expect now.\nwhat uh minsky and pepper show that even for very simple functions right where this is not a very complex.\nfunction but a very simple function like the xor function for two variables right just take one say a and.\nb as input and you know what the xor function should give an output the perceptron is not capable of.\nlearning that also that means i cannot come up with a perceptron model which takes a and b as input.\nand if i change a and b from 0 0 0 1 1 0 1 1 the output would be.\nthe same as the truth table of the xor function right so that's what they showed right and that kind.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "and if i change a and b from 0 0 0 1 1 0 1 1 the output would be.\nthe same as the truth table of the xor function right so that's what they showed right and that kind.\nof led to significant disappointment because now you have like a very simple function and you're claiming that a perceptron.\ncannot even solve that then how do you expect you to solve much more complex real-world examples like the translation.\nexample or complex decision making like giving a bunch of parameters about a patient right blood sugar level cholesterol and.\nso on and trying to predict the life expectancy or the possibility of a person getting a cardiac ailment in.\nthe future and so on and those are much more complex decisions than just a simple xor function right so.\nthis led to a lot of disappointment but unfortunately right so while this statement is true right and it's a.\nclassic example that we also use today i'll also be doing this in the course uh minsky and pepper said.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "classic example that we also use today i'll also be doing this in the course uh minsky and pepper said.\nthis only in the context of a single percept right they said they still said that if you have a.\nnetwork of perceptrons you could solve uh complex function right but somehow this second part that you cannot do this.\nwith a single neuron but with a layer of multiple multiple or multi-layer network of perceptrons but you can do.\nit with a multi-layer network of perceptrons the second part often got lost and the first part got emphasized that.\nhey you cannot do this and that led to a lot of disappointment and following this and of course i.\ncan't say that this was the primary reason but this and similar uh evidences which started emerging there's a slight.\ndecline in the interest in funding ai for almost the next two decades and this is what we call the.\nwinter of ai where the interest in funding large scale projects in ai started declining and people started i would.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "winter of ai where the interest in funding large scale projects in ai started declining and people started i would.\nsay rationalizing their expectations from ai a bit more right but that does not mean that work completely stopped no.\none was doing uh yeah there was still work happening and small progress was being made in different uh areas.\nright in particular in 1986 uh yeah so as i said this was the ai winter of connectionism and for.\nsome of you are initiated in this there's connexious ti and there's a symbolic ai so people were still interested.\nin symbolic ai but lost interest in what is known as connectionist ai whereas today's ai is largely dominated by.\nconnectionist ai and we want to again try to see if you can come up with hybrid models because again.\ntoday people are realizing that this way of doing ai the deep learning view of ai has its limitations and.\nwe need to start looking at hybrid methods or complete different alternatives to really push the frontier a bit more.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "we need to start looking at hybrid methods or complete different alternatives to really push the frontier a bit more.\nright and this during this period which i'm calling the ai winter not i it's generally called the ai winter.\nthere's also this abandonment of these ideas which were similar to deep learning at that time but some workforce of.\ncourse continued and in particular there was this back propagation algorithm which all of you would have read about in.\nblogs or various articles that you have read on deep learning this was again not something which has been discovered.\nin the last 10 15 years in fact it was discovered and rediscovered several times throughout 1960s and 70s and.\nof course there's not the benefit of having internet where if something gets discovered in one part of the world.\nit immediately propagates so you might not be aware that someone else has discovered it and independently discovered this algorithm.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "it immediately propagates so you might not be aware that someone else has discovered it and independently discovered this algorithm.\nthat's what was happening at that time and around uh 1986 uh rumel heart and others that including jeffrey hinton.\nuh popularized in the this in the context of neural networks and they showed that this back propaganda propagation algorithm.\ncan be used for training uh neural neural networks and this was an important breakthrough because even to this day.\nuh most modern deep neural networks are trained using the back propagation algorithm right there was another important and uh.\nan interesting fact is this back propagation uh within it right or at the heart of it there's also gradient.\ndescent right which was much older right like almost one century one and a half century before back propagation and.\nthis was discovered or proposed by koshi and for a very different purpose right and he was trying to use.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "this was discovered or proposed by koshi and for a very different purpose right and he was trying to use.\ngradient descent to compute the orbit of heavenly body so there's a lot of interest in astronomy at that time.\nand he was trying to look at what is the orbit of heavenly bodies and in that context he had.\ndiscovered gradient distance right so again whatever benefits we are enjoying today they come through the work of many great.\nscientists over centuries in different fields and that's where that has helped us reach where we are today right in.\nvery unexpected uh ways so while we were still in this winter period again this 1989 what came up is.\nthis universal approximation theorem which is again something that will come up in the course right and what this said.\nagain to repeat what i said earlier that if you have a function which takes certain x as input and.\ngives you y and in real world you don't know what this f is right so in real world if.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "again to repeat what i said earlier that if you have a function which takes certain x as input and.\ngives you y and in real world you don't know what this f is right so in real world if.\ni want to say okay what is my decision in real examples suppose f is the function that i'm using.\nto decide how to hire people or how to predict the life expectancy of a person i don't know what.\nthis f is that is not really known i just know it depends on certain factors right but and what.\ni have is several examples of these inputs and outputs right i know a person who had a certain blood.\npressure level cholesterol level and so on sugar level and then i know how long that person lived and so.\non right so i have many such examples now what this theorem said is that you could come up with.\na multi-layered network of neurons right not unlike what papad and minsky had said that a single neuron cannot do.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "a multi-layered network of neurons right not unlike what papad and minsky had said that a single neuron cannot do.\nit they said that if you have a multi-layered network of neurons right then you could take an x given.\nmany such examples you could learn a model such that now if you feed an x to it the y.\nthat it predicts would be very close to f of x which is the true value of the y that.\nyou would have got if you knew what that function is right so what it means is that what i'm.\ntrying to show in this diagram is this orange curve that you see here right okay somehow the pen is.\nbehaving a bit strange yeah the orange curve that you see here suppose this is what my actual f is.\nand i don't know that right and now i'm trying to approximate it and those i'm doing that approximation with.\nthe help of these uh rectangular bars that you see right and what this was uh this theorem said is.\nthat if you have a very deep neural network or a multi-layer in fact it said even if you have.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "the help of these uh rectangular bars that you see right and what this was uh this theorem said is.\nthat if you have a very deep neural network or a multi-layer in fact it said even if you have.\na one layer network with a large number of neurons then you can approximate this very very well right and.\nthat's what we want to do we had the true function which we did not know all we knew was.\nseveral instances of x1 and y1 x2 y2 what this theorem says is that if you have many such instances.\nyou could come up with a neural network right and that those rectangular bars are in some sense the output.\nof the neural network such that that output would be very close to the real output right and the more.\nneurons you add the better your approximation would be if you had fear of neurons your approximation would be poor.\nlike you see in this case but the more neurons you add it will be better right this theorem an.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "like you see in this case but the more neurons you add it will be better right this theorem an.\nillustrative proof of it is something that we will see but this was a real breakthrough right because now it.\nhas almost the reverse effect of what we had for the proof by pepper and minsky which said that even.\nfor simple functions a single neuron does not work now what this is saying is that for any arbitrary complex.\nfunction not even boolean functions any arbitrary function that you have no longer what the function looks like you can.\nalways come up the neural network which will be able to approximate that function to any desired degree of precision.\nright and that desired degree of precision is controlled by the number of neurons that you have if you're on.\nfewer neurons my approximation would be very bad but if i keep adding neurons my approximation would become better and.\nbetter right so this is a real breakthrough that was there and after this of course nothing changed right but.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "better right so this is a real breakthrough that was there and after this of course nothing changed right but.\nfor nothing changed much in the sense that people realize okay there is a lot of power in deep neural.\nnetworks but for the next uh 20 years or so or maybe 17 18 years on the back of these.\ntwo discoveries one is back propagation which allows you to train deep neural networks and the other is this universal.\napproximation theorem which says that there is value in training deep neural networks because then you can approximate arbitrary real.\nworld functions uh people try to apply these ideas right to real world problems but what they notice is that.\ntraining a multi-layer network of neurons using back propagation is not very stable and it often does not lead to.\nconvergence so while in theory you can use back propagation while in theory you can approximate any function but in.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "convergence so while in theory you can use back propagation while in theory you can approximate any function but in.\npractice when you're trying to train this really deep neural networks it's not working right so uh in the next.\nfew years from 1989 to not few almost two decades uh much practical progress did not happen in terms of.\ndeep learning right people knew these two things but they were not really able to make them work in most.\ncases there were of course still a lot of progress happened on convolutional neural networks which you'll see soon right.\nso that's that's where we were so we started with the spring where there was a lot of hype and.\nenthusiasm around ai then things collapsed and then things kind of stabilized okay let's not completely ignore it let's keep.\nmaking some progress and see if you can actually use back propagation to train the steep neural networks.", "metadata": {"video_title": "Brief history of Deep learning - Neuron Doctrine - AI Winter"}}
{"text": "[Music] so while all this is happening great we're getting very good good models big models trained on very large.\namounts of data very good performance on very complex tasks like generation but there are also calls for sanity right.\nwe also need to be sure that whatever we are doing is interpretable fair responsible green right and these terms.\nwhich are not uh so popular like 10 15 years back i mean they were just maybe being discussed on.\nthe sidelines now have become quite mainstream right because of these rapid advancements that we are making people are realizing.\nhey with all this happening we need to also be a bit conscious about the implications of what we are.\nbuilding right and that's what this section would be about right so i'll start with the calls for sanity right.\nso so this is the paradox of deep learning right so why does deep learning work so well despite high.\ncapacity these are very large models and when you start do a basic course on machine learning you will the.", "metadata": {"video_title": "Call for sanity in AI - Efficient Deep learning"}}
{"text": "capacity these are very large models and when you start do a basic course on machine learning you will the.\nfirst thing that you learn is that hey if you have very complex models then they are susceptible to overfitting.\nthey'll completely learn the training data but they'll not perform well untested right but this is not what is happening.\nfor deep learning models right they are high capacity but they are still somehow being able to overcome this problem.\nof overfitting right to a certain extent some cases they do overfit then there is this numerical instability i spoke.\nabout this vanishing exploding gradients and even gans and diffusion based models they are often hard to uh not diffusion-based.\nmodels but at least gans are often very hard to train and get unstable during uh training right then they.\nhave this sharp minima right because this uh the uh the the loss function that you have in deep data.", "metadata": {"video_title": "Call for sanity in AI - Efficient Deep learning"}}
{"text": "have this sharp minima right because this uh the uh the the loss function that you have in deep data.\nnetworks is not a good convex loss function right so it has a many multiple local minima and some of.\nthese are very sharp and then it leads to certain kind of challenges during our training despite all this deep.\nlearning works quite well in practice right it's also not very robust right so this is an example where you.\nhave the image of a stop symbol and now you just put some stickers on it right and for a.\nhuman it does not matter right it's still the stop signal right but now uh the machine is reading it.\nas a speed limit of 45 right so now it's like completely different from what the intended meaning here was.\nright so it's not very robust in practice right so we still don't know right why this ah this happens.\nright because it looks like it should not work it is also not very robust but still in very many.", "metadata": {"video_title": "Call for sanity in AI - Efficient Deep learning"}}
{"text": "right because it looks like it should not work it is also not very robust but still in very many.\nreal world applications we are seeing it perform very well and give deliver results right so we still don't know.\nwhy this happens uh and there are many such examples right so so this is again another example right so.\npeople have been able to come up with images which can fool deep neural networks right so i just i.\nmean scientists have come up with these regular patterns right but now when you feed this to a deep learning.\nbased image classifier it thinks that the first image is of a king penguin right then you could just uh.\nlet your imagination run a build wit and realize why it could be thinking that right i mean there are.\nsome patterns here similarly the second image it classifies as starfish right so these these challenges do exist but still.\nuh that has deep learning is the current popular paradigm and delivering good results in various fields right so now.", "metadata": {"video_title": "Call for sanity in AI - Efficient Deep learning"}}
{"text": "uh that has deep learning is the current popular paradigm and delivering good results in various fields right so now.\nwhile there are no clear answers yet but slowly and steadily there is an increasing emphasis on explainability it's not.\njust good that you tell me your model gives me a good accuracy you also need to tell me why.\ni am getting this accuracy right so that then i have a better grasp of these kind of issues that.\nwhy is it seeing a speed limit fortify just because i put some stickers here and there right so these.\nthese uh things are uh coming into focus and a lot of research is happening on uh the theoretical justifications.\nfor why deep learning works why the uh research on explainability of these models how do you explain the performance.\nof a deep learning model and so on right and hopefully all of this will bring some sanity right so.\nhere are a few advances in that direction right so there's now a workshop on human interpretability in machine learning.", "metadata": {"video_title": "Call for sanity in AI - Efficient Deep learning"}}
{"text": "here are a few advances in that direction right so there's now a workshop on human interpretability in machine learning.\nwhich started in 2016. uh there's this clever hands uh toolkit which was developed right so so clever hands was.\nthis horse right and i think in the uh last century where um so so the horse had an owner.\nand then he used to do the street show where uh where someone in the audience would ask it uh.\narithmetic question right what is 5 plus 2 right and the way the horse would answer is i think by.\nnodding or by tapping its feet and it would kind of stop right so 5 plus two is seven so.\npeop i mean during the shows it would do it seven times right and then people thought hey this horse.\nis able to actually compute the answer and so on right but that was not the case what people later.\non realize is that everyone in the audience was giving it cues naturally they're cheering for it right so when.", "metadata": {"video_title": "Call for sanity in AI - Efficient Deep learning"}}
{"text": "on realize is that everyone in the audience was giving it cues naturally they're cheering for it right so when.\nthe when when it nodded seven times they would all like erupt in say in an applause or something and.\nthe horse would take that as a signal oh i should stop now and it would get the right answer.\nright so it's this thing where the answer is correct but there is no reason for the answer to be.\ncorrect right so that's what was happening in the case of clever hands and like based on that anecdote or.\nthat incident right this clever hands tool kit was designed to again look at similar problems uh why do you.\nget the answers uh right uh instead of just saying that oh i have got the answer right right so.\nthis toolkit allows uh machine learning systems uh to be tested on adversarial examples right so i showed you a.\nfew examples of examples on the previous slide where some stickers were added to the sign or some random patterns.", "metadata": {"video_title": "Call for sanity in AI - Efficient Deep learning"}}
{"text": "few examples of examples on the previous slide where some stickers were added to the sign or some random patterns.\nwere generated and now uh the models this this repository allows you to benchmark the models on such adversarial examples.\nand if they don't do well then you know that they're not really learning to solve the problem but are.\nsomewhat like clever hands where they are relying on some other cues to arrive at the right answers without really.\nunderstanding what is the question being asked right and then there are now several workshops on blackbox nlp where the.\nfocus is on analyzing and interpreting the neural networks which are used for nlp and there's this nice uh textbook.\ninterpretable machine learning a guide for making black box models explainable right so the reason i'm bringing up all this.\nis that this was not so much in focus a few years back right but now this has become quite.", "metadata": {"video_title": "Call for sanity in AI - Efficient Deep learning"}}
{"text": "is that this was not so much in focus a few years back right but now this has become quite.\nmainstream and a lot of you who are looking for interesting research problems to solve in this space this is.\na very active area that you could pursue the second is about fairness and responsibility right so again if you.\nlook at a brief history around 2011 there are hardly any papers talking about fairness or responsible ai and so.\non and by 2017 when deep learning had almost taken over completely taken over nlp speech and vision there was.\na massive increase in the number of papers talking about fairness and responsible ai and that has just grown in.\nthe last has grown further in the last three to four years right and i'll tell you what fairness and.\nresponsibility i why are we worrying about this problem right so look at this uh image right so this was.\npassed through a facial recognition kind of a software right which looks at images and try to make certain judgment.", "metadata": {"video_title": "Call for sanity in AI - Efficient Deep learning"}}
{"text": "passed through a facial recognition kind of a software right which looks at images and try to make certain judgment.\nabout the people right and the first image it has classified as low risk so this software was trying to.\npredict whether uh the people in the images being shown could become criminals in the future or might commit some.\ncrime in the future right so the first person it's showing it's low risk on a scale of one to.\nten just three whereas for the second person it's showing high risk on a scale of one to ten it's.\neight right but now if you look at the past record of these people then the person in the first.\nimage has two armed robberies one attempted armed robbery and then after that again there was one grand theft right.\nwhereas the other person who has been marked as high risk had four uh juvenile offenses right or misdiminis and.\nthen after that there were no offenses right but what is happening here right is that since the second person.", "metadata": {"video_title": "Call for sanity in AI - Efficient Deep learning"}}
{"text": "then after that there were no offenses right but what is happening here right is that since the second person.\nis black and the model has a certain bias towards black people it is recommending or it is uh falsely.\njudging right or in a very biased way uh judging that this person has a higher chance of becoming a.\ncriminal in the future right so it's biased against blacks it's not fair to black set and this is a.\nrepeating theme in various ai models where they are biased based on the biases that they see in the training.\ndata maybe this model saw a lot of images during training of black people who were criminals maybe that's the.\nbias in the data that you had and most white people images that it show they were not criminals right.\nand that bias then gets reflected in the outputs that the model generates uh when deployed in real-world situations right.\nuh similar studies were done on facial recognition software and it was found people took the uh facial recognition systems.", "metadata": {"video_title": "Call for sanity in AI - Efficient Deep learning"}}
{"text": "uh similar studies were done on facial recognition software and it was found people took the uh facial recognition systems.\nfrom deep tech companies right microsoft ibm and so on and they found that if you are a white male.\nthen the facial recognition accuracy is very high right this corresponds to uh the second last column right so light.\nno sorry the third last column which is lighter male right you are a light skinned or a white skinned.\nmale then all of these are giving you close to 100 accuracy if you are light skinned female then the.\naccuracy decreases and the worst accuracy is for darker females which is the column in which you see the red.\nbars and it's like atrociously low right so again this shows that the model has not really been trained for.\nblack women and hence it's not able to do a good job of recognizing faces of black right and the.\nhuge gap between the performance of the best category uh or the best performing category right versus the category for.", "metadata": {"video_title": "Call for sanity in AI - Efficient Deep learning"}}
{"text": "huge gap between the performance of the best category uh or the best performing category right versus the category for.\nwhich the model has the worst performance and then because of these things right because this is not fair this.\nis biased many civil rights and other research groups have been writing to prominent people this is a letter to.\njeff bezos demanding that it should start providing facial recognition technology governments because this technology is giving you biased outputs.\nand then the government might take actions based on those biased outputs for example the criminal classification which example which.\ni showed earlier right and now uh around 2020 ibm amazon microsoft all of them have stopped selling uh facial.\nrecognition technology they have taken a public stand that will no longer be supporting facial recognition technology for a police.\npurpose for supplying it to police or even other government agencies right again like i spoke about daily 2 this.", "metadata": {"video_title": "Call for sanity in AI - Efficient Deep learning"}}
{"text": "purpose for supplying it to police or even other government agencies right again like i spoke about daily 2 this.\nis a recent example now if you ask it to generate images of success you can see that most of.\nthe images are of males if you ask it to generate images about sadness most of the females are about.\nmost of the images are about females right so this kind of gender bias or bias towards certain sections of.\nthe population is very common in these most deep learning models because of the bias and the training data that.\nthey have been trained with right and this again happens in various domains right so now if you use an.\nai model to decide whether you should give loan to someone or not again it would be biased towards certain.\nsections of the society because it would be the case that in its training data there were certain sections which.\ndid not often pay their loans back right so it's biased for example against women migrants or people of color.", "metadata": {"video_title": "Call for sanity in AI - Efficient Deep learning"}}
{"text": "did not often pay their loans back right so it's biased for example against women migrants or people of color.\nright which is not fair right i mean you have to judge each case individually and not based on similar.\nuh things that you have seen in the past right so there's and definitely not like two people who have.\nthe same profile but only differ in their terms of their skin color you cannot have a different decision for.\nthem right so that's what is happening in many of these models hence this calls for being fair and responsible.\nand there is again a challenge right so now again people are becoming aware of these and trying to push.\nresearch in these areas so stanford has this ai audit challenge where the idea is to build models which are.\ncompliant and do not do any illegal discrimination okay so while we are talking about fairness and responsible ai there.\nis also another axis around which we need to talk about being responsible right so i was talking about the.", "metadata": {"video_title": "Call for sanity in AI - Efficient Deep learning"}}
{"text": "is also another axis around which we need to talk about being responsible right so i was talking about the.\nhuman brain right so it has 10 is to 15 synapses but it only consumes 15 watts of power right.\nwhereas if you look at these current deep learning models i showed you one model which was trained on 2048.\ntpus which is a huge amount of power and compute right and in the period from 2012 to 2018 there.\nhas been a 300 000 folds increase in the amount of uh compute compute that is being used for training.\nthese models and this has been doubling every few months right and you can see uh in the in the.\nshaded portion here right that in the past few years there's a rapid increase in the size of these models.\nor the uh computes that you're using uh for training these models right and that that has to stop right.\nbecause you are being irresponsible in some sense and this should give it make it clear so if you have.", "metadata": {"video_title": "Call for sanity in AI - Efficient Deep learning"}}
{"text": "because you are being irresponsible in some sense and this should give it make it clear so if you have.\nlike a take an air travel from new york city to san francisco then this is the carbon footprint for.\none passenger right which you see on the extreme left-hand side and average uh human in one year uh does.\nthe amount of co2 emission that it does is marked as 11 right whereas if you look at the amount.\nof co2 emission that happens for training a transformer based ai model just unimaginable right i mean you have the.\nthe entire just the graph before it is the amount of co2 emission in the full lifetime of a car.\nright the entire lifetime of a car this is the amount of co2 emission that it does and what you.\nget by training one single transformer right it's just like at least five times that right and this is again.\nirresponsible and that's it again we are talking about climate change we are talking about uh being responsible citizens of.", "metadata": {"video_title": "Call for sanity in AI - Efficient Deep learning"}}
{"text": "irresponsible and that's it again we are talking about climate change we are talking about uh being responsible citizens of.\nthe earth and if you train such models there's clearly a problem there right so there's a lot of push.\nfor green ai now to make models smaller and smaller and also these issues mix right so this is also.\nnow uh not just a problem of environment it's also a problem of being responsible it's also a problem of.\nbeing fair right i'll just read out this quote from this famous paper you can read google a bit about.\nthis paper so is it fair that the residence of maldives right which is likely to be underwater by 2100.\nor the 800 000 people in sudan are affected by drastic floods who are affected by drastic floods is it.\nfair that they pay the environmental price of training and deploying even larger english language models right so all a.\nlot of this is english centric work and a lot of computers being used for training english models so is.", "metadata": {"video_title": "Call for sanity in AI - Efficient Deep learning"}}
{"text": "lot of this is english centric work and a lot of computers being used for training english models so is.\nit fair that people living in maldives or sudan are paying the environmental price for training these models when similar.\nmodels are not being produced for their own languages right so then what is the price they are paying for.\nit's a very valid question to think about and again this is a very active area of research not trying.\npeople are trying to rethink about hardware itself to make the compute uh much more efficient right uh so that.\nwas slightly on a somber note right it was saying as we started off well we spoke about all the.\nsuccesses of deep learning but then we had this somber note on hey we should be responsible fair talk about.\ngreen ai talk about having sanity in what we are doing just not put out models but also try to.\nexplain why they work and so on so i didn't want to end on that somber note so i'll again.", "metadata": {"video_title": "Call for sanity in AI - Efficient Deep learning"}}
{"text": "green ai talk about having sanity in what we are doing just not put out models but also try to.\nexplain why they work and so on so i didn't want to end on that somber note so i'll again.\ntalk about some exciting times right so the ai revolution is now uh driving scientific research also and let me.\ntell you what i mean by that this is the uh protein folding uh uh problem right and deep mine.\nin 2020 released a model for predicting the protein structure of sorry the the three structure of proteins right so.\nwhat do i mean by that right so we uh every protein right is made up of a sequence of.\namino acids bonded together right and we know these amino acids this sequence for all the proteins that are there.\nin the body right but now this is just like a i would say a illustrative representation or just a.\nlinear representation of what the protein looks like but that's not the reality right these amino acids they actually interact.", "metadata": {"video_title": "Call for sanity in AI - Efficient Deep learning"}}
{"text": "linear representation of what the protein looks like but that's not the reality right these amino acids they actually interact.\nwith each other and based on these interactions the protein starts taking certain shapes and it actually has this 3d.\nstructure right now knowing this 3d structure is important because if you know this 3d structure then it helps in.\ndrug design because if you develop a structure which is compatible with this 3d structure then it would lead to.\neffective drug delivery for example and a lot of you could relate to this in terms of this covet pandemic.\nright so we heard that this virus has this spike protein right which has a certain crown like shape and.\nbecause of that it is able to attach on certain things well it's a similar analogy here that you have.\na certain structure a 3d structure it has a certain shape if you know what that shape is then you.", "metadata": {"video_title": "Call for sanity in AI - Efficient Deep learning"}}
{"text": "a certain structure a 3d structure it has a certain shape if you know what that shape is then you.\ncould design things which could attach or be compatible with that structure and that helps in drug design right so.\nthis structure 3d structure is not really known for all the proteins in the body right and now what uh.\ndeepmind has been able to do with its alpha fold model that it has been able to get a significant.\nincrease in the uh accuracy with which we are being able to predict these 3d structures right of course the.\nproblem is nowhere close to being solved but there is a remarkable increase in where we were like just in.\n2019 versus where we are in 2022 today right so that's exciting and maybe it would take a while for.\nthings to uh because as we have again realized during the pandemic that things move very different in at a.\nvery different pace in the scientific field of medicine and so on where there are a lot of regulations and.", "metadata": {"video_title": "Call for sanity in AI - Efficient Deep learning"}}
{"text": "very different pace in the scientific field of medicine and so on where there are a lot of regulations and.\na lot of other things need to fall in place but at least the science is getting pushed and that.\nmight show soon show effects in actual applications right yeah similarly in astronomy right so there is an interest that.\nyou have a certain very i'll just try to explain this at a very high level right because in fact.\neven i understand at a very high level so suppose you have a certain galaxy right and you're interested in.\nhow it will age or how it will look when it ages now this problem people have tried for human.\nfaces right they have been able to generate these images of how you will look like say 5 years from.\nnow 10 years from now 100 years old now can you apply the same at the galaxy scale and try.\nto predict about how the galaxy would look right and now if you can predict that then if you can.", "metadata": {"video_title": "Call for sanity in AI - Efficient Deep learning"}}
{"text": "now 10 years from now 100 years old now can you apply the same at the galaxy scale and try.\nto predict about how the galaxy would look right and now if you can predict that then if you can.\ngenerate that image right then you then scientists can make certain claims about what is going to happen there right.\nis that star becoming older or what exactly are some of the physical phenomena that might be linked because of.\nwhich this image is getting generated right so this is again a very different way of doing astronomy which is.\nnot being thought about or popular a few years back right okay uh similar uh stuff has happened uh for.\nuh uh for finding uh fundamental variables right which are hidden in experimental data right so i'll not try to.\nexplain this i'll just i don't think i have time to play this video also but there's a very interesting.\nvideo here i would recommend that you see the first five minutes of this video to understand uh what this.", "metadata": {"video_title": "Call for sanity in AI - Efficient Deep learning"}}
{"text": "video here i would recommend that you see the first five minutes of this video to understand uh what this.\nis about right okay now the last section that i want to talk about is efficient deep learning so we.\nare living in the era of mobile phones right and the number of smartphone subscriptions has increased at least in.\nthe context of a country like india there is increasing mobile penetration in the rural areas right what that means.\nis that now you should start thinking about how do you serve this population which might not have computers right.\nbut they have access to mobile phones so can you build ai models which run for this particular form factor.\nright and people have this uh good connectivity also so now there is increasing uh emphasis on designing models which.\ncan run on the edge right so if you have a drone can you which would have limited memory limited.\ncompute can you take your ai models and make them run on these devices right so that's another push that.", "metadata": {"video_title": "Call for sanity in AI - Efficient Deep learning"}}
{"text": "compute can you take your ai models and make them run on these devices right so that's another push that.\nwe have and a lot of things are being done to address these constraints of power storage you want real-time.\noutput so you cannot have like you submit a sentence and the result comes back after one or two seconds.\nyou want it to be in milliseconds right and what if you have drones which are not connected to internet.\nright they're just stand alone can you do computations on the devices a lot of research is also happening in.\nmaking these large models work for small devices so i do how do you take these multi-billion parameter models and.\nmake them smaller while not compromising on the accuracy right so this is all i had so i'll just stop.\nhere there are a few reference material here that you could look at but i'm done with the lecture i.\nhope you enjoyed the history of course there are many other things which i might not have covered in particular.", "metadata": {"video_title": "Call for sanity in AI - Efficient Deep learning"}}
{"text": "hope you enjoyed the history of course there are many other things which i might not have covered in particular.\ni have not talked much about the advances in things like text to speech voice conversion even automatic speech recognition.\nbut the idea was to give you a very high view of how the field has evolved from early neural.\nnetworks to recurrent networks to transform the base models gans diffusion-based models and so on so with that i'll end.\nthis lecture thank you.", "metadata": {"video_title": "Call for sanity in AI - Efficient Deep learning"}}
{"text": "foreign [Music] simple but very effective technique so in data set augmented augmentation this is what you do right so.\nsuppose your training data set was about digit classification right and many of you would have experimented with the mnist.\ndataset uh so this these are some images given in the MS data set so this is say one of.\nthe images that was given and whose label is of course uh two right so this is the digit two.\nnow what you could do is that from this image you could create some other images right so what what.\nare the other images you could create you could Define certain uh transitions right so you could rotate this image.\nby 20 20 degree and get a new image right and that image the label would still be two so.\nyou don't need to manually label this you know that even if I rotate the image by a small amount.\nof course for some things like 6 and 9 if you rotate by 180 degrees then you will have a.", "metadata": {"video_title": "Dataset Augmentation and Parameter Sharing"}}
{"text": "you don't need to manually label this you know that even if I rotate the image by a small amount.\nof course for some things like 6 and 9 if you rotate by 180 degrees then you will have a.\nproblem but for most digits you will not have this problem right so you could rotate by a small amount.\nyou could shift it vertically right so the two being at the center of the image you could bring it.\ndown you could take it up and you would get a different image you could shift it horizontally or you.\ncould blur the image you could also change some pixels and add some noise right and both doing all of.\nthis you have just created more training data and what you're doing here is actually you are exploiting the knowledge.\nof the task right so you know that if you rotate the image if you do any of these augmentations.\nthat you have seen and there are many such more augmentations in the possible in case of image classification the.", "metadata": {"video_title": "Dataset Augmentation and Parameter Sharing"}}
{"text": "that you have seen and there are many such more augmentations in the possible in case of image classification the.\nlabel does not change so I can create a lot of data for free from the data that was queued.\nand now how does this help so now you have more training data and now it's no longer enough for.\nthe network to kind of uh overfit that okay whenever I see this combination of pixels I will mark it.\nas two because now that will not work for this guy because now the pixels have changed a bit the.\norientation of the pixels have changed and so on so this this will not uh it cannot easily overfit right.\nit has a harder task so you have given it much more training data that it has to by heart.\nnow right it has to memorize now and that will make it uh make it less prone to overfitting right.\nbecause there's just so much to learn that it cannot hold over fit possibly on all of this of course.", "metadata": {"video_title": "Dataset Augmentation and Parameter Sharing"}}
{"text": "because there's just so much to learn that it cannot hold over fit possibly on all of this of course.\nmodern deep neural networks will still over fit but it's found that doing this kind of data augmentation which comes.\nfor free it's not additional cost for you then why not just try it and you would be able to.\nget uh a lesser chance of overfitting in this case right that's all there is to this technique there's nothing.\nmore to be said here the only intuition here is that you have more training data you have more variants.\nof the same image and all of that you have to be able to classify as two so now this.\nchance of just memorizing and not understanding reduces a bit right that's all that happens and this works well for.\nimage classification object recognition it has already also been tried well for speech now it has also been tried for.\ntext there are libraries which allow you to do data augmentations but in text it's still a bit harder to.", "metadata": {"video_title": "Dataset Augmentation and Parameter Sharing"}}
{"text": "text there are libraries which allow you to do data augmentations but in text it's still a bit harder to.\ncome up with very meaningful augmentations uh the main difference is in image and speech you have like continuous signals.\nwhereas in text you have discrete uh you have just words whether the speech you have a continuous signal similarly.\nin image you have these pixels and so on so that's the main uh difference but even for text nowadays.\nthere are certain annotation techniques available right so that's all to be said about data set augmentation the next technique.\nis about parameter sharing and tying and we will see this a bit more detail when we talk about ah.\nconvolutional neural networks so in convolutional neural networks you have if you want to process an image you do what.\nis you use what is known as a convolutional filter and the same filter is applied to different parts of.\nthe image right so what I'm trying to show you here and this is of course become much more clear.", "metadata": {"video_title": "Dataset Augmentation and Parameter Sharing"}}
{"text": "the image right so what I'm trying to show you here and this is of course become much more clear.\nwhen we do convolutional neural networks is that I have a matrix you can think of there's a matrix of.\nWeights now instead of defining a separate Matrix for every part of the image I am going to use the.\nsame Matrix and apply it to different parts of the image right so that's the basic idea I'll not say.\nanything more at this point because you need to understand what convolution is and so on and we will see.\nthis in more detail when we reach there right okay and also in the case of Auto encoders which we'll.\nsee after a few lectures is also known as something known as weight tying so what you do in weight.\ntying is that if you look at this suppose this is n dimensional and this is D dimensional and this.\nis again n dimensional so then the weight Matrix that you will have here would be n cross T dimensional.", "metadata": {"video_title": "Dataset Augmentation and Parameter Sharing"}}
{"text": "is again n dimensional so then the weight Matrix that you will have here would be n cross T dimensional.\nright and the weight Matrix that you will have here would be D cross n dimensional let's call this p.\nand let's call this Q then what you could do is you could say that I'll just enforce P transpose.\nequal to q that means I'll not have a separate P Matrix here I'll just use Q transpose and Q.\ntranspose is indeed D cross n The Matrix that I was looking for so instead of having separate weights in.\nthese two layers I am using the same weight in both the layers right so that's called weight tying and.\nthis is also a common way of doing regularization it has also been tried in the modern Transformer based models.\nwhere you have eight different layers and multiple attention heads and you kind of do some parameter tying that means.\nyou across all the layers instead of having a separate Matrix of Weights in each of these layers you you.", "metadata": {"video_title": "Dataset Augmentation and Parameter Sharing"}}
{"text": "you across all the layers instead of having a separate Matrix of Weights in each of these layers you you.\njust reuse the weights across all the layers or you share or tie the weights across all the layers right.\nso this will again become make more sense when we do either when we do auto encoders or later on.\nwhen we talk about Transformers but for the sake of completeness I'm just mentioning it here here so you know.\nthat these are also regularization techniques right and this here the regularization in both these cases is clear right so.\nhere I'm using the same weight Matrix that means I am not using separate weights that means I'm using fewer.\nweights and fewer weights means smaller models smaller model means smaller complexity and the same thing I'm use doing here.\ninstead of using 2 times the number of Weights that I should have used I'm just using uh one time.\nthat right so I'm just using W instead of using two W's so again I'm reducing the model complexity right.", "metadata": {"video_title": "Dataset Augmentation and Parameter Sharing"}}
{"text": "that right so I'm just using W instead of using two W's so again I'm reducing the model complexity right.\nso in both these cases it should be clear how this is acting as a regularizer so I'll end this.\nvideo here thank you.", "metadata": {"video_title": "Dataset Augmentation and Parameter Sharing"}}
{"text": "foreign [Music] ER and we start looking at the decoder right so the decoder of course operates in the sequence.\nhere it will operate sequentially because it generates one output then that output feeds in as the input while generating.\nthe next output and so on right so now what does the decoder look like right so the decoder of.\ncourse takes input from the encoder also so as I said the output of the encoder is E1 up to.\ne t right this layer so this is of course fed as the decode right because the decoder needs the.\ncontext of what the input was what is it translating right so that contextual representation you have captured at the.\nlast layer and that you feed to the decoder so the decoder is going to get inputs from the encoder.\nit's also going to have his self inputs right which is the inputs of the words that have been decoded.\nso far right so it'll have these two types of inputs and let's see how it deals with these two.", "metadata": {"video_title": "Decoder block"}}
{"text": "it's also going to have his self inputs right which is the inputs of the words that have been decoded.\nso far right so it'll have these two types of inputs and let's see how it deals with these two.\ntypes of right so so what would be the output Dimension right so let's first look at that too so.\nwe know what the inputs are now we look at the output so the output and what do I need.\nto generate at the output again this is not new to you we have already seen this in the context.\nof rnns so each output here would tell you a probability distribution over the vocabulary right and you will take.\nthe arc Max from there right so you would want that if there are 37 000 words so that's what.\nthe number 37 000 is then you'll get the probability of each of these 37 000 tokens and hopefully uh.\nat the first position uh this should have been go actually right so let's assume there is a go input.\nhere and this is the first position so you hope that at the first position Nan has the maximum uh.", "metadata": {"video_title": "Decoder block"}}
{"text": "here and this is the first position so you hope that at the first position Nan has the maximum uh.\nprobability because that's the correct translation then at the second position Transformer has the right probability because that's the correct.\ntranslation and so on like you continue uh like that till you produce stop uh at the last position right.\nso your output is uh probability distribution over the entire vocabulary hence the output Dimension is one cross 37 000.\nwhere 37 000 is the size of the vocabulary so in general I should just say it is of size.\nV right that's what I should have said so now we understand what the inputs and outputs for the decoder.\nare so now let's just zoom into what the decoder actually contains right so just like the encoder the decoder.\nwould also be a multi-layered network so it would have say typically 6 to 12 8 any I mean depending.\non the kind of uh training data you have the amount of training data you have the kind of problem.", "metadata": {"video_title": "Decoder block"}}
{"text": "would also be a multi-layered network so it would have say typically 6 to 12 8 any I mean depending.\non the kind of uh training data you have the amount of training data you have the kind of problem.\nthat you're dealing with you would have any number of layers but the most basic ones have six eight or.\ntwelve layers right so there are 12 layers of processing happening and just as in the case of encoder each.\nof these layers has an identical structure right so whatever is happening within a layer is the same is just.\nthat the output of one layer acts as the input to the next layer right so that's the same as.\nwhat we had in the encoder but now what we need to look at is what is inside each of.\nthese layers right that's what we need to understand so let's zoom into one such layer so you will have.\nthe inputs so these are the uh small H number of time steps which have been decoded so far right.", "metadata": {"video_title": "Decoder block"}}
{"text": "the inputs so these are the uh small H number of time steps which have been decoded so far right.\nall the inputs have not been available because whatever has been decoded that comes in as input so whatever how.\nmany steps have been decoded so far only that many inputs would exist right then you have something known as.\nthe Mast multi-head self attention right uh so there's this Mast word which has been introduced here okay then again.\nat the output of one layer you have the feed power Network so this as I said is the same.\nexactly the same as what we saw earlier right so again the same thing is going to happen here you.\nwould have these inputs H1 up to h capital T right which would give you some intermediate outputs S1 S2.\nup to St and maybe there's some space here maybe something else would come uh so maybe you'll get some.\nother outputs uh let me call them M1 M2 up to NT and then finally all of that will pass.", "metadata": {"video_title": "Decoder block"}}
{"text": "up to St and maybe there's some space here maybe something else would come uh so maybe you'll get some.\nother outputs uh let me call them M1 M2 up to NT and then finally all of that will pass.\nto the feed forward Network and you will get the outputs Z1 Z2 up to set T right and then.\nthis process will repeat across layers and in the final layer you will have the output projection to a soft.\nMax layer right you'll have a soft Max layer which would predict a probability distribution over the vocabulary which will.\nhopefully Peak at the right word right so this is what is happening here so I need to explain what.\nis this mask and what is happening in between here right so these are the two things that I want.\nto explain which are different than what happens in the encoder right uh yeah so this is what happens so.\nyou have the encoder inputs coming right those were the E's right so in addition it's of its own inputs.", "metadata": {"video_title": "Decoder block"}}
{"text": "you have the encoder inputs coming right those were the E's right so in addition it's of its own inputs.\nwhich is the self part it also gets inputs from the encoder and helps you have a multi-headed cross attention.\nright cross because this is between the encoder and the decoder hence you have the cross attention layer then the.\noutput of the Cross attention layer goes to the feed forward Network right so again I will repeat this as.\nmany times as required at every stage you have I should not call it t but say suppose Capital T1.\nright because the number of words in the input may be different for the number of words in the output.\nright in Source language you might have six words in target language you might have eight words to say the.\nsame thing so you'll have Capital T1 inputs here again at this point Capital T1 intermediate representations would be computed.", "metadata": {"video_title": "Decoder block"}}
{"text": "same thing so you'll have Capital T1 inputs here again at this point Capital T1 intermediate representations would be computed.\nagain here Capital T1 intermediate representations would be computed and then again here Capital T1 output representations would be computed.\nthis is one layer then these T1 outputs would feed to the next layer and again the same processing would.\napp right and all of this this entire block is happening in parallel right all these computations are happening in.\nparallel of course like what I mean by that is all these T1 outputs are getting computed in parallel then.\nall these T1 outputs are getting computed in parallel then all these T1 outputs are getting computers right so now.\nwhat is this Mast and what is the multi-head cross attention these are the two things that we need to.\nunderstand okay so now we'll try to understand uh what this uh uh the Mast self-attention and the multi-headed cross.", "metadata": {"video_title": "Decoder block"}}
{"text": "understand okay so now we'll try to understand uh what this uh uh the Mast self-attention and the multi-headed cross.\nattention looks like right so we have these inputs so we'll assume right in any case now one question of.\ncourse is that in the case of the encoder you know that the sequences of length T because that is.\nthe input given to you right but in the case of the decoder how do you know what the sequence.\nlength is right because you don't know what the output is you're trying to generate the output so the length.\nof the output is unknown to you because unless you generate the stop signal or the stop word you don't.\nreally know what the length of the output is right so you'll assume some Max sequence length and let me.\ncall that Max sequence length as uh T1 right so that's for the max sequence limb that you will assume.\nbut now when you start producing the first word right at that time you have T1 inputs right of which.", "metadata": {"video_title": "Decoder block"}}
{"text": "but now when you start producing the first word right at that time you have T1 inputs right of which.\nonly one is valid because that's the go word and you know that okay go is the start signal that.\nI get but all of these are junk right you don't know what these are right and now what are.\nyou going to uh do uh for each of these you are going to compute the q k and V.\nvectors right so these are the capital T1 inputs given to you right and you're going to compute the q1.\nK1 V1 right the QQ Matrix the uh the query Matrix the key Matrix and the value Matrix right but.\nwhile doing that you are aware that whatever value you're getting for these guys right that does not make sense.\nwhatever key you are getting for these guys does not make sense whatever query you are getting for these guys.\nthis does not make sense because these are some junk inputs you don't even know right this might just be.", "metadata": {"video_title": "Decoder block"}}
{"text": "this does not make sense because these are some junk inputs you don't even know right this might just be.\na special symbol saying empty right you don't know what it is right now right so when you are Computing.\na refined representation for go right then let me call that as Z1 or rather sorry let me not make.\nthose mistakes again right so I'm going to call this as S1 right uh when I'm going to compute a.\nrefined representation for S1 I don't so S1 if you remember is going to be summation Alpha over all the.\nVJs where J's go from 1 to T1 right but from V2 to VT I don't trust my inputs because.\nthere are some junk inputs right I don't even know what symbols should come there so it does not make.\nsense for me to compute a refined representation using V2 to VT right so what I will do and that's.\nwhy this is called masked self attention is I'll I'll make these Alphas zero right so whatever Alpha is greater.", "metadata": {"video_title": "Decoder block"}}
{"text": "why this is called masked self attention is I'll I'll make these Alphas zero right so whatever Alpha is greater.\nthan the current decoded word I will make those zero right so Alpha 2 to Alpha capital T will become.\n0 so these V's will not participate in the computation so S1 would only depend on the representation of go.\nright and in the first case it makes sense right but now as you go deeper at the first case.\nyou might think this is just trivial right but now as you go deeper let's understand what will happen right.\nso now suppose you have decoded this much right you have go none and Transformer right so you have these.\nthree inputs available to you now you are Computing a represent refined representation Z1 Z2 Z3 all the way up.\nto Z T one right now what will you do when you're Computing Z1 your formula was Alpha into v.\nj j is equal to 1 to Capital T1 but now since you have decoded up till time step 3.", "metadata": {"video_title": "Decoder block"}}
{"text": "to Z T one right now what will you do when you're Computing Z1 your formula was Alpha into v.\nj j is equal to 1 to Capital T1 but now since you have decoded up till time step 3.\nyou will set you will let alpha 1 Alpha 2 Alpha 3 be whatever they were right and then you.\nwill set the remaining ones to zero right that's what you will do that's what masking does so the masking.\nmeans that zeroing out the attention weights which do not matter right I'm just doing a slight uh abuse of.\nexplanation here in the sense that uh the alphas need to sum up to one right so while you mask.\nout the zero you'll also do something so that these three now scale them up so that they become they.\nsum up to one right but that is internal detail what you need to understand conceptually is that when you're.\nComputing a refined representation of a Z1 you will set all the other Alphas to zero right similarly so this.\nI should call as alpha 1 J so this is Alpha 1 1 Alpha 1 2 Alpha One three now.", "metadata": {"video_title": "Decoder block"}}
{"text": "Computing a refined representation of a Z1 you will set all the other Alphas to zero right similarly so this.\nI should call as alpha 1 J so this is Alpha 1 1 Alpha 1 2 Alpha One three now.\nsimilarly when you're Computing the defined representation for Z2 your equation would be Alpha to J into v j and.\nagain Alpha 2 1 Alpha 2 2 Alpha 2 3 is what you care about and all the other Alphas.\nyou will set to zero right so that all that's all that masking does right now the question would be.\nthat what happens to Z4 right so I will I'm going to compute Z4 right Z4 would also have a.\nformula so in that formula what do I do right so you you will again have only these Alphas ready.\nin that formula all the other Alphas would not be of use but actually it doesn't matter right because again.\nwhen you are doing so the key thing to realize here is that these did not participate in the computation.\nright these gave you the uh corresponding q k and V but when it was time to use those V's.", "metadata": {"video_title": "Decoder block"}}
{"text": "when you are doing so the key thing to realize here is that these did not participate in the computation.\nright these gave you the uh corresponding q k and V but when it was time to use those V's.\nyou just zeroed out the waves right so irrespective of what your input was this did not participate in the.\ncomputation similarly whatever you compute here will not participate in the rest of the computation because the corresponding Alphas would.\nalways be zeroed out you'll always use this masking right here also you will have a masking so only that.\npart of the input which is currently being decoded is going to participate in the computations the rest of the.\ninput will get masked out because you will set the corresponding Alphas to zero right so that's what masking means.\nso we are done with the first masked self-attention block here which is it's just going to use a only.\nthose many entries right one to h means the number of time steps which have been decoded so far instead.", "metadata": {"video_title": "Decoder block"}}
{"text": "those many entries right one to h means the number of time steps which have been decoded so far instead.\nof using one to Capital T1 it will only use those many entries which have already been decoded right so.\nthat's what you will have here at this point you'll have Z1 Z2 Z3 it also have some other things.\nbut you don't care about them because they will not participate in the rest of the computation right so now.\ngoing forward you have the encoder representation coming up so this was the Mast self attention now this is the.\nMast cross attention right this is the layer which is new I need to understand what is happening in that.\nlayer right so let's see what happens there yeah so you have this S1 to T right all the s.\nrepresentations are computed and now again the ease that you had right the uh you had these capital T representations.\ncoming out from the encoder you'll again pass them through your standard QE uh key and value matrices right and.", "metadata": {"video_title": "Decoder block"}}
{"text": "coming out from the encoder you'll again pass them through your standard QE uh key and value matrices right and.\nthen you will have to words within the decoder or the representations coming from the decoder as the query right.\nso whatever has been decoded so far that will become the query and then you will pass it through this.\nnormal attention Network right so now what happens is the following that let's understand this conceptually so This S1 to.\nSt so far right I should call it S1 to S small Edge right that is the number of words.\nthat have indicated so far those have been uh refined representations of your original words by looking at the context.\nalso right the context came in through that summation Alpha V right so these are refined representations already but taking.\ncare only of self attention that means within the decoder now you want to refine these representations further in the.\nlight of whatever you had seen in the encoder right so now you want to take S1 as the input.", "metadata": {"video_title": "Decoder block"}}
{"text": "light of whatever you had seen in the encoder right so now you want to take S1 as the input.\nyou want to generate a q1 from that okay then using q1 and these K1 K2 up to K capital.\nT where capital T was the number of tokens in the uh encoder right you will compute the corresponding Alphas.\nright so you'll have alpha 1 1 Alpha 1 2 Alpha One T so these Alphas tell you how much.\nattention should you give to the ith input in the encoder while Computing a refined representation for the jth decoder.\nvalue right and now using this you will compute your Z1 as whatever Alphas you have computed Alpha 1j into.\nthe values which are coming out of here right so these are going to be some VJs and you're going.\nto sum over J equal to 1 to T right the same computation happens again you have the key query.\nand value it's just that now the attention is between the encoder the decoder representation and the encoder representation and.", "metadata": {"video_title": "Decoder block"}}
{"text": "and value it's just that now the attention is between the encoder the decoder representation and the encoder representation and.\nyou're Computing a new representation for the decoder using an attention weighted sum of the encoder values right so that's.\nwhat is happening in the Mast uh cross attention you are doing a cross between the encoder and the decoder.\nand at this level you will then perhaps get out I don't know what I was calling it earlier but.\nmaybe I'll just call it M1 up to M T1 and then these will pass through the feed forward Network.\nto give you Z1 Z2 up to Z D1 right so this is what the entire decoder block looks in.\nthe new edition was this cross attention where now the key and the value come from the encoder and the.\ndecoder brings the query and this Mast self-attention where the only concept was of this masking that The Words which.\nhave not been decoded so far you don't let them participate by setting the corresponding Alphas to zero right so.", "metadata": {"video_title": "Decoder block"}}
{"text": "have not been decoded so far you don't let them participate by setting the corresponding Alphas to zero right so.\nthat's all there is to learn about the decoder for now right now the Last Detail that we need is.\nthe output from the decoder so you have multiple such layers within the decoder right and let's assume this is.\nthe last layer which again has the same identical structure it receives the inputs from the previous layer it has.\nthe Mast self-attention must cross attention then the feed forward Network and then whatever comes out right so suppose you.\nget these five and two dimensional embeddings right uh and at each stage you want to predict what the next.\noutput is going to be so you take this 512 dimensional input embedding uh use a matrix of size 512.\ncross V to give you a v dimensional output and then you apply a soft Max on that to give.\nyou the distribution over the vocabulary and then pick the arc Max from there to feed it as the input.", "metadata": {"video_title": "Decoder block"}}
{"text": "cross V to give you a v dimensional output and then you apply a soft Max on that to give.\nyou the distribution over the vocabulary and then pick the arc Max from there to feed it as the input.\nfor the next time step right so that's what happens at the output layer right so we have seen the.\nfull encoder decoder we have seen all the blocks within an encoder layer so the encoder has many layers each.\nlayer is again composed of sub layers so you have the multi-added self attention and the feed forward neural network.\nthe decoder again has many layers each layer is composed of sub layers and you have three sub layers here.\nthe self attention within the decoder the cross attention between the encoder and the decoder then the feed power Network.\nand one key uh property or key difference in the decoder is that when you're looking at the inputs you.\nonly look at whatever has been decoded so far that's why you need to do this masking so that the.", "metadata": {"video_title": "Decoder block"}}
{"text": "only look at whatever has been decoded so far that's why you need to do this masking so that the.\ninputs which are not available so far do not participate in the computation by setting the corresponding Alphas to zero.\nright so that's all I have to say about Transformers I'll end this lecture here and this would probably be.\nthe last recording for the course I will see you in the next session if to clear any doubts that.\nyou might have but we are done with the syllabus for this part of the course thank you everyone.", "metadata": {"video_title": "Decoder block"}}
{"text": "foreign [Music] functions and initialization methods and these are all geared towards making deep learning or deep neural networks train.\nbetter right so we'll first do a quick recap set the context about why we are talking about activation functions.\nand initialization methods and then introduce a bunch of activation functions as well as initialization methods okay so let's start.\nwith a quick recap as I said so uh when you train neural networks so we started with this very.\nsimple Network which had just one parameter W in fact we started with two parameters wnb but I've just kept.\none parameter and we already saw how to train this network the idea was to use gradient descent or any.\nof its variants and the main ingredient there was to update the weight using some kind of an update rule.\nwhich internally contained the derivative right so this is the quantity that was important and we saw various variants of.", "metadata": {"video_title": "Deep Learning revival"}}
{"text": "which internally contained the derivative right so this is the quantity that was important and we saw various variants of.\nthe gradient descent algorithm but in all of these the gradient shows up in one way or the other right.\nso this is a quantity which is important ah and uh we also saw how to this how to compute.\nthis quantity right so uh we saw we had derived this for the simple Network and the key observation that.\nwe had made there was that the derivative is actually proportional to the input X right that's the one important.\nobservation that we had made and that also had kind of aided our discussion on what happens when the input.\nis passed because in most cases this x would be 0 and then we came up with these adaptive methods.\nand so on right so this observation we have made a column of couple of times before about the derivative.\nformula having this X as a factor and hence if x is large something can happen if x is small.", "metadata": {"video_title": "Deep Learning revival"}}
{"text": "and so on right so this observation we have made a column of couple of times before about the derivative.\nformula having this X as a factor and hence if x is large something can happen if x is small.\nsomething can happen and so on right yeah so uh then from this very thin and very shallow Network we.\nwent to wider Network which had many inputs but it's still a shallow Network there's only one layer input and.\nnow in fact there's no layer input and output that's it and even in this case when the update rule.\nRemains the Same it's just that the same update will applies to all the parameters and the derivative for any.\nparameter again shows up this red term here which is the input connected to that way right so again this.\nterm was showing up and if this is high low 0 and so on uh we saw what are the.\nramifications of that right similarly now if you have a thin network but a deeper Network we still use the.", "metadata": {"video_title": "Deep Learning revival"}}
{"text": "term was showing up and if this is high low 0 and so on uh we saw what are the.\nramifications of that right similarly now if you have a thin network but a deeper Network we still use the.\nderivative it's just that we compute the derivative using a chain rule but nothing else changes right I mean the.\nconceptually everything Remains the Same and again in this chain rule this H 0 has shown up here which is.\nagain the input to the network and this is for the weight W1 but in general for any weight you.\nhad some formula for the derivative we don't care what the actual formula was but all we care about is.\nthere was this term h of I minus 1 uh sorry this should have been suffix it's I minus 1.\nand not like h i minus 1 right so it's just the suffix is I minus one and for w.\none of course I minus 1 would be 0 so H 0 showed up here and at 0 was the.\nsame as X the input but for any layer if I'm looking at this layer then the derivative of the.", "metadata": {"video_title": "Deep Learning revival"}}
{"text": "one of course I minus 1 would be 0 so H 0 showed up here and at 0 was the.\nsame as X the input but for any layer if I'm looking at this layer then the derivative of the.\nloss function with respect to this weight is going to be proportional to H2 that means the input that this.\nweight was connected right so the H's are the inputs coming from the previous layer they are of course also.\nthe output of some layer but for this current layer they are the input right so the derivatives are always.\nproportional to the inputs connected to the weight so that's the main observation that we had made and I'm just.\nrepeating that in this uh recap that we are doing right uh okay now uh if if there is a.\nnetwork which is deep and wide again we calculated the same thing we calculated the derivative of the loss function.\nwith respect to any weight by using this chain rule applied across multiple Parts not just one path but three.", "metadata": {"video_title": "Deep Learning revival"}}
{"text": "with respect to any weight by using this chain rule applied across multiple Parts not just one path but three.\ndifferent parts here and again we saw uh some uh formula for this and we derived this in quite detail.\nwhen we studied the back propagation algorithm right so now the question is are the points to remember right now.\nare that training neural networks is a game of gradients you have you compute gradients at every layer and then.\nyou use whatever variant or your favorite variant of the gradient based approach it could be momentum nag atom add.\na Max whatever you want to use but the derivators will get used inside them right and this gradient is.\nthe way of quantifying the responsibility of the parameter towards the loss the higher the gradient higher the responsibility lower.\nthe gradient lower the responsibility right and the gradient with respect to a parameter is proportional to the input connected.", "metadata": {"video_title": "Deep Learning revival"}}
{"text": "the gradient lower the responsibility right and the gradient with respect to a parameter is proportional to the input connected.\nto that parameter in the single or the input output Network this input was just X in a multi-layer network.\nit's just the input from the previous layer which is h i minus 1 right so that's these are things.\nto remember now things to uh wonder about are that we learned this back propagation algorithm right and we said.\nthat this is the basis for training all the Deep neural networks right and we saw feed forward neural networks.\nalready later on in the course we will see convolutional neural networks recurrent neural networks and then Transformers and for.\nall of them training happens using the back propagation algorithm right so is it that this back propagation algorithm was.\nsomething that was discovered in the last decade maybe around 2009 2010 and then deep learning became so popular because.", "metadata": {"video_title": "Deep Learning revival"}}
{"text": "something that was discovered in the last decade maybe around 2009 2010 and then deep learning became so popular because.\nwe have been uh we I mean kind of know that deep learning has been popular since 2009 2010 in.\nNLP maybe around 2014 and so on but in the last decade right so is it that around that time.\nthis algorithm got discovered and then we all started switching to deep neural networks no actually right so the back.\npropagation algorithm existed much before anything late 70s or even before that if for all I know but I definitely.\nknow that in 90 1986 there was this it was made popular in the context of neural networks by rumala.\nheart and team right so it has existed for a long time so what was happening since from 1986 to.\n2009 2010 when deep learning became really popular right why was deep learning not so popular in the 90s or.\nearly 2000s given that the algorithm used for training it existed back then right so what was stopping it from.", "metadata": {"video_title": "Deep Learning revival"}}
{"text": "early 2000s given that the algorithm used for training it existed back then right so what was stopping it from.\nbecoming popular so the issue is that while this algorithm existed in theory you know that you can train a.\ndeep neural network by just chaining the gradients and Computing the gradients using the chain rule in practice when you're.\ntrying to train deep neural networks as deep as four or five layers it was not very successful right and.\nwhat I mean by successful not successful is that the networks did not converge right did not converge reliably of.\ncourse a lot of other things have changed now you have faster compute so earlier if you had to use.\na certain number of flops then you would need so many days of computation now maybe you need a few.\ndays of computation so that has changed but in general there were other things other more uh theoretical things because.\nof which it was hard to train deep neural network it's not just a com issue of compute right so.", "metadata": {"video_title": "Deep Learning revival"}}
{"text": "of which it was hard to train deep neural network it's not just a com issue of compute right so.\nthere are three things that have changed since the 1990s right one is of course we have much more data.\nnow so if you have many parameters to train as a deep neural network would have you need larger amount.\nof data that we have you need faster compute we have that but there were some other things also which.\nneeded to fall in place and that's what we'll focus on in this lecture right so it's not that this.\nsuddenly got discovered in 2019 and then people started using deep learning right so until 2006 it was very hard.\nto train them and in 2006 there was a seminal work that I'll talk about which allowed us to train.\ndeep neural networks and that suddenly sparked or revived the interest in deep neural networks and from then on we.\nhave seen the success story which has led us to where we are currently in 2022 right so we will.", "metadata": {"video_title": "Deep Learning revival"}}
{"text": "have seen the success story which has led us to where we are currently in 2022 right so we will.\ntalk about what happened in this initial years from 2006 to 2019 which kind of helped us train this deep.\nneural networks and what was the effect of that and how is that connected to the lecture that we are.\nlooking at today right so that's going to be the focus so I'll end this video here and I'll come.\nback and talk about unsupervised pre-training which is something that happened in 2006 and enabled the training of DPR Networks.", "metadata": {"video_title": "Deep Learning revival"}}
{"text": "foreign [Music] methods are good right so what does that mean that if we average the output of several models.\nthen it always helps right but now in the context of deep neural networks this is of course a challenge.\nright so if you have to train several large neural networks then it is going to be prohibitively expensive right.\nbecause then you are going to combine these neural networks but then you have to train so many of those.\nright and what uh there are two options here right one is you train several neural networks having different architectures.\nso that's what I've shown in the first figure here where you have all of these are neural networks but.\nthey have different architectures or the other option is that you have the same neural network same architecture rather and.\nyou're going to train it using different subsets of the training data right now both these options are obviously expensive.", "metadata": {"video_title": "Dropout"}}
{"text": "you're going to train it using different subsets of the training data right now both these options are obviously expensive.\nright because training a deep neural network is going to be expensive and now you're talking about training K of.\nthose right and even if I were able to train K neural networks at test time again I have a.\nproblem right because in test time typically I need results faster but again every test instance that I'm going to.\nget I'm going to pass it through all the K uh neural networks and then combine their output rate so.\nit's not just a training time I have to have a lot of compute but even at test time I'm.\ngoing to have a challenge right so how do we then use this model averaging idea which we just saw.\nin the last video is useful how do you use it in the context of deep learning right so the.\ntwo challenges are that at training time your cost is high and even a test time your cost is high.", "metadata": {"video_title": "Dropout"}}
{"text": "in the last video is useful how do you use it in the context of deep learning right so the.\ntwo challenges are that at training time your cost is high and even a test time your cost is high.\nright irrespective of whether you use option one or option two right so Dropout actually addresses both these issues so.\nwhat it does is it effectively allows us to train several neural networks without incurring the cost of actually training.\nK neural networks right and we'll see how exactly it does that and it also at inference time we don't.\nneed to pass the test instance to several networks we just need to pass it through one neural network but.\njust do a simple trick to get the averaging effect right so we'll see how that works so the idea.\nin Dropout is that you have this original Network which I have shown on the left hand side you drop.\nout some units from it that means you drop out some new neurons from it so this is I've dropped.", "metadata": {"video_title": "Dropout"}}
{"text": "in Dropout is that you have this original Network which I have shown on the left hand side you drop.\nout some units from it that means you drop out some new neurons from it so this is I've dropped.\non some randomly some six neurons from here and then you get a different network right so you have got.\na different architecture now right so just as an option one uh you are training neural networks with different architectures.\nand you are training key of those I have got the same effect here by dropping some neurons so it.\nhad the base architecture I dropped some neurons and I now my neural network looks a bit different right and.\njust doing it temporarily just for that current training instance I'm going to drop a few neurons or that batch.\na mini batch I'm going to drop a few neurons right and then again do something different in the next.\ndevice right now each node is retained with some fixed probability right so every uh so every time I get.", "metadata": {"video_title": "Dropout"}}
{"text": "device right now each node is retained with some fixed probability right so every uh so every time I get.\na mini batch for every node I take a decision whether I should drop it or retain it and this.\ndecision is taken with a fixed probability right that means with 80 chance I'll retain it and 20 chance I'll.\ndrop right so that's uh the decision that I take so that every time I receive a mini batch I.\nvisit every node and decide whether to drop it or retain it of course it's not done like sequentially the.\nway I'm saying it uh you have more efficient ways of doing it but conceptually this is what you will.\ndo for every mini patch right now suppose the neural network has n nodes then using this Dropout idea how.\nmany neural networks can you create right so I have n nodes given to you and for every node I.\ncan either retain it or drop it right so it's a binary decision I'm going to take for every node.", "metadata": {"video_title": "Dropout"}}
{"text": "many neural networks can you create right so I have n nodes given to you and for every node I.\ncan either retain it or drop it right so it's a binary decision I'm going to take for every node.\nand each such configuration gives me a new neural network right so I just think of it that I have.\nthese n nodes here right and I'm going to write a 0 if I'm going to drop that node and.\na 1 if I am going to retain that load right so now this is my n dimensional vector and.\nthis will decide which neurons here get dropped or uh retained right and there are such two raised to n.\nvectors that I can construct right which means there are 2 raised to n networks that I can construct from.\na given neural network right so if I use the Dropout idea and if I have n neurons then I.\ncan actually construct 2 raised to n different neural networks right but then how does it help me I just.\nsaid that ah even if I have like K different neural networks and K was of course much smaller than.", "metadata": {"video_title": "Dropout"}}
{"text": "said that ah even if I have like K different neural networks and K was of course much smaller than.\n2 raised to n then I have a challenge in training it so now I'm talking about two raised to.\nn different neural networks which are possible so how am I going to train these uh many networks right so.\nthe trick that we use is that you share the weights across all the networks so you have two raised.\nto n networks but the weight matrices are the same right so now suppose this weight exists or this node.\nis retained suppose in such uh 2 raised to n by 2 of the networks right because half the networks.\nif you are retaining and dropping with 50 probability then half the networks would have this node how the networks.\nwould not have this node right so that means this weight will be retained in half the networks right but.\nthis weight would remain the same in all the networks right I'll be using the same copy of the weight.", "metadata": {"video_title": "Dropout"}}
{"text": "this weight would remain the same in all the networks right I'll be using the same copy of the weight.\nuh that's the first trick I'm going to use what's the implication of that we will see soon and the.\nsecond trick that we are going to use is that we are going to sample a different network for each.\ntraining instance so it's not that I have like these two raised to n networks created at beginning and then.\ntrain all of these two raised to N I just have one network whenever I get a mini batch I'm.\ngoing to sample one of these two raised to a networks that means I'm going to randomly drop some nodes.\nin the network and then train using that uh Network right so let's see what that means right it's like.\na bit difficult to understand but we will break it down into steps and then it should become clear right.\nso we initialize all the weights of the network so there are in this network this is my base Network.", "metadata": {"video_title": "Dropout"}}
{"text": "so we initialize all the weights of the network so there are in this network this is my base Network.\nright I'm not creating multiple copies of this this is my only Network so whatever weights are there so let's.\nassume there are n parameters everywhere uh sorry n nodes everywhere so you have have a n cross n weight.\nsitting here you have an N cross n weight Matrix sitting here I'm ignoring the biases for the while and.\nsuppose you have only one output neuron then you are n weights sitting here right so these all these uh.\n2N squared plus n weights I have initialized okay and I have started training now I received the first mini.\nbatch for training and I just drop some notes from the network right so I've got a thinned version of.\nthe network for the first mini batch right and now I'm just going to assume that this is what my.\nneural network looks like and just do my forward propagation and backward propagation right so I'll compute the loss using.", "metadata": {"video_title": "Dropout"}}
{"text": "neural network looks like and just do my forward propagation and backward propagation right so I'll compute the loss using.\nforward propagation and then I'll back propagate now when I do back propagation which are the parameters that need to.\nbe updated I had n square plus n parameters now if I do backward propagation which are the parameters that.\nI should update yet only the ones which had participated in the computation right so this parameter for example was.\nnot there this was because both these nodes were dropped so this parameter was not there this parameter was also.\nnot there this parameter was also not there so obviously they did not participate in forward propagation so they will.\nnot get updated in backward propagation right that's as simple as that so let's see what that means right so.\nthese were the ones which were active so these will get updated these will get updated and these will get.", "metadata": {"video_title": "Dropout"}}
{"text": "these were the ones which were active so these will get updated these will get updated and these will get.\nupdated right the other weights will not get updated and it's a bi-directional Arab that means in the forward propagation.\nalso these only participated and the backward propagation also only those will participate right so this is what I have.\ndone for the first mini batch that I received now when I received the second mini batch I again sample.\na new neural network from my original Network that means I just drop some other set of nodes right because.\nthis is a random process that every mini batch I'm going to decide for every node whether to retain it.\nor drop it so my decisions would change from one batch to another and I'll get a different version of.\nthe original neutral Network and now again I'm going to update only those weights which actually participate in the computation.\nright and now if you look at it right so let's look at some weight which was there this one.", "metadata": {"video_title": "Dropout"}}
{"text": "right and now if you look at it right so let's look at some weight which was there this one.\nright so if you look at this weight this was present in both the networks there were some other weights.\nalso which were present in both the networks maybe I should use a different color right so this weight participated.\nin both the networks so let me just call that weight as W okay so uh at time step one.\nI had updated the value of w using whatever update rule I wanted right let's assume I was just using.\ngradient descent so my value of w has changed now at time Step 2 since my weights are shared I.\nwill start with the updated value of w I will not start from W naught I'll start with W 1.\nbecause that weight has already been upgraded and then update it again using this equation right so that's what sharing.\nweights means these are two different networks conceptually but actually it's the same network from which certain weights have been.", "metadata": {"video_title": "Dropout"}}
{"text": "weights means these are two different networks conceptually but actually it's the same network from which certain weights have been.\ndropped right and those weights which are participated in all in the first batch as well as the second batch.\nfor those weights I have done two updates and for those weights which only participated in the first batch or.\nonly participated in the second batch I would have done only one update right so the main thing to notice.\nhere is that even though I am kind of it looks like I'm using a different neural network at every.\ntime step it's not the case because the weights are the same they are the shared weights and I just.\nstart from the previous value of the weight which was at the say the kth iteration and then update that.\nvalue I don't start with the value which was at the zero titration right okay yeah yeah so that's what.\nis being said in this slide so each thinned Network will get trained rarely because there are two raised to.", "metadata": {"video_title": "Dropout"}}
{"text": "is being said in this slide so each thinned Network will get trained rarely because there are two raised to.\nend networks right so it's very unlikely that this network if n is very large right which is typically the.\ncase that the same network will get sampled many times because there are two ways to end different configurations possible.\nso it was also possible that some of these networks will never get sampled at all right because if your.\nnumber of steps which you run the data for is less that run the training for is less than 2.\nraised to n right suppose you have a million nodes the two raised to million is going to be very.\nlarge and you're going to train for much fewer steps than that right so all these two raised to million.\nnetworks will not even get sampled some of them would get sample right but still because of its weight sharing.\nit's like every network is getting trained because even if this network was never sampled there were other networks which.", "metadata": {"video_title": "Dropout"}}
{"text": "it's like every network is getting trained because even if this network was never sampled there were other networks which.\ngot sampled in which these weights would have been active and hence those weights were getting regular updates right and.\nsince every uh weight is going to be present with an 80 probability because you are going to retain 80.\npercent with probability 80 percent you're going to retain a node right and also okay so 80 into 80 because.\nboth these nodes need to be remained so with 80 probability this will be retained and with 80 probability this.\nwill be retained so with 64 probability both will be retained and hence this way it would be retained right.\nso every weight with 64 probability it will be retained right and hence it will get updated many times it.\nwill get 64 percent of the times that you are doing training it will get updates if you run for.\na thousand steps every weight will get updated around 640 different uh times right at least 640 times okay so.", "metadata": {"video_title": "Dropout"}}
{"text": "a thousand steps every weight will get updated around 640 different uh times right at least 640 times okay so.\nnow what do you do at test time this is what you are doing at training time at training time.\nevery node was present only with probability P right now what do you do at test time at test time.\nnow again you cannot sample these two raised to n networks past the output through all of those and then.\ntake the final decision right so at test time you can use this neat trick which is saying that hey.\nevery node was only present with probability P that means it was only present P faction of the times right.\nso that's let's say 80 percent of the times or sixty percent of times whatever is the probability that you.\nhave chosen right so then I should trust the output of this node with only that fraction this guy only.\nparticipated in 80 of the discussions so whatever it says I only trust it with 80 value that's the same.", "metadata": {"video_title": "Dropout"}}
{"text": "participated in 80 of the discussions so whatever it says I only trust it with 80 value that's the same.\nas saying that whatever output this guy gives just scale it by that fraction as simple as that right so.\nyou participated only in 80 of the discussions so I'm going to trust your output with only 80 confidence that's.\nthe same as saying that whatever output you give which is going to be passed to the next layer I'm.\njust going to scale it down by B right so that's all you do at test time so again at.\ntest time you're just passing through a single Network and every weight in the network is going to get Scaled.\nor the output of every neuron in that network is going to be scaled by this probability P okay that's.\nall you are doing right uh so now what let's let's try to get some more intuition right into what.\nDropout is actually trying to do and why does it act as a regularizer right so it actually applies a.", "metadata": {"video_title": "Dropout"}}
{"text": "Dropout is actually trying to do and why does it act as a regularizer right so it actually applies a.\nmasking noise to the hidden knit so what does that mean is that in every you could think that Suppose.\nthere are n vectors n nodes here at every mini batch I am creating an N dimensional uh Vector where.\nsome values are one say 80 of the values are one and the remaining 20 are zeros right and that's.\nthe same as whatever output you produced here I'm going to multiply this by this mask right so if you.\nwere a masked out then you are not going to participate in the computation that means your output would be.\nzero because I'm just multiplying You by this Mass Vector so that is what is happening here some units are.\ngetting dropped and this is how you actually implement it it's not that every time you visit every node and.\ndecide whether to keep it or not you just create this random vector by saying that I want 20 of.", "metadata": {"video_title": "Dropout"}}
{"text": "decide whether to keep it or not you just create this random vector by saying that I want 20 of.\nthe units to be off and then at this layer whatever output you have computed you just multiply it by.\nthis mask so 20 of the outputs will be set to zero that's the same as this node not participating.\nin the computation at all right now when you're masking it what is it that you are effectively doing right.\nso what happens is it prevents the nodes from co-adapting right so what does co-adapting mean that if you have.\nall the nodes active right then there could be this one lazy guy who says that okay I mean this.\nother guy suppose I'm trying to detect faces and this other guy is going to detect noses a nose and.\nsomeone is going to detect eyes and I don't need to do anything then right I can slack I can.\nI did not actually work much the other guys can do the work right so that's what co-adapting means right.", "metadata": {"video_title": "Dropout"}}
{"text": "someone is going to detect eyes and I don't need to do anything then right I can slack I can.\nI did not actually work much the other guys can do the work right so that's what co-adapting means right.\nor it could be that hey you detect knows and I will detect ice I will only fire if there.\nis a clear eyes in the picture if it's taken from the side maybe the eyes are not visible and.\nyou fire only when there is no sweat of course this is not that the neurons are talking to each.\nother but this kind of co-adapting could happen where every neuron just focuses on doing one thing right but now.\nif you are going to drop the nodes randomly right so what will happen in that case suppose this was.\nthe guy which was focusing on detecting noses now you have received a training instance and that training instance say.\nit has a nose right and you have dropped this guy house now there is no node in this layer.", "metadata": {"video_title": "Dropout"}}
{"text": "it has a nose right and you have dropped this guy house now there is no node in this layer.\nwhich is detecting noses hence the other guys will have to wake up and say that hey I cannot rely.\non anyone else I cannot co-adapt because this guy is unreliable and sometimes he is active sometimes he is not.\nactive so better I also learn how to detect nose in addition to learning how to detect say ice right.\nso every node will now have to act independently and take more responsibility right so this is what actually happens.\nin a Dropout and I'll just now Flash the content on the slides yeah so either the multiple nodes need.\nto learn how to detect noses or other nodes to need to learn how to detect noses how to detect.\nthe face even if the nose detecting neuron is not active that means some other node needs to now detect.\nhow to uh I mean detect a face using ice let's say what the problem here is simple and I'm.", "metadata": {"video_title": "Dropout"}}
{"text": "the face even if the nose detecting neuron is not active that means some other node needs to now detect.\nhow to uh I mean detect a face using ice let's say what the problem here is simple and I'm.\nshowing you an image and I want to tell you whether there's a human face in it or not and.\nyou can use the features of a human face to detect right so some neurons could fire if their eyes.\nis there some neurons could fire if nose is there but now if you drop out things then some of.\nthese guys will no longer be available so then the other guys have to become creative and earlier maybe the.\nnetwork was never learning to rely on the eyebrows to detect a face but now some neurons will start picking.\nthat up because the other guys are not working so I better become more creative right so that way the.\nnetwork is becoming more robust right so that's all I had about Dropout and what I'll do next is I'll.\nquickly give you a summary of all the regularization techniques that we have studied need.", "metadata": {"video_title": "Dropout"}}
{"text": "foreign [Music] I just started talking a bit about errors I was moving the line a bit and saying over.\nhere we're making a few errors and so on let's let's just dig a bit deeper in this concept of.\nerrors and I'll also introduce error surfaces which will again stay with us for a long time in the course.\nright today we'll see something which is Trivial but this error surface or lost surface will keep reefers resurfacing and.\nwe'll see it quite often in the course at least in the initial part of the course right uh so.\nagain this is the example we were dealing with so now I'm looking at the uh and function now okay.\nand for the and function the output should be one only for one input which is shown in green here.\nand it should be 0 for the three red inputs right that's what the and function is and now my.\nuh decision is of course W1 it's uh sorry yeah so it's w naught plus W1 X1 plus W 2.\nx 2 greater than equal to 0 then the output would be 1 otherwise it would be zero right so.", "metadata": {"video_title": "Errors and error surfaces"}}
{"text": "uh decision is of course W1 it's uh sorry yeah so it's w naught plus W1 X1 plus W 2.\nx 2 greater than equal to 0 then the output would be 1 otherwise it would be zero right so.\nnow let me just fix this as W naught s minus one right so that is what I have done.\nhere so my w naught is fixed to minus 1 okay again I did the same thing okay okay so.\nw naught is minus one and I will try different values of w one W two right so it just.\nso happens that the first value that I tried was W one equal to minus 1 w t equal to.\nminus 1 and that is the line that I have drawn here so this since W naught is minus 1.\nof course the Y intercept is minus 1 and then I have minus x 1 minus x 2 and this.\nis the line that I get now what is wrong with this line yeah so obviously if you look at.\nthe negative half space of the line which is shown in the red region right and now this is an.\nexample where it's clear that it's not always correct to say lies above the line right because these are all.", "metadata": {"video_title": "Errors and error surfaces"}}
{"text": "example where it's clear that it's not always correct to say lies above the line right because these are all.\npoints which are lying above the line but they are actually in the negative half space so above and below.\ndoes not really Define negative and positive half space it simply boils down to equation the inequality right so minus.\n1 minus ah x 2 minus sorry let me just erase this and write it properly yeah so this is.\nthe equation of the line so all the points which satisfy this greater than equal to 0 lie in the.\npositive half space and this happen to be those points or the points in this region and all the points.\nwhich satisfy the inequality less than 0 are in the negative half space and it so happens that this entire.\nred region is those points right so it's not above or below it's about ah whether greater than equal to.\nzero or less than equal to zero which of these inequalities is satisfied okay so these these points line the.", "metadata": {"video_title": "Errors and error surfaces"}}
{"text": "zero or less than equal to zero which of these inequalities is satisfied okay so these these points line the.\nnegative half space as shown in the red region and then of course one of the points which should not.\nhave been in the negative half space which is the point one comma 1 is lying in the negative half.\nspace so with these particular values of the perceptron or the weights of the perceptron function I am making error.\non one of the four inputs right so if I just want to tabulate so I took the W and.\nW2 values as minus 1 minus 1 and then I made an error of one of the inputs now let.\nme take the values 1.5 and 0 okay let me try those values so I will oops yeah I'll make.\nthis as 1.5 ok so this is 1.5 and I want W2 to be 0 so I'll just make it.\n0. okay so this is the line that I get and I do not of course I have fixed at.\nminus one and again you can see the negative and the positive half spaces so I'm again making an error.", "metadata": {"video_title": "Errors and error surfaces"}}
{"text": "0. okay so this is the line that I get and I do not of course I have fixed at.\nminus one and again you can see the negative and the positive half spaces so I'm again making an error.\nof 1 it's a different error this time one of the points which should have been in the negative half.\nspace which is this point uh is actually in the positive half space now right so again I made an.\nerror of one now let us try a different value of w one W two so I will take the.\nvalues 10 and minus 10 right so I'll just make this 10 and I'll make this as minus 10 okay.\nnow this is what is happening right so again uh I'm making an error of 2 because one of the.\nred points is in the positive half space and the Green Point is actually in the negative half space right.\nso only these two red points uh which are 0 1 and 0 0 have actually been ah are in.\nthe right place right they are in the negative half space and they should be in the negative half space.", "metadata": {"video_title": "Errors and error surfaces"}}
{"text": "so only these two red points uh which are 0 1 and 0 0 have actually been ah are in.\nthe right place right they are in the negative half space and they should be in the negative half space.\nbut the other two points I am making so what is happening here is that as I'm changing the value.\nof w and W 2 the error or the number of Errors is changing right so now I can think.\nof error itself why is this not working yeah I can think of error as a function of w 1.\nW 2 because I have fixed W naught right and depending on the values of w 1 W 2 y.\nthat I try the error is changing so now I think of error as a function of W1 W2 and.\nnow once you have digested that idea we are ready for the next slide where I have error which is.\non the Z axis okay as a function of let me just try to see if I can zoom into.\nthis yeah so error is a function of W1 and W 2 right so this is my W1 this is.\nmy W2 and along the Z axis I am plotting the error and what are the values that the error.", "metadata": {"video_title": "Errors and error surfaces"}}
{"text": "this yeah so error is a function of W1 and W 2 right so this is my W1 this is.\nmy W2 and along the Z axis I am plotting the error and what are the values that the error.\nfunction can take integer values right it can either be zero error one error two error or three errors right.\nthose are the values that it can take so that is why you see this step function here so for.\ncertain values of w the error would be 0 which is the dark blue region for certain values it would.\nbe 1 which is the purple region for certain values it would be 2 which is the orange region and.\nfor certain values it would be 3 which is the yellow region right so that's those so now what I.\nhave done is I've taken all possible values of oops yeah I've taken all possible values of w one W.\ntwo I've just restricted to minus four to plus 4 because that's all I can show on the plot and.\nthis and I have substituted that those values in the equation and then just counted the number of errors that.", "metadata": {"video_title": "Errors and error surfaces"}}
{"text": "this and I have substituted that those values in the equation and then just counted the number of errors that.\nI made so we were doing that visually but I could just write a program to calculate those errors and.\nthat's how the code there was displaying it there was two errors one errors and so on and now these.\nare the set of values for w and W2 if I plug in these values and you can go back.\nand check them the error would be 0. these are here right these are the values of w one and.\nah W 2 right for which the error is going to be 1 and so on 2 and 3. so.\nwhat I have done here is I have actually ah yeah so the question here is for this particular case.\ncan the error not be 4 right so now you can see so any line that you draw okay so.\nthe only possibilities are that all the Four Points lie in the uh in the positive half space in which.\ncase you would be making an error on three points or all the Four Points lie in the negative half.", "metadata": {"video_title": "Errors and error surfaces"}}
{"text": "case you would be making an error on three points or all the Four Points lie in the negative half.\nspace that's the worst case scenario right in which case you will be making an error on three points right.\nbecause one one of the two things would be right right I mean it would either be the positive point.\nwould be correct or the negative points will be correct so that's why it can be only maximum three you.\ncannot make an error on all the Four Points um okay so now what I've done here actually is just.\nuh plotted the error for all possible values of w one W two and now from the plot I can.\nsee which are the values which are useful right and I can just pick any of those W and W2.\nvalues which correspond to that dark blue region of the plot right ah so this is what we have done.\nbut this is okay visually to do but ideally we would want an algorithm such that I don't now I.\ncan plot this visually for two inputs right but if I had n inputs I can't even plot and visualize.", "metadata": {"video_title": "Errors and error surfaces"}}
{"text": "but this is okay visually to do but ideally we would want an algorithm such that I don't now I.\ncan plot this visually for two inputs right but if I had n inputs I can't even plot and visualize.\nit right so now I'm looking for an algorithm just as visually by looking at this plot I can find.\nout what they write w and W two values are can I have an algorithm which allows me to find.\none such W one W two such that if my input is linearly separable then for that value of w.\nand W 2 my positive points would be on the right side and my negative side points would also be.\non the right side right on the correct side or in the right half space right so that's the algorithm.\nthat we are looking for and that's the perceptron learning algorithm that we will see in the next module thank.\nyou.", "metadata": {"video_title": "Errors and error surfaces"}}
{"text": "foreign [Music] data as D which contains M plus n points where you have M ah or other n training.\npoints and M test points right or the other way around as we defined on the previous slide now we.\nalso know that ah some true function exists such that Y is f of x plus Epsilon what does that.\nmean that Y is is related to x i by some true function f but there is also some noise.\nEpsilon in the relationship right just like in any in relation that you might have some noise so you ideally.\nwant Y is equal to f of x but there might be some minor noise and for Simplicity we assume.\nthat this noise is zero Center that means it has zero mean and a small variance Sigma square right so.\nthat's what we are going to assume and of course we do not know F right if you knew F.\nthen there's nothing to do but we don't know f so what we do is we approximate f using F.\nhat and then we estimate the F hat the parameters of f hat using uh the training data which is.", "metadata": {"video_title": "Estimate error from Test data"}}
{"text": "then there's nothing to do but we don't know f so what we do is we approximate f using F.\nhat and then we estimate the F hat the parameters of f hat using uh the training data which is.\ngoing to be a subset of this capital D right so I am going to refer to the training data.\nas T and it's going to be a subset of the D which I have defined earlier and ah Y.\nis going to be F hat of X now right so that is the approximation that we have made instead.\nof Y is equal to F X Plus Epsilon which was a true relation and we did not know F.\nwe have come up with some approximation F hat and we have freedom in choosing what F hat we want.\nto choose from like a family of functions and we could have different parameters for that right so we are.\ninterested in knowing ah the expected value of F hat x minus f of x the whole square right so.\nthis is the mean square error ah indeed so here expectation stands for the mean this is the error this.", "metadata": {"video_title": "Estimate error from Test data"}}
{"text": "interested in knowing ah the expected value of F hat x minus f of x the whole square right so.\nthis is the mean square error ah indeed so here expectation stands for the mean this is the error this.\nis the square of the error so this is the mean square error so the simply put the square of.\nthe difference between the predicted value from R approximation and the True Value and this expectation computed over a large.\nnumber of samples right that is what we are interested in estimate but of course we cannot estimate this directly.\nright because this requires us to know f of x and we do not know f of x right so.\nhow do we compute this because this has a quantity that we do not even know right but we will.\nstill be able to estimate this empirically because although we do not know F we do not know the true.\nfunction f we have been given a lot of training points right where these y's are known so those y's.", "metadata": {"video_title": "Estimate error from Test data"}}
{"text": "function f we have been given a lot of training points right where these y's are known so those y's.\nwe know are coming from some relation f of x plus Epsilon so even if you don't know what the.\nrelationship is we are given some of these x's and the corresponding y's so we know the X comma y.\npairs right so we have some idea of what f of x looks like for the given X's that we.\nhave we do not know the full function but at least for the X's that are there in our training.\nset we know what the Y's look like right so we have some of these and similarly F at X.\nis also we can compute because we can pass the X through the function that we have approximated or the.\napproximate function that we approximation function that we have used and that could give us y hat I right and.\nthen we could compute this as a difference so f x you could replace by Y and ah F hat.\nX you could replace by this y hat right this is y hat which I often call it as just.", "metadata": {"video_title": "Estimate error from Test data"}}
{"text": "then we could compute this as a difference so f x you could replace by Y and ah F hat.\nX you could replace by this y hat right this is y hat which I often call it as just.\nY and it's from the context it's clear whether I am referring to F at X X or the original.\nfunction there but now I will make it explicit that one is all I do not need to actually use.\ny hat here I can just say F hat X which you understand what is and f x is just.\nY which are the training points that are given to me right so now ah let us go further so.\nthis is the quantity that I am interested in then right so I have just replaced f of x by.\nY and F hat of X by y hat right ah so this is what the quantity is that I.\nam interested in and now I know that Y is actually f of x minus Epsilon right so I can.\njust substitute sorry f of x plus Epsilon so I can just substitute that and now I'm just going to.\nlike kind of open up this square and rearrange some terms right so I'm looking at this as a minus.", "metadata": {"video_title": "Estimate error from Test data"}}
{"text": "just substitute sorry f of x plus Epsilon so I can just substitute that and now I'm just going to.\nlike kind of open up this square and rearrange some terms right so I'm looking at this as a minus.\nB the whole Square where a is equal to F hat x minus f x and B is equal to.\nEpsilon right so that is how I have expanded this uh formula I just open the bracket rearrange some terms.\nand so on so a expectation of a sum is the same as the sum of the expectations so I.\ncan write this expectation of the three terms that I had a square minus 2 a B plus b square.\nas the sum of the expectations of these terms so expectation of a square minus expectation of 2 a b.\nuh plus expectation of b square right with a and b as defined earlier and of course a constant I.\ncan take outside ah so now if you look at it this is the quantity that I was interested in.\nand I did not know how to estimate it but now I have been able to rewrite that quantity in.", "metadata": {"video_title": "Estimate error from Test data"}}
{"text": "can take outside ah so now if you look at it this is the quantity that I was interested in.\nand I did not know how to estimate it but now I have been able to rewrite that quantity in.\nterms of things here and here I see some hope now because now I have this y hat and Y.\nhere which is just the training data that was given to me those are the Y's and the predictions that.\nare my model made so this is something I can estimate expectation of Epsilon Square I can estimate it's just.\nthe I'd already assumed that Epsilon comes from the normal distribution right so expect expectation of Epsilon square is just.\ngoing to be Sigma Square so that I can compute again here I have a slight problem but we will.\ncome back to this right so I have just now Rewritten the quantity of interest in terms of a set.\nof quantities of which some quantities I can compute and there is one quantity which I can still not compute.\nbut we will see how to deal with that ok so now what we will do is now we will.", "metadata": {"video_title": "Estimate error from Test data"}}
{"text": "of quantities of which some quantities I can compute and there is one quantity which I can still not compute.\nbut we will see how to deal with that ok so now what we will do is now we will.\ntake a small detour to understand how to empirically estimate an expectation and then return to our derivation let's see.\nwhat I mean by that so suppose we have observed the goals scored in K matches you are watching some.\nfootball uh a lot of football games and you've watched some 100 200 games and you have observed the number.\nof goals scored say 100 200 games between if you want the same pair of teams or uh all cities.\none team is constant there right so how many goals on average does say Brazil score or some other team.\nscores right I never observed this and now the way you compute the expected value we are interested in what.\nis the expected value of Z so our random variable here is z and it can take any value 0.\n1 2 3 4 5 6 7 maybe as many goals as you can score and you are interested in.", "metadata": {"video_title": "Estimate error from Test data"}}
{"text": "is the expected value of Z so our random variable here is z and it can take any value 0.\n1 2 3 4 5 6 7 maybe as many goals as you can score and you are interested in.\nthe expected value right now expectation as you define in probability of course is you have to have repeat this.\na large number of times but you only have like some K matches that you have seen the way you.\ncompute the estimate or you empirically estimate the expectation is by using this formula right this is something that we.\nhave been doing for ages right I mean in in our studies in high school engineering and so on where.\nwe just compute we are given a set of samples and we compute the average as using this formula and.\nthat average is actually the empirical estimate of the expectation right and the analogy to our our derivation where we.\nwere stuck would be the following that I am interested in the expected value of the square error between y.", "metadata": {"video_title": "Estimate error from Test data"}}
{"text": "were stuck would be the following that I am interested in the expected value of the square error between y.\nhat and Y and that I can just approximate from the data that I have so I have seen M.\ntraining points for each of those training points I know what my y hat is because I can pass it.\nthrough my F at X and get the answer I know what the true Y is because that was given.\nto me in the training data and I can compute the difference between them Square it and take the average.\nso this would be my empirical estimate for the expectation and that's all I can do right I cannot really.\ncompute the true expectation because that would mean to expectation or all possible X's that I can get and that's.\nnot possible to do for all the possible X's I need to compute the Y hats also know the true.\ny but if I already knew the true y for all possible X's then there's nothing to estimate right so.", "metadata": {"video_title": "Estimate error from Test data"}}
{"text": "not possible to do for all the possible X's I need to compute the Y hats also know the true.\ny but if I already knew the true y for all possible X's then there's nothing to estimate right so.\nI just have a small sample of reasonable size so I'm just going to empirically estimate the expectation using that.\nsample right so wherever I see expected value of y hat minus y the whole Square I am going to.\nreplace it by this formula okay so that is the main idea and that's why on the previous slide I.\nwas saying that this is something that we can compute and now I have shown you how to compute that.\nright so let us return back ah to our derivation now so this is the expectation this is the expected.\nerror right the true error that we were interested in Computing I call this a true error because this is.\nthe difference between the predicted value and the true function value right but we do not know the true function.", "metadata": {"video_title": "Estimate error from Test data"}}
{"text": "the difference between the predicted value and the true function value right but we do not know the true function.\nvalue so we cannot estimate this quantity and now we have written it as a sum of three terms of.\nwhich I just showed you how to compute the first one empirically right so now you could compute it empirically.\nfrom the data but there are two cases you can either use the test data to compute this or to.\nestimate this quantity or you could use the training data right so let us look at both the cases so.\nfirst we look at the case when we are looking at the test data okay so now suppose I look.\nat the test data then this is the true error that I am interested in now the first quantity here.\nI'm going to replace it by the empirical estimate that I just showed on the previous slide which is the.\naverage computed from the m test points that you had and the test points were the points from n plus.", "metadata": {"video_title": "Estimate error from Test data"}}
{"text": "average computed from the m test points that you had and the test points were the points from n plus.\n1 to n plus M I know what the predicted value is I know what the true value is because.\nI was given the data as x i comma y i pairs right so I had these pairs I know.\nthe what the true value is and I just compute this difference and this is of course easy to compute.\nthis is expectation of Epsilon square and I just told you earlier that explain expectation Epsilon comes from 0 comma.\nSigma Square so expected value of Epsilon square is just the variance which is Sigma Square so Sigma square is.\ngoing to be small so this is a small ah constant that will get added here and then you have.\nthis third quantity that you still don't know how to deal with right ah so let's just see how to.\ndeal with that quantity so what I've written here is that this is the covariance between uh these two uh.\nvalues right ah and the yeah so what I've written here is the this this quantity here I mean keeping.", "metadata": {"video_title": "Estimate error from Test data"}}
{"text": "values right ah and the yeah so what I've written here is the this this quantity here I mean keeping.\nthe two aside is actually the covariance between these two random variables Epsilon which is the noise and F at.\nx minus f x these two random variables and I'll tell you why I am saying that right ah so.\nyou have the covariance between X comma Y is e of X the expected value of x minus the MU.\nmean of X and Y minus the mean of Y ah r x is Epsilon so the mean of X.\nis of course 0 ah so the mean of X will disappear and then you have y minus mu y.\nnow I can open up the bracket so I get e of X Y minus ah X into mu X.\nand now again the sum of expectations is the expectation of sum so I can take the two terms out.\nso expectation of X Y minus expectation of X into mu y now mu Y is of course a constant.\nright because you compute the expectation and then that's a constant so that will come out so you have e.\nx y minus mu Y into e of X but again your V of X because X is Sigma is.", "metadata": {"video_title": "Estimate error from Test data"}}
{"text": "right because you compute the expectation and then that's a constant so that will come out so you have e.\nx y minus mu Y into e of X but again your V of X because X is Sigma is.\ngoing to be 0 so your ah what you remain with is e of X comma y right so in.\nthis case when Epsilon was a random variable with mean 0 this product between treating Epsilon as X and this.\nquantity as Y is actually the covariance between these two random variables right so that's that's something that we are.\nnoting now how do we use this fact what do we do with this is something that we will see.\non the next line right so so far ah so good uh we were interested in this we cannot estimate.\nthis so we extract estimated this as a sum of these three quantities of which the first two we know.\nhow to deal with the third one is still not clear right because it again has f of x so.\nwe need to see if we can somehow get rid of this third quantity or make some comment on that.", "metadata": {"video_title": "Estimate error from Test data"}}
{"text": "how to deal with the third one is still not clear right because it again has f of x so.\nwe need to see if we can somehow get rid of this third quantity or make some comment on that.\nright so that part is still remaining so let us look at the third part and make some observations about.\nit so ah remember that Epsilon here what is Epsilon actually so Y is equal to f of x plus.\nEpsilon so Epsilon is actually ah y minus f of x right ah so now you are trying to find.\nthe covariance right so this quantity as I said is the covariance so between Epsilon and this ah other random.\nvariable and I just told you that Epsilon is actually equal to Y minus f of x side we just.\nsaw that that simply comes from the fact that Y is equal to f of x plus Epsilon now the.\nstatement that I am going to make is that this y minus f of x is actually independent of f.\nhat x minus f of x why am I saying that because when I am talking about the Y's here.", "metadata": {"video_title": "Estimate error from Test data"}}
{"text": "statement that I am going to make is that this y minus f of x is actually independent of f.\nhat x minus f of x why am I saying that because when I am talking about the Y's here.\nI am talking about the Y's coming from the training data because this expectation we are Computing from the training.\ndata right oh sorry ah test data right so this expectation we are Computing from the test data whereas F.\nhat of X was estimated from the training data your test points did not participate in the X in the.\nestimation of f hat of X and these y's are actually corresponding to the test data right hence these two.\nrandom variables are actually independent because why the Y's that you are seeing here did not participate in F hat.\nof X that they did not have any say in the estimation estimated parameters of f hat X because this.\nbelongs to test data and the test data did not participate in the estimation of f hat X right so.", "metadata": {"video_title": "Estimate error from Test data"}}
{"text": "of X that they did not have any say in the estimation estimated parameters of f hat X because this.\nbelongs to test data and the test data did not participate in the estimation of f hat X right so.\nnow these two quantities are actually independent that means Epsilon and this quantity is independent right because y Epsilon is.\nsimply y minus f of x and that means the covariance between Epsilon and this other quantity is going to.\nbe 0. right because these two are independent random variables so there ah covariance is going to be zero right.\nso that's why you can say that in this case when you are trying to estimate these expectations from the.\ntest data your the covariance term is actually going to be 0. yeah so I mean sorry I should have.\nsaid it a bit differently so the expectation of e Epsilon comma this random variable can just be written as.\nthe product of the two expectations and ah so sorry I shouldn't have said that they are in sorry so.", "metadata": {"video_title": "Estimate error from Test data"}}
{"text": "the product of the two expectations and ah so sorry I shouldn't have said that they are in sorry so.\nthis is correct so the expectation of this quantity which was what you had here you can now since Epsilon.\nand the other random variable are independent you can write it as the product of their expectations and since the.\nexpected value of Epsilon is 0 you get the final answer as 0 right so in the case when you.\nare trying to estimate this expectation from the test data this quantity disappears right so then the true error is.\nactually equal to the empirical test error plus a small constant so what does this mean that if you are.\ngoing to X going to estimate the true error empirically from the test data which is the first quantity that.\nyou see here if you are going to estimate it empirically from the test data then your true error is.\nactually very close to the empirical test error so if this is what your estimate is going to be then.", "metadata": {"video_title": "Estimate error from Test data"}}
{"text": "actually very close to the empirical test error so if this is what your estimate is going to be then.\nyou are not making a very ah large gap right because there is just plus a small constant right that.\nis what we derived here because it was the sum of this quantity plus this quantity plus this quantity we.\nproved that this quantity is 0 this is a small constant so if I were to expect if if I.\nwere to estimate the true error using the test data then my uh exp my up my estimation is actually.\nvery good right so if I compute the error from the test data then I'm actually very close to the.\ntrue error right ah now let's see what happens if we had to estimate the error from the training data.\nso that's why whenever you want to estimate the model so now you have trained the model and you want.\nto see how good the model is it does not make sense as I'll show soon to estimate it from.", "metadata": {"video_title": "Estimate error from Test data"}}
{"text": "so that's why whenever you want to estimate the model so now you have trained the model and you want.\nto see how good the model is it does not make sense as I'll show soon to estimate it from.\nthe training data because the training does is always going to be optimistic right as you keep increasing the model.\ncomplexity it will go to zero but your test error might still be high and your test error actually gives.\nyou the true picture because if you empirically estimate the error from the test data then you are actually very.\nclose to the true error is what we have just derived.", "metadata": {"video_title": "Estimate error from Test data"}}
{"text": "[Music] now so where are we in the history right we did a bit of back and forth so we.\ncame from the 1950s where there was enthusiasm then the and the spring period then the winter period then the.\nmore stable period where people were still looking at things there were some success in the 1990s in terms of.\nconvolutional neural networks actually being used for real world problems and but still some disappointment in terms of very large.\nnetworks not being able to train despite back propagation being known and universal approximation theorem being understood right and then.\n2006 around that time these things changed and then from 2006 to 2012 a lot of other advances happened which.\ngave us a better understanding of deep learning and we were able to train these networks to work for the.\nimage net and similar challenges and started showing uh again it's not just gain but starting winning some of these.", "metadata": {"video_title": "Faster-Higher-Stronger"}}
{"text": "image net and similar challenges and started showing uh again it's not just gain but starting winning some of these.\nuh prestigious competitions right so that's where we are and then after that from 2016 onwards there was further acceleration.\nright so we wanted to come with better optimization methods so now we know that it's possible to train deep.\nneural networks but now can we do it faster can we lead to convergence faster right what this animation here.\nis trying to show that i want to come to this configuration where the two points on the left-hand side.\nfigure exactly fit or the sigmoid function exactly fits to those two points and i'm trying to solve some optimization.\nproblem for that and it's taking me a while to reach to the solution right so can i speed that.\nup right of course faster convergence was always a goal all right which existed in machine learning when we had.", "metadata": {"video_title": "Faster-Higher-Stronger"}}
{"text": "up right of course faster convergence was always a goal all right which existed in machine learning when we had.\nthe machine learning approaches which were again solved using optimization problems we always wanted faster conversions and nestrov in 1983.\nhad proposed a method which does better than the gradient descent approach which i had mentioned earlier right it leads.\nto faster convergence now that idea got scaled further and a series of uh optimization algorithms all of which we.\nare going to cover and discuss on most of this not maybe all of this adagrad rms prop adam and.\nadam adam w random and so on right this continues and all of these the goals was to have models.\nor have algorithms which lead to faster and better conversions right you lead in deep learning there are multiple minima.\npossible so can you lead to the can you reach to a better minima and faster time right and there's.", "metadata": {"video_title": "Faster-Higher-Stronger"}}
{"text": "possible so can you lead to the can you reach to a better minima and faster time right and there's.\nbeen a series of algorithms proposed to that which has led to now deep learning models being able to train.\nfaster in parallel that also be in progress in what are known as learning rate schedules now we have much.\nmore complex learning rate schedules than what we use 10 years back and all of that has led to stable.\nas well as faster training of deep neural networks of course still much more is desired but we have made.\na lot of progress and these advances are something that we'll cover in the course uh yeah this is still.\nabout the better optimization methods and then we also had better activation functions right so earlier uh in the 1980s.\nto 90s there's only the logistic function right this is the first function the blue colored function that you see.\non the slide here right but since then there's been like an industry of activation functions that has been proposed.", "metadata": {"video_title": "Faster-Higher-Stronger"}}
{"text": "on the slide here right but since then there's been like an industry of activation functions that has been proposed.\nand each of them with the motivation of stabilizing the training right or leading to better performance and faster conversions.\nright and many of these activation functions we are going to cover in the course tannic is something that we'll.\nsee we'll see relu we'll see leaky relu parametric relu and some of these other functions right galu-lu and so.\non right so many of these functions have been uh proposed and they all have shown to be useful in.\ndifferent contexts right so we will look at many of these throughout the course and all of this is also.\nled to better stability and performance right so so once we discovered that deep neural networks can really be trained.\nthere was a lot of interest in that and making them better and better and that's where better activation functions.", "metadata": {"video_title": "Faster-Higher-Stronger"}}
{"text": "there was a lot of interest in that and making them better and better and that's where better activation functions.\nbetter optimization functions better regularizations i forgot to include a slide on that so regularization in the terms of a.\ndropout or even batch normalization you could think of it as a regularization in some sense have been proposed to.\nimprove the training and stability of these networks right so that's what this uh period was about okay so so.\nfar we have discussed about uh the progress of deep learning in general and then zoomed into a bit of.\nimage processing where we uh or image applications where we talked about convolutional neural networks right the other important category.\nof problems that you deal with in your life and which are popular in the deep learning context is the.\nproblems involving sequences so let's look at what these problems are so you encounter sequences everywhere right so okay so.", "metadata": {"video_title": "Faster-Higher-Stronger"}}
{"text": "problems involving sequences so let's look at what these problems are so you encounter sequences everywhere right so okay so.\nwe encounter sequences everywhere right if you beat videos or if you have speech a speech is like a sequence.\nof uh phonemes right and we need to do speech processing we need to do video processing and these are.\nall inputs here are naturally sequenced right so we need sequence processing angles of course this need were discovered i.\nmean reid was realized long back and he as is the case with convolutional neural networks very long back in.\n1982 we had something known as an off field network i'm not going to the details of it it's not.\nsomething that we'll cover in the course also but it was something which was able to uh do some kind.\nof a sequence processing right more relevant to this course is this what is known as recurrent neural networks where.\nthe idea is that you have a sequence of inputs and you want to have some interaction between these inputs.", "metadata": {"video_title": "Faster-Higher-Stronger"}}
{"text": "the idea is that you have a sequence of inputs and you want to have some interaction between these inputs.\nright so for example when i'm talking it's not that every phoneme or every microsecond of speech can be ah.\na millisecond of speech can be analyzed independently it has some connection to what i just spoke previously right so.\ni should have networks which are aware that such dependency between the inputs or interactions between the inputs are useful.\nand should allow for those interactions right so the jordan network which is one time of recurrent neural network allowed.\nthe output at each time step to be fed as input for the next time step and that will allow.\ninteractions between different times so think of a sequence of images or sequence of frames which forms a video and.\nyou want to do some kind of video classification at the end so it's important that every sequence interacts with.", "metadata": {"video_title": "Faster-Higher-Stronger"}}
{"text": "you want to do some kind of video classification at the end so it's important that every sequence interacts with.\nthe other sequence so that it understands what is happening in the entire sequence as a whole so rick canadian.\nneural network allows us to do that it was it entered natural language processing in around 2013 or 14 but.\nthis idea has been there much before that right and there were two different types of neural recurrent neural networks.\nwhich were popular well the question of course is that if they were proposed in 1990 and if it made.\nsense for sequences and people were interested in a lot of nlp and speech at that time both nlp and.\nspeech have sequence of words and sequence of phonemes and why were these networks not being used again the same.\nchallenge a lot of work around that time which showed that it's very hard to train these networks in particular.", "metadata": {"video_title": "Faster-Higher-Stronger"}}
{"text": "challenge a lot of work around that time which showed that it's very hard to train these networks in particular.\nthere was this problem of exploding and vanishing gradients which will again cover in the course which did not allow.\nthese networks at that time to be used for uh real world problems right but as things changed after 2006.\n2006 is when we discovered hey we can suddenly strain deep neural networks and in the next seven eight years.\nwe made uh advances in optimization algorithms activation functions uh regularization and so on things started improving and uh again.\nin 1997 uh sorry i should have before going to 2006 i should have finished this in 1997 uh long.\nshort term memory cells lstms was proposed which again alleviated this problem of uh exploding uh of vanishing gradients in.\nparticular and but still they were not being used for real-world applications due to the other reasons which i had.", "metadata": {"video_title": "Faster-Higher-Stronger"}}
{"text": "particular and but still they were not being used for real-world applications due to the other reasons which i had.\nsaid right i mean compute was a challenge large scale data sets were a challenge but now when we enter.\n2014 a lot of things fell into place right deep learning got better we had a better understanding of how.\nto stabilize the training we had much more compute by this time gpus had already entered the scene we had.\nmuch more data large-scale data sets which allowed training these networks right so and that's when in 2014 uh deep.\nneural networks or recurrent neural network based models entered nlp and since then there's been almost no looking back right.\nall these traditional models which were probabilistic and statistical in nature of course you also have some probabilistic modeling here.\nbut the statistical machine translation models which were popular at that time uh were almost completely replaced by these recurrent.", "metadata": {"video_title": "Faster-Higher-Stronger"}}
{"text": "but the statistical machine translation models which were popular at that time uh were almost completely replaced by these recurrent.\nneural networks and lstms right and then in 2017 came by what are known as transformer networks and we'll have.\na separate section on them later on which then slowly started replacing these rn and lstm it was but it.\ntook a couple of years at least for that transition to happen so i think around 2014 to 2019-ish is.\nwhen this rnns and lstms which were proposed way back in 1990s and 1997 dominated the at least the nlp.\nand speech uh scene uh as far as uh uh deep learning was considered concerned and then they were slowly.\nreplaced by transformers right and today we are in the era of transformers and we'll have a separate section discovering.\nuh discussing the history of transformers later all right so i think i'll end this section here where we looked.", "metadata": {"video_title": "Faster-Higher-Stronger"}}
{"text": "uh discussing the history of transformers later all right so i think i'll end this section here where we looked.\nat a new type of input sequences and again the same story nothing new these models the need for processing.\nsequences was felt much earlier the relevant neural network models itself for proposing speech were proposed in 1986 1990 1997.\nbut at that time due to various reasons it was not conducive to train them on large scale data sets.\nwith good compute trained for longer durations they had also this vanishing exploding gradient problem some of these things were.\nfixed in the period of 2006 to 2014 and then they became popular in the sequence learning problems in nlp.\nand speech it and even envision for example captioning a video or an image okay now the next thing that.\ni would like to talk about is another class of problems where deep learning became very popular which was at.", "metadata": {"video_title": "Faster-Higher-Stronger"}}
{"text": "i would like to talk about is another class of problems where deep learning became very popular which was at.\ngame playing right and we started seeing various results where in extremely complex game environments uh rl reinforcement learning agents.\nstarted beating humans right so let's start around 2015 you have these atari games and reinforcement learning was always used.\nfor uh playing games so there was many uh classical textbooks on that where you had certain gaming environments like.\nthe pac-man environment or the ping-pong environment and you could train a rl agent to kind of learn how to.\nplay those games so these are those console based games that were popular many years ago right and in 2015.\ncame this deep reinforcement learning paradigm which was able to play all these atari games and win all of them.\nhands down right and beat humans at them what i mean by that then uh the one popular breakthrough which.", "metadata": {"video_title": "Faster-Higher-Stronger"}}
{"text": "hands down right and beat humans at them what i mean by that then uh the one popular breakthrough which.\nwas which came was this uh the reinforcement learning agent which could play this game of go right if you're.\nnot aware of go then in very layman terms i would tell you that it's something much more complex than.\nchess you all know that chess has many rules and there's an exponential number of possibilities there depending on what.\ni play and then there are many roots or many parts which open up right so go is something which.\nis even more complex than it is again a strategy based game and unlike previous algorithms which used brute force.\nkind of strategies this reinforcement learning agent based on deep reinforcement learning was able to beat the best go players.\nat that time right in 2015 and this of course gained a lot of popularity and attention and deep rl.\nbecame a very popular area and it still continues to be right then in 2016 again using the same uh.", "metadata": {"video_title": "Faster-Higher-Stronger"}}
{"text": "at that time right in 2015 and this of course gained a lot of popularity and attention and deep rl.\nbecame a very popular area and it still continues to be right then in 2016 again using the same uh.\ndeep reinforcement learning based players or agents people were able to beat professional poker players right i mean these are.\nthe people who participate in these world championships in pokers and they were able to beat many of them in.\nthe game of poker right similar advances in much more complex strategy games which have complex visual environments as well.\nas complex strategies which for example the defense of the ancients some of you would have played it and openly.\ni did this very interesting thing that it just made an agent play with itself right and it played like.\n10 000 years of gaming and just imagine that i mean we play i mean there's no way you can.\ncompare to this kind of scale right and so 10 000 years of training they did on of course a.", "metadata": {"video_title": "Faster-Higher-Stronger"}}
{"text": "10 000 years of gaming and just imagine that i mean we play i mean there's no way you can.\ncompare to this kind of scale right and so 10 000 years of training they did on of course a.\ndistributed set of compute and they were able to demonstrate that just by playing with itself the agent is able.\nto get this expert level performance and beat the best dota players at that time right so again very complex.\nproblems very strategy based games with the inputs also being complex you're getting the inputs in form of images right.\nit's not like some signals but an entire image is being fed and looking at the image you're trying to.\ndecide what the next step is going to be and there are various possibilities there various strategies to learn it's.\nable to learn all of this and being able to beat humans right and then around the same time to.\nkind of popularize this research in rl right where are these complex games which require this 10 000 hours of.", "metadata": {"video_title": "Faster-Higher-Stronger"}}
{"text": "kind of popularize this research in rl right where are these complex games which require this 10 000 hours of.\nplaying and so on maybe only the big deep tech companies can do that but can you have simpler environments.\nwhich have a lot of this complexity but you can you use those environments to train some of these or.\nadvance the research and deep reinforcement learning so that's why this open ai gym was released which became a toolkit.\nfor developing and comparing reinforcement learning algorithms because this entire deep reinforcement learning based work is something that we will.\nnot cover in this course but it's important that you are aware of that that's one branch which you could.\nexplore as you continue on your journey on deep learning right similarly there was another gym retro right a newer.\nversion of that which had thousand games very different environments which people could experiment with right and again complex strategy.", "metadata": {"video_title": "Faster-Higher-Stronger"}}
{"text": "version of that which had thousand games very different environments which people could experiment with right and again complex strategy.\ngames then uh similarly alpha star which uh has to balance to learn short-term and long-term goals these are all.\ngames which you are aware of these are complex strategy games which you play and you can imagine as humans.\nalso we take a while to understand what needs to be done and we are never perfect in any of.\nthese but now rl agents are being able to learn how to be play these games on their own right.\nthis was again a very interesting uh demo video which uh or of the work that they had done where.\nthey had these agents the the blue orange guys that you see in the screenshot there where you have this.\na flat floor right you can imagine it's a floor in which you could have some places where you could.\nhide you could construct a wall so that you can hide from the person who is chasing you if you.", "metadata": {"video_title": "Faster-Higher-Stronger"}}
{"text": "a flat floor right you can imagine it's a floor in which you could have some places where you could.\nhide you could construct a wall so that you can hide from the person who is chasing you if you.\nare a chaser you could destroy a wild wall to find to look for people inside or you could build.\na ramp you can see that yellow ramp in the figure so you could climb up the ramp and peep.\ninside the structure and see whether someone is hiding right and all of this the rl agents were able to.\nlearn uh quite well right that is the ability to chase and hide build a defensive shelter break a shelter.\nuse a ramp to search inside and so on as a very interesting demo video that you should see right.\nand what is emerging in all of this the underlying theme is that very complex decision making strategies were being.\nable to learn using these different deep reinforcement learning agents and while all of these specialized in like one task.", "metadata": {"video_title": "Faster-Higher-Stronger"}}
{"text": "able to learn using these different deep reinforcement learning agents and while all of these specialized in like one task.\nthen came this mu zero which can do multiple things right because you always aim for always one argument is.\nthat hey your ai agent can do only one thing whereas a typical human and you might be able to.\nbeat a human in that one thing but a human can do many other things right so if you have.\na good go player go rl agent maybe it beats the best go player but that player can also play.\nchess that could also play badminton could also play table tennis and so many other things whether our agent is.\ndoing one thing itself right and so now there's this push to have what are known as master of all.\nmodels so this mu zero was the first among the first step in that direction which could not only play.\ngo but also chess shogi and atari right these are again all uh decision making games and so it could.", "metadata": {"video_title": "Faster-Higher-Stronger"}}
{"text": "go but also chess shogi and atari right these are again all uh decision making games and so it could.\ndo multiple of these games uh in a single rl agent right so going towards more uh general intelligence right.\nand becoming a master of many uh traits right and similar research is progressing where you now have this player.\nof games a general purpose algorithm which learns to play in different conditions it's a lot of progress has happened.\nin this deep reinforcement learning over the years none of this will cover in the course but this is important.\nfor you to know as i said and you could explore this on your own once you understand the other.\nbasic concepts in the course so i'll end this module here.", "metadata": {"video_title": "Faster-Higher-Stronger"}}
{"text": "[Music] hi everyone uh welcome back uh today we'll uh start with feed forward neural networks and talk about the.\nback propagation algorithm right so where are we so far in the course right so we started with mp neurons.\nthen we moved on to perceptrons we dived quite a bit deep into perceptrons where we spoke about the representation.\npower of perceptrons that it's just a linear separator and it can only work for data which are linearly separable.\nthen we looked at the perceptron learning algorithm we also looked at why there's a guarantee that it will converge.\nfor linearly separable data right and then we moved on that okay this linear boundary or this threshold kind of.\na thing which we have in a perceptron that's that's not very convenient and in real world we take decisions.\nwhich are much more smoother so we move to the sigmoid neuron right and then for a single sigmoid neuron.", "metadata": {"video_title": "Feed forward neural networks"}}
{"text": "which are much more smoother so we move to the sigmoid neuron right and then for a single sigmoid neuron.\nwhich are just two weights uh w and b we looked at the gradient descent algorithm for learning these parameters.\nright and then we also spoke about the representation power of a kind of a network of sigmoid neurons right.\nand so we saw that if you have quite many layers in fact the universal approximation theorem said that if.\nyou have one non-linear layer then you could approximate any arbitrary function to the desired degree of precision right and.\nwe went through that statement we also saw an illustrative proof of what that statement does what that statement uh.\nmeans and then of course in the proof we had more than one layer but that was just an illustrative.\nproof the formal proof uh which is beyond the scope of this lecture you could prove it in with one.\nlayer right but uh it doesn't matter whether we had one or two or three i mean just a small.", "metadata": {"video_title": "Feed forward neural networks"}}
{"text": "proof the formal proof uh which is beyond the scope of this lecture you could prove it in with one.\nlayer right but uh it doesn't matter whether we had one or two or three i mean just a small.\nnumber of layers we were able to show that you could up our approximate arbitrary complex functions right and that's.\nthe main takeaway that we had and that was the power of the deep uh a network of sigmoid neurons.\nso now we are going to formalize this concept of uh deep layer of sigmoid neurons by uh introducing some.\nnotation and a network and then see when you have this deep network how do you learn the weights in.\nthat deep network okay so before i begin some references and acknowledgements i learned this from uh from the lectures.\nthat hugo larochel has on back propagation way back maybe six seven years back you could still they're still available.\non the net and you could take a look at them right so with that i start the first module.", "metadata": {"video_title": "Feed forward neural networks"}}
{"text": "on the net and you could take a look at them right so with that i start the first module.\nwhich is feed forward neural networks or a multi-layered network of neurons and in our case it would be it.\ncould be sigmoid neurons or any other non-linear function that we might consider right so let's start defining every uh.\ncomponent of this feed forward neural network right so at the input we have an n dimensional vector so these.\nare some n inputs that we might have and we have uh i think in the past refer to this.\noil uh drilling example have you referred to that the or whether i can find oil from a given location.\ni think you have done that where you have a bunch of parameters right so you might consider the uh.\nsalinity the pressure the temperature and so on right so these are some n variables that are given to you.\nand based on that you want to make a certain decision and this is an n dimensional input and i'll.", "metadata": {"video_title": "Feed forward neural networks"}}
{"text": "and based on that you want to make a certain decision and this is an n dimensional input and i'll.\nsoon be just referring to it i've already referred to as rn right so this is an input which belongs.\nto r n okay then we have a network which contains l minus one hidden layers okay and in the.\nexample that i'm going to show there will be just two hidden layers so the network has a total of.\nl layers of which l minus one layers are hidden and i'll tell you what the ellipt layer is going.\nto be so in this case i have shown two hidden layers and each of them have n neurons right.\nand for a large part of this discussion i'll just assume n equal to three right so i'll explain with.\nthe help of n equal to three but in general they're going to be n neurons right and also uh.\nit it need not be like all of these are n that's just for convenience i've taken it this could.\nbe n1 this could be n2 and this could be n3 and so on that could be different the value.", "metadata": {"video_title": "Feed forward neural networks"}}
{"text": "it it need not be like all of these are n that's just for convenience i've taken it this could.\nbe n1 this could be n2 and this could be n3 and so on that could be different the value.\nof n could be different across layers is what i mean but for introducing the concepts i'm just going to.\ntake n it does not have any bearing on the concepts that we learn it's without loss of generality it.\ncould be any n right or my explanation holds for any value of n so it's just for convenience that.\ni'm keeping it as n throughout okay finally there is an output layer containing k neurons right so you have.\nthese l minus 1 so that is 2 and then the third layer is going to be the output layer.\nin this example so i have a total of l layers in this case l is equal to three and.\nn minus one hidden layers and one output layer right now each neuron in the hidden layer and the output.\nlayer i'm going to split it into two parts right so what are these two parts uh i'll let the.", "metadata": {"video_title": "Feed forward neural networks"}}
{"text": "n minus one hidden layers and one output layer right now each neuron in the hidden layer and the output.\nlayer i'm going to split it into two parts right so what are these two parts uh i'll let the.\nannotations be for now so there's going to be a part called the pre-activation so the bottom part which is.\nthe white part that you see i'm going to refer it to as pre-activation and the shaded part is the.\nactivation right so this is how you refer to it in neural network terminology so you have the input x.\nthen you have a1 the bunch of outputs that you are producing the first hidden layer a2 the outputs that.\nyou are producing in the second hidden layer and a3 these are all the pre-activations that you have in these.\nlayers right now maybe i'll get rid of the activations and then you have the activation which is the shaded.\npart h1 h2 and then for the last layer it could be h3 or hl i could also call it.", "metadata": {"video_title": "Feed forward neural networks"}}
{"text": "layers right now maybe i'll get rid of the activations and then you have the activation which is the shaded.\npart h1 h2 and then for the last layer it could be h3 or hl i could also call it.\nas y hat because that's the output that i'm interested in so for the last guy the dark green or.\nthe shaded green guys there are multiple names right i could call it h l or the y hat because.\nit's the output or i can also call it as f hat of x right because this is my approximation.\nokay so they have the pre-activation and the activation now how to compute these uh pre-activations and activations is something.\nthat we'll see today but remember that each of these is a vector right so now here the entire input.\ni could call it as x and i already said that x belongs to rn and since i have assumed.\nthat all these layers the hidden layers have n neurons so remember that a1 is also rn right and just.\nto clarify this is a11 this is a12 all the way up to a 1 n similarly this is a.", "metadata": {"video_title": "Feed forward neural networks"}}
{"text": "that all these layers the hidden layers have n neurons so remember that a1 is also rn right and just.\nto clarify this is a11 this is a12 all the way up to a 1 n similarly this is a.\n2 1 a 2 2 all the way up to a to n right and the same analogy for h.\nevery a has a corresponding edge so just as you have a 2 1 a 2 up to a 2.\nn you'll have h 2 1 h 2 2 up to h 2 n right so these are vectors in.\nthis case they are vectors belonging to rn okay the input layer can be called the zeroth layer and the.\noutput layer can be called the l3 right so you have layer 0 hidden layer 1 hidden layer 2 and.\nthen the output layer i'm going a bit slow about this i'm being very deliberate about every statement that i'm.\nmaking because this is something that will stay with us for the rest of the course so these small things.\nthat okay the input layer is actually layer 0 the output layer is actually layer uh l is something that.", "metadata": {"video_title": "Feed forward neural networks"}}
{"text": "that okay the input layer is actually layer 0 the output layer is actually layer uh l is something that.\nyou would not need to remember right so i'm going to then soon start calling this layer h zero right.\nso h0 is the same as x in my notation okay okay now let's get rid of the annotations again.\nyeah so now every neuron in the previous layer so now this is the previous for this layer this is.\ngoing to be the previous layer right this is the previous layer so every neuron in the previous layer is.\nconnected to every neuron in the next layer by a weight right so there are n neurons here each of.\nthem is connected to n neurons in this layer right so how many weights would you have you would have.\nn cross n weights right so all those weights i'm going to put together in a matrix and call it.\nw 1 for here this is w 1 as already mentioned there so w1 belongs to n cross n and.\nthey'll also be a bias right so every neuron in this layer is going to have a bias connected to.", "metadata": {"video_title": "Feed forward neural networks"}}
{"text": "w 1 for here this is w 1 as already mentioned there so w1 belongs to n cross n and.\nthey'll also be a bias right so every neuron in this layer is going to have a bias connected to.\nit so i'll have n such biases so i'll have b1 equal to rn right and since i have all.\nthe hidden layers have the same number of neurons and i've also assumed the input also has the same number.\nof neurons for sake of convenience all of these are going to be n cross n matrices right so i.\nhave w1 which is n cross n and b1 which belongs to rn similarly i'll have w2 which is also.\ngoing to be n cross n because there are n neurons here connected to each of the n neurons here.\nso you'll have n square weights so that's the n cross n matrix and they'll again be n biases okay.\nthen the output layer of course here there are k neurons and here there are n neurons so each of.\nthese n neurons is connected to each of the k neurons in the output layer so you'll have a total.", "metadata": {"video_title": "Feed forward neural networks"}}
{"text": "then the output layer of course here there are k neurons and here there are n neurons so each of.\nthese n neurons is connected to each of the k neurons in the output layer so you'll have a total.\nof n cross k or k cross n weights right and you will have only k bias term so it's.\none for each of the output you know right so so that's the overall uh structure of the network so.\ni'll just quickly summarize input layer some hidden layers output layers within all the layers including the output layer you.\nhave the pre activation and then the activation we'll see how to compute the pre-activation and the activation all of.\nthese are vectors the a's and the edges the output layer is spatial you could call it h l or.\ny hat or f hat of x right and then you have weights every neuron in every layer is connected.\nto every neuron in the next layer i'm going to refer to the input also as a spatial layer as.", "metadata": {"video_title": "Feed forward neural networks"}}
{"text": "y hat or f hat of x right and then you have weights every neuron in every layer is connected.\nto every neuron in the next layer i'm going to refer to the input also as a spatial layer as.\nh0 right so that's and you also have biases connected to every neuron right so that kind of summarizes the.\nslide and you should so we'll do more of this but you should get used to these dimensions okay so.\nnow with that let's go to the next side now how do you compute the ai's okay so let's see.\nthat now remember a i is a vector so let me just focus on a 1 right so that means.\ni am focusing on this white part here okay so there are let's assume n equal to 3 for this.\nexample there are only three neurons that have drawn so i'm actually a 1 1 a 1 2 a 1.\n3 okay this is actually equal to the vector a right and how am i computing that i have b.\n1 1 b 1 2 b 1 3 plus so this is also a vector okay because i have three.", "metadata": {"video_title": "Feed forward neural networks"}}
{"text": "3 okay this is actually equal to the vector a right and how am i computing that i have b.\n1 1 b 1 2 b 1 3 plus so this is also a vector okay because i have three.\nbiases one bias for every neuron in neuron in this layer then i have w what was the dimension of.\nw it was a three cross three uh matrix i am going to squeeze it in here so i have.\nw1 this is the weight layer 1 matrix then the first element of it w 1 1 2 w 1.\n1 3 right and then w 2 1 w 1 2 1 w 1 2 2 w 1 2 3.\nw 1 3 1 okay let me just write this one all the way up to w 1 3 3.\nright so this is a 3 cross 3 matrix and that's going to get multiplied by x what is x.\nx again has three components the input x one x two x three right so this is how a is.\ngoing to be computed it's a simple matrix vector multiplication so this is a matrix vector multiplication and followed by.\na addition with a vector and all the dimensions here make sense right so a is equal to r a.", "metadata": {"video_title": "Feed forward neural networks"}}
{"text": "a addition with a vector and all the dimensions here make sense right so a is equal to r a.\nbelongs to r n b also belongs to r n we just did that on the previous slide w belongs.\nto r n cross n and i can multiply it with h i minus 1 h i minus 1 is.\nh 0 in this case which is just the input and that also i had assumed belongs to r right.\nso all this makes sense so this is just a simple matrix vector multiplication followed by an addition right so.\nthere's absolutely nothing uh fancy happening here right so just keep that in mind and that's how you compute a1.\nthat's all that's there's nothing more to it right so this is the pre-activation now what is the activation the.\nactivation takes the pre-activation right so this is and passes it through a function so what does that mean i.\nhave already computed a 1 1 a 1 2 a 1 3 okay and now i am going to pass.\nthis vector to a function right a 1 1 a 1 2 a 1 3 okay and now there are.", "metadata": {"video_title": "Feed forward neural networks"}}
{"text": "have already computed a 1 1 a 1 2 a 1 3 okay and now i am going to pass.\nthis vector to a function right a 1 1 a 1 2 a 1 3 okay and now there are.\nvarious functions which can operate on vectors but in this case this is going to be an element wise function.\nwhat does that mean that the output is just going to be g of a 1 1 g of a.\n1 2 g of a 1 3 right so i've just put the function inside that means it's just an.\nelement wise function these are known as element wise functions which operate on vectors that means they operate on every.\nelement of the vector right now if i choose sigmoid as the function then g of a 1 1 is.\nsimply going to be 1 over 1 plus e raised to minus a 1 1 right so that's all so.\nonce i have computed each of these a elements the computation of h is very simple and i'm going to.\ncompute these each of the edges so i'll get these three dimensional uh h1 right so this is what my.", "metadata": {"video_title": "Feed forward neural networks"}}
{"text": "once i have computed each of these a elements the computation of h is very simple and i'm going to.\ncompute these each of the edges so i'll get these three dimensional uh h1 right so this is what my.\nh1 looks like it's a three dimensional vector so this is what it looks like and every element there is.\nvery easy to compute because you already have the a's and you're just applying a function onto every element of.\na right so that's that's all there is right so this is called the activation function there are many activation.\nfunctions uh in the deep learning literature we'll be covering a few in this course we already saw the logistic.\nfunction from the sigmoid family there's also tannage function that could be linear later on we'll be seeing functions like.\nrelu uh leaky relu and so on and we'll be seeing a whole bunch of functions for g right and.\nall of them are going to operate element whether this one thing you need to just get into your heads.", "metadata": {"video_title": "Feed forward neural networks"}}
{"text": "relu uh leaky relu and so on and we'll be seeing a whole bunch of functions for g right and.\nall of them are going to operate element whether this one thing you need to just get into your heads.\nis that i'm talking about non-linear functions and all that but at the end it's just simple right it's just.\noperating element wise you give me a vector i'll take every element of the vector and pass it through a.\nfunction right and just tan h is a function that you know so i'm just taking every element of the.\nvector and passing it to the tanning function right so nothing very complex is happening there i mean in that.\none operation of course all of this gets combined and gives you a very complex composite function but in that.\nparticular computation there is nothing great right so the activation at the output layer is given by uh so you.\nhave alx right so this is your free activation at the output layer and i'm going to use a sum.", "metadata": {"video_title": "Feed forward neural networks"}}
{"text": "have alx right so this is your free activation at the output layer and i'm going to use a sum.\nspatial function right so i have called g as the functions for the hidden layers the activation functions for the.\nhidden layers but for the output i'll need some spatial function right so i'll tell you give you some intuition.\nwhy so example i cannot always use the sigmoid function at the output because the sigmoid function will only give.\nme values between 0 to 1 whereas in some cases my output could be greater than that so what if.\ni am trying to predict say the rating of a movie on a scale of 0 to 100 or the.\nscore of a test on a scale of 0 to 100. if i bound it to sigmoid function then it.\nwill only give me values between 0 to 1. so depending on the problem that i'm handling i would want.\nsome output functions which allow me to cover those range right so we'll do that in more detail in this.", "metadata": {"video_title": "Feed forward neural networks"}}
{"text": "some output functions which allow me to cover those range right so we'll do that in more detail in this.\nlecture but for now just remember that all the hidden activation functions i'm calling as g and the output activation.\nfunction i'm calling as o okay and one example of the output activation function is softmax the other is a.\nsimple linear function both of these we are going to see soon right now to simplify notation what i'm going.\nto do is i'm going to get rid of this right so it's understood that all of these are actually.\nfunctions of the input right i don't need to write it again and again that i have an input and.\ni'm computing a function of that input right it's understood that all of these are functions so i'm just going.\nto remove that instead of a i of x that converts something i'm just going to write it as a.\ni h i and so on right so that off x is something that i have deleted but it's understood.", "metadata": {"video_title": "Feed forward neural networks"}}
{"text": "to remove that instead of a i of x that converts something i'm just going to write it as a.\ni h i and so on right so that off x is something that i have deleted but it's understood.\nthat you have this input everything that is getting computed here all the white guys red guys and the green.\nguys they are all the functions of the input right so that's very obvious i'm not going to write it.\nexplicitly it just makes the notation more cumbersome right so i have introduced a feed forward neural network now in.\nthe previous lecture we saw this supervised machine learning setup right where we said that in machine learning you always.\nhave an input x you know there's some true function which x is between x and y but you don't.\nknow that so you come up with an approximation of that function and that is your y hat right so.\ni see a y hat here and this should have been f hat of x you can ask the t.\nis to correct that with the f f x right so i have the x here and i have the.", "metadata": {"video_title": "Feed forward neural networks"}}
{"text": "i see a y hat here and this should have been f hat of x you can ask the t.\nis to correct that with the f f x right so i have the x here and i have the.\ny hat but what is the function right i see what i'm seeing here is some kind of a network.\nbut what is the function f hat of x i mean can you write the output as a function of.\nx all these arrows and all make sense and i can see that somehow these computations are happening but if.\ni ask you to write that function so that function in the previous lecture was just the sigmoid function right.\nso i could write it it's 1 over 1 plus e raised to minus w transpose x plus b i.\ncould write that y hat explicitly now can i write this y hat explicitly or all that explanation that i.\nhad in the last class where f hat is your approximation and now instead of telling you what f hat.\nis i've just given you a diagram where is the f hat in this diagram right so that's something that.", "metadata": {"video_title": "Feed forward neural networks"}}
{"text": "had in the last class where f hat is your approximation and now instead of telling you what f hat.\nis i've just given you a diagram where is the f hat in this diagram right so that's something that.\nwe should try to figure out right what does the f hat look like and that should give you confidence.\nthat this is just yet another choice of function families within the machine learning paradigm where you could choose different.\nfunctions you could choose the linear function quadratic function and this is yet another function it just happens to be.\na very complex composite function but you can still write down what f hat is right so we will try.\nto do that so i'll go back to that setting of a typical machine learning supervised machine learning setup where.\nyou have data which comprises of x's and y's so that is there i have the x i have the.\ny's and i want to now come up with a model okay which is my approximation of the result of.", "metadata": {"video_title": "Feed forward neural networks"}}
{"text": "you have data which comprises of x's and y's so that is there i have the x i have the.\ny's and i want to now come up with a model okay which is my approximation of the result of.\nthe relation between x and y and here i'm not come up with a model i'm not given you a.\nfunction actually i've come up with a model but i've not given you what that function is i've just given.\nyou a diagram right so now can you write down that function right can you write down f hat so.\nyou could pause the video here and try to write it on your own and when you come back i'll.\njust talk about it okay so let me talk about it so this is what the function is right this.\nis exactly what the function looks like so let me explain you had the input x what did you do.\nto the input you multiplied it by the w1 weight and added the b1 vector what did you get here.\nyou got a one right you got this output okay then what did you do you passed that through a.", "metadata": {"video_title": "Feed forward neural networks"}}
{"text": "to the input you multiplied it by the w1 weight and added the b1 vector what did you get here.\nyou got a one right you got this output okay then what did you do you passed that through a.\nnon-linear function g and that function was simple in the sense that it was just operating on every element of.\na1 and then from here you got sorry oops i okay so this was a one this gave you h1.\nonce you had h1 that became the input for the next layer what did you do with that you again.\nmultiplied it by w2 right and then you added b2 and that gave you a2 then you passed it through.\nthis non-linearity and you got not g2 sorry h2 right then h2 became the input for the next layer so.\nyou multiplied it by w3 and then you added b3 and that gave you a3 and then you passed it.\nthrough a spatial output function to get h 3 which is the same as y hat which is the same.\nas f hat of x right so you have actually written the output as a function of the input it.", "metadata": {"video_title": "Feed forward neural networks"}}
{"text": "through a spatial output function to get h 3 which is the same as y hat which is the same.\nas f hat of x right so you have actually written the output as a function of the input it.\njust happens to be a very complex composite function right and let's just do a bit more about this so.\njust to make sure that we understand all the dimensions right so remember that this was our n okay this.\nwas r n cross n so i'm multiplying an n cross n matrix with an n-dimensional vector so the output.\nis going to be n-dimensional and then i'm adding an n-dimensional vector to it so that output is also going.\nto be n-dimensional then i am passing it to an element-wise function so that is also going to be n-dimensional.\nnow you can go along this chain and convince yourself that all the di all the dimensions are compatible right.\nso now that's that's the main thing i wanted to say uh yeah now one last thing i'll say suppose.", "metadata": {"video_title": "Feed forward neural networks"}}
{"text": "so now that's that's the main thing i wanted to say uh yeah now one last thing i'll say suppose.\ni have just assumed everything is n right for the sake of convince convenience now let me just assume uh.\nmake it different let's let this be p i'm sorry m and let this b be p now what would.\nthe weights w1 v w1 would belong to what so you have n inputs each of them connected to these.\nm neurons so you have m cross n weights so this would be m cross n okay and what would.\nb1 be you have one bias for every neuron in this layer so you will have m such right and.\nnow again you can see that these computations go well so this is an m cross n matrix multiplied by.\nan n dimensional input okay and then add it with an m dimensional vector right so m cross n multiplied.\nby n dimensional vector will give you an m dimensional output and then you can add it to a m.\ndimensional vector and then when you pass it to this element wise non-linearity you will again get an m dimensional.", "metadata": {"video_title": "Feed forward neural networks"}}
{"text": "dimensional vector and then when you pass it to this element wise non-linearity you will again get an m dimensional.\nvector right and now if the next layer has p neurons then this w 2 should belong to p cross.\nm and again this multiplication goes through this is p cross m multiplied by m which is a valid operation.\nand now you can again justify the whole series of computations that you are doing okay so i want you.\nto be comfortable with this computation so this should give you some com confidence that although this network diagram looks.\na bit complex there are so many connections going from one layer to the other layer at the end it's.\njust a series of matrix vector operations followed by some element-wise non-linearities which are very simple at every element you.\njust pass it through a function and you get the output right so it's not as complex as it looks.\nyou can actually write it down and even if they give you a hundred layered network technically you could still.", "metadata": {"video_title": "Feed forward neural networks"}}
{"text": "you can actually write it down and even if they give you a hundred layered network technically you could still.\nwrite it down it just would be a very laborious equation to write down that's all right now what are.\nthe parameters all the w's all the b's are the parameters and those are the parameters that you had introduced.\neverything else was just x but these are the parameters that you have introduced and now you want to learn.\nthese parameters so you'll use gradient descent with back propagation so we'll see what back propagation that is one of.\nthe main topics of this lecture and then what's the loss function you could use uh the squared error loss.\nfunction right so you are predicting k quantities you know what the true k quantities are right so you just.\ntake the squared error difference between those k quantities sum it over all the n training examples that you have.\nand then take the average so that's what your loss function is and you want to minimize this loss function.", "metadata": {"video_title": "Feed forward neural networks"}}
{"text": "and then take the average so that's what your loss function is and you want to minimize this loss function.\nthat means you want to learn the parameters theta which is all of these such that this equation or this.\nexpression gets minimized or this quantity l theta gets minimized right so i'm going to call this as l of.\ntheta this quantity gets minimized so you want to find theta which is a collection of all these parameters such.\nthat this quantity gets minimized so this deep learning still fits in the paradigm of machine learning that we had.\nthese five components which we had data model parameters algorithm objective function and we'll have to come up with objective.\nfunctions which will help in guiding the training uh criteria i mean guiding the training right so so we'll end.\nthis module here and then we'll come back and talk a bit more about output functions and loss functions.", "metadata": {"video_title": "Feed forward neural networks"}}
{"text": "foreign [Music] function that I want to talk about is gelu which is gaussian error linear unit it was first.\nintroduced in the context of Transformer model the first Transformer model around 2017 used this gelu activation function so let's.\nmotivate that so first we'll start with uh noting some similarity between relu and this hard thresholding right so this.\nis what our hard thresholding looks like it is 1 if x is greater than 0 and it is 0.\nif x is less than or equal to zero right that's what the hard thresholding looks like and now let's.\nlook at what the value function looks like the value function can also be written in a similar form as.\nf of x is equal to 1 into x if x is greater than 0 and maybe 0 into X.\nif x is less than or equal to 0 this is how I'm going to rewrite it on the next.\nslide so main thing to note here is that both the activation functions the output depends on the sign of.", "metadata": {"video_title": "GELU to SILU"}}
{"text": "if x is less than or equal to 0 this is how I'm going to rewrite it on the next.\nslide so main thing to note here is that both the activation functions the output depends on the sign of.\nthe input if the sign is positive then you get a certain output if the sign is negative then you.\nget a certain output right and now I'm just going to rewrite the radioactivation function as I said on the.\nprevious slide I could think of it as 1 into x if x greater than 0 or 0 into x.\nif x is less than equal to 0 right now uh the Dropout function also does the same right I.\nmean the Dropout idea of Dropout is that you have a certain number of neurons and if you apply the.\nmask then you could think of this mask as the activation function which either zeros out the neuron or lets.\nit pass through right and then apply some activation function on top of that right so it also does the.\nsame the only difference is that instead of one and zero it has this stochastic uh uh nature right so.", "metadata": {"video_title": "GELU to SILU"}}
{"text": "same the only difference is that instead of one and zero it has this stochastic uh uh nature right so.\nit's not always one not always zero it's decided based on a coin toss right so it could be uh.\nthe output is 1 into X if with probability p and 0 into x with probability 1 minus P right.\nso all of these uh you just I want you to notice the similarities between this right so this function.\nform it's looking similar but the conditions are a bit different it is X greater than 0 x less than.\nor equal to 0 here it is with probability P it is 1 into x with probability uh uh 1.\nminus P it is 0 into X right so then it does make sense right it makes a case for.\ncombining these ideas where in the relu you have this uh similarity to the hard activation function and Dropout brings.\nin this stochasticity so can you combine these two ideas and try to come up with a different activation function.", "metadata": {"video_title": "GELU to SILU"}}
{"text": "in this stochasticity so can you combine these two ideas and try to come up with a different activation function.\nand that's where we are headed so how about writing the following right that I have an activation function which.\nis M into X right and now m is a Bernoulli random variable so that means uh M takes on.\nthe value 0 or 1 with probability Phi X right so when I say I think you're used to this.\nBernoulli um of P which means that with probability P the output would be heads or 1 and with probability.\n1 minus P the output would be Tails or zero right so now instead of P the parameter is Phi.\nX and Phi X is itself a function of uh the input right so that's how you are setting up.\nthe activation function and now what could this Phi X be right so let's make some observations this Phi X.\nis supposed to be a probability right because it has to be the probability of heads or probability of one.\nso it should lie between 0 to 1. right so that's the idea the Phi of x should lie between.", "metadata": {"video_title": "GELU to SILU"}}
{"text": "is supposed to be a probability right because it has to be the probability of heads or probability of one.\nso it should lie between 0 to 1. right so that's the idea the Phi of x should lie between.\nuh zero to one right that's the uh range that it can have right now what is the function that.\nyou know which gives you output between zero to one so the obvious choice which comes to minus the logistic.\nfunction right so you could think of Phi of X as a logistic function of the input that you have.\nnow what is the input here let's try to understand that clearly right I am saying X but let's not.\nconfuse uh say this is your deep neural network and this is some hidden layer right let's call this the.\nH2 hidden layer okay and now you have this preactivation let me call it a21 and this is the activation.\nh21 right so this is the function that I am defining right h21 is equal to F of a to.\n1 and I have defined it as M into a to 1 where M itself is a function of a.", "metadata": {"video_title": "GELU to SILU"}}
{"text": "h21 right so this is the function that I am defining right h21 is equal to F of a to.\n1 and I have defined it as M into a to 1 where M itself is a function of a.\n2 sorry M uh m is a random variable which is Bernoulli distributed with the parameter Phi E21 right so.\nthat's the what I'm trying to that's how this whole picture is right so you have a neural network within.\nthat you have multiple hidden layers I'm looking at one of those hidden layers I'm looking at one of those.\nneurons and I want to define the activation function which is this and this is how I have defined the.\nactivation function and now you can see that X actually is a21 which is just the preactivation of that neuron.\nand now I want to uh get a value between 0 to 1 based on a21 so one of the.\nsimplest choice would be to just do one over one plus e raised to minus a218 this is the logistic.\nfunction that we had seen and we know that it gives values between 0 to 1 right so that is.", "metadata": {"video_title": "GELU to SILU"}}
{"text": "simplest choice would be to just do one over one plus e raised to minus a218 this is the logistic.\nfunction that we had seen and we know that it gives values between 0 to 1 right so that is.\none choice the other choice since we are in the domain of probability is to use the cumulative distribution function.\nof the normal distribution right so the probability density function of the normal distribution looks like this right this is.\na bell shaped and if you look at the cumulative density uh function then it again has this s shape.\nright because it again uh at minus infinity it's around zero and around 1 when you have integrated over the.\nentire uh possibility of values you will get a probability of one right because a cumulative density function tells you.\nP of X taking on values less than equal to X so P of x less than equal to Infinity.\nis going to be 1 right so as this approaches Infinity uh your CDF would hit one right so that's.", "metadata": {"video_title": "GELU to SILU"}}
{"text": "P of X taking on values less than equal to X so P of x less than equal to Infinity.\nis going to be 1 right so as this approaches Infinity uh your CDF would hit one right so that's.\nanother function that you can use so you could make the this function Phi of X as uh take it.\nas the cumulative density function uh and parameterize of of the normal uh uh distribution and then parameterized by this.\ninput that you have right so these are the possibilities so overall I mean I said a lot of things.\nlet me quickly summarize so we first do this analogy between relu and hard activation and then rewrote a value.\naccordingly then we try to see that relu and Dropout seem to be doing something with this stochastic uh difference.\nbetween them right Dropout does this stochastically withdrawal DP and minus P so why not bring that stochasticity into the.\nfunction itself and how I brought it in I have defined a random variable M right so now this random.", "metadata": {"video_title": "GELU to SILU"}}
{"text": "function itself and how I brought it in I have defined a random variable M right so now this random.\nvariable will take on certain value 1 or 0 with a certain probability and hence the activation function would differ.\nwould depend on X as well as have this stochastic feature because of this random variable and now this random.\nvariable needs to follow a certain distribution so it's a Bernoulli distribution because this random variable can take on value.\n0 to 1 so it's a binary random variable so it will follow a Bernoulli distribution and the bernalia distribution.\nhas a parameter p and now I am saying in this case the parameter P would just be a function.\nof the input right which is as I and I also explained what this input is this input is the.\npre-activation at that neuron right so that's The quick summary of whatever I have said so far ah so it.\ncan be the logistic function or it can be the cumulative distribution of the standard normal distribution right so that's.", "metadata": {"video_title": "GELU to SILU"}}
{"text": "can be the logistic function or it can be the cumulative distribution of the standard normal distribution right so that's.\nwhat I already uh explained uh earlier okay so now let's uh go further yeah so what is the expected.\nvalue of f of x let's try to find that so expected value of f of x is the same.\nas the expected value of M into X and uh that since it follows the Bernoulli distribution I can just.\nwrite it with probability uh Phi X it would take on the value 1 then multiply it by X right.\nand then with probability 1 minus 5x it will take on the value 0 and then multiply it by X.\nand this effectively would just be Phi of x into X right and what was Phi of X it was.\nthe cumulative density function of the normal distribution so Phi of X just basically means the probability of X being.\nless than or equal to X right that's how what that's what Phi of X means right so if I.\nwere to draw the cumulative density function then if I were to this is my x axis so if I.", "metadata": {"video_title": "GELU to SILU"}}
{"text": "less than or equal to X right that's how what that's what Phi of X means right so if I.\nwere to draw the cumulative density function then if I were to this is my x axis so if I.\ntake any X I'll get a value here and this value is essentially the P of X taking on values.\nless than or equal to x z because it has been obtained by the integration of this curve and then.\ntill this point whatever was the area under the density curve that is what gets plotted here right so this.\nis just probability of X less than or equal to X right and now this I could uh since this.\nthis is the probability density function of the uh of the normal distribution I know how to compute this there.\nare certain approximations available for that and I can use those approximations right so what my FX actually is I.\ncan write F of uh I can write this quantity okay let me just draw the other one so so.\nI can write this quantity okay you ignore the X here because this x corresponds to this x right so.", "metadata": {"video_title": "GELU to SILU"}}
{"text": "can write F of uh I can write this quantity okay let me just draw the other one so so.\nI can write this quantity okay you ignore the X here because this x corresponds to this x right so.\nthis quantity P of X less than or equal to is given by this nasty formula without this x right.\nand then I can just multiply it by X to get the formula that I have here another approximation of.\nit is this right so it can also be given as X into Sigma of 1.702 X and I've drawn.\nboth these formula here and you can see that they are very close to each other and look like the.\nCDF of the normal distribution right so that's why effectively uh the gelu activation function turns out to be f.\nof x is equal to X into Sigma of 1.702 X quite a weird function but well motivated right so.\nthis formula if I just give you you don't you won't understand where it came from but now you from.\nthis derivation you understand that it's multiplying the CDF value with the X itself and the CD F value there.", "metadata": {"video_title": "GELU to SILU"}}
{"text": "this derivation you understand that it's multiplying the CDF value with the X itself and the CD F value there.\nare multiple ways of approximating it and we have chosen one particular way of approximating it which is this right.\nso that's that's how the Galu activation function is arrived at and again The quick summary was that you took.\nrelu you wrote it as 1X into 0x you took Dropout you understood that it's 1 and 0 with probability.\np n minus p and then you combine these two ideas brought it this random variable M then wanted to.\ndefine the function X so you said okay what's the expected value of this function so this is what the.\nexpected value is you know how to deal with this quantity you can just write it as the formula for.\nthe CDF and there are multiple choices you chose one of the simpler choices and you came up with this.\nneat uh slightly neat looking formula for the Galu activation function right so that's how you arrive at gelu uh.", "metadata": {"video_title": "GELU to SILU"}}
{"text": "neat uh slightly neat looking formula for the Galu activation function right so that's how you arrive at gelu uh.\nand this is how it would look if you plot it so you have seen a relu we had seen.\nthe exponential uh linear unit and now we have seen this gelu which is this red colored uh right uh.\nsimilarly you could have uh another a few other activation functions which are again kind of variants of relu itself.\nso you have the scale exponential linear unit so remember we had seen the exponential linear unit and now how.\nhow was scaled exponential linear unit motivated from there we observed that these leaky relu and those variants they are.\nnot completely uh zero center right and we will see that when we draw the plot so we want this.\nto be zero centered so one way to do that is using uh something known as normalization techniques which we'll.\nsee later on the course the other way is to kind of change the activation function itself so it has.", "metadata": {"video_title": "GELU to SILU"}}
{"text": "see later on the course the other way is to kind of change the activation function itself so it has.\nsome normalization in it and that's exactly what cellu does right so let's let's try to understand what I was.\nsaying right so if you look at relu none of them are zero centered right and we saw what zero.\ncentered means in the case of the tan H function that there is equal distribution around a negative and positive.\nclearly relu is not zero centered because along the positive axis it can take very large values along the negative.\nside it only takes zero value so it's not zero centered exponentially uh Lu is slightly better because it has.\nsome values towards uh the negative so it has some balance right I mean it's still tilted towards a positive.\nbut at least some weight along the negative same with gelu but now if you scale this exponential linear unit.\nby an appropriate Lambda then you could have some of equal distribution along these two paths and then you get.", "metadata": {"video_title": "GELU to SILU"}}
{"text": "by an appropriate Lambda then you could have some of equal distribution along these two paths and then you get.\nalmost zero centering and it depends on how you choose the value of Lambda and a few values of Lambda.\nthat people have suggested uh empirically uh derived are these Lambda values are Lambda equal to 1.05 or Lambda equal.\nto 1.67 if you do this then you get some sort of zero centering and that's exactly what cellu tries.\nto do right uh now upper now after these activation functions uh came up right then of course this question.\narises I mean till when can I keep discovering these functions right is there a better way of searching these.\nactivation functions can I automatically search for them right and there was this work which tried to search for Activation.\nfunctions so think that there are many functions right this is a space of all functions possible you have Max.\nyou have I mean you have the relu here you have the logistic sigmoid tan H everything here right and.", "metadata": {"video_title": "GELU to SILU"}}
{"text": "you have I mean you have the relu here you have the logistic sigmoid tan H everything here right and.\nthere are many functions here now what they observed is that any such function is composed of two operations or.\nunary operation this is what unary operation is so negation of x which you see here which operates only on.\na single input is a unary operator absolute value of x which again operates on a single input is a.\nunary function a into X is again a unary function or exponent of all of these are unary functions so.\nwhat are binary functions binary functions are functions which take two inputs right so this Max of X1 comma X2.\nis a binary function X1 into sigmoid or some X2 is again a binary function right so either these have.\nunary so these have a combination of unary and binary uh operators so what if I tried many possible combinations.\nof these unary and binary operators right these are easier to Define there's a limited space of unity operators that.", "metadata": {"video_title": "GELU to SILU"}}
{"text": "of these unary and binary operators right these are easier to Define there's a limited space of unity operators that.\nI consider a limited space of binary operators that I consider of course there are many possibilities here but if.\nI restrict that search space and then try out all these combinations and then try to see which one gives.\nme the best performance then can I discover better activation functions and this requires some sort of reinforcement learning which.\nis beyond on the scope of this but what you can think right at a Layman level this is how.\nhow you could understand it there's a large space of functions right and you uh You observe that these functions.\nare composed of certain operations so can you compose functions in different ways right and then try to see which.\ncomposition gives you a better way and do this search in an Optimum way using some ideas from RL and.", "metadata": {"video_title": "GELU to SILU"}}
{"text": "composition gives you a better way and do this search in an Optimum way using some ideas from RL and.\nthen do you arrive at better activation functions right so they did this and the many activation functions popped out.\nright so you can see for reference here this one looks somewhat like the elu gelu family right but there.\nare other activation functions which also popped out and what they uh arrived it as that any function of this.\nform which is X into sigmoid of a beta into X is a good activation function and this is actually.\nuh the swish activation function and if you set the constant to 1.702 then you get uh gelu or if.\nyou make it a learnable parameter then you have uh the switch activation function right so they again when they.\ntry to search this they again arrived at this form which was similar to the form that gelu had and.\nthe generic form which is parametric is called Swish and if you set it to a specific value then you.", "metadata": {"video_title": "GELU to SILU"}}
{"text": "the generic form which is parametric is called Swish and if you set it to a specific value then you.\nget the gelu activation function right and so so we call this one as swish we call the specific instance.\nas gelu now there's what would we just call X Sigma x i so this is a specific case where.\nbeta is equal to 1 and this is called the silu which is sigmoid weighted linear unit right so you.\nhave the uh linear unit here and then you have the sigmoid weighted so that's what silu is uh so.\nmany activation functions right so we have seen quite a few now uh let me just three calls sigmoid tan.\nH relu then we had leaky relu then we had parametric relu then we had elu then we had max.\nout then we had gelu then we came to swish which was a generalization of gelu then we again saw.\nsilu which is yet another specialization of glue so this kind of covers all the popular activation functions that are.", "metadata": {"video_title": "GELU to SILU"}}
{"text": "silu which is yet another specialization of glue so this kind of covers all the popular activation functions that are.\nout there now of course the main question is given so many activation functions which one should I actually use.\nright so that's one question of interest and uh this is this plot right from the website papers with code.\nshows how these uh how the use of these activation functions has evolved over time so this was around the.\ntime uh the Transformer idea became popular and that is the context in which the Galu activation function was proposed.\nand now as the as Transformers are kind of popular this is one of the most popular activation functions today.\nbut relu over a large I mean almost greater than a decade now it has maintained its popularity so relu.\nand Galu are two important activation functions which are uh quite uh popular even today sigmoid is again used it.\nhas because of its nice property of zero to one which makes it like a uh something that can look.", "metadata": {"video_title": "GELU to SILU"}}
{"text": "has because of its nice property of zero to one which makes it like a uh something that can look.\nlike a probability function it still has use in some activation functions inside in some attention functions and so on.\nuh so it's still uh in some use so sigmoid and tanh are again used despite their multiple disadvantage advantages.\nthat we spoke about but they are still popular but relu and gelu are the most popular activation functions out.\nthere today right so with that a quick summary so sigmoids are generally bad we saw them but whenever you.\nneed something to be between 0 to 1 then we'll see a few cases where we want that uh and.\nthat time sigmoids are useful uh relu is more or less a standard unit for convolutional neural networks you can.\ntry some of these variants of value but they have not been so popular uh tanh is still used in.\nlstms and rnns which we'll see later and gelu is commonly used in Transformer right so I would say tannage.", "metadata": {"video_title": "GELU to SILU"}}
{"text": "lstms and rnns which we'll see later and gelu is commonly used in Transformer right so I would say tannage.\nbecause it's required in lstm and rnns it's still popular and then gelu for Transformers and then relu for convolutional.\nneural networks right so these are the three top activation functions which are currently still popular right so with that.\nI'll end this discussion on activation functions and that's the end of the lectures for this week and next week.\nwe'll come and talk about weight initialization so that was the fourth pillar along which people made uh progress after.\nthis 2006 to 2009 period when people realized that there is a way to make deep learning work or deep.\nneural networks train better and to to these are the four axes along which we should investigate which is optimization.\nregularization activation function and weight initialization so the first three we are done with now we look at the fourth.\nin the next lecture thank you.", "metadata": {"video_title": "GELU to SILU"}}
{"text": "foreign [Music] okay so I'll just demonstrate that algorithm in a toy setup right now whatever we did right so.\nthis is a function that is given to us and as you can see that this function has multiple minimally.\nthat's one here there's one here there's one here and I'm just going to try to follow the gradient descent.\nrule right which is wherever I am currently which is w I'm just going to do W minus ETA W.\nright so that's my going to be my new value so right now I want a value of w which.\nis 1.2 okay I have done the computation there I have calculated the derivative okay so remember that derivative of.\nx square for example right is 2X but now if I compute the uh this value at x equal to.\n1.2 then I get some scalar number right so that's what I'm going to do here so my w is.\nequal to 1.2 so whatever is the derivative in that formula I'm going to plug in W equal to 1.2.\nand I'm going to get some answer for what the derivative is right and then my ETA is set to.", "metadata": {"video_title": "Gradient Descent Weight update rule"}}
{"text": "equal to 1.2 so whatever is the derivative in that formula I'm going to plug in W equal to 1.2.\nand I'm going to get some answer for what the derivative is right and then my ETA is set to.\n1 so that will give me what n n into derivative should be and that value is what I'll uh.\nkeep changing okay so let's start and I'm not going to do this in um great detail as you can.\nsee this is like set up as a game for you to practice and try to reach at the Minima.\nso I'll just show you a few steps of how to do it so now my 1.2 is what my.\nvalue was I have calculated the derivative and that ETA into DW turns out to be 0.31 right so I'll.\nhave to move in the direction oppose it to the gradient so I'll do 1.2 minus 0.31 so I'll just.\nenter that in this uh box here right so 1.2 minus 0.31 is going to be 0.89 okay so this.\nis where I end up right so I have moved and looks like I have moved in a good direction.", "metadata": {"video_title": "Gradient Descent Weight update rule"}}
{"text": "enter that in this uh box here right so 1.2 minus 0.31 is going to be 0.89 okay so this.\nis where I end up right so I have moved and looks like I have moved in a good direction.\nbecause I'm headed towards one of the Minima right uh so zero point uh now uh at this point I.\nhave 0.89 is my current value I've again computed a derivative and my ETA into DW is 1.25 so I'm.\nagain just going to ah 0.89 minus 1.25 is what I'm going to do and that would be around minus.\n0.36 right so I'm going just going to set the value as minus 0.36 and this is where I get.\nright so as you can see I have moved a bit away right I should have I would have liked.\nto end at the Minima but now I've gone in the opposite direction right and why this is happening what.\ncould you do to make sure that this doesn't happen is something for all of us to figure out in.\nthe next couple of lectures right now just keep trying this right I've shown you what to do you just.", "metadata": {"video_title": "Gradient Descent Weight update rule"}}
{"text": "could you do to make sure that this doesn't happen is something for all of us to figure out in.\nthe next couple of lectures right now just keep trying this right I've shown you what to do you just.\nfollow this and as I said it's like a setup as a game you could just keep trying this and.\nsee right and now you can notice it uh so far I was moving in this direction right I started.\nhere then I went to a point here then I went to a point here and now it looks like.\nI have to come in the back Direction I have to come back and so on so you just play.\nwith this and you just keep following the update Rule and see that you'll slowly start moving towards the uh.\none of the minimas that are there in this function right and you could also initialize the W from a.\ndifferent point right you could initialize it from say this point here and then you will see that it will.\nstart going towards this Minima if you initialize it to a point here it will start going towards this Minima.", "metadata": {"video_title": "Gradient Descent Weight update rule"}}
{"text": "start going towards this Minima if you initialize it to a point here it will start going towards this Minima.\nright so what I've done is I'd initialize it at this point right which was on this side of the.\nslope so that's why it went there if I had initialize it here at this value then it would have.\ngone into this minimal so just play around with this try different values of w and keep following the formula.\nand see whether you are able to reach the Minima I'm sure you will not be easily able to reach.\nthe Minima and one of the problems there is that ETA is a bit large so it's not helping you.\nto control ETA into DW here I wanted the ETA to be small which so that I could have reached.\nthe Minima instead of Crossing it I've crossed the minimine going in the upper Direction so all of this is.\nsomething that we'll do in detailer and at this point I just want you to kind of break your heads.", "metadata": {"video_title": "Gradient Descent Weight update rule"}}
{"text": "something that we'll do in detailer and at this point I just want you to kind of break your heads.\nwith this and try to understand the problems that show up right I don't want you to fully understand this.\nnor do I want you to get the solution but I just wanted to see what kind of problems you.\nface and how would you change them I already gave you a few hints right that uh you might think.\nthat oh this is the global Minima but I started from some point here 1.2 and I'm not being able.\nto reach there right then what if I had initialized it a bit differently would I be able to do.\nthat so what is the effect of initialization what is the effect of ETA as you make ETA very large.\nwhat happens if you make ETA very small what happens right so just try to experiment with those things and.\njust get your hands a bit dirty with this okay yeah so now we have the gradient descent rule that.\nthe direction U that we intend to move in should be at 180 degrees with respect to the gradient in.", "metadata": {"video_title": "Gradient Descent Weight update rule"}}
{"text": "just get your hands a bit dirty with this okay yeah so now we have the gradient descent rule that.\nthe direction U that we intend to move in should be at 180 degrees with respect to the gradient in.\nother words move in the direction opposite to the gradient so this phrase I'm sure you have heard a million.\ntimes but now you know where this phrase comes from uh and this is what my update would look like.\nwhatever is my current value of w uh sorry whatever is my current value of w I just take the.\ngradient or the derivative right and then move in the direction opposite to that because of the minus here and.\nby a small conservative step because of ETA and this ETA comes from the fact that Taylor series works well.\nin a small neighborhood hence we are using a small ETA there right now this year if I put it.\nin a vector this is nothing but Theta t plus 1 okay maybe not the best choice of colors here.\nif I put this in a vector this is Theta t and if I put this quantity in a vector.", "metadata": {"video_title": "Gradient Descent Weight update rule"}}
{"text": "in a vector this is nothing but Theta t plus 1 okay maybe not the best choice of colors here.\nif I put this in a vector this is Theta t and if I put this quantity in a vector.\nthen this is the gradient of the loss function with respect to Theta right okay at time step T right.\nat the time step T right so now let's try to again uh clarify the notation so this are the.\nentire thing I can write in Vector form that's what I've just shown now some more clarification so what does.\nthis quantity Delta WT mean right so Delta WT is the partial derivative of the loss function which is a.\nfunction of two variables W and B with respect to W and then evaluated at w is equal to WT.\nand B is equal to BT what does that mean so now if I have the function is W square.\nplus b square then the formula would be 2W 2B right this is what the formulaic representation of the gradient.\nwould be but now I can evaluate this at the current value of my w right so I'll just substitute.", "metadata": {"video_title": "Gradient Descent Weight update rule"}}
{"text": "would be but now I can evaluate this at the current value of my w right so I'll just substitute.\nthe value of w and if I do that then I'll get a two real numbers so I'll get a.\nreal valued Vector right as opposed to a formula I'll get a vector so this is always this that you.\nhave the formula for the derivative then you can evaluate the derivative at a specific point by putting it those.\nuh points uh the that point into the formula so that's what I mean by evaluated at w is equal.\nto WT right so that's what this quantity is signifying that it's the partial derivative evaluated at that point and.\nthat's exactly what we are doing in the previous slide we are getting a real number at the end which.\nwe could add and subtract and so on right so that's the idea yeah so now we have a more.\nprincipled way of moving in the WB plane we are no longer doing our guesswork we know that if we.\ndo we just saw a proof or we arrived with the formula mathematically that if we keep moving in the.", "metadata": {"video_title": "Gradient Descent Weight update rule"}}
{"text": "principled way of moving in the WB plane we are no longer doing our guesswork we know that if we.\ndo we just saw a proof or we arrived with the formula mathematically that if we keep moving in the.\ndirection opposite to the gradient we are guaranteed that the loss at every step will uh decrease not that it.\nwill just decrease it will also decrease by uh the largest amount right now one thing you should have noticed.\non the previous slide in my last update actually you could go back and check whether the loss had increased.\nor not and if the loss had increased what could have been the reason for that and the answer there.\nis that the learning rate or the ETA was quite high but all of these are things that we'll come.\nback to so don't worry about okay so this is the algorithm that we have created uh we'll start at.\ntime step 0. we'll set up some Max iterations we'll initialize W and B randomly and well while we are.", "metadata": {"video_title": "Gradient Descent Weight update rule"}}
{"text": "time step 0. we'll set up some Max iterations we'll initialize W and B randomly and well while we are.\nless than the max iterations we'll just keep doing this uh repeatedly right whatever is the current value of w.\nwe're just going to get a new value of w from that by following the gradient descent rule right and.\nwe keep doing this for the max number of iterations and by then uh the we should be at a.\npoint which is very close to the Minima right that's what's the hope is okay now uh everything in this.\nalgorithm is known right I could have actually run this algorithm it's just that I don't know what is Delta.\nW and Delta B is right for so let's see what I've given you the definitions of them right but.\nwhat is that formula how do I actually compute this the partial derivative of w with respect to the loss.\nfunction because I have not even told you very clearly what the loss function is in so far in the.", "metadata": {"video_title": "Gradient Descent Weight update rule"}}
{"text": "function because I have not even told you very clearly what the loss function is in so far in the.\ndiscussion if you're just working with a generic function L of w so we now need to look at the.\nwhat the actual loss function is right so here's coming back to our original example where we had two parameters.\nW and B this was our function okay and our loss function was defined as this right that let's assume.\nthere is only one point there's only one data point so a loss function was summed over all the points.\nand then take in the average but now I assume that there's only one data point so there's no summation.\nhere and summed over what it was the prediction minus the true and the square of that right so that's.\nwhat this formula is f of x is our approximation Y is the true value so I am taking the.\ndifference between the two and squaring it right and there's one by two I've just kept for some convenience it.", "metadata": {"video_title": "Gradient Descent Weight update rule"}}
{"text": "difference between the two and squaring it right and there's one by two I've just kept for some convenience it.\ndoes not uh I could use any multiple here right it does not change because I'm looking for the lowest.\nvalue of the loss whatever is the lowest value if I If I multiply all values by half that value.\nof w comma B which at which I get the lowest value does not change right but it just makes.\nit a bit convenient for me from a mathematical perspective I'll become clear why how okay so this is uh.\nwhat our loss function is now I come to the definition of delta W which was there on the previous.\nslide is the partial derivation of derivative of the loss function with respect to W which is the partial derivative.\nof this function with respect to W so this is the quantity that I want to compute now right uh.\nso this is the partial derivative now this is easy right so this I can just I know that this.", "metadata": {"video_title": "Gradient Descent Weight update rule"}}
{"text": "of this function with respect to W so this is the quantity that I want to compute now right uh.\nso this is the partial derivative now this is easy right so this I can just I know that this.\nis a function of w comma B right this is my sigmoid function which has the parameters W and B.\nso this is a function of w and B so I'll just take the derivative and then push the derivative.\ninside it so this is the derivative of uh of fun of the square of a function of w so.\nit will just be 2 times that function into the derivative of the function right you just you this chain.\nrule of derivative that you know so this is some function which depends on W then you have taken the.\nsquare of that function and now you are taking the derivative the first thing would be 2 times the function.\nand then you push the derivative inside so you get the derivative of the function with respect to W okay.\nnow this is of course a constant right because this is the value True Value which is given to us.", "metadata": {"video_title": "Gradient Descent Weight update rule"}}
{"text": "now this is of course a constant right because this is the value True Value which is given to us.\nit does not depend on wnb I can choose whatever W and B I want the True Value will not.\nchange so that the derivative with respect to that will be zero so the only thing that I am read.\nis the partial derivative of the function with respect to uh uh the W right and you can see why.\nI had chosen this half here because this half N2 got canceled it just makes my life easier so and.\nnow if I just substitute the value of f of x so this is the value and this is the.\nderivative that I want to compute so let's try to compute right right so this is the derivative of again.\nof the inverse of a function of w right so this is 1 over something which is a function of.\nw so the derivative would be minus 1 over that function square and then the derivative pushed inside so derivative.\nof this should have been 1 plus that e raised to minus W X plus b but 1 is a.", "metadata": {"video_title": "Gradient Descent Weight update rule"}}
{"text": "w so the derivative would be minus 1 over that function square and then the derivative pushed inside so derivative.\nof this should have been 1 plus that e raised to minus W X plus b but 1 is a.\nconstant so it's anyway is going to the derivative will be 0 so I have not written that right so.\nnow derivative of e raised to minus W X plus b so that would be e raised to minus W.\nX plus b into the derivative of minus W X plus b and now again B is a constant so.\nthat would just give me minus one so oh sorry uh minus X sorry derivative of w x with respect.\nto W would be minus X so I have these two negatives minus 1 and minus X so that will.\nbecome a positive okay so this is what I get now what is this this is just my original f.\nof x right so this is f of x so I can just write f of x and you can.\ngo back and check this this is essentially 1 minus f of x right so you can go back and.\nwork it out so if you just take 1 minus 1 over this formula right then you will sorry 1.", "metadata": {"video_title": "Gradient Descent Weight update rule"}}
{"text": "go back and check this this is essentially 1 minus f of x right so you can go back and.\nwork it out so if you just take 1 minus 1 over this formula right then you will sorry 1.\nminus this formula then you will get this right so I can just write it as f of x into.\n1 minus f x right so now I'm going to substitute this value back in my original equation so this.\nis what I get I had this term already from earlier and now this entire quantity is this as I.\nhave just derived right so now I exactly know what Delta W looks like and now if I give if.\nyou give me a value of w and B I can substitute that in this formula and I can get.\na real numbered value right so I know the formula that I was looking for okay so if you have.\nonly one point then we have the Delta W is equal to this if we add more than one points.\nthen it will just be a sum of all those right so this is uh the derivative of uh the.\nloss function with respect to W when there was only one one point right so in that case LW itself.", "metadata": {"video_title": "Gradient Descent Weight update rule"}}
{"text": "then it will just be a sum of all those right so this is uh the derivative of uh the.\nloss function with respect to W when there was only one one point right so in that case LW itself.\nwas uh just f of x minus y the whole square right but now if I have many points then.\nI'll have a summation here and I'll Index this by I I'll Index this also by I right so I.\nhave a summation so I have told you the derivative for one term in this summation if there are such.\nn terms in the summation then the the derivative will also get added up right so the derivative of a.\nsum is just the sum of the derivatives right so that's what is happening here so I have just taken.\nthe sum now so if there are two points I'll have this the sum here so I have this right.\nnow similarly you can compute the derivative of B and you can see that all of this is same the.\nonly thing extra here is this x i and that is coming from the fact that you had W X.", "metadata": {"video_title": "Gradient Descent Weight update rule"}}
{"text": "now similarly you can compute the derivative of B and you can see that all of this is same the.\nonly thing extra here is this x i and that is coming from the fact that you had W X.\nplus b so when you take the derivative with respect to W this x is what you get but when.\nyou take the derivative with respect to B you don't get this x that you just get 1 here that's.\nwhy you are having this uh into 1 here and into X there right so now you have the formula.\nfor Delta uh for the uh partial derivative of the loss function with respect to W and the partial derivative.\nof the loss function with respect to B right so you have both of these and now we can construct.\nan algorithm out of this okay so let's see what that algorithm is so I started off wow so this.\nI hope you can see it so these were the two points given to me these are the same two.\npoints that we have been looking at since the last lecture so x equal to 0.5 where the value of.", "metadata": {"video_title": "Gradient Descent Weight update rule"}}
{"text": "I hope you can see it so these were the two points given to me these are the same two.\npoints that we have been looking at since the last lecture so x equal to 0.5 where the value of.\nthe function was 2.5 and x equal to 0.2 where the value of the function was 0.9 okay now I.\nwant to do gradient descent to find the right W's and B's so I've set W to minus 2 my.\ninitialization is minus 2 B is also minus 2 I have kept ETA is 1 and I have set the.\nmax epox as 1000 right so this is what my initializes are now for I in range of Max epox.\nthat means for thousand iterations I have initialized the derivatives to zero and for X Y in my data points.\nright so all the data points and iterate over all the data points I'll compute the gradient with respect to.\nW or the derivative partial derivative with respect to W I'd also compute so let's see now I want to.\ncompute the partial derivative with respect to W how will I do that so I have this function uh here.", "metadata": {"video_title": "Gradient Descent Weight update rule"}}
{"text": "compute the partial derivative with respect to W how will I do that so I have this function uh here.\nand I know how to do this right so I can this is the formula I just computed this on.\nthe previous slide f of x minus y into f of x into 1 minus f of x into X.\nright and what is my X here I am looking at the first point here so my X is going.\nto be 0.5 so I can just substitute that here and what is my f of x let's see what.\nf of x is so this is how I'll compute f of x right so f of x is I'll.\njust substitute the value of x in this formula and I'll just substitute the current values of w and B.\nright which are 2. so I can compute this and I'll get some real number and that is what I'll.\nput here right so now I can compute that quantity similarly I can compute the grad of the partial derivative.\nwith respect to B again I have a function written for that and the same idea that I want to.", "metadata": {"video_title": "Gradient Descent Weight update rule"}}
{"text": "with respect to B again I have a function written for that and the same idea that I want to.\ncompute this f of x which I'll compute from this function here by substituting the current values of w b.\nand X that I am dealing with right okay so far so good let me just erase it okay and.\nthen I'm going to sum this over all the points right because I visited the first point which was x.\nequal to 0.5 and Y is equal to 2.5 then the loop will go to the second point this Loop.\nwill go over all points and I'm just collecting all the uh I'm just adding up all the derivatives because.\nas I said the this derivative of a sum is just the sum of the derivatives right once I've computed.\nthe derivatives I'm just going to apply the gradient descent formula which is W is equal to W minus ETA.\ninto W did the DW and B is equal to B minus ETA into d w DB right so this.\nis I'll just keep doing this in the loop and I'll keep moving along the error surface so this is.", "metadata": {"video_title": "Gradient Descent Weight update rule"}}
{"text": "into W did the DW and B is equal to B minus ETA into d w DB right so this.\nis I'll just keep doing this in the loop and I'll keep moving along the error surface so this is.\nwhat my error surface is so I have this is what my error surface looks like and how have I.\nplotted the error surface I have plotted it using something like this right so uh yeah so this is how.\nI plotted for all possible values of w comma B actually I've just computed the error and then I have.\nplotted it right so don't worry too much about what is happening here this is just a formula for computing.\nthe loss function which is f of x minus y the whole square right so I've just computed that and.\nI've plotted this uh so now if I just keep changing the values of w and B as shown in.\nthis algorithm following the gradient recent rule then no matter where I start from I start moving in a principled.\nway and at every point the loss will just keep decreasing and I'll reach the minimum that is what is.", "metadata": {"video_title": "Gradient Descent Weight update rule"}}
{"text": "way and at every point the loss will just keep decreasing and I'll reach the minimum that is what is.\ngoing to happen and now let me try to see if I can demonstrate that right I'm just going to.\ngo here okay okay so this is what my loss function looks like this is actually the same as the.\nloss on the previous slide just scaled a bit differently and what I know is that I want to reach.\nsomewhere here because in this surface it's clear that the error is minimum there but right now I am way.\nfar I want to reach somewhere here but I am very far my initialization was randomly done right so now.\nwhat I'm going to do is I'm going to run this algorithm and you just observe how the things change.\nboth on the loss surface you should see that the loss at every step should keep decreasing and the W.\nshould keep W comma B the point on the yeah on the W comma B plane right so this here.\nis the W comma B plane so the W comma B plane I should keep moving till I reach this.", "metadata": {"video_title": "Gradient Descent Weight update rule"}}
{"text": "should keep W comma B the point on the yeah on the W comma B plane right so this here.\nis the W comma B plane so the W comma B plane I should keep moving till I reach this.\nnice optimal value here at which the loss is minimum okay so let's see if I can run this so.\nthe algorithm is running it's initially quite slow you can see that the yellow dots are moving right it's slowly.\nslowly moving it has finished some 27 iterations it's the loss is still a bit high it's 0.81 as you.\ncan see here and it's still running running running but it seems to be going in the right direction at.\nno point am I seeing that the loss is increasing and it keeps going I'll Just Fall Down good the.\nloss has decreased quite a bit and I'm very happy that I'm moving in the right direction again it has.\nslowed down a bit why is it slowing down in certain regions why is it not going fast in other.\nregions is something that we'll figure out but now it has started moving and you can see that the loss.", "metadata": {"video_title": "Gradient Descent Weight update rule"}}
{"text": "slowed down a bit why is it slowing down in certain regions why is it not going fast in other.\nregions is something that we'll figure out but now it has started moving and you can see that the loss.\nis around 0.02 I just run it for 100 iterations I've stopped it now I could keep running it a.\nbit more of the side set to 200 so let me just make it 500 and I can keep running.\nthis you can again experiment with this at your own Leisure right so now you can see that it's moving.\nfurther and it'll keep going patiently it'll keep going but it'll go in the right direction right I'm not worried.\nthat it will eventually lease the loss function right even 500 iterations does not seem to be enough in this.\ncase uh but if I just land it for a thousand two thousand iterations then it will reach there right.\nbut the key thing that you need to at least be happy about right now is that unlike a random.\ngas guess algorithm here you are not making a mistake at any point at every time step your loss is.", "metadata": {"video_title": "Gradient Descent Weight update rule"}}
{"text": "but the key thing that you need to at least be happy about right now is that unlike a random.\ngas guess algorithm here you are not making a mistake at any point at every time step your loss is.\ndefinitely decrease right so that's that's what is happening here so this is a demonstration of the gradient descent algorithm.\non the toy example that we had using the code that I had on the previous slide which was in.\nturn based on the gradient descent update right um so later on in this course we look at gradient descent.\nin much more detail and we look at a lot of its variance right so for now it suffices to.\nknow that we have an algorithm for learning the parameters of a sigma material and I've just shown you the.\nalgorithm I have shown you the Mac behind it I have shown you the visual interpretation of that and I've.\nalso shown you how to code it and then run it so that you move on the loss function in.", "metadata": {"video_title": "Gradient Descent Weight update rule"}}
{"text": "algorithm I have shown you the Mac behind it I have shown you the visual interpretation of that and I've.\nalso shown you how to code it and then run it so that you move on the loss function in.\na principle way right so I'll uh end this video here before just telling you quickly where do we head.\nfrom here right so now the next thing to do we have done this completed this cycle that we introduced.\nsigmoid neuron just as we had introduced perceptron then we introduce the learning algorithm just as we had introduced The.\nperceptron Learning algorithm then we discussed the error and we ran the algorithms in both cases in perceptron as well.\nas in the sigmoid neuron and we saw that the learning algorithm actually learns well and it keeps moving in.\nthe right direction so all of that we have seen then in the perceptron case we had moved on to.\nthe representation of the power of the perceptron perceptron and now we want to talk about the representation power of.", "metadata": {"video_title": "Gradient Descent Weight update rule"}}
{"text": "the representation of the power of the perceptron perceptron and now we want to talk about the representation power of.\nthe sigmoid neuron right so that's what we are going to do uh next so I'll end this video here.\nand in the next class we'll continue with the representation power of Sig Point here.", "metadata": {"video_title": "Gradient Descent Weight update rule"}}
{"text": "foreign [Music] so we are in on this journey of looking at different variants of gradient descent and we looked.\nat quite a few of them in the last lecture and the main takeaways was the idea of momentum and.\nthen how do you correct for momentum because momentum often takes you very fast so the correction was done through.\nnatural accelerated gradient descent and then we saw the stochastic and the mini batch versions of these algorithms and also.\ntalked a bit about how do you come up with learning rate schedules right and during the discussion at some.\npoint we felt the need for like an Adaptive learning rate right so when you are on the Steep regions.\nyou want the learning rate to be small and when you are on the gentle regions you want the learning.\nrate to be fast right so that's what I mean by adaptive so it should look at okay how is.\nthe history been and where am I currently and can I accordingly slow down or maybe speed up a bit.", "metadata": {"video_title": "Gradient Descent with Adaptive learning rate"}}
{"text": "rate to be fast right so that's what I mean by adaptive so it should look at okay how is.\nthe history been and where am I currently and can I accordingly slow down or maybe speed up a bit.\nright and the gradients will of course not change if you are in the Steep region they will be large.\nand if you're in the general region they will be small right you can't do much with the gradients but.\nthen the multiplying factor which is the learning rate can you change that so that you can scale up and.\nscale down the updates accordingly right so uh that's the idea that we want to explore in today's lecture right.\nso let's first again make a case for it so here's again a simple neural network there's a slight change.\nin notation which I had to do because now we have four inputs so it's not a change in notation.\njust a new notation so we have these four inputs so far we have been dealing with a simple neural.", "metadata": {"video_title": "Gradient Descent with Adaptive learning rate"}}
{"text": "in notation which I had to do because now we have four inputs so it's not a change in notation.\njust a new notation so we have these four inputs so far we have been dealing with a simple neural.\nnetwork where we had one input one bias and then the output I'm looking at four inputs X1 X2 X3.\nX4 and then the bias which is a constant input of 1 and of course each of these inputs has.\nan Associated weight W1 W2 w3w4 right so we have been using the subscript for the training instance so there.\nare M training instances X1 to xn which is X is bold which is a vector but now we have.\nthese superscripts for the different uh inputs within a given training instance right so now you could think of X.\nas in X as a vector X1 X2 X3 X4 and this is one input right so let's say this.\nis the data about one movie and there would be many such movies so say this is the movie one.\nso then we have x 1 1 x 1 2 x 1 3 x 1 4. right so that's the.", "metadata": {"video_title": "Gradient Descent with Adaptive learning rate"}}
{"text": "is the data about one movie and there would be many such movies so say this is the movie one.\nso then we have x 1 1 x 1 2 x 1 3 x 1 4. right so that's the.\nnotation that we are using so these are all the individual inputs that are going to the network and a.\ncollection of such inputs forms one training point or one uh data point right so that's the idea okay uh.\nand so this is the notation and now uh so we have multiple weights now right we have W and.\nW2 w3w4 and for the minute I have ignored the bias so now given this network that it should be.\neasy that for a given a single data point right which is uh X and it should have been X.\ncomma y not X comma B sorry this should have been y so a single training point is the input.\nX and the output y now if you wanted the derivative of the loss function with respect to W1 which.\nis one of the weights in this network right so remember earlier we had derived when we just had one.", "metadata": {"video_title": "Gradient Descent with Adaptive learning rate"}}
{"text": "X and the output y now if you wanted the derivative of the loss function with respect to W1 which.\nis one of the weights in this network right so remember earlier we had derived when we just had one.\nweight w we had derived this quantity which was the derivative of the loss function with respect to uh W.\nand this is the formula that we had got and the only thing that has changed is that that time.\nwe had only one input which was X now we have four inputs X One X two x three x.\nfour so if you derive the formula this part which I have the under braces that would remain the same.\nand it would just get multiplied by the appropriate input right the input to which the weight uh corresponds to.\nsimilarly for the second weight W2 the derivative of the loss function with respect to the weight right so this.\nwould be uh so this quantity here is derivative of the loss function with respect to the weight W2 and.", "metadata": {"video_title": "Gradient Descent with Adaptive learning rate"}}
{"text": "would be uh so this quantity here is derivative of the loss function with respect to the weight W2 and.\nif you solve for that you will get this is the answer and this is not very different from what.\nwe had already done right so we had this network earlier which was uh a single input x 1 and.\nthen you had this com constant one and in this case when we had taken the loss function and then.\ncomputed the derivative of the loss function with respect to this weight the formula that we had got was this.\nmultiplied by the corresponding input which is X right and all I'm saying is that now if you have four.\ndifferent inputs and four different weights associated with each of those inputs then if you take the derivative with respect.\nto any of these weights the formula will not change only the last X will get replaced by the appropriate.\nX right so you can go and check this but it should also be straightforward because you have this W.", "metadata": {"video_title": "Gradient Descent with Adaptive learning rate"}}
{"text": "X right so you can go and check this but it should also be straightforward because you have this W.\ntranspose X here which is W1 X1 plus W2 X2 and so on so when you take the derivative of.\nthis quantity right so finally when you apply the chain rule at some point you will take the derivative of.\nthis quantity with respect to W1 and all the other quantities will disappear and the derivative of this quantity would.\nbe X1 right so that's how this X1 is showing up here right so that's the idea so if there.\nare end points we can just sum the gradients over all the endpoints to get the total gradient right so.\nwhat does that mean that this was with respect to a single point where the input was 1X but I.\ncould have inputs which are X1 x 2 all the way up to x m right so here I have.\nshown the derivative with respect to one such input but if you have many inputs then you will just sum.\nthe derivative across all these inputs at this we have again done some before so the the derivative of w.", "metadata": {"video_title": "Gradient Descent with Adaptive learning rate"}}
{"text": "the derivative across all these inputs at this we have again done some before so the the derivative of w.\nderivative of the loss function that will secure W 2 would be summation I equal to 1 to M right.\nand then uh you will have this entire quantity inside and in the end that would be multiplied by x.\n2 of the appropriate I right so that's what the derivative would look like right so uh maybe this is.\nnot clear here let me just undo that yeah so you'll have this so you'll have this term which is.\nin the bracket and then multiplied by x 2 of the corresponding input right so that's what the total derivative.\nwould look like right so you're just the derivative of the loss function with respect to W2 is going to.\nthe sum of this quantity that I have shown here okay now now what happens if a feature X2 is.\npassed right so X2 is one of the features so in our oil drilling example it could be salinity pressure.", "metadata": {"video_title": "Gradient Descent with Adaptive learning rate"}}
{"text": "the sum of this quantity that I have shown here okay now now what happens if a feature X2 is.\npassed right so X2 is one of the features so in our oil drilling example it could be salinity pressure.\ntemperature Etc in our movie example it could be the director actor and so on right now if one of.\nthese features is sparse what does that mean that across all the M training points that I had happen this.\nfeature is always in many cases is going to be 0 right in a classic example for this would be.\nif I have data for the past say 1000 Bollywood movies right and if I have one of the features.\nas is actor Amir Khan now Amir Khan acts in very few movies so for these thousand movies maybe there.\nwould be four to five movies for which this feature is on and everywhere else this feature would be zero.\nright so that's what a sparse feature looks like and again as you can imagine in real world applications there.", "metadata": {"video_title": "Gradient Descent with Adaptive learning rate"}}
{"text": "right so that's what a sparse feature looks like and again as you can imagine in real world applications there.\nare many features which are sparse now what's the consequence of that right so if my derivative formula that I.\njust showed you as the sum right is summation I equal to 1 to M then that quantity that I.\njust mentioned multiplied by this feature value right this is what my derivative is going to be right and now.\nif this feature is very sparse that means when I am taking the sum for if this m is equal.\nto thousand and out of 1000 times if 990 times this feature is going to be 0 then 990 terms.\nin this derivative are going to be 0 right that means my total derivative that I am going to compute.\nis going to be very sparse for features uh is going to be very small for weights corresponding to sparse.\nfeatures now what's the consequence of that the consequence consequence of that is that in my any gradient descent based.", "metadata": {"video_title": "Gradient Descent with Adaptive learning rate"}}
{"text": "features now what's the consequence of that the consequence consequence of that is that in my any gradient descent based.\nupdate my new weight is going to be my old weight right so let me just call this new and.\nthis old minus ETA times whatever derivative I compute and I've just told you that if X is sparse or.\nthat feature is pass then the derivative is going to be small and if the derivative is going to be.\nsmall then it means that my updates are going to be small and if my updates are going to be.\nsmall then I'm not making much changes along that direction right and that is not something that I desire right.\nand now a wish list here would be that if I know my derivatives are going to be small can.\nmy learning rate would have been high for such sparse features so if the learning rate would have been high.\nthen still my updates would have been larger right so this is one more case for having an Adaptive learning.", "metadata": {"video_title": "Gradient Descent with Adaptive learning rate"}}
{"text": "then still my updates would have been larger right so this is one more case for having an Adaptive learning.\nrate so we talked about two cases one is in the Steep regions I want the learning rate to adopt.\nand in the flat regions again I wanted to increase and the Steep regions I wanted to be small the.\nother cases which is again related is that if you have sparse features where you know that the when you.\ncompute the total derivative it's going to be very small because in most cases this x i the circled x.\ni is going to be 0 and then hence my total derivative is small and if my total derivative is.\nsmall can and I just jack up the learning rate so that my updates are a bit larger right because.\nnow I don't need to be conservative because anyways I'm going to get very sparse updates for this feature okay.\nso that's the overall idea yeah so that's what is happening here now why is this important right so my.", "metadata": {"video_title": "Gradient Descent with Adaptive learning rate"}}
{"text": "so that's the overall idea yeah so that's what is happening here now why is this important right so my.\nuh my weight now you might say that if this feature is passed then why do I care about it.\nright I mean let it not get updates let those weights not get updated because then maybe this feature will.\nnot contribute much to my final classification but that's not the case right so there could be a case that.\na feature is passed as well as important so returning back to the movie example right there could be an.\nactor or a director right take maybe Christopher Nolan who directs very few movies right maybe one in few years.\nright so if I look at all the data for the past 10 years we will have four to five.\nof his movies right but this would be a very important deciding fact uh feature because his movies are generally.\ngood right so now if you completely ignore that feature if you are not bothered that hey this weights are.", "metadata": {"video_title": "Gradient Descent with Adaptive learning rate"}}
{"text": "good right so now if you completely ignore that feature if you are not bothered that hey this weights are.\ncorresponding to this feature are not getting updated so whatever initial value you had started if that value was small.\nand you ran this algorithm for thousand time steps and this feature got updated only a few times and that.\ntoo with very small quantities then you are missing out on an important feature right so it is often the.\ncase that's passed the features has passed but at the same time they are also important whenever they are present.\nthey play a very important role in the decision right so that's why you cannot ignore this so we want.\nthese updates to be good so that we get a good uh contribution from this feature in our W transpose.\nX Plus y right because W decides how much the feature X contributes and if that feature if W is.\nnot changing much you have initialized it to a very small value and then it's not changing much because the.", "metadata": {"video_title": "Gradient Descent with Adaptive learning rate"}}
{"text": "not changing much you have initialized it to a very small value and then it's not changing much because the.\nderivatives are small as I just Illustrated then you are losing out on the information in this feature right so.\nyou can't let that happen right so then can we have a different learning rate for each parameter which takes.\ncare of the frequency of the features let's say if there are certain features which are very sparse then you.\njust jack up the learning rate for them right and of course this all has to be done in a.\nnon-neuristic non-hacky way right you just have some equations which automatically take care of okay if this was sparse then.\nthe learning rate is adjusted to high value if this was dense then the learning rate is adjusted to a.\nsmall value right you cannot you know there are millions of features so you cannot have if else kind of.\nconditions you just have to have some equations which inherently take care of this right so now you have to.", "metadata": {"video_title": "Gradient Descent with Adaptive learning rate"}}
{"text": "conditions you just have to have some equations which inherently take care of this right so now you have to.\nconvert this intuition to a mathematical equation or to an update rule right.", "metadata": {"video_title": "Gradient Descent with Adaptive learning rate"}}
{"text": "foreign [Music] okay so now let's come to the next station in our journey which is we wanted to compute.\nthe gradients with respect to the hidden unit so it's again following this outline so we are done with this.\npart right and now we want to talk to the hidden units and remember there could be many hidden layers.\nright but I want to come up with a generic formula so that irrespective of whether I am in layer.\nhidden layer 1 2 3 4 it doesn't matter I should have a similar formula or a similar set of.\ncomputation and I don't want to derive this formula independently for every layer right the output layer of a spatials.\nI am okay with deriving a formula for that but I don't want to do it for every hidden layer.\nright so that's what we'll focus on okay so let's start so before that right so let's try to see.\nwhat we are where we are headed right so this is our hidden layer okay this is the activation at.", "metadata": {"video_title": "Gradient wrt Hidden Units"}}
{"text": "right so that's what we'll focus on okay so let's start so before that right so let's try to see.\nwhat we are where we are headed right so this is our hidden layer okay this is the activation at.\nthe hidden layer so in particular this is say h to 2 because is the second hidden layer and the.\nsecond neuron in the second hidden layer and I want to take the derivative of the loss function with respect.\nto H22 and what this diagram is saying that there are multiple paths from the loss function to this H2.\n2 right that's what it is saying and there is some significance of that so what we'll try to do.\nright um suppose I have variable Z okay and I compute say one function of Z right and let me.\ncall the function as q1 and suppose Q 1 of Z is just Z squared okay so I compute that.\nso I've computed uh q1 of Z from Z similarly I say have another function right say Q2 of Z.\nlet that be Z Cube and I have computed that also from Z right and then let me have say.", "metadata": {"video_title": "Gradient wrt Hidden Units"}}
{"text": "so I've computed uh q1 of Z from Z similarly I say have another function right say Q2 of Z.\nlet that be Z Cube and I have computed that also from Z right and then let me have say.\nQ 3 of Z which is equal to e raised to Z it's all of these are functions of Z.\nand then say using these three as inputs I am Computing some p okay and suppose that P is just.\nsay q1 divided by Q2 I'm just not writing the off Z in the bracket right just to avoid I.\nmean this is cumbersome to write and now is p a function of Z yes it is a function of.\nz y because q1 is a function of Z Q2 is a function of Z Q3 is a function of.\nZ so this p is also a function of Z because it has been computed using quantities which in turn.\nwere computed using right so I have this kind of a situation so now I can can I ask the.\nquestion do can I ask you to compute the derivative of P of Z with respect to Z what do.\nI mean by that would this derivative exist like or would it just be 0 the derivative would exist right.", "metadata": {"video_title": "Gradient wrt Hidden Units"}}
{"text": "question do can I ask you to compute the derivative of P of Z with respect to Z what do.\nI mean by that would this derivative exist like or would it just be 0 the derivative would exist right.\nbecause I just said that P of Z is a function of Z right because it is computed using quantities.\nwhich in turn depend on zerlet so this function this question is a valid question now how will you compute.\nthis derivative this derivative would simply be the derivative of P right I'm just going to use shortcuts now with.\nrespect to q1 into the derivative of q1 with respect to Z plus the derivative of p with respect to.\nQ2 into the derivative of Q2 with respect to Z Plus the derivative of p with respect to Q3 into.\nthe derivative of Q3 with respect to Z right so what am I telling you I am going to sum.\nthe derivatives along these three parts right I'm just going to apply the chain rule along these three parts and.", "metadata": {"video_title": "Gradient wrt Hidden Units"}}
{"text": "the derivatives along these three parts right I'm just going to apply the chain rule along these three parts and.\nthen sum the derivatives right so that's what you do when you have this kind of a situation this is.\njust from basic calculus I've just revised that and now I'll just write this as a more compact formula right.\nso this is the formula that I would write that if I want to compute the derivative of P of.\nZ with Z such that there are multiple paths right there could be as many rights as such m paths.\nsuch that I have Z I am Computing some intermediate quantities q1 Q2 up to q m using this Z.\nthen if I want to compute the derivative of p with respect to Z then it would be p with.\nrespect to q1 into q1 with respect to Z plus p with respect to Q2 into derivative of Q2 with.\nrespect to Z and I'm just going to sum all these parts that means there are M such parts and.\nI'm going to sum the chain rule across each of these parts set and in any One path the chain.", "metadata": {"video_title": "Gradient wrt Hidden Units"}}
{"text": "respect to Z and I'm just going to sum all these parts that means there are M such parts and.\nI'm going to sum the chain rule across each of these parts set and in any One path the chain.\nrule this is what the chain rule looks like it's a product of these two quantities which is derivative of.\np with respect to q and then derivative of Q with respect to Z right so that's what it looks.\nlike straight forward this is what you know already from calculus and just quickly revise that right and that's exactly.\nthe kind of situation that we have here you have this quantity H22 you are interested in the derivative of.\nthe loss with respect to H22 you know that the loss depends on H to 2 because it is it.\ndepends on quantities which you are computed using H22 right which dependent on H2 so the loss depends on H2.\nand there are multiple paths how many parts are there there are K Parts here right because from s to.\n2 you go to a31 okay and from a31 you can go to the loss a32 you go to the.", "metadata": {"video_title": "Gradient wrt Hidden Units"}}
{"text": "and there are multiple paths how many parts are there there are K Parts here right because from s to.\n2 you go to a31 okay and from a31 you can go to the loss a32 you go to the.\nloss because the loss depends on a31 a32 all the way up to A3 K all of this the loss.\ndepends on and all of these in turn depend on H22 so there are K parts from H22 to the.\nloss function so if I want to compute the derivative of the loss function with respect to H22 I'll have.\nto sum across these K paths right so in my case the P of Z is the loss function my.\nZ is equal to h i j I took a specific h 2 2 but it's in general h i.\nj and my intermediate outputs are these alms right so al1 al2 Al 3 and so on those were my.\nintermediate outputs right okay so with that let's try to see how to find the derivative so again we are.\ninterested actually we are interested in the derivative of the loss function or the gradient of the loss function with.", "metadata": {"video_title": "Gradient wrt Hidden Units"}}
{"text": "interested actually we are interested in the derivative of the loss function or the gradient of the loss function with.\nrespect to H2 remember H2 is a vector but I won't go to the vector directly I'll focus on one.\nelement of this maybe H22 and in general I'm just going to call it as i j right where I.\nis the layer number and J is the neuron number right neuron number in that layer okay let's remember that.\nso that is the setup and based on the formula that I had there are K paths which take me.\nfrom an H I J to the loss function okay so I'm going to sum over all those K paths.\nand each path I'll have this chain rule which I am going to compute and sum over those chain groups.\nand what is the chain rule saying derivative of the loss function with the a unit in the next layer.\nright so I want I so in the next layer the the a neuron in the next layer depends on.\nthe uh H neuron in this layer so that's a i plus 1 okay and there are K of those.", "metadata": {"video_title": "Gradient wrt Hidden Units"}}
{"text": "right so I want I so in the next layer the the a neuron in the next layer depends on.\nthe uh H neuron in this layer so that's a i plus 1 okay and there are K of those.\nso I'll sum over M to 1 to K and then the derivative of that with respect to h i.\nj right and I'm confidently doing this because just on the previous lecture or the previous video I have actually.\nshown you how to compute the derivative of the loss function with respect to A3 so I already have this.\nquantity right so I am happy that this quantity is in the path because that quantity I have already computed.\nokay so let's go ahead so as I just said uh this quantity I have already computed in the previous.\nvideo right you can see that we had computed the partial derivative of the loss function with respect to every.\nneuron in the uh output layer right a31 a32 all the way up to A3 K so this quantity I.\nhave already computed right here 3 is equal to I plus 1 and these are the m so this I.", "metadata": {"video_title": "Gradient wrt Hidden Units"}}
{"text": "neuron in the uh output layer right a31 a32 all the way up to A3 K so this quantity I.\nhave already computed right here 3 is equal to I plus 1 and these are the m so this I.\nhave already computed right what I didn't know was this right so what is this saying let's see I want.\nto compute the derivative of one of these guys okay with respect to one of these guys that's what I.\nwant to do and I am saying that that derivative is this quantity most of you would get that if.\nyou don't get it let's try to see how we get it so give me a couple of minutes while.\nI write a few things on the slide foreign how I went from here to here right so let's first.\nunderstand how did we compute A3 this is what our formula for A3 was it was the weighted sum of.\nthe inputs plus the bias Vector right so it was just I took this as the input this red guys.\nas the input which is the entire H2 Vector then multiplied them by the weights which was W3 and then.", "metadata": {"video_title": "Gradient wrt Hidden Units"}}
{"text": "the inputs plus the bias Vector right so it was just I took this as the input this red guys.\nas the input which is the entire H2 Vector then multiplied them by the weights which was W3 and then.\nadded the biases which was B3 right so that's what this formula is capture now I've just expanded that formula.\nI have taken the case where I have only two units two neurons in the output layer that's why I.\nhave only a31 and a32 and I have three neurons in the previous hidden layer that's why I have it's.\n2 1 h 2 2 h 2 3 so this weight Matrix would then be or 2 cross 3 Matrix.\nmultiplied by a three cross one vector and the bias is I would have just two biases corresponding to the.\ntwo output neurons so I have this two biases right so that's how this computation is now let's write down.\none of of these guys right so a 3 1 is what is that equal to w three one one.\nI'm just going to do the multiplication into H to 1 plus W 3 1 2 into h 2 2.", "metadata": {"video_title": "Gradient wrt Hidden Units"}}
{"text": "one of of these guys right so a 3 1 is what is that equal to w three one one.\nI'm just going to do the multiplication into H to 1 plus W 3 1 2 into h 2 2.\nplus W3 1 3 into h 2 3 plus the bias term which was B 3 1 okay now what.\nare the indices here this is I plus 1 right so I plus 1 is equal to 3 this is.\nmy M right because I am trying to take the derivative of uh a i plus 1 comma M with.\nrespect to h i j right so my I plus 1 is equal to 3 and my m is 1.\nin this case and of course my I would be 2 in that case okay so my I is 2.\nNow what is my J I had taken the specific case of h 2 2 so my J is equal.\nto 2 right so now suppose I want to take the derivative of this quantity which I'll write here derivative.\nof a 3 1 with respect to H to 2 okay where this is I this is J and this.\nis M okay now what would that be I have written the expanded formula of a31 and I am taking.\nthe derivative with respect to h 2 2 so this will disappear because this does not depend on its 2y.", "metadata": {"video_title": "Gradient wrt Hidden Units"}}
{"text": "is M okay now what would that be I have written the expanded formula of a31 and I am taking.\nthe derivative with respect to h 2 2 so this will disappear because this does not depend on its 2y.\nH22 this will also disappear this will also disappear so what will remain is w312 into H22 and the derivative.\nof that with respect to H to 2 would just be w312 and what is w312 what are these indices.\n3 is actually equal to I plus 1 okay one is coming from the m and this 2 is coming.\nfrom the J right so that's why what I get w i plus 1 M J right so this is.\nthe mgth entry of the w i plus 1 made by tricks in this case it is the 2 comma.\n2 entry of W3 okay so that's how you should read this so I hope this is clear now we'll.\nmove ahead so this we had already computed in the previous lecture and this we just uh proved why this.\nis equal to this way so am I done here not quite yet so let's let's uh spend some more.", "metadata": {"video_title": "Gradient wrt Hidden Units"}}
{"text": "move ahead so this we had already computed in the previous lecture and this we just uh proved why this.\nis equal to this way so am I done here not quite yet so let's let's uh spend some more.\ntime on this so now suppose I have this Vector right what is this Vector this is the derivative of.\nthe loss function with respect to the layer I Plus 1. this is how I will write it it will.\nbe a collection of the partial derivatives so in this case since I plus 1 is equal to l i.\nhave K elements here so I have the derivative of the loss function with respect to the first neuron in.\nthe output layer with respect to the second neuron in the output layer third neuron all the way up to.\nkth neuron right and now what is this w i plus 1 so I had the w three Matrix okay.\nand this dot here means here I should have had the row index instead of the row index I have.\na DOT here that means I don't care about the row I'm going to take all the rows and all.", "metadata": {"video_title": "Gradient wrt Hidden Units"}}
{"text": "and this dot here means here I should have had the row index instead of the row index I have.\na DOT here that means I don't care about the row I'm going to take all the rows and all.\nthe rows from which column the jth column right so that is what this notation means I have the W3.\nMatrix I'm going to take all the rows of that Matrix because I have not specified an index for the.\nrow but I haven't specified an index for the column so that means I'm taking all the rows for the.\njth column which is the same as taking the entire jth column right so how would I write that Matrix.\nthis is how I'll write this this is w 3 the first row jth column second row jth column all.\nthe way up to care through jfcon right so these are two uh matrices that I have now if sorry.\nthese are two vectors that I have I have still not shown why I came from here to here right.\nthat will become clear in a while why did I start discussing about these two just assume that these two.", "metadata": {"video_title": "Gradient wrt Hidden Units"}}
{"text": "these are two vectors that I have I have still not shown why I came from here to here right.\nthat will become clear in a while why did I start discussing about these two just assume that these two.\nare the vectors given to you right now what is the uh dot product of these two vectors going to.\nbe I'm going to take the dot product of these two vectors so it's going to be this multiplied by.\nthis this multiplied by this all the way up to this multiplied by this and the summation of those right.\nso they are going to be K terms in the summation and every term in the summation is going to.\nlook like the product of these two quantities right so this quantity that you have here is essentially the dot.\nproduct between the jth column of the W of the weight vector and this gradient Vector which you had already.\ncomputed right so that's all it says this is the same formula that I have here and again this is.", "metadata": {"video_title": "Gradient wrt Hidden Units"}}
{"text": "computed right so that's all it says this is the same formula that I have here and again this is.\na not very difficult right so even if you didn't get it now you could just go back and take.\na look at the slides all the notations have been explained clearly if you understand the notations this is a.\nstraightforward leap from there right so whatever we had computed what did I what am I trying to say what's.\nmy summary I have computed the derivative of the loss function with respect to one of the Hidden units right.\nI wanted to in fact not even this right just one the dark red guy right I actually wanted to.\ncompute the derivative with respect to entire H2 but I've just computed with one element of H2 right and that.\nis this quantity it's the dot product between two vectors one of the vectors is the jth column right and.\nJ comes from here the jth column of the weight Matrix and the gradient Vector right the gradient Vector is.", "metadata": {"video_title": "Gradient wrt Hidden Units"}}
{"text": "J comes from here the jth column of the weight Matrix and the gradient Vector right the gradient Vector is.\nthe uh what we had already computed earlier okay so where do I go from here I have the formula.\nfor one of these guys I want the formula for all of these guys right I want now to compute.\nthe derivative of the loss function with respect to any hidden layer h i okay so what will it be.\nit will just be the collection of the partial derivatives with respect to all elements of those that hidden layer.\nwhich is h i y and h i 2 all the way up to h i n and I just.\nknow the formula for one of these guys at h i j so I can just substitute the value of.\nh i of I and J accordingly so I'll keep the I as it is because I have I here.\nand wherever I see a j I am going to substitute 1 2 3 all the way up to n.\nright so this is what it's going to look like it's a DOT product between the First Column of the.", "metadata": {"video_title": "Gradient wrt Hidden Units"}}
{"text": "and wherever I see a j I am going to substitute 1 2 3 all the way up to n.\nright so this is what it's going to look like it's a DOT product between the First Column of the.\nweight Matrix with the gradient Vector the dot product of the second column of the weight Matrix with respect to.\nthe gradient Vector the dot product of the last column of the weight Matrix with respect to the with the.\ngradient Vector right so what is that looking like this is the so let me just try to write the.\nMatrix W transpose right w i plus 1 transpose so this would be the first column of w i plus.\n1 this would be the second column of w i plus 1 all the way up to the nth column.\nof w i plus one remember I am writing the transpose hence I am drawing rows but saying Columns of.\nw i plus 1 and all of this is getting multiplied by this gradient Vector right this gradient so essentially.\nwhat I'm doing is that every uh uh every uh element of this Vector is essentially the product of one.", "metadata": {"video_title": "Gradient wrt Hidden Units"}}
{"text": "what I'm doing is that every uh uh every uh element of this Vector is essentially the product of one.\nrow of the Matrix with this gradient Vector right so now I can write it even more compactly I can.\njust write it as the product between the Matrix W transpose and the gradient Vector right that's what I have.\nshown here because the first element of the resulting product is going to be the dot dot product between the.\nfirst row of this Matrix and the gradient vector the second row of this Matrix and the gradient Vector the.\nthird row of this Matrix and the gradient vector and I can write this entire thing as just very compactly.\nas a matrix Vector computation and this is what I have done here right again if you are confident with.\nyour linear algebra if not there's a separate set of lectures that I put out on linear algebra you can.\ngo and look at them but this I'm sure most of you would be aware of it so I have.", "metadata": {"video_title": "Gradient wrt Hidden Units"}}
{"text": "your linear algebra if not there's a separate set of lectures that I put out on linear algebra you can.\ngo and look at them but this I'm sure most of you would be aware of it so I have.\ntaken the Matrix and I could write the entire gradient now this is the gradient of the loss function with.\nrespect to the hidden Unit H I which I can just compute as this and why am I confident of.\ncomputing this this I already know it whatever are my current weights what is the current W3 Matrix looking like.\nwhatever it looks like I just have to take that and this I already computed in the previous lecture this.\nwas the gradient of the loss function with respect to the last layer right because here I is equal to.\n2 so I plus 1 is equal to 3. but now I have a problem right I have a slight.\nproblem here what is my problem now suppose this was derivative of the loss function with respect to H1 right.\nso then what would this have been so I is equal to 1 now so I'll just substitute this this.", "metadata": {"video_title": "Gradient wrt Hidden Units"}}
{"text": "so then what would this have been so I is equal to 1 now so I'll just substitute this this.\nwould just be W2 transpose this is not a problem for me I know the weight Vector whatever is the.\nweight Vector I'll use that but this would have been derivative of a 2 with respect to loss function right.\nthat means this guy and this I have not computed yet right so if I compute this if I have.\na generic formula for computing this I already have a jumbled formula for computing the derivative of the loss function.\nwith respect to h i right if I have a generic formula for computing the derivative of the loss function.\nwith respect to any AI then I am done right because if I have any AI that I could be.\n1 I could be 2 I could be 3 I can just substitute it here depending on which h i.\nI am dealing with right so I have not done that yet I've only done for the spatial case when.\nuh I is equal to l right the last layer so this part I need to do now so that.", "metadata": {"video_title": "Gradient wrt Hidden Units"}}
{"text": "I am dealing with right so I have not done that yet I've only done for the spatial case when.\nuh I is equal to l right the last layer so this part I need to do now so that.\nis what I am going to work on so this is what I said right I don't know how to.\ncompute this quantity when I is less than L minus 1 that means I have only done this I have.\nnot done these two guys here right so let's see how to compute it and that's going to be again.\neasy so this is what this looks like so I already know how to compute the derivative with respect to.\neach of these guys right now I want the derivative of the loss function with respect to this so again.\nI'm going to do the same thing I'm trying to compute the derivative with respect to one of the elements.\nhere here okay and what would that be that would be the derivative of the loss function I've already computed.\ntill here so then I can just use that part as it is and then the derivative of H I.", "metadata": {"video_title": "Gradient wrt Hidden Units"}}
{"text": "here here okay and what would that be that would be the derivative of the loss function I've already computed.\ntill here so then I can just use that part as it is and then the derivative of H I.\nJ with respect to a i j right and remember that uh how did we compute the edges so this.\nis suppose I look at H2 I had h21 H22 H2 3 and how did I compute that I had.\ncomputed that from the a vector and there was a one-to-one correspondence right so H22 was just the G function.\napplied to a22 h21 was just the G function applied to a to 1 and h23 was just the G.\nfunction applied to a to 3. so if I am trying to compute the derivative of H I J with.\nrespect to a i j it's just the derivative of this G function that I had used right so I'm.\njust trying to compute the derivative of this with respect to a i j I'm just going to call that.\nas G Dash right because it could be any function but I am just taking the derivative so this is.", "metadata": {"video_title": "Gradient wrt Hidden Units"}}
{"text": "just trying to compute the derivative of this with respect to a i j I'm just going to call that.\nas G Dash right because it could be any function but I am just taking the derivative so this is.\nwhat it looks like this guy I have already computed this is just G Dash a i j right so.\nnow again from this one element let me go to the entire Vector so now I want the derivative of.\nthe loss function with respect to a i the entire Vector so it is again going to be a collection.\nof the partial derivatives and every element I know how to compute so I have a formula for a i.\nj so now I will go from AI 1 all the way up to a i n I'll just substitute.\nJ equal to 1 to n everywhere in the formula and this is what I get right so this is.\none vector that I have I could think of this as another Vector this is not the dot product between.\ntwo vectors right because the dot product between two vectors gives me a scalar this is the element wise multiplication.", "metadata": {"video_title": "Gradient wrt Hidden Units"}}
{"text": "two vectors right because the dot product between two vectors gives me a scalar this is the element wise multiplication.\nbetween two vectors right this first Vector is simply the the gradient of the loss function with respect to H.\nand this second Vector is just a collection of the derivatives of the uh activations with respect to preactivations right.\nso this element wise matrix multiplication I can show it as this this is called the hadamard product so I.\ncan just multiply every element of the first Vector with every element of the second vector and I'll get this.\nquantity right so now I have computed a i Theta and let's see what it depends on right so it.\ndepends on the derivative of the loss function with respect to h i right so now let's see if I.\nwanted to compute the derivative of the loss function with respect to A2 then I should know the derivative of.\nthe loss function with respect to H2 and then I should know these G's so I already know that that.", "metadata": {"video_title": "Gradient wrt Hidden Units"}}
{"text": "the loss function with respect to H2 and then I should know these G's so I already know that that.\nis not a problem right now suppose I wanted to compute the derivative of the loss function with respect to.\na 1 then I need the derivative of the loss function with respect to H 1 do I know that.\nI know that because I had computed the previous formula and the derivative of H the loss function with respect.\nto H1 depending on the derivative with respect to A2 and I just computed that right so I can just.\ndo it one by one at a time and keep going down the chain so now I have the formula.\nfor computing the derivative of the loss function with respect to the preactivation at any layer with respect to the.\nactivation at any layer and I just keep doing this one by one and whenever I want to compute something.\neverything that I need for that is already compute right I'll just explain that intuition again so I started by.", "metadata": {"video_title": "Gradient wrt Hidden Units"}}
{"text": "everything that I need for that is already compute right I'll just explain that intuition again so I started by.\nComputing the derivative of the loss function with respect to a 3 right that is the first thing that I.\ndid and there was nothing special about A3 it was the activation at the output layer now when I want.\nto compute this okay let me just do this on the previous slide Maybe yeah so I started by Computing.\nthe derivative of the loss function with respect to A3 so this I had already done right then I wanted.\nto compute the derivative of the loss function with respect to h 2 and I was all set because I.\nneed W3 for that and I need the derivative of the loss function with respect to A3 which I had.\nalready computed right so I am ready to compute derivative of H2 so this part is done I have now.\nbeen able to compute this then I wanted to compute the derivative of a 2 with respect to the loss.", "metadata": {"video_title": "Gradient wrt Hidden Units"}}
{"text": "already computed right so I am ready to compute derivative of H2 so this part is done I have now.\nbeen able to compute this then I wanted to compute the derivative of a 2 with respect to the loss.\nfunction and the formula on the next slide I'll just go to the next slide now the formula on the.\nnext slide say is that the derivative of loss function with respect to A2 just depends on H2 and some.\nquantity right which is easy to compute right this G Prime is easy to compute and we'll compute it at.\nthe end right for now just convince yourself because it's a simple function it suppose my uh my my activation.\nfunction was the sigmoid function then I have 1 over 1 plus e raised to minus a 2 2 right.\nH22 is equal to 1 over e raised to 1 over 1 plus e raised to minus a22 and if.\nI want to compute the derivative of H22 with respect to a22 I can do that right I can apply.\nthe UV Rule and keep going and I'll be able to get it right so this quantity is easy to.", "metadata": {"video_title": "Gradient wrt Hidden Units"}}
{"text": "I want to compute the derivative of H22 with respect to a22 I can do that right I can apply.\nthe UV Rule and keep going and I'll be able to get it right so this quantity is easy to.\ncompute okay so that's why I'm not worrying about that yeah but apart from that it only depends on derivative.\nof h 2 and that we just computed on the previous slide right so I'm I have everything that I.\nwant till this point then again I want to compute the derivative of the loss function with respect to H1.\nand I know for that I just need the derivative of the loss function with respect to H2 and I.\nneed W2 these are the two quantities involved and I have that because I just come sorry not H2 uh.\nA2 and W2 right so these are the two quantities that I need for computing derivative of the loss function.\nwith respect to H1 and I just computed now I want to compute the derivative of the loss function with.\nrespect to A1 so for that the two con the quantity that I need is the derivative of the loss.", "metadata": {"video_title": "Gradient wrt Hidden Units"}}
{"text": "with respect to H1 and I just computed now I want to compute the derivative of the loss function with.\nrespect to A1 so for that the two con the quantity that I need is the derivative of the loss.\nfunction with respect to H1 right and apart from that I need this simple quantity which I just showed is.\neasy to compute right so that's if I just keep going one by one at every stage I have all.\nthe quantities that I wanted to uh want or want at that stage for computing the quantity of Interest right.\nso now at this point you should be convinced that we have a generic formula for computing the derivative of.\nthe loss function with respect to the hidden layer no matter where the hidden layer is the first hidden layer.\nsecond third fourth everyone we have this formula so we are almost done and now we'll reach the final part.\nin our uh chain rule so let's look at that in the next module.", "metadata": {"video_title": "Gradient wrt Hidden Units"}}
{"text": "[Music] okay so we have done the intuition behind back propagation now we want to take this intuition forward and.\nget into the mathematical details of how back propagation or how do you compute these partial derivatives or gradients that.\nyou're interested right so this was the outline so we said that we are going to slowly talk to all.\nstakeholders in the neural network and we had divided this into three parts talk to the output layer talk to.\nthe hidden layers and talk to the weights so i'll start with the first part which is talk to the.\noutput layer right so that's what we're going to do now right so let's see what's what do you what.\ndo you mean by talk to the output layer i am interested in computing the derivative of the loss function.\nwith respect to the output layer right so this is the output layer right i collectively call it as the.\noutput layer although it has two parts activation and pre-activation uh i am going to call uh this layer which.", "metadata": {"video_title": "Gradient wrt output units"}}
{"text": "output layer although it has two parts activation and pre-activation uh i am going to call uh this layer which.\nis a the a's is what i am going to call the output layer right which i want i am.\ninterested in the derivative of the output layer of the loss function with respect to this output activations or output.\npre-activations okay so that is the quest that i have and this i know would be a collection of some.\npartial derivatives right so derivative partial derivative of the loss function with respect to the first uh output neuron with.\nrespect to the second output neuron and so on right and this would be the loss function here similarly the.\nloss function here right so this is the quantity this entire vector which is the gradient vector of the loss.\nfunction with respect to the output layer is what i am interested in okay now to begin with right so.\nand i know what the loss function is i know that this is minus log of y hat l where.", "metadata": {"video_title": "Gradient wrt output units"}}
{"text": "function with respect to the output layer is what i am interested in okay now to begin with right so.\nand i know what the loss function is i know that this is minus log of y hat l where.\nl is the true class label okay and what i'm going to do is i'm going to focus first on.\nthe upper half right which is the green dark shaded green portion right so what do i have here i.\nhave y 1 hat y 2 hat all the way up to y k hat okay and my loss function.\nis minus log y hat l that means one of these the negative of the log of one of these.\nvalues which value the one which corresponds to the true class label right so that's what the situation is so.\nthat's what my loss function is and i'm trying to take its derivative with respect to one of the values.\nit could be y one y two all the way up to y k right so now this is what.\ni am trying to do i am trying to take the derivative of the loss function which has the term.\ny hat l with respect to one of the y i's okay and we know that this log y hat.", "metadata": {"video_title": "Gradient wrt output units"}}
{"text": "i am trying to do i am trying to take the derivative of the loss function which has the term.\ny hat l with respect to one of the y i's okay and we know that this log y hat.\nl depends only on one of the elements in this array the one which corresponds to the true loss function.\nright so in particular let's take an example suppose l is equal to 2 that means the second class was.\nthe correct class then the loss function would in effect be minus log hat y2 and now i have trying.\nto take the derivatives of this with respect to any one of these right because i have considered a generic.\ni here right so this is an i so any one of those elements so if i'm trying to compute.\nwith respect to y1 hat of course the loss would be zero right because this quantity does not depend on.\ny one hat there is no y one hat that shows up here if i try to compute with y.\ntwo hat then it would be minus 1 over y2 hat because that's the derivative of so if you want.", "metadata": {"video_title": "Gradient wrt output units"}}
{"text": "y one hat there is no y one hat that shows up here if i try to compute with y.\ntwo hat then it would be minus 1 over y2 hat because that's the derivative of so if you want.\nto compute the derivative of log x with respect to x that's 1 over x so the same thing apply.\nhere instead of x we have y 2 right if i want to compute with any of the other guys.\ny 3 hat y 4 add up to y k hat it's going to be 0 right so that's that's.\nvery straight forward so what does it mean it's like an if else condition right so this is going to.\nbe equal to minus 1 of 1 over y hat l if i and l tend to be is are.\nthe same that means the quantity that i am taking the derivative with is also the quantity that is there.\ninside the log then it would be minus 1 over y hat l otherwise it would be 0 because then.\nthere is i'm taking the derivative of one variable with respect to another and they don't depend on each other.", "metadata": {"video_title": "Gradient wrt output units"}}
{"text": "there is i'm taking the derivative of one variable with respect to another and they don't depend on each other.\nright so this is very straightforward i have computed the derivative of the loss function with one of the entries.\nhere any generic entity here and it would either be zero or one right so how do i write this.\nformally does anyone know that so some of you might have heard of this indicator variable right so what does.\nan indicator variable mean so let me just explain so i am going to write the derivative of l theta.\nwith respect to y i and again i repeat i can go from 1 to up to k right is.\nequal to the indicator variable so this one here is the indicator variable i should have written it as this.\nkind of a one okay and what's the condition associated with that i have suffixed something there so it's this.\nis what it looks like i equal to l so what that means is that if i is equal to.", "metadata": {"video_title": "Gradient wrt output units"}}
{"text": "kind of a one okay and what's the condition associated with that i have suffixed something there so it's this.\nis what it looks like i equal to l so what that means is that if i is equal to.\nl then this indicator will take on the value 1 otherwise it will take on the value 0. so that's.\nwhat an indicator value variable means it just encodes this if else condition right so indicator variable is this is.\nhow you write it and it will take on the value one if the condition is satisfied that the condition.\nin the suffix is satisfied otherwise not right so you can now agree that these two are the same right.\nthis is just a more compact way of writing that right okay so what do i have so far i.\nhave the derivative the partial uh derivative of one uh of the ayat da shaded green guy sorry the partial.\nderivative of the loss function with respect to the ith shaded green guy right but we are all we are.", "metadata": {"video_title": "Gradient wrt output units"}}
{"text": "derivative of the loss function with respect to the ith shaded green guy right but we are all we are.\nnot interested in one partial derivative we are interested in the entire collection of partial derivatives so i want to.\ntalk about the gradient with respect to the vector y hat not just the partial derivative with respect to one.\nof the elements of the screen right so what do i mean by that so this is i'm going to.\nwrite the gradient of the loss function with respect to y hat right so this is what my notation is.\nand what does this mean right this is what it means it's just a collection of the partial derivatives of.\nthe loss function with respect to all the elements of y hat and what are all the elements of y.\nhat y 1 hat y 2 hat all the way up to y k hat right so it's just a.\ncollection of all these guys and i have the formula for any one of them right so this is what.\nthe formula looks like the formula was indicator variable l equal to i divided by y hat l and the.", "metadata": {"video_title": "Gradient wrt output units"}}
{"text": "collection of all these guys and i have the formula for any one of them right so this is what.\nthe formula looks like the formula was indicator variable l equal to i divided by y hat l and the.\nminus of that right so uh if i want to compute the derivative of the loss function with respect to.\ny 1 hat in that case i is equal to 1 so hence i will write i equal to 1.\nhere and l of course remains the same right l does not change so that 1 by y hat l.\nand the negative of that comes outside and this indicator variable just changes as i go down the vector the.\ncondition associated with the indicator variable changes right and since only one of the uh i mean l could take.\non any one of the k values this vector will have one in only one position and zero everywhere else.\nright for example uh on the previous slide we were discussing that suppose l equal to two that means the.\ncorrect class is the second class then when i evaluate this indicator variable which is l equal to 1 i'll.", "metadata": {"video_title": "Gradient wrt output units"}}
{"text": "correct class is the second class then when i evaluate this indicator variable which is l equal to 1 i'll.\nget the answer as 0 for the next one i'll get the answer as 1 and everywhere else i'll get.\na 0 right similarly if l is equal to 3 then i'll get 0 0 1 and then all 0.\nright so any only one of these elements would be one so these kind of vectors are called one hot.\nvector because only one of their entries is going to be one right so let me just clear the slide.\nokay so this is what it looks like i'm going to call this vector as the one hot vector e.\nl where this contains a one in the l position this is a vector containing one in the l position.\nand zero everywhere else and that's just a more compact way of writing this i have not done anything new.\ni've just introduced a new notation for the vector e right okay ah so so far so good right so.\nwhat have we been able to do so far remember that this was h3 this part was h3 which was.", "metadata": {"video_title": "Gradient wrt output units"}}
{"text": "i've just introduced a new notation for the vector e right okay ah so so far so good right so.\nwhat have we been able to do so far remember that this was h3 this part was h3 which was.\nthe same as y hat sorry i should have written h 3 in the suffix right so i have the.\nderivative of the loss function with respect to y hat but that is not the quantity that i was interested.\nin i was interested in the derivative of the loss function with respect to a 3 right or a l.\nbecause l equal to 3 in this case right so that i have not done yet but what i have.\ndone the reason i did this part was to give you the uh give you like with help of a.\nsmaller example right try to explain how this entire discussion is going to go what did we do we first.\ncomputed the partial derivative with respect to one of the elements so we were actually interested in the partial in.\nthe gradient with respect to the entire vector but we didn't go there directly we first computed the partial derivative.", "metadata": {"video_title": "Gradient wrt output units"}}
{"text": "the gradient with respect to the entire vector but we didn't go there directly we first computed the partial derivative.\nwith respect to one of the elements and then once we had that we were able to write down the.\nformula for the entire vector right uh so this same recipe is going to repeat everywhere we are going to.\nfirst try to if you have any interest in a derivative of some quantity with respect to vector we'll first.\ntry to find out the partial derivative with respect to one element of that vector right and then generalize for.\nthe entire vector okay that's what we are going to do so now with that let me just try to.\nfind the derivative of the loss function with respect to a l and what i am going to do is.\nfirst try to find the derivative with respect to one of the elements which is the ith element so again.\na l is actually a l 1 a l 2 all the way up to a l k right so.\nfrom with respect to any generic element of that vector i am trying to find the derivative of the loss.", "metadata": {"video_title": "Gradient wrt output units"}}
{"text": "a l is actually a l 1 a l 2 all the way up to a l k right so.\nfrom with respect to any generic element of that vector i am trying to find the derivative of the loss.\nfunction what is the loss function minus log y hat l so now what is this going to be i.\ncan split it into two parts i can write it as the derivative of the loss function with respect to.\ny hat l and then the derivative of y hat l with respect to a l i right they can.\nsplit it into these two paths okay and now the question is does y hat l depend on a l.\ni right so what am i asking this is what i'm trying to ask right so this is what my.\na vector looks like okay and my i here in this is anything from 1 to k right and let.\nus again consider the case when the correct output was uh 2 right so that means my loss function is.\nminus log y hat 2 okay and that's why this i am trying to compute the derivative of y hat.\n2 with respect to some ith element right it could be a l 1 a l 2 a l 3.", "metadata": {"video_title": "Gradient wrt output units"}}
{"text": "minus log y hat 2 okay and that's why this i am trying to compute the derivative of y hat.\n2 with respect to some ith element right it could be a l 1 a l 2 a l 3.\nanything right so now i'm asking does y hat to depend on all these a's or does it only depend.\non one of them right because earlier we saw that the loss function depended on only on one of the.\ny hats and not the entire vector for all the other elements the derivative was 0 but now for y.\nhat 2 which is this green guy not the loss function right this shaded green guy does it depend on.\neverything every element of a it does right because how did you compute y hat y hat was computed using.\nthe soft max function so y hat 2 in particular was e raised to a l 2 divided by the.\nsummation of e raised to all a i's right so it depends on all the ais because all the ai.\nali's appear in the denominator right so it's a function of all the allies right this is how we had.", "metadata": {"video_title": "Gradient wrt output units"}}
{"text": "summation of e raised to all a i's right so it depends on all the ais because all the ai.\nali's appear in the denominator right so it's a function of all the allies right this is how we had.\nuh computed it right so this this should be clear uh that now we are trying to take the derivative.\nof y hat l with respect to a l i and hence it's a function of all the variables okay.\nso now that we have understood this and we understood what the final formula for y hat l is let's.\ntry to find it so let's try to derive this okay okay so this is what we are trying to.\ndo right we are trying to compute the derivative of the loss function with respect to one of the elements.\nin the al vector okay now this is going to be and i've already established that y hat l depends.\non all the a's right so i it's not going to be the case of 1 in some cases and.\n0 in other cases right that the derivative is 0 in some cases and not 0 in some cases the.", "metadata": {"video_title": "Gradient wrt output units"}}
{"text": "on all the a's right so i it's not going to be the case of 1 in some cases and.\n0 in other cases right that the derivative is 0 in some cases and not 0 in some cases the.\nderivative would always exist now this is a derivative of the form derivative of the form of log of a.\nquantity with respect to a variable and that quantity depends on that variable so this would just be uh 1.\nby y hat l my 1 by y hat l and then the derivative of y hat l with respect.\nto a li right so this is just the chain rule applied right so this if you have derivative of.\nlog of x square with respect to x right then you will first write it as 1 by x square.\nand the derivative of x square with respect to x so the same thing is happening here here now instead.\nof x and x squared which is very straightforward i have a l i and i have y hat and.\njust as x square is a function of x y hat is a function of a l i so the.", "metadata": {"video_title": "Gradient wrt output units"}}
{"text": "of x and x squared which is very straightforward i have a l i and i have y hat and.\njust as x square is a function of x y hat is a function of a l i so the.\nsame thing applies here i'll first take the derivative of log of y hat with respect to ali which would.\nbe just minus 1 by y hat li and then the derivative moves inside just as what has happened here.\nright this high school calculus i am sure you know this but i just did it in some detail okay.\nnow let me just replace y hat l by what exactly is y hat n right so let me explain.\nwhat this formula means right so what was y hat is actually the soft max of a l what does.\nthat mean that a l was a vector right which had these components a l 1 a l 2 all.\nthe way up to a l k and then how did i compute y hat from that y hat was.\ncomputed by applying the soft max function which was e raised to a l 1 divided by the summation e.\nraised to a l 2 divided by the summation and so on right so this y hat is a vector.", "metadata": {"video_title": "Gradient wrt output units"}}
{"text": "computed by applying the soft max function which was e raised to a l 1 divided by the summation e.\nraised to a l 2 divided by the summation and so on right so this y hat is a vector.\na l is a vector so this soft max of a l is actually a vector and what this is.\nsaying is that i am looking at the lth element of that vector which is the same as y hat.\nl right so that's what this formula is saying nothing great here and what is the l element of y.\nhat l it's going to be e raised to a l again suffix with l divided by the summation of.\nall the exponents right so that's what y hat l means that's what softmax off there's a softmax of the.\nvector a l and then taking the small l component of that right so that's what that notation means okay.\nlet's go ahead so i'm just going to uh as i said what i just explained that y hat l.\nwhich is the same as soft max a l l is essentially this guy right so i've just put in.", "metadata": {"video_title": "Gradient wrt output units"}}
{"text": "let's go ahead so i'm just going to uh as i said what i just explained that y hat l.\nwhich is the same as soft max a l l is essentially this guy right so i've just put in.\nthe formula for that sorry this should have been i right because here the index is over i uh so.\ni'm summing over all the exponents okay now this is the derivative of the form u by v or i.\ncan call it as derivative of the form g x by h x right and we all know the formula.\nfor that this is the formula for the derivative of u by v right and you would have again known.\nthis formula i will not go into the details of that you can refer to it if you don't know.\nthis just maybe quickly brush up some calculus and you will realize that this is the formula okay okay so.\nlet's go ahead now uh so i'm just going to supply substitute this formula blindly right so this is g.\nof x for me and this is h of x for me so i'm just going to substitute g of.\nx and h of x so this is derivative of g of x with respect to x so i'll have.", "metadata": {"video_title": "Gradient wrt output units"}}
{"text": "of x for me and this is h of x for me so i'm just going to substitute g of.\nx and h of x so this is derivative of g of x with respect to x so i'll have.\nderivative of this is my g of x with respect to a l i into 1 by h x so.\ni'm just dividing it by h of x right and then uh i'll not explain this entire formula it's like.\ntwo conversion uh to speak it out right it's not uh difficult but it's just too cumbersome to speak it.\nout this is again g of x as you would as you would notice this is again h of x.\nso this is just what i'm substituting then this is h of x square right so all of this is.\nuh the routine substitutions that i'm making in the original formula once you get that this is g and this.\nis h and you buy this formula the rest of it is just plain substitution right now let's try start.\ncomputing these derivatives so now i am taking derivative of a l i with respect to uh the exponent of.\na l l right so let's see what am i trying to do again i had this vector a l.", "metadata": {"video_title": "Gradient wrt output units"}}
{"text": "computing these derivatives so now i am taking derivative of a l i with respect to uh the exponent of.\na l l right so let's see what am i trying to do again i had this vector a l.\nwhich had components a l 1 a l 2 all the way raised to a l k right now i.\nam considering this quantity which is what e raised to a l l okay there's no denominator here right it's.\njust e raised to a l i'm trying to take the derivative of that with respect to one of the.\nuh guys here right and again you can notice that this only depends on uh one of the elements here.\nwhere corresponding to the l entry here it does not depend on the other guys so this would be zero.\nif i happens to be anything other than l otherwise it would be 1 right and we just saw how.\nto deal with this on the previous slide so i am going to write it as this that it's going.\nto be e raised to x right you understand what i am calling as e raised to x so the.", "metadata": {"video_title": "Gradient wrt output units"}}
{"text": "to deal with this on the previous slide so i am going to write it as this that it's going.\nto be e raised to x right you understand what i am calling as e raised to x so the.\nderivative of e raised to x with respect to x is e raised to x itself here instead of x.\nwe just have a l i right so it's going to be e raised to x if l is equal.\nto i just as i explained here sorry oops okay so it's going to be uh if l is equal.\nto i then the derivative would be e raised to this quantity otherwise it would be 0 hence the indicator.\nvariable then i have copied the denominator as it is from the other place right now this quantity i have.\ncopied as it is here the denominator i have split into two parts right so it's squares i've just put.\ninto two separate parts and now i have this right so let's see what that quantity is so here i.\nhave the sum of the exponents so what does that mean i have e raised to a l 1 plus.\ne raised to a l 2 plus all the way e raised to a l k and i'm taking the.", "metadata": {"video_title": "Gradient wrt output units"}}
{"text": "have the sum of the exponents so what does that mean i have e raised to a l 1 plus.\ne raised to a l 2 plus all the way e raised to a l k and i'm taking the.\nderivative of this sum with respect to one of the e l i's right and what will happen all the.\nelements in this sum will disappear except the one which corresponds to e a l i hence the derivative would.\njust be e raised to a l i right so this is again straightforward i have a sum of terms.\nonly one of them depends on the variable with respect to where i'm com i'm computing the derivative so only.\nthat term will rem remain everything else will disappear right so all of this looks uh pretty straightforward don't worry.\nif you're not getting is as i'm explaining it you can just pause the video and look at it it's.\nvery mechanical steps right so it's this exp just looks very annoying but it's just e raised to something right.\nand you know the derivative of e raised to something with respect to that same variable is just e raised.", "metadata": {"video_title": "Gradient wrt output units"}}
{"text": "and you know the derivative of e raised to something with respect to that same variable is just e raised.\nto x right so that's the only formula that is being applied here right uh okay so now let's get.\nback what is this we just did that do you remember what that is so in the bottom i have.\nthe sum of all the exponents right so again let me just do this so i had a l1 al2.\nup to al k so this the guy the denominator is just the summation of the exponents of all these.\nquantities so that's what this what denominator is and what is the numerator the numerator is the exponent with respect.\nto the ith guy so that means it's just the soft max if you consider the soft max vector which.\ni had said which was y hat then am just looking at the ith entry of that vector so this.\nis just y hat i right that's what it is this is the ith entry of the softmax vector similarly.\nthis is the lth entry of the softmax vector similarly this is the ellith sorry this should not have been.", "metadata": {"video_title": "Gradient wrt output units"}}
{"text": "this is the lth entry of the softmax vector similarly this is the ellith sorry this should not have been.\nl equal to i so this should have been l entry right not i dash this should have been the.\nl entry of the vector and now i can just write all this as this guy i can just write.\nit as y hat l this guy i can write it as y hat l and this guy i can.\nwrite it as y hat i right and now of course the y hat l here and here gets cancelled.\nand so i have i'm just left with this which is uh minus 1 l equal to i minus y.\nhat i right so this is what i have left with again don't fret too much if you did not.\nget it at one go you just pause the video and you will get it at this simple set of.\nsteps so this is where we are right so we have the derivative of one of the loss function with.\nrespect to one of the elements of my output vector right and this is what it turns out to be.\nand now i want to compute the derivative of the loss function with respect to my entire output vector so.", "metadata": {"video_title": "Gradient wrt output units"}}
{"text": "respect to one of the elements of my output vector right and this is what it turns out to be.\nand now i want to compute the derivative of the loss function with respect to my entire output vector so.\nthis is the quantity that i am interested in and i know that is just a collection of these partial.\nderivatives right so it's the derivative with respect to a l 1 a l 2 all the way up to.\na l k and i know the formula for each of these guys right because this formula is for i.\nany generic i so i can substitute i equal to 1 2 3 so on right so i can just.\nsubstitute i as the appropriate value here and this is what i'll get right so minus indicator variable l equal.\nto 1 y minus y 1 hat y 2 all the way up to y k so i have just.\nsubstituted the i by the appropriate index and just expanded the vector right i've written down the full vector now.\nthis here is just my y hat vector and this here is just my one hot vector which in which.", "metadata": {"video_title": "Gradient wrt output units"}}
{"text": "this here is just my y hat vector and this here is just my one hot vector which in which.\nonly the lth entry would be one and everything else would be zero just as we had done on the.\nprevious slide so this is just the difference between two vectors okay and the minus sign has been accommodated for.\nappropriately right so i think [Music] yeah okay this minus sign should have been outside right because this was there.\nyeah whichever way you could have thought of this as minus plus minus plus minus plus and then you are.\nleft with this right so these signs here are consistent with the signs that you had here right that's what.\nis important right okay so we are done with this so what have we done so far uh we have.\ndone the derivative we have done till this far right remember we were talking to every layer and this was.\nour first goal that talked to the output layer so we now know the derivative of the loss function with.", "metadata": {"video_title": "Gradient wrt output units"}}
{"text": "our first goal that talked to the output layer so we now know the derivative of the loss function with.\nrespect to the output layer that means we know our first gradient right which is the gradient of the loss.\nfunction with respect to output layer this is a scalar quantity this is a vector so what this tells you.\nis that what happens if i change one value of each of the values of this al vector then what.\nis the change in l theta right that is what each of these elements captures and the collection of all.\nthose elements is just the gradient vector the gradient of the loss function with respect to a f right so.\nwe are done with the first step that we had which was to talk to the output layer now we.\nare going to talk to the hidden layers and then we are going to talk to the waves okay so.\nwe will come back.", "metadata": {"video_title": "Gradient wrt output units"}}
{"text": "foreign [Music] with respect to the parameters where are we we finished this part then we finished this part we.\ndon't care how many hidden layers are there we'll just keep applying the same formula again and again so we.\nare done with this very nasty part which could have repetitions there could be many hidden layers and now we.\nare finally ready to compute this okay so let's go there okay uh so let's look at that right so.\nwhat do we want now we want the derivative of the loss function with respect to one of the weight.\nmatrices right w k and I want this to be again generic I don't it should not the formula I.\nneed not have to recompute the formula for W1 W2 W3 I should just be able to do it for.\na generic uh WK that's what I'm interested in and again my recipe is going to be the same instead.\nof computing this uh gradient at one go right so this is the derivative of a scalar with respect to.", "metadata": {"video_title": "Gradient wrt Parameters"}}
{"text": "a generic uh WK that's what I'm interested in and again my recipe is going to be the same instead.\nof computing this uh gradient at one go right so this is the derivative of a scalar with respect to.\na matrix so this is again going to be a matrix but instead of computing all the elements the entire.\nMatrix at one go I am first going to compute the derivative of one of the elements of this Matrix.\nright so what I'm going to do is I just ignore this part for now I'm going to let it.\nbe I'm going to compute the derivative of the loss function with respect to one element of the Matrix so.\nthat could be say w k i j right so this Matrix has n cross n or N1 cross N2.\nentries so I'm going to focus on Computing the derivative with respect to one of these entries right and now.\nlet me just link it to the diagram that I have suppose this is the entry that I am looking.\nfor which in this case tragically happens to be W 2 2 right so it's the yeah it's the weight.", "metadata": {"video_title": "Gradient wrt Parameters"}}
{"text": "let me just link it to the diagram that I have suppose this is the entry that I am looking.\nfor which in this case tragically happens to be W 2 2 right so it's the yeah it's the weight.\nconnecting the second neuron in this layer to the second neuron in this layer so it's uh the 2 comma.\n2 entry of the W2 mid so that's what that is and what have I done so far right so.\nfar I have done derivatives up to this point right I've already done the derivative of the loss function with.\nrespect to every element of this layer right and now I see that the weight is only connected to one.\nof the guys here right so that guy may be a two two the weight is only connected to that.\nso now I'm going to exploit this fact right so if I want to compute yeah so I want the.\nderivative of the loss function with respect to one of the entries in The Matrix and now I can again.\nsplit it into two parts right so let's see I already know the derivative of the loss function with respect.", "metadata": {"video_title": "Gradient wrt Parameters"}}
{"text": "split it into two parts right so let's see I already know the derivative of the loss function with respect.\nto this guy right which is uh a k i right this is the ith neuron in the kth layer.\nI already know the derivative of the loss function with respect to that and so I can find the derivative.\nof the loss function with respect to a k i I already have some formula for that from the previous.\nlectures and then the derivative of a k i with respect to w k i j right now this how.\ndo I compute this is the question and this is again going to be very uh straight forward so let's.\nsee what that is it just turns out to be h k i j right so let's see why that.\nis the case okay so now uh let's see a k again I'll take a simple case of a 2.\ncross so Suppose there are only two neurons here or maybe let's take all the three neurons okay right and.\nnow this so this is the AK Vector I'm going to just ignore B K I'm just going to write.", "metadata": {"video_title": "Gradient wrt Parameters"}}
{"text": "cross so Suppose there are only two neurons here or maybe let's take all the three neurons okay right and.\nnow this so this is the AK Vector I'm going to just ignore B K I'm just going to write.\nit at the end somewhere now this is my w Matrix so this is w k 1 1 uh this.\nis going to be w k 1 3 uh this is w k 2 1 I'm just going to write.\nthe second row completely w k 2 3 okay and this multiplied by h k minus 1 which is just.\ngoing to be h uh K minus 1 in this case uh so I had taken certain values okay let.\nme just write it as K in this case was 2 right K was equal to 2 uh so my.\nh k minus 1 is going to be H 1 1 H 1 2 H 1 3 right and plus.\nI have the B Vector which is going to be I I'll ignore the B Vector right because you'll see.\nthat it does not show up in the formula right now this a K2 how do I get a K2.\nI get it by multiplying this by this right so if I open that up it's going to be w.\nk 2 1 into h11 plus w k 2 2 into H 1 2 plus w k 2 3 into.", "metadata": {"video_title": "Gradient wrt Parameters"}}
{"text": "I get it by multiplying this by this right so if I open that up it's going to be w.\nk 2 1 into h11 plus w k 2 2 into H 1 2 plus w k 2 3 into.\nh 2 3 and plus there would be this bias term right which would be B oh okay 2 in.\nthis case Okay so that's that's what it is going to be and now I'm taking the derivative of this.\nquantity with respect to uh one of the weights right so what is the weight that I am looking at.\nI'm looking at w k i j in which this case the wkij is w k 2 2. so all.\nof these other terms are going to disappear right that's why I said the bias would matter in fact more.\nterms don't matter and the derivative of this quantity with respect to w k 2 is just going to be.\nH 1 2 right and now let's understand what is uh H what is this what are these indices one.\nand two here so this was just the uh I and J right so that's what this one and two.\nare so I was trying to take the derivative of uh a I was trying to take the derivative of.", "metadata": {"video_title": "Gradient wrt Parameters"}}
{"text": "and two here so this was just the uh I and J right so that's what this one and two.\nare so I was trying to take the derivative of uh a I was trying to take the derivative of.\na K2 with respect to w k this was the guy right so this was again 2 2 and my.\nthe answer which I got was h one two in this case right so this is actually I minus 1.\nand this is J so that's what our formula is going to be right so let me just delete all.\nof this and show you the formula it's going to be uh K minus 1 sorry that was K minus.\n1 so I got H 1 2 so this one is for the previous layer so it's K minus 1.\nand the jth neuron in that layer so G right so that's how I got this answer so we just.\ndid that derivative I'm sorry I made some mistakes there but if you go back and look at it I.\nthink it should be fine so we got this as the answer so this is the derivative of the loss.\nfunction with one element of the weight Matrix w k right so if I look at the derivative of the.", "metadata": {"video_title": "Gradient wrt Parameters"}}
{"text": "think it should be fine so we got this as the answer so this is the derivative of the loss.\nfunction with one element of the weight Matrix w k right so if I look at the derivative of the.\nloss function with respect to the entire Matrix then it's going to be a collection of these entries and for.\nany guy here I now have the formula I can just substitute those formula right so that's what I'm going.\nto do now and I'm going to do that for this simple case when w k is 3 cross 3.\nMatrix right so the derivative of the loss function with respect to w k is going to be a part.\ncollection of the partial derivatives with respect to all elements of w k and there are nine such elements okay.\nand the formula for any such guy was this so now I'm going to substitute this formula to get this.\nMatrix right so what is this I'm going to just substitute the right values of K and I so this.\nis going to be derivative of K1 and then this is going to be K minus 1 1 this is.", "metadata": {"video_title": "Gradient wrt Parameters"}}
{"text": "Matrix right so what is this I'm going to just substitute the right values of K and I so this.\nis going to be derivative of K1 and then this is going to be K minus 1 1 this is.\nthe formula that we had computed on the previous slide right this was h of K minus 1 comma J.\nso I'm just going to substitute the values of I and J right so this is of K and J.\nso this is J and you already know K so this is what I get right so this uh I've.\njust substituted everything from the previous slide onto this slide okay now let's look at notice something about this Matrix.\nright so all the guys in this column all the terms that are getting multiplied are the same same for.\nthe next column all of these guys are the same next for same for the next column the other interesting.\nthing is that across the rows the entries are the same right these three guys are the same these three.\nguys are the same and these three guys are the same right so if I were to take say if.", "metadata": {"video_title": "Gradient wrt Parameters"}}
{"text": "thing is that across the rows the entries are the same right these three guys are the same these three.\nguys are the same and these three guys are the same right so if I were to take say if.\nI were to take two vectors right a one A2 A3 multiplied by B1 B2 B3 right so this is.\nknown as the outer product of two vectors this is a three cross one vector multiplied by a one cross.\nthree Vector so the product is going to be a three cross three Matrix right so let's look at what.\ndo the entries in The Matrix look like the entries in The Matrix would look something like this it would.\nbe a one into B1 A1 into B2 A1 into B3 okay and then uh a 2 into B1 A2.\ninto B2 A2 into B3 and then A3 into B1 A3 into B2 A3 into B3 right so you have.\nthe same situation that you have uh here right if you compare all these guys were the same again all.\nof these are b1s all these guys are the same all of these are V2s all these guys are the.", "metadata": {"video_title": "Gradient wrt Parameters"}}
{"text": "the same situation that you have uh here right if you compare all these guys were the same again all.\nof these are b1s all these guys are the same all of these are V2s all these guys are the.\nsame all of these are p3s right and similarly all these three guys are the same just as these three.\nguys were the same and so on right so what I'm trying to say is that this Matrix here can.\nactually be written as the outer product of two vectors and what are those two vectors this is one vector.\nright so this will be your B vector and uh this will form your a vector right so that's how.\nI'm going to write it now the outer product between these two vectors so this is what it's going to.\nlook like this is the a vector and this is the B Vector the transpose operated because that was the.\nsleeping vector and the a vector was the standing Vector right so now you have a formula for the derivative.\nof the loss function with respect to K and what do you need for that you need two quantities one.", "metadata": {"video_title": "Gradient wrt Parameters"}}
{"text": "of the loss function with respect to K and what do you need for that you need two quantities one.\nis the derivative of the loss function with respect to a k that we have already done in the previous.\nlecture so I can compute this the other is the activation at the K minus 1 layer and that you.\nanyways compute in the forward pass right so you compute you start with the input you do the First Transformation.\nto that which is W X plus b then you get the first activation layer preactivation layer then you apply.\nthe activation on that to get the activation layer so this is does not require any derivatives any computations right.\nthis is just a forward pass and you know how to compute every element in the network during the forward.\npass so this does not have a gradient Associated it is not some derivative it's just h k minus this.\none which is the activation at the K minus 1 at layer so that you already have so both these.", "metadata": {"video_title": "Gradient wrt Parameters"}}
{"text": "pass so this does not have a gradient Associated it is not some derivative it's just h k minus this.\none which is the activation at the K minus 1 at layer so that you already have so both these.\nquantities you have so you now know how to compute the derivative of the loss function with respect to w.\nk okay now going to the last part we just come find the biases so you want to compute the.\nderivative of the loss function with respect to bias again the same idea so if I look at this bias.\nokay and this is connected to this activation pre-activation here and I know what the formula for computing that pre-activation.\nwas so now if I want to take the derivative of the loss function with respect to one of the.\nelements of the bias Vector then I can split it into these two parts this again I already know how.\nto compute and what is the derivative of a k i with respect to bki you can just see from.", "metadata": {"video_title": "Gradient wrt Parameters"}}
{"text": "elements of the bias Vector then I can split it into these two parts this again I already know how.\nto compute and what is the derivative of a k i with respect to bki you can just see from.\nthis formula that it's just equal to 1 8 so this part will disappear and you're just taking the derivative.\nof b k i with respect to bki which is one so the only thing that you will get is.\nthis so this is now we can have the gradient Vector so uh the derivative of the loss function with.\nrespect to the BK Vector is just going to be a collection of all these partial derivatives which is just.\nthe derivative of the loss function with respect to a k right because each of these entries is just the.\npartial derivative of the loss function with respect to one of the elements of a k so collection of all.\nthose is just going to be the gradient of the loss function with respect to a k right so we.\nare done we have done the derivatives of the loss function with respect to the weights and the biases that.", "metadata": {"video_title": "Gradient wrt Parameters"}}
{"text": "are done we have done the derivatives of the loss function with respect to the weights and the biases that.\nis what our eventual goal was right now we are going to just recap all of this and try to.\nput this into one algorithm in the next video thank you.", "metadata": {"video_title": "Gradient wrt Parameters"}}
{"text": "foreign [Music] technique that we're going to see is about adding noise to the inputs so earlier I used to.\nhave a different sequence in the course where I used to First teach Auto encoders and then do that but.\ndo this lecture but now I've changed that so I'll just make some changes on this slide accordingly so we.\nwill see this in the auto encoder but for now let's assume you have the following setup that you are.\ngiven a certain input and you have say a one layer Network just ignore this for now right I'll come.\nback to this soon so this is your inputs assume this layer is not there this layer is directly feeding.\ninto this and then you have the output right so this is what your normal neural network would look like.\nnow what weight uh adding weight uh noise to the weights what this method does is you take your inputs.\nlet's assume your inputs were say binary for the sake of simplicity right now what you do is you add.", "metadata": {"video_title": "Injecting Noise at Inputs"}}
{"text": "let's assume your inputs were say binary for the sake of simplicity right now what you do is you add.\na noise process such that for every input say with probability 80 percent it will keep the input as it.\nis and with probability 20 percent it will flip the input right that is a very simple noise process so.\nif you have like 100 uh digits as input or an input belongs to R 100 or in this case.\nuh 0 comma 1 raised to 100 because there are only binary inputs then uh with probability eighty percent it.\nwill keep each of those hundred values as they were and with probability 20 it will flip it so in.\neffect what you would expect is that twenty percent of your inputs has been corrupted right because twenty percent of.\nit will get a change right so now you have added noise to the input and now once you have.\nadded noise to the input what is a deep neural network good at it is good at mapping x to.", "metadata": {"video_title": "Injecting Noise at Inputs"}}
{"text": "it will get a change right so now you have added noise to the input and now once you have.\nadded noise to the input what is a deep neural network good at it is good at mapping x to.\nthe true y right that means it's very good at reducing the training error but now from this x you.\nhave given created a random X like a modified X or a corrupted X sorry not a random X but.\na corrupted X and now it is trying to map this corrupted x to the input right and now every.\ntime you see this training instance this corruption would be slightly different so across epochs as you are modifying the.\nevery time you are applying this noise process you are seeing a slightly different corrupted version of the original input.\nand every time it has to map that to the same y so now again the job of the network.\nhas become harder because it's it's in a way similar to what you did with the augmentation of the data.\nset right so you had an original two and then you passed it uh shifted to or rotated two and.", "metadata": {"video_title": "Injecting Noise at Inputs"}}
{"text": "has become harder because it's it's in a way similar to what you did with the augmentation of the data.\nset right so you had an original two and then you passed it uh shifted to or rotated two and.\nnow both of these it needs to map to the label two right so now it has a more to.\nlearn and same thing you're doing here you had an original X you corrupted it and it has to map.\nit to Y next Epoch you again corrupted it but this time the corruption would be different maybe in the.\nfirst Epoch uh the say the first 20 bits were corrupted now maybe the bits from 20 to 40 are.\ncorrupted and so on it's every time it's seeing a different corrupted version of the input right and this this.\nindex here is the epoch or that I'm looking at and it has to map all of those two wire.\nright so now memorizing the input becomes or this taking the input memorizing it and mapping to the Y becomes.\ninput because now your input is every time a bit corrupted so it has to deal with these Corruptions also.", "metadata": {"video_title": "Injecting Noise at Inputs"}}
{"text": "input because now your input is every time a bit corrupted so it has to deal with these Corruptions also.\nand now what we can actually show right that for a simple input output Network right that means there is.\nno uh hidden layer so you have a uh so I have a bunch of inputs and then you directly.\nhave the output right so there is no in no input output layer so your inputs are say X1 to.\nx n and now if you add some noise to all of your inputs right and if that noise is.\na gaussian noise that means say that noise is coming from a gaussian distribution zero mean and some variance then.\nyou can show that this is actually equal to L2 regularization right and we will see that on the next.\nslide so this is the setup that we are considering this is exactly what I drew on the previous slide.\nthat you have these n inputs and to each of these inputs you have added some gaussian noise okay and.", "metadata": {"video_title": "Injecting Noise at Inputs"}}
{"text": "that you have these n inputs and to each of these inputs you have added some gaussian noise okay and.\nthe noise added to each input is independent of the other inputs and then you're trying to predict a y.\nokay and the noise is coming from zero mean and some variance so now your X tilde that the corrupted.\nX is the original X plus this noise and now your original y hat right without the corruption your this.\nis what your y hat should have looked like right there's no surprising here no surprising I have not used.\nany non-linearity this is just like a linear transformation right so your model is simply Y is equal to W.\ntranspose X this is a linear model that you're using here right and so this is what your y hat.\nshould be but now instead of Y hat you are actually Computing y tilde and why do I say y.\ntilde because now you don't have X's you have X tildas right you have the corrupted axis now for the.", "metadata": {"video_title": "Injecting Noise at Inputs"}}
{"text": "should be but now instead of Y hat you are actually Computing y tilde and why do I say y.\ntilde because now you don't have X's you have X tildas right you have the corrupted axis now for the.\nX if I substitute the value of x tilde as X Plus Epsilon then I can just rewrite this summation.\nas follows this is what will happen right so it is x i x i is equal to X Plus.\nEpsilon so that summation now splits into two parts and the first part of course is the same as your.\ny hat so I can write y tilde as y hat plus W I uh plus summation w i Epsilon.\nI okay now what are we interested in we are interested in this expected error now the expected error what.\nis it that I am Computing this is the output based on the corrupted input and this is the true.\noutput right this is not y hat remember this is not y hat this is the true output the true.\nlabel that was given to me right and we are interested in this expected error this is what we have.", "metadata": {"video_title": "Injecting Noise at Inputs"}}
{"text": "output right this is not y hat remember this is not y hat this is the true output the true.\nlabel that was given to me right and we are interested in this expected error this is what we have.\nbeen doing throughout when we are looking at bias variance trade-off right so this is the quantity that I am.\ninterested in so let me try to find out what that quantity is so I'm just going to now the.\nrest of it is just now going to be a set of steps so I'll just maybe write down everything.\nfirst yeah so I'll just now go over it one by one this will be easier to do it that.\nway so I had y tilde okay so now what I have done is I have written y tilde as.\ny hat plus this quantity so no uh no issues there then I have three quantities here A B and.\nC so I have grouped them so I have a minus B plus sorry A minus C plus this B.\nquantity and of course there was the whole Square outside always okay so now this now Becomes of the form.", "metadata": {"video_title": "Injecting Noise at Inputs"}}
{"text": "C so I have grouped them so I have a minus B plus sorry A minus C plus this B.\nquantity and of course there was the whole Square outside always okay so now this now Becomes of the form.\nlet me just say p plus Q the whole Square so it is going to be p square plus Q.\nsquared plus 2 p q so it is going to be the expectation of p square plus Q squared plus.\n2 PQ this is an expectation of a sum so I can write it as the sum of expectations so.\nexpectation of P Square expectation of 2pq and expectation of our Q square right now let's make a few observations.\nso this quantity here there are n terms here right so you could think of it as a 1 plus.\na two plus plus all the way up to a n square right so now when you expand this what.\nare the kind of terms that you are going to get you are going to get the squares of all.\nof these guys A1 square plus a 2 square all the way up to a n square and then you.\nare going to get these uh and choose two terms where you will have a 1 a 2 of course.", "metadata": {"video_title": "Injecting Noise at Inputs"}}
{"text": "of these guys A1 square plus a 2 square all the way up to a n square and then you.\nare going to get these uh and choose two terms where you will have a 1 a 2 of course.\nmultiplied by something plus ah A2 A3 again multiplied by something and so on right and so this is going.\nto be a very long sum and you are taking the expectation of that sum so it's going to be.\na sum of the expectations so this is what it would look like right and my a a one a.\ntwo here is of course W 1 Epsilon 1 W 2 Epsilon 2 and so on right now wherever you.\nencounter A1 A2 remember that a one a two here is Epsilon some Epsilon I Epsilon J and since the.\nepsilons are independent then all these except all these could be written as expectation of Epsilon I into expectation of.\nEpsilon J and of course some other quantities here but the main thing here is that Epsilon expectation of Epsilon.\nis zero because Epsilon is a zero mean noise so all those terms which contain A1 A2 are going to.", "metadata": {"video_title": "Injecting Noise at Inputs"}}
{"text": "is zero because Epsilon is a zero mean noise so all those terms which contain A1 A2 are going to.\nbe disappearing why will they disappear because you are taking the expectation of a product and the two random variables.\nin the product are independent so that expectation can be written as the product of expectations and in the individual.\nterms are there now become zero so all these terms are going to be disappearing and so you will only.\nbe left with the uh with the quantities which contain the squares so those are these quantities right so w.\ni square plus Epsilon I into Epsilon I square and you'll have n such terms so that's why you have.\nn terms here right so that's that's a pretty long ah um yeah I mean long set of explanation for.\nwhat how I went from here to here right and again I have said that this quantity also becomes zero.\nso why does that quantity become zero so this is y hat minus y okay so here you are a.", "metadata": {"video_title": "Injecting Noise at Inputs"}}
{"text": "what how I went from here to here right and again I have said that this quantity also becomes zero.\nso why does that quantity become zero so this is y hat minus y okay so here you are a.\nrandom variable is y hat and this is w i Epsilon I so Y is of course not a random.\nvariable w i is also not a random variable so what is the random variable y hat and Epsilon I.\nright so this is again the product expectation of the product of two random variables and now again y hat.\nand Epsilon I are independent because y hat had nothing to do with Epsilon that was the uncorrupted output whereas.\nEpsilon is the noise that you have added and the noise that you have added had nothing to do with.\nthe uncorrupted output so these two are independent random variables so this expectation you can also show that will go.\ndown to 0 right so then the only thing that you are left with is this and this quantity uh.\nso let's see from there where do we go so this becomes the expected value of this error right plus.", "metadata": {"video_title": "Injecting Noise at Inputs"}}
{"text": "down to 0 right so then the only thing that you are left with is this and this quantity uh.\nso let's see from there where do we go so this becomes the expected value of this error right plus.\nah the the summation w i square is not the random variable so expectation of Epsilon Square into this sum.\nright this sum here and the expectation of Epsilon square is just Sigma Square so what you get effectively is.\nyour loss term right which this was the training error right so if you estimate this expectation from the training.\nerror from the training data that is what you are going to do because you only have the training data.\nat training time so this is your L Theta and this is your Omega Theta and this is actually the.\nsame as the L2 loss because you are minimizing the sum of the squares of the weights which is the.\nsame as the L2 Norm penalty right so in the simple input output Network without any non-linearity uh adding noise.", "metadata": {"video_title": "Injecting Noise at Inputs"}}
{"text": "same as the L2 Norm penalty right so in the simple input output Network without any non-linearity uh adding noise.\nto the inputs is the same as using a weight DK right so this is the L2 Norm penalty is.\nalso called weight Decay because as we saw it decays the weights right by this Factor Lambda by Lambda Lambda.\nplus Alpha okay so that's this is another regularization technique and we have seen its relation to uh weight DK.\nin the simple input output Network without any non-linearity okay uh silent this here.", "metadata": {"video_title": "Injecting Noise at Inputs"}}
{"text": "foreign [Music] technique that we are going to look at is adding noise to the outputs so we saw adding.\nnoise to the inputs but you could also add noise to the output so let's see how what that means.\nso this is what your output is right so you are given a certain image so let's look at the.\nclassification problem and this is what your true Y is going to look like all the probability masses on the.\ncorrect level which is 2 in this case so 0 1 2 all the way up to 9 and everything.\nelse is 0 right and now this is what your true Y is and you are trying to predict uh.\ny hat or f x ah F hat of X which is very close to this 2 y right and.\nthat could lead to overfitting because if you have you if you have a lot of parameters you know that.\nthis error you can drive down to zero but now what if I do the following right so I instead.\nof using this output the true output that was given to me uh and this is what my loss this.", "metadata": {"video_title": "Injecting Noise at outputs"}}
{"text": "this error you can drive down to zero but now what if I do the following right so I instead.\nof using this output the true output that was given to me uh and this is what my loss this.\nis what I'm going to minimize right this is my cross entropy loss that I'm going to minimize so P.\nhere is the true label and Q is the predicted label so instead of using the true distribution what if.\nI say that I don't trust the true labels they may be noisy or I don't want to trust the.\ntrue labels because then I am trying to map my input exactly to the output and that is what overfitting.\nis right so instead I am going to add some noise to the outputs what I'm going to do is.\nthat in my original distribution all my probability Mass was on the correct label right so what I'm going to.\ndo is I'm going to take away a small probability Mass from there and distribute it to the other labels.\nright so now instead of all the mass being on one and everything else being 0 I'm instead of using.", "metadata": {"video_title": "Injecting Noise at outputs"}}
{"text": "right so now instead of all the mass being on one and everything else being 0 I'm instead of using.\nthose hard targets now I'm using the soft targets where there's a small non-zero probability for all the other outputs.\nalso right so this I have corrupted the output in some sense but not corrupted it by a lot right.\nbecause still if Epsilon is small and Epsilon will be small my majority of the probability mass is still on.\nthe correct level but now when I apply my formula the cross entropy formula earlier in my Pi log UI.\nif you remember only one term remained right the one which correspond to p i equal to 1 so only.\nthis the label the second term would remain but now because all the other values were 0 but now all.\nthe other values are not zero right so my computation is changing so the loss that I am actually trying.\nto minimize is uh changing and that will act as a regularizer right because now you are not trying to.", "metadata": {"video_title": "Injecting Noise at outputs"}}
{"text": "to minimize is uh changing and that will act as a regularizer right because now you are not trying to.\nminimize the true loss but a slightly corrupted version of the loss because you have made some change to the.\noutputs right so that's what uh that's what you do in uh adding noise to the outputs and now since.\nyou are not what what you're doing done is again you have added some kind of a corruption to the.\nloss function right so earlier you are looking at L theta plus Omega Theta right and now again if you.\nopen this up right and now again what has happened is earlier you had only uh so what you have.\nnow is Epsilon I Epsilon into log of P1 plus Epsilon into log of uh sorry P0 P1 Plus Epsilon.\ninto log of P3 log of P 9 and then you also have this 1 minus Epsilon into log of.\nP2 which was the correct label right for the correct label this is the weight so now you can again.\nthink of this this was actually your true loss earlier without regularization right so you can think of this as.", "metadata": {"video_title": "Injecting Noise at outputs"}}
{"text": "think of this this was actually your true loss earlier without regularization right so you can think of this as.\nL Theta so now you have some weighted L Theta so let me just call it as alpha 1 L.\nTheta where alpha 1 is 1 minus Epsilon and then you have Plus or let me just call it 1.\nminus Epsilon only right so you have 1 minus Epsilon times your earlier loss plus Epsilon times some other loss.\nwhich I can call as Omega Theta now so again you can see that you are doing some kind of.\na regularization here and that will help avoid overfitting right so that's the idea behind adding noise to the output.\nlevels so we'll end this here.", "metadata": {"video_title": "Injecting Noise at outputs"}}
{"text": "foreign [Music] so welcome to lecture six of the course and in this lecture we'll be talking about regularization right.\nand different types of regularization ah which includes L2 regulation early topic and a bunch of other things but before.\nI talk about the types of regularization that you use in the context of deep learning of course some of.\nthem are also in the context of machine learning I'll first tell you give you intuition for why we need.\nregularization and that's where we'll focus on the discussion on the bias various trade-off right so that's where we'll start.\nwe'll make a case for regularization understand what regularization is and then look at different types of regularization okay so.\nhere are some acknowledgments I have referred to several sources for preparing the this lecture this includes a deep learning.\nbook Ali goat sees video lectures and regularization in the context of deep learning then this paper on dropout right.", "metadata": {"video_title": "Introduction to Bias and Variance"}}
{"text": "book Ali goat sees video lectures and regularization in the context of deep learning then this paper on dropout right.\nso all of these I have referred to so let's first look at what the problem is right so so.\nfar we have focused on minimizing the objective function using a variety of optimization algorithms right so we had this.\nloss function and all our Focus was to how to minimize this loss how to minimize it faster how to.\nmake sure that we don't overshoot uh the Minima and land in the right Minima and so on that's what.\nall our Focus has been on right but now the issue is that uh deep learning models uh typically have.\nbillions of parameters right I wouldn't say typically have billions but I would say typically have like hundreds of millions.\nof parameters and the training data may have only a few millions of samples right so the main point here.\ninstead of focusing on billions and millions is that deep learning models are often over parameterized right that means they.", "metadata": {"video_title": "Introduction to Bias and Variance"}}
{"text": "instead of focusing on billions and millions is that deep learning models are often over parameterized right that means they.\nhave a large number of parameters which are often more than the training points that you have right and in.\nsuch over parameterized models it's a well-known uh fact right that they are prone to overfitting right this is true.\nfor machine learning that you have a large number of parameters and only a few points then you can easily.\nover fit on the training data that means you can drive the training error to zero uh for all the.\ntraining points right now that may be good but what happens is and that's what we will see in the.\nbias variance trade-off is that if you try to overfit the data on the training data right so you're trying.\nto kind of memorize everything that you see in the training data then you may not be able to generalize.\nwell on the test data because that is unknown data so you have focused so much on uh they fixated.", "metadata": {"video_title": "Introduction to Bias and Variance"}}
{"text": "well on the test data because that is unknown data so you have focused so much on uh they fixated.\nso much on what was given to you that you are now not going to be able to generalize on.\nsome unknown data right and we will see all of this in detail in this lecture and that's where the.\nconcept of bias and variance comes in and its relation to the capacity of the model and what I'm trying.\nto say on this slide and which you all know is that deep learning models have a very high capacity.\nbecause they have a large number of parameters okay so that was the context and with that context I'll start.\nthe discussion on bias and variance so let's look at this right so we look at an example where uh.\nwe want to fit a curve right so what do I mean by that right so I have been given.\nsome data points the data points have actually come from the sinusoidal function that means I know what the true.\nf of x is in this case right so I have X and my true relation between X and Y.", "metadata": {"video_title": "Introduction to Bias and Variance"}}
{"text": "some data points the data points have actually come from the sinusoidal function that means I know what the true.\nf of x is in this case right so I have X and my true relation between X and Y.\nwhich is given by f of x for a change for once in my life I know what that is.\nthat is the sinusoidal function because I've actually picked up the data points from there I know this right but.\nI am going to assume that I do not know this I have just been given some points and now.\nI'm going to try to fit a model to this points what does that mean that now I am going.\nto do the following I'm trying to going to come up with an approximation of what the relation is I'll.\ncompletely ignore the fact that I actually know what the true relationship is because in real world I wouldn't know.\nthis right just for illustration I have taken points from sinusoidal functions so that's it's easy for me to illustrate.", "metadata": {"video_title": "Introduction to Bias and Variance"}}
{"text": "this right just for illustration I have taken points from sinusoidal functions so that's it's easy for me to illustrate.\nwhat the true function is and how good or bad my approximation is right so now in a typical deep.\nlearning machine learning setup I am going to talk about different approximations right so I will take one very simple.\napproximation and then one slightly complex approximation and then I'll try to make some comments based on the approximations that.\nI have made right so let's ah see how that goes okay yeah so we consider two models so one.\nis a simple model as I said degree one model where I assume that Y is related to X using.\na linear function right so I just assumed that Y is equal to MX plus C or Y is equal.\nto W and X plus W naught right so I have only two parameters here clearly not an over parameterized.\nmodel and the relationship that I have assumed is very simple the other I am going to assume is a.", "metadata": {"video_title": "Introduction to Bias and Variance"}}
{"text": "to W and X plus W naught right so I have only two parameters here clearly not an over parameterized.\nmodel and the relationship that I have assumed is very simple the other I am going to assume is a.\nslightly complex model which is a degree 25 ah polynomial right so what does that mean that ah I am.\nsaying that my ah Y is related to X by the following relationship W 25 x raised to 25 plus.\nW 24 x raised to 24 all the way up to WX then W naught right ah so w One.\nX plus W naught right so clearly I have 26 parameters here and my function is also not linear it's.\na polynomial so this is clearly a complex function at least in relation to the ah first function where I.\nhave just assumed a linear relationship so here I have only two parameters here I have 26 parameters so I.\nclearly have more capacity here as compared to the simple model right and if I take this to the extreme.", "metadata": {"video_title": "Introduction to Bias and Variance"}}
{"text": "clearly have more capacity here as compared to the simple model right and if I take this to the extreme.\nif I had approximated this relationship by a deep learning model extremely deep 10 layer deep model with many neurons.\nper layer then I would have had that many more parameters but I am not going there I am just.\ngoing to stop at this level of complexity where I have some 26 parameters and a degree 25 polynomial as.\nan approximation of the relationship right so this is what my approximation of the relationship between ah F and also.\nbetween X and Y is right this is what my f hat of X is okay ah ok now in.\nboth cases we are making assumption right we have no idea about a true relationship again of course in this.\ncase I know other two relationship is but if you had given me this problem I wouldn't know so I.\nwould have just assumed something so in both cases we are making an approximation ah now ah the training data.", "metadata": {"video_title": "Introduction to Bias and Variance"}}
{"text": "would have just assumed something so in both cases we are making an approximation ah now ah the training data.\nthat is given to me it contains some 500 points right so I have been given some 500 x comma.\ny pairs which I have been given some 500 of these X comma y Pairs and using these I have.\nmy job is to learn W1 W naught or these 26 parameters depending on which model I am going to.\nchoose right so that's the setup so now we are going to uh sample sum actually not 500 I think.\nit should have been 100 points or here so I was maybe given okay let's assume a bit differently let's.\nassume that I was given some 500 points and I'm going to sample randomly pick up some 400 or 300.\npoints from this and train a simple and a complex Point model I am going to repeat this process K.\ntimes what does that mean that this is what I mean by that right so in this model let us.\ntry to understand what my full procedure looks like right so I have my loss as I equal to 1.", "metadata": {"video_title": "Introduction to Bias and Variance"}}
{"text": "times what does that mean that this is what I mean by that right so in this model let us.\ntry to understand what my full procedure looks like right so I have my loss as I equal to 1.\nto M where as m is the number of training points and in this case I am going to Define.\nmy loss as y i which is a true y that was given to me minus F hat of x.\ni which is my function that I have chosen right and the parameters here are all the W's that I.\nhave right it's either so this is a minimization problem with respect to the W's so the W's are either.\nW 1 and W naught or they are W 25 all the way up to W 29 right and this.\nyou know now how to solve this or how to ah find this Minima using the gradient descent or any.\nof the variance of the gradient descent algorithm right now let us consider the simple model which was W 1.\nx 1 plus W naught right so at the end of training suppose I I took some ah 400 points.", "metadata": {"video_title": "Introduction to Bias and Variance"}}
{"text": "of the variance of the gradient descent algorithm right now let us consider the simple model which was W 1.\nx 1 plus W naught right so at the end of training suppose I I took some ah 400 points.\nand I do just do not want to take all the 500 points because then I cannot repeat this process.\nK times right because if I have the same 500 points and I repeat the process K times then I.\nam going to get very similar Solutions so I am just going to take a different random sample right so.\nI have 500 points total and I'm just going to take some random 400 points from there and try to.\nsolve this optimization problem one which will result in some W one and W naught then again I will take.\na separate 400 points again try to solve this optimization problem and I might get a different W one and.\nW naught right so that is the idea that's why I am not taking the entire training data but just.\ntaking like a a significantly large sample of it but in every each of these K times and I'm going.", "metadata": {"video_title": "Introduction to Bias and Variance"}}
{"text": "W naught right so that is the idea that's why I am not taking the entire training data but just.\ntaking like a a significantly large sample of it but in every each of these K times and I'm going.\nto try to solve this optimization problem my m points are going to be different and hence the solution that.\nI lend up for W1 and W naught would be slightly different right so that is the idea that's what.\nI am trying to do here right and now let us look at one of these right so the first.\ntime I took the 400 points and solved this using gradient descent or any of the other optimization algorithms then.\nI have come up with this equation that Y is equal to W 1 x 1 plus W naught and.\nI could plot that equation right so the blue line that I have here that is for a given value.\nof w 1 and W 9 8 so that is the same as MX plus C so I have the.\nx axis I have the X and I have the y axis and I have just drawn the line which.", "metadata": {"video_title": "Introduction to Bias and Variance"}}
{"text": "of w 1 and W 9 8 so that is the same as MX plus C so I have the.\nx axis I have the X and I have the y axis and I have just drawn the line which.\nis given by this our equation right and now if I repeat it K times every time I will get.\na different line and I will keep drawing all those K lines right so that's what I am going to.\nshow you in the animation here so every time a new line let's just see what happens here oops um.\nokay so this is okay I'll come back to this animation later let's see yeah so this is what happens.\nso this is what I'm going to do now right so this optimization problem I am going to solve for.\nthe simple model which is y equal to W 1 x 1 plus W naught and I am going to.\ndo that a k number of times right and every time I solve that optimization problem I get a slightly.\ndifferent value of w and W naught and I will keep plotting those lines okay so I am plotting all.", "metadata": {"video_title": "Introduction to Bias and Variance"}}
{"text": "do that a k number of times right and every time I solve that optimization problem I get a slightly.\ndifferent value of w and W naught and I will keep plotting those lines okay so I am plotting all.\nthose different lines which I get right and here iteration you can think of iteration not as the iteration in.\nthe case of gradient descent but this is the kth so now I have repeated the experiment 25 times 27.\ntimes 28 times 29 times 30 times right so I have repeated the experiment a total of 30 times from.\n0 to 29 and every time I got certain value of w 1 and W naught and I just plotted.\nthose ah values right I just plotted those lines okay similarly I can do the same thing I can solve.\nthe same equation or I can try to find this minimum except that now my F at X I is.\ngoing to be that degree 25 polynomial which I show right so that does not change anything I'll just have.", "metadata": {"video_title": "Introduction to Bias and Variance"}}
{"text": "the same equation or I can try to find this minimum except that now my F at X I is.\ngoing to be that degree 25 polynomial which I show right so that does not change anything I'll just have.\nthe gradient computed accordingly and I could still use the gradient descent based algorithms to find the W's right so.\nnow I need to find 26 different values and once I find the values I can plug in those values.\nin this equation and then for every value of x on the x axis I can find what F hat.\nwould be and I can plot that right and now again this I am going to repeat some 30 different.\ntimes each time I am going to take a different 400 samples from the training data right a different sample.\nof size 400 from the training data and I am just going to plot that curve right so let's see.\nwhat that curve looks like so every time you can see I am getting a different curve right you can.\nsee all the different curves that I am getting here okay I am done now right so you understood the.", "metadata": {"video_title": "Introduction to Bias and Variance"}}
{"text": "what that curve looks like so every time you can see I am getting a different curve right you can.\nsee all the different curves that I am getting here okay I am done now right so you understood the.\nprocedure now let's try to make some observations from these plots so simple methods trained on different samples of the.\ndata do not differ from each other right so I had a total of 500 training points given to me.\nand I took different random samples of 400 multiple times I took a different sample of 400 and each of.\nthese lines corresponds to one of those sample training data sets and I found the values of w and W.\nnaught and plotted this and you can see that all these lines are very close to each other right they.\nare not really giving me different solutions right but the same thing when I do with the complex function you.\ncan see that all of these are quite different right so you have something which is going like this you.", "metadata": {"video_title": "Introduction to Bias and Variance"}}
{"text": "can see that all of these are quite different right so you have something which is going like this you.\nhave something which is going like this you have something which is going like this and so on right so.\nall these functions which I am getting and these functions are plotted by ah substituting values for the following equation.\nright so whatever I solve whatever Minima I get that will give me the value of the parameters I just.\nplug in those parameters and plot it and every time I use a different sample of 400 my my uh.\nthe the the function which I get is different right it's quite all these I'm getting a lot of variety.\nhere a lot of variance here as opposed to in the simple model right so that's the observation that we.\nare making so simple models do not differ from each other but they are far from the true sinusoidal curve.\nright they should have been very close to the red curve but I mean the moment I chose a simple.", "metadata": {"video_title": "Introduction to Bias and Variance"}}
{"text": "right they should have been very close to the red curve but I mean the moment I chose a simple.\nmodel I was just going to get a line right and my actual function happened to be a sinusoidal curve.\nso I know that my model is going to be very bad no matter how well I try to fit.\nit it's just trying to pass it like through the center if you are I mean some sort of ah.\npass it through the average uh it's getting some sort of a line which is kind of balancing the positive.\nand negative points right so that's all I would get right and it's very far off from the ah True.\nValue so what do I mean that mean by that is that suppose I substitute the value of x here.\nright then my my predicted value is this whereas my true value is this right and there is a clear.\ngap between the predicted and the True Value this is happening for the simple models but for the complex models.\nwhile there is a huge variance in them you can see that on average the gap between the True Value.", "metadata": {"video_title": "Introduction to Bias and Variance"}}
{"text": "while there is a huge variance in them you can see that on average the gap between the True Value.\nand the predicted value is much smaller if you look at on average across the entire x axis then if.\nyou plug in different values of X and if you have to compute the error for all the values of.\nX it's much smaller here as compared to the simple model that's visually clear right I mean the complex model.\nis at least able to give me a sinusoidal shape whereas the simple model was not even able to do.\nthat right okay so these are just observations that you are making and then we will try to formalize these.\nobservations right yeah so simple models very close to each other not much variance between them complex models they're far.\nfrom each other that means there's High variance but simple models tend to underfit they are not even able to.\ngive me a zero error on the training data itself whereas complex models are able to overfit they are giving.", "metadata": {"video_title": "Introduction to Bias and Variance"}}
{"text": "give me a zero error on the training data itself whereas complex models are able to overfit they are giving.\nme close to zero error on the training data at least some of them on average are giving me close.\nto zero error on the training data right so this is what is happening now so now what I have.\ndrawn here I have the red curve which is my true sinusoidal function then the green curve that you see.\nthat is the that is the average value of x hat for the simple model so what do I mean.\nby that what do I mean by that so I have computed 25 different models right I have each of.\nthese models now I plug in a value of x into each of these models okay and I get y's.\nfrom each of these models right now I take the average value of that Y and I plot it so.\nif I plug in x equal to 0 I compute ah sorry there were 30 models not 25 right so.\nI compute ah F hat of x from all the 30 models and then take the average and plot it.", "metadata": {"video_title": "Introduction to Bias and Variance"}}
{"text": "if I plug in x equal to 0 I compute ah sorry there were 30 models not 25 right so.\nI compute ah F hat of x from all the 30 models and then take the average and plot it.\nsimilarly I plug in the value as 0.1 pass it through all the 30 value or 30 models I substitute.\nx equal to 0.1 I get 30 different y's I take the average of that and I plot it right.\nso what I am plotting here is the average value of f hat X of the simple for the 30.\nsimple models similarly the blue curve is the average value of f hat X for the complex model right so.\nah this is the what what I am trying to do here is the average value and you know that.\nthe average I can also call it as the expected value right so the mean is the same as the.\nexpected value so that is what I am drawing here empirically right I'm just trying to draw the expected value.\nfor the green curve as well as the for the simple model which is the green line and for the.", "metadata": {"video_title": "Introduction to Bias and Variance"}}
{"text": "for the green curve as well as the for the simple model which is the green line and for the.\ncomplex model which is the blue curve right and now I can define a quantity called bias which is the.\ndifference between the average value and the True Value right so it's for the simple model its difference between the.\nGreen curve or the green line and the red curve right and how do I compute that difference I can.\njust compute it by summing it up over all the points right so for every Point Let's assume you just.\nhave 100 points on this axis then for all the hundred points I can compute what the average value is.\ngoing to be right that is the green value minus the red value and the square of that right so.\nthat is what my ah average error is going to be but what I am defining as the bias is.\nthat whatever is the average value right that divided by sorry that minus the True Value right so that's what.", "metadata": {"video_title": "Introduction to Bias and Variance"}}
{"text": "that is what my ah average error is going to be but what I am defining as the bias is.\nthat whatever is the average value right that divided by sorry that minus the True Value right so that's what.\nthe bias is so the bias is defined as the expected value of the prediction minus the True Value that.\nyou would have had right so that's what the bias is so in simple terms this quantity here is the.\ngreen ah value and this is the red value right so that is what you are defining as the ah.\nexpected uh as the bias okay now what is clear is that for the simple model the average value is.\nvery far from the True Value that means the bias is going to be very high for a simple model.\nah whereas for the complex model the bias is going to be low because the average value which is the.\nblue curve is very close to the red curve right so bias is the difference between the average value or.\nthe expected value of the model ah so why is there an expectation here because where is the randomness coming.", "metadata": {"video_title": "Introduction to Bias and Variance"}}
{"text": "the expected value of the model ah so why is there an expectation here because where is the randomness coming.\nfrom you are taking different random samples of the training data right so you might get if I give you.\none lakh samples for training I I might just have gotten this random sample from somewhere and giving it to.\nyou right so I might be able to depending on what random sample of the training data I have given.\nyou you would get different uh F hat X which means you will get different values for the parameters so.\nacross all these different ah values of the parameters if I take the expected value and then try to find.\nthe difference from the True Value then that's what the bias is and you can see that the green curve.\nis ah much farther from the red curve so the bias is high whereas the blue curve is closer to.\nthe red curve so the bias is low right so informally we are seeing that simple models have a high.", "metadata": {"video_title": "Introduction to Bias and Variance"}}
{"text": "is ah much farther from the red curve so the bias is high whereas the blue curve is closer to.\nthe red curve so the bias is low right so informally we are seeing that simple models have a high.\nbias and complex models have a low bias right so let us continue with the discussion and now we will.\nDefine one more quantity which is the variance right and variance is something that I think most of you know.\nso variance is just the expectation of the of how far is a given value away from the expected value.\nright so what does that mean so you have this ah suppose you take any one blue line from here.\nand then you had that green line or the green curve which was the average right maybe I should have.\nused the right colors let me just use the right colors yeah so suppose I take any one of these.\nblue lines from here and then I had the green curve which was the average right and it looks something.", "metadata": {"video_title": "Introduction to Bias and Variance"}}
{"text": "used the right colors let me just use the right colors yeah so suppose I take any one of these.\nblue lines from here and then I had the green curve which was the average right and it looks something.\nlike this then ah for every blue line I can find the difference from the green line right that is.\nwhat I am trying to do and I can take the square of that difference and then I can do.\nthis for all the blue lines that I had and that's where the expectation is coming from so that's the.\nvariance right so it's the difference between uh the predicted value minus sorry it's the difference between the given model.\nand the expected value of the model the square of that difference and the expectation of that right so this.\nis very similar to I mean this is the definition of variance that you know from statistic right so you.\ncould look at it as suppose a fat of X you can think of it as a random variable Z.", "metadata": {"video_title": "Introduction to Bias and Variance"}}
{"text": "is very similar to I mean this is the definition of variance that you know from statistic right so you.\ncould look at it as suppose a fat of X you can think of it as a random variable Z.\nthen this is essentially expected value of Z minus expected value of Z the whole Square and this is essentially.\nthe definition of variance so all you have is here the Z is the F hat X which is a.\nrandom variable because it is random depending on the training samples that you have you will get different blue lines.\nand hence there is a Randomness there right so this is the standard definition nothing much to say here but.\nthis is what the bias looks like now what do we off uh see that all the blue lines are.\nactually very close to the ah green line right which was there on the previous slide that means they are.\nnot very far away from the average and hence the variance is going to be small for the simple models.", "metadata": {"video_title": "Introduction to Bias and Variance"}}
{"text": "not very far away from the average and hence the variance is going to be small for the simple models.\nright whereas for the complex models you can see that some of the blue lines are actually very far off.\nfrom the average that we had right so the blue curve sorry not the blue lines so the here you.\nhave quite a bit of variance that you have right you can see that visually so you for the same.\ninput X there's a quite a bit of difference between the predicted value based on the different models that you.\nhave trained right there's quite a bit of gaps one model is predicting the value here the other model is.\npredicting a value here similarly here if you look at for this point one model is predicting the value here.\nthere's another model which is predicting value here so there is quite a bit of variance in the values that.\nare you are obtaining for f hat of X right and hence the variance so for the complex models the.", "metadata": {"video_title": "Introduction to Bias and Variance"}}
{"text": "are you are obtaining for f hat of X right and hence the variance so for the complex models the.\nvariance is going to be high right so that's that's what we have observed so in summary informally what have.\nwe observed that simple models have high bias low variance complex models have low bias High variance and now there.\nis always a trade-off between the bias and the variance right so now I can't say that let's always choose.\na complex model because I know that if I choose a complex model I'll have a low bias right which.\nmeans I'll be able to overfit the training data but I also have a high variance that means that depending.\non the training data that I had gone my models would be quite different from each other and that's not.\nwhat I want right because if my models depending on the training data that I've got if my models are.\nlargely different from each other then I don't know how they'll perform on a test data so let's let's look.", "metadata": {"video_title": "Introduction to Bias and Variance"}}
{"text": "largely different from each other then I don't know how they'll perform on a test data so let's let's look.\nat what I mean by that right so suppose my model has a very high variance so this is one.\ntraining data this was the other training data right and I predicted some f hat of X using this training.\ndata or estimated some effect of X and now again using this if these two are very different from each.\nother right so now let us assume this was the actual training data given to me then this is what.\nmy model would have been right but now if I get a test instance which belongs here then this model.\nwill do a bad job on that right because if I had been given this as the training data then.\nmy F at X would have been quite different which means the current F at X would not have been.\nvery similar to this F hat X and that means if I had a test instance from here this model.\nwould perform very bad right whereas if my model has a low variance then irrespective of what training data I.", "metadata": {"video_title": "Introduction to Bias and Variance"}}
{"text": "would perform very bad right whereas if my model has a low variance then irrespective of what training data I.\nhave all my f hat X's would be very close to each other so now if I had used this.\nas a training data and then my test data comes from any of these other remaining data then I do.\nnot have a problem because my f hat X would have been similar right but at the same time it.\nshould not be the case that this low variance is coming with high bias because if there is high bias.\nthen I know that all my f hat X's would be close to each other but then they will be.\nfar away from the true function which is again useless to me right so there has to be this balance.\nthat you want like you want a low bias by the same time you don't want a high variance or.\nyou do other way of looking at it is that you want low variance but at the same time you.\ndon't want a high bias in the model right so you want like reasonable variance reasonable bias so that your.", "metadata": {"video_title": "Introduction to Bias and Variance"}}
{"text": "you do other way of looking at it is that you want low variance but at the same time you.\ndon't want a high bias in the model right so you want like reasonable variance reasonable bias so that your.\ndifferent models are not far off from each other at the same time your model is not far off from.\nthe true uh function that you have it so that's why this trade-off is there that you want medium variance.\nand medium bias right and turns out ah that both bias and variants actually contribute to the mean square error.\nright so let's see what the mean square error is of course we have seen it a million times but.\nlet us see again in the context of bias and variance right so I'll end this video here and then.\nwe'll talk about uh mean square error in the next video.", "metadata": {"video_title": "Introduction to Bias and Variance"}}
{"text": "foreign [Music] ERS so we'll do an introduction to Transformers and compare them largely with the recurrent neural networks which.\nwas the previous dominant set of models in various applications in NLP as well as a few Vision applications like.\nimage captioning and so on right and now a lot of them have been replaced by Transformers so that's what.\nwe're going to talk about today so to uh motivated what what is the content today going to look like.\nright so we have seen three types of architectures in this course one was the feed forward neural networks then.\nthe convolutional neural networks and then recurrent neural networks right and in each of this we saw the building block.\nand then once we knew understood the building block properly we could Envision deep wide networks right so in the.\ncase of feed forward neural networks the building block was essentially this sigmoid neuron right or any non-linear neuron which.", "metadata": {"video_title": "Introduction to transformer architecture"}}
{"text": "case of feed forward neural networks the building block was essentially this sigmoid neuron right or any non-linear neuron which.\nused to take um a bunch of inputs do a weighted aggregation and then pass it through a non-linearity right.\nso that was the basic building block and then we had many of these connected in a layer and across.\nlayers to get a deep and wide neural network okay similarly in the case of convolutional neural networks the basic.\nbuilding block was the convolution operation and maybe even the max pooling operation then the case of recurrent neural networks.\nthe basic building block was this recurrent equation where you could compute HT which was a state at time t.\nas a function of HT minus 1. and the input at that time step right so this is what the.\nbasic building block was and then we saw the attention-based recurrent neural network where we had the attention function which.", "metadata": {"video_title": "Introduction to transformer architecture"}}
{"text": "basic building block was and then we saw the attention-based recurrent neural network where we had the attention function which.\nwas like a basic building block which I'm calling these basic building blocks because if you understand these then it's.\nnot very difficult to understand the full Network right so similarly today we'll focus on the basic building blocks of.\nTransformers which is lastly the attention uh the self attention and the cross attention uh layers that it uses and.\nthen try to relate it to what we have already seen in attention-based models in the context of recurrent neural.\nnetworks right so that's the idea so with that let's zoom into uh the RNN based models and just see.\nsome limitations of it right so one challenge in RNN based models was that if I'm looking at the use.\ncase of translation right then my input is I am going home and I want to produce its translation in.\nthe target language right now the input is given to me at one go right the input is not like.", "metadata": {"video_title": "Introduction to transformer architecture"}}
{"text": "case of translation right then my input is I am going home and I want to produce its translation in.\nthe target language right now the input is given to me at one go right the input is not like.\ncoming to me one word at a time where I just tell the machine hey this is the sentence that.\nI want to translate right but the computation is not happening at one go right so let's see what I.\nmean by that right so what happens here is that you get the uh you have the initialization Vector right.\nand then you have this word I and then you pass it through the nuclear neural network so what you.\nhave here is say the embedding of the word I right so let's call this as X1 so this is.\nthe first words you have the embedding of that and then what the output is H1 right or what you're.\nComputing here is H1 which is a function of h0 and x 1 right and now once I have done.\nthat this H1 then becomes an input to the next computation because H2 will be computed as H1 comma x.", "metadata": {"video_title": "Introduction to transformer architecture"}}
{"text": "Computing here is H1 which is a function of h0 and x 1 right and now once I have done.\nthat this H1 then becomes an input to the next computation because H2 will be computed as H1 comma x.\n2 right so let's see this in the figure now so I have which one I compute H1 then I.\nget the next input which is enjoyed right it has some another sentence so I think it's I enjoyed the.\nmovie Transformers so I got H1 and now I use that and H1 right so essentially I'm using this function.\nwhich is H1 comma X2 to compute this hidden representation or this yellow representation that you see here right so.\nwhat is happening here is that my computations are happening sequentially although my input was given to me at one.\ngo I had the entire sentence at one go and now let's just get rid of some of these annotations.\nand just see the whole thing right so I had the entire sentence I am still Computing it one step.\nat a time and I'll compute it at Z H1 then H2 then I am Computing H3 then H4 and.", "metadata": {"video_title": "Introduction to transformer architecture"}}
{"text": "and just see the whole thing right so I had the entire sentence I am still Computing it one step.\nat a time and I'll compute it at Z H1 then H2 then I am Computing H3 then H4 and.\nso on right so I'm just doing all of this one step at a time although this entire sentence was.\navailable to me right at the beginning right so now the question is what's the benefit of what is happening.\nhere right so why am I Computing H2 only after I have looked at H1 right so the reason for.\nthat is that I know that for every word I already have the word embedding right so that's what my.\ninput is and this word embedding could be computed from your favorite algorithm like what to make or fast X.\nor glove embedding or it could just be randomly initialized it could just be a learnable parameter in the network.\nright but that is the word that is the embedding of the word computed from a corpus but I want.\nto know the embedding of this word in the context of the center so that's what these Yellow Boxes are.", "metadata": {"video_title": "Introduction to transformer architecture"}}
{"text": "right but that is the word that is the embedding of the word computed from a corpus but I want.\nto know the embedding of this word in the context of the center so that's what these Yellow Boxes are.\nallowing me to compute because they are taking input from the rest of the sentence right now you might ask.\nhey I'm only taking input from one side right I am not really considering Transformers when I am Computing the.\nyellow box for uh movie right but there are also known as something known as bi-directional rnns or bidirectional lstms.\nwhere you start from right to left right so this is how the computation proceeds for all practical purposes you.\ncould think of I have one sentence I enjoyed the movie Transformers so I computed these Yellow Boxes the way.\nRNN does it then I reverse the sentence so I feed in Transformers movie is the Android I and I.\nagain do the computation and then again compute some other say green boxes right and now I have two representations.", "metadata": {"video_title": "Introduction to transformer architecture"}}
{"text": "again do the computation and then again compute some other say green boxes right and now I have two representations.\nfor every word same movie I have one representation computed from the forward Direction and another representation computed from the.\nbackward Direction and I could just simply concatenate the those two representations to get the final representation of the word.\nmovie right and the reason I'm doing this or reason I am Computing this one step at a time is.\nbecause I am interested in the context of the sentence right I want a contextual representation for the word movie.\nso that's why I was doing this one step at a time to get the contextual representations for every word.\nin the input right so which is also aware about what is happening in the words around it so this.\nis important it's a contextual representation is important what is not good is in the interest of doing this contextual.", "metadata": {"video_title": "Introduction to transformer architecture"}}
{"text": "is important it's a contextual representation is important what is not good is in the interest of doing this contextual.\ncomputation I'm not being able to do a parallel processing I have to wait every time one word at a.\ntime which is significantly which reduces my computational efficiency right because I am just doing things sequentially right so now.\nmy wish list would be to be able to do this to get the contextual representation that means when I'm.\nComputing the yellow box for the word movie I want to know what is happening around me I want to.\ntake inputs from the other words and right now these inputs are flowing through these hidden represent additions at 0.\nH1 h2h3 so I want that to continue right and of course I'm going bi-directional so it's flowing from both.\nsides so I want that to happen I want to take the inputs from all the surrounding words but I.\ndon't want to do this in a sequential manner I want a model which is not recurrent in nature because.", "metadata": {"video_title": "Introduction to transformer architecture"}}
{"text": "sides so I want that to happen I want to take the inputs from all the surrounding words but I.\ndon't want to do this in a sequential manner I want a model which is not recurrent in nature because.\nthe idea behind a recurrent equation is that you do something at time step T minus 1 and then feed.\nit as input to time step T so I don't want that to happen right so that's the basic problem.\nI have with the sequencer sequence a models which I would like to overcome right and problem as well as.\na good thing a good thing is that I want contextual representations the bad thing is I don't want to.\ndo parallel sequential process right and once this is done then you have the entire Encore decoder block right which.\nthen takes in the final representation so ignore this for some reason this s2s1 is showing up on the slide.\nit's not supposed to but it continues to show up oh no yeah so this is the final State and.", "metadata": {"video_title": "Introduction to transformer architecture"}}
{"text": "then takes in the final representation so ignore this for some reason this s2s1 is showing up on the slide.\nit's not supposed to but it continues to show up oh no yeah so this is the final State and.\nthen it's passed to the decoder and then the decoder again does this sequential processing right and then of course.\nI don't have much of a choice because I produce the first output which say in this case is none.\nand then that has to be fed to the next state anyways right so the decoder uh this processing will.\nstill happen sequentially because I need to know what was produced and then feed it as the next input so.\nunlike here where the entire input was available at one go here the input itself right is being generated one.\nstep at a time so I'll have to do a sequential processing right but then the encoder can I do.\nsomething to speed up the computation is what my uh question is right so we'll go towards answering that question.", "metadata": {"video_title": "Introduction to transformer architecture"}}
{"text": "something to speed up the computation is what my uh question is right so we'll go towards answering that question.\nokay so this is for the decoder where I'm doing one word at a time but in the traditional encoder.\ndecoder model the problem is that I just do this computation once I have computed this H1 to H5 and.\nthen I take H5 as the like my final representation for the entire sentence and then this is the only.\nthing which is fed to the uh decoder and then the decoder just produces the entire output based on this.\none representation that was given to it there's no notion of alignment right so there's no notion that when I'm.\nproducing none I should actually focus more on I when I am producing a Transformer I should actually pay more.\nattention to Transformer and so on right so that notion is not there and you know where that notion comes.\nfrom or what kind of models have that notion and that is the attention based model so it's in the.", "metadata": {"video_title": "Introduction to transformer architecture"}}
{"text": "from or what kind of models have that notion and that is the attention based model so it's in the.\nattention-based models I have that where I have I compute the RNN encodings right and now I'm just going to.\nlook at it a bit differently right so once I have computed this I once I've computed H1 to H5.\nnow I don't need the RNN block I just need these yellow representation that I have computed which are the.\nH1 to H5 which I can just take them out and those can be my those are now with me.\nright so I don't need to do any further computation on this right and once these five blocks are available.\nto me right so I've just made a copy of those representations and kept it sorry the network seems to.\nbe yeah so once all these vectors are available I can just throw away the encoder and just have the.\noutput of the encoder which is these five vectors in this case in general it would be capital T vectors.", "metadata": {"video_title": "Introduction to transformer architecture"}}
{"text": "output of the encoder which is these five vectors in this case in general it would be capital T vectors.\nwhere T is the length of my input sequence right so that's what I'll have now once I have these.\nthe I'll feed these as input to the attention mechanism right and this is what the attention mechanism does at.\nevery point it now feeds a contextual representation to the decoder so it will just compute uh what is the.\nmost important word at this point right so you start okay go start Computing the output or start building the.\noutput so it will just take a weighted representation of all the inputs to compute the contextual representation which is.\njust going to be like a attention weighted uh some of the inputs right so that's what it's going to.\nbe so I have these Alphas coming in here so for some reason the animations are showing up very slowly.\nyeah so I have these Alphas showing up here and then I take a weighted sum and I compute this.", "metadata": {"video_title": "Introduction to transformer architecture"}}
{"text": "be so I have these Alphas coming in here so for some reason the animations are showing up very slowly.\nyeah so I have these Alphas showing up here and then I take a weighted sum and I compute this.\nuh with a contextual representation uh C Wagner so this is called the context Vector it's also called The Thought.\nVector uh and for the rest of the discussion I'll typically call it the context Vector if I'm calling it.\nsomething else I'll let you know uh at that point right so just think of this C1 as a context.\nVector which is a weighted sum or the attention weighted sum of the inputs right so you have just taken.\nthe outputs of the encoder which were these yellow representations which are context aware representations and now again to the.\ndecoder you are feeding a contextual representation which is now here the context is basically where am I on the.\noutput I'm producing the first word so what is the most important uh set of weights or what is the.", "metadata": {"video_title": "Introduction to transformer architecture"}}
{"text": "output I'm producing the first word so what is the most important uh set of weights or what is the.\nmost important words that I need to focus right and then you keep doing this at every time step so.\nyou could think of is that you had this H1 to H5 weights and then you're multiplying them uh sorry.\nthe H1 to H5 vector us right so you can think of putting them in a matrix and then you.\nhave this Vector of Weights so now you're taking the doing this Matrix Vector multiplication which is essentially taking a.\nlinear combination of all these columns right so alpha 1 1 into this alpha 1 2 into this Alpha One.\nthree into this and so on right so that's the operation that is happening here and then you get the.\nuh you feed it to the decoder RNN which then produces a output at the end right and you keep.\nrepeating this at every time step you do it at C2 then you compute C3 C4 C5 and so on.\nright so you keep doing that now what I'm showing here is what is known as a heat map so.", "metadata": {"video_title": "Introduction to transformer architecture"}}
{"text": "repeating this at every time step you do it at C2 then you compute C3 C4 C5 and so on.\nright so you keep doing that now what I'm showing here is what is known as a heat map so.\nthis is what you typically look at when you are using an attention-based model so this has uh this is.\na say a T1 cross T2 Matrix where T1 is the length of the input and T2 is the length.\nof the output so or the other way around whichever way you can look at it and now in this.\nwherever you see a light spot that is the place where the attention weight is maximum right so when I.\nwas generating I my attention on none was maximum when I was generating enjoyed my attention on uh Racine was.\nmaximum generating fill this was the maximum weighted word and similarly here right it makes sense because these are almost.\nlike one-to-one correspondences in the translation output right so that's what the heat map shows you yeah so this is.", "metadata": {"video_title": "Introduction to transformer architecture"}}
{"text": "like one-to-one correspondences in the translation output right so that's what the heat map shows you yeah so this is.\nhow the heat map relates to what is happening in the sentence as I said that when I'm using none.\nuh the maximum attention is on I and the color coded you can understand the colors here so this blue.\ncolor I'm focusing here and this is the corresponding weight and so on right and you have some attention function.\nand then we had seen this function so used to compute the attention weight right as some function of the.\nprevious state of the decoder and the any input vectors that you had so this was the attention to be.\npaid to input I at time step T which depended on the state of the decoder at time step T.\nminus 1. and input I right and then this had this soft Max function to make sure that this uh.\nalign this Alphas form the distribution rate so they summed up to one right so that's what we have seen.", "metadata": {"video_title": "Introduction to transformer architecture"}}
{"text": "minus 1. and input I right and then this had this soft Max function to make sure that this uh.\nalign this Alphas form the distribution rate so they summed up to one right so that's what we have seen.\nand you can use any alignment function here uh we had seen one specific function in when we were discussing.\nrecurrent neural networks so just showing the alignment of function again right and here's a question all right so and.\nthis is what will lead us to our eventual discussion on Transformers right so can Alpha TI be computed in.\nparallel for all I at time step T so what is the question that I'm asking so I'm at a.\nparticular time step say I'm at time step 4 right so I'm asking can all these Alphas Alpha TI and.\nT is equal to 4 so I'm asking whether Alpha 4 1 Alpha four two four three four four four.\nfive because I can take values from one to five can they be computed in parallel right and the answer.", "metadata": {"video_title": "Introduction to transformer architecture"}}
{"text": "T is equal to 4 so I'm asking whether Alpha 4 1 Alpha four two four three four four four.\nfive because I can take values from one to five can they be computed in parallel right and the answer.\nis yes right because this only depends on St minus 1 which is already available and on hi which is.\nalready available right so it depends only on these two values so you can compute this this in parallel right.\nand of course for normalization you need all the values but you can compute the scores in parallel and then.\nonce you have this course you can again compute the normalization in parallel right so for a given T you.\ncan compute the alpha tis in parallel for All Eyes no matter how many eyes you have right so no.\nmatter how long your sequence is you don't need to wait on the previous computation to happen or to have.\nsomething happen at I minus 1 to be able to compute Alpha TI right you can just compute all of.", "metadata": {"video_title": "Introduction to transformer architecture"}}
{"text": "something happen at I minus 1 to be able to compute Alpha TI right you can just compute all of.\nthat in well right now the other question is so the main takeaway here is that the attention can be.\nparalyzed but now the other question that I have is that can you compute this in parallel for all T's.\nright so I said that at a particular T you can compute it in parallel right now I'm asking that.\nsuppose there's this time step 5 also here so can I compute Alpha 4 eyes right all the alpha force.\nand all the alpha phi's and all the alpha threes right so all these Alphas across different T's right so.\nmy T is changing here can I compute them in parallel a given set of values all the Alpha Four.\nStars right Alpha Four one up to Alpha 40 I can compute in parallel that we have already seen but.\ncan I compute all of these in parallel at one go and the answer is clearly no right the reason.\nis that it depends on S T minus y right so unless I have computed St minus 1 I cannot.", "metadata": {"video_title": "Introduction to transformer architecture"}}
{"text": "can I compute all of these in parallel at one go and the answer is clearly no right the reason.\nis that it depends on S T minus y right so unless I have computed St minus 1 I cannot.\ncompute any of the alpha T's right and St minus 1 actually depends on Alpha T minus 1 because that's.\nthe input right so St minus 1 here this one would depend on uh sorry this would depend on c.\n3 and C3 in turn would be a function of alpha 3 all the alpha threes right so unless I.\nhave computed Alpha 3 I cannot compute C3 unless I have computed C3 I cannot compute S3 unless I have.\ncomputed S3 I cannot compute the alpha Force right so all the alphas cannot be computed in parallel but for.\na given uh T the alphas can be computed in parallel right so now the two things to notice here.\nright so one is that the attention can be parallel at least in a given T it can be parallelized.\nright it cannot be paralyzed across T's but for a given T it can be parallelized and this is something.", "metadata": {"video_title": "Introduction to transformer architecture"}}
{"text": "right it cannot be paralyzed across T's but for a given T it can be parallelized and this is something.\nthat we would like to exploit and see that if we can get rid of this recurrent connection right because.\nthis recurring connection is still a problem for us right because because of the recurrent connection we have to do.\nthings sequentially but if we could get rid of the recurrent connections and then rely on the fact that the.\nalphas can still be computed in parallel then can we get to an architecture which allows us to compute these.\nAlphas in parallel right so it's still a bit hard to visualize where we are headed but just keep these.\nquestions in mind along the way and once we read there all these questions and the answers will make sense.\nso just to summarize the discussion so far right so I mean everything about the RNA model is good right.\nso what do I mean by that uh we saw that across papers right I mean we saw the architecture.", "metadata": {"video_title": "Introduction to transformer architecture"}}
{"text": "so just to summarize the discussion so far right so I mean everything about the RNA model is good right.\nso what do I mean by that uh we saw that across papers right I mean we saw the architecture.\nthey are used for machine translation summarization video captioning image captioning right so they gave very good performance and a.\nwide variety of tasks right but the only uh issue that we have is that given a training example we.\ncannot paralyze the sequence of computations because each of these guys needs to be computed one at a time that.\nI cannot compute all of them in parallel of course on top of that if I have attention attention at.\na given time step can be computed in power right so now a wish list would be can we come.\nup with a new architecture right that incorporates the attention mechanism and also allows us to do things in parallel.\nso we don't want to get rid of the attention mechanism because the attention mechanism helps us to compute the.", "metadata": {"video_title": "Introduction to transformer architecture"}}
{"text": "so we don't want to get rid of the attention mechanism because the attention mechanism helps us to compute the.\ncontextual representation but we want parallelism we also don't have a problem with the basic idea of recurrence right that.\nthe recurrence actually allows us to compute things which are contextual right so we don't have a problem with this.\nhere so we don't have a problem with this here this is fine because this is allowing us to compute.\nthese recurrent uh or the contextual representations but we have a problem with the computational curves which comes with recurrence.\nthat's the problem that we want to solve okay so that's the context so that's a quick recap of recurrent.\nneural networks and what we see as problems in the recurrent neural network and now we'll try to go towards.\na solution for that right and that might probably lead us to a new architecture so I'll end this video.\nhere and we'll come back and continue from this point.", "metadata": {"video_title": "Introduction to transformer architecture"}}
{"text": "foreign [Music] trade-off and from there we went on to this equation which involved the regularization term so what we.\nsaid is that so far we were discussing about minimizing the training error which was computed as the average error.\nover the M training examples and this was empirically computed as say the difference between the true value and the.\npredicted value right and then we realized that if you minimize this you are not actually minimizing the true error.\nbecause this is not a good approximation for the true error it does not account for this additional term which.\ncomes from model complexity and instead of just minimizing this it's possible that you might minimize this to zero but.\nstill your true error would be high because your model complexity is high right and hence instead of minimizing just.\na training error you should minimize training error plus some regularization term and this regularization terms should serve as a.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "a training error you should minimize training error plus some regularization term and this regularization terms should serve as a.\nproxy for the model complexity means which means that if the model complexity is high then this value should be.\nhigh if the model complexity is low then this value should be low right so then you are aiming to.\nnot just minimize the training error the empirical training error but also minimize the model complexity and find some sweet.\nspot in this trade-off right that's where we were and then I said that this kind of forms the basis.\nfor many regularization methods and today we are going to look at a a few regularization methods starting with L2.\nregularization then we look at data set augmentation parameter sharing adding noise to the inputs adding noise to the outputs.\nearly stopping Ensemble methods drop out right so quite a bit of a long list and we will maybe look.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "early stopping Ensemble methods drop out right so quite a bit of a long list and we will maybe look.\nat this formula in detail at least in the context of L2 regularization right so let's start there so in.\nL2 regularization this is what we do right so we Define our the loss that we want to minimize or.\nthe effective loss that we want to minimize we Define it as a sum of the training loss which is.\nthis so which is again I will just write it down once more this is again summation I equal to.\n1 to m or rather average oops M of whatever your training losses and we have been dealing with a.\nsquared error loss right so in the LT case of L2 regularization instead of just minimizing this you also try.\nto minimize this right that means you are trying to minimize the L2 Norm of your weight say that's what.\nthis quantity is and this is some multiplying factor which decide how much weightage should be given to uh this.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "this quantity is and this is some multiplying factor which decide how much weightage should be given to uh this.\nuh term in the total loss function right so this is the total loss function and this decides how much.\nweight to be given to this component if you set Alpha to zero then essentially you are not doing any.\nkind of regularization your total loss is just the empirical training loss and you are back to uh the non-regularization.\nnon-regularization loss function right but if you set Alpha to a certain value it tells you how much do you.\ncare about this regularization term now the question is ah how does this act as a regularization so this is.\nindeed a function of w right there is no denying that so we wanted the loss function to be of.\nthis form so it is indeed in this form right but minimizing this how does this control for model complexity.\nthat's the question that we would like to understand right so what are we saying right we are saying that.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "that's the question that we would like to understand right so what are we saying right we are saying that.\nyou find me weights such as that the training loss is minimized and also the weights values are very small.\nright so what does that actually mean suppose we consider only two weights right say w one and W two.\nnow in the absence of this I'm just saying that you go ahead and pick up whatever weight values you.\nwant right and just minimize my training error that means I could pick up very large values of the weight.\nof the weights rights all of the entire R2 plane is open to me right so let's just extend this.\nso any value in the R2 plane is open to me right now I am saying that hey you know.\nyou can minimize the loss but I do not want these W values to blow up because if these W.\nvalues blow up my regularization term will blow up and hence my effective loss would still be high right so.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "you can minimize the loss but I do not want these W values to blow up because if these W.\nvalues blow up my regularization term will blow up and hence my effective loss would still be high right so.\nI am telling you that here you cannot allow the weights to grow in other words I am just drawing.\nsome kind of a boundary here and saying that the more you go out of this boundary right the more.\nuh bigger my W1 and W2 are going to be the more bigger my L2 Norm is going to be.\nof the weights and more bigger this loss term is going to be and hence my effective loss will not.\nbe minimized right so now I am saying that you add as many parameters as you want you add more.\nmodel complexity you have a million parameters a billion parameters but for any given parameter I will not allow you.\nto grow those values a lot because the moment you grow those values a lot then this loss term will.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "to grow those values a lot because the moment you grow those values a lot then this loss term will.\nincrease and that will increase my total loss right so in some sense I am restricting the model complexity by.\nallowing you to have have many weights but I am not giving you a lot of freedom in choosing the.\nvalues of those ways right so that's how this is acting as a proxy for controlling the model complexity okay.\nso now if that is the loss function then we are interested in the derivative of the loss function with.\nrespect to W right because for all gradient descent methods and its variance this is what our in quantity of.\ninterest is so the derivative of the quantity on the left hand side is the derivative of this quantity plus.\nthe derivative of this quantity with respect to W and this quantity is actually Alpha by 2 W transpose W.\nso if you take the derivative of that with respect to W you can think of this as W Square.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "the derivative of this quantity with respect to W and this quantity is actually Alpha by 2 W transpose W.\nso if you take the derivative of that with respect to W you can think of this as W Square.\nso one of the W's will disappear and you'll have a two term here which will cancel with this so.\nw square is the derivative is 2W so you the twos will cancel and you'll just be left with Alpha.\nW right so that's that's how you can think of how we arrived at this formula right now ah so.\nnow what happens to your update rule right so your update rule earlier was the new value of loss is.\nequal to the old value minus ETA times the derivative and now this is what your derivative looks like right.\nand I've just opened up the brackets so this is what your earlier update rule would have looked like if.\nyou did not have the alpha term now since you have the alpha term this term gets added right so.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "you did not have the alpha term now since you have the alpha term this term gets added right so.\nL2 regularization is very easy to implement if you have already implemented gradient descent then all you need to do.\nis change this update Rule and add this term or subtract this term and you have the L2 regularized update.\nrule for green interesting right now this becomes more complicated as you look at other algorithms like atom and variance.\nof it and so on but we will not discuss those but at least in the case of vanilla gradient.\ndescent where the update rule was just move in the direction opposite to the gradient it's now subtract that term.\nand then further subtract this term that's all your update rule is so extremely easy to implement now let us.\nsee what is the geometric interpretation of this right so to understand the geometric interpretation we will do slightly uh.\nlong ish I would say a derivation right we'll just try to uh go a bit deeper uh into what.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "long ish I would say a derivation right we'll just try to uh go a bit deeper uh into what.\nthe Taylor series says and from there what do we derive and so on right so let's assume that W.\nstar okay is the optimum yeah so let's assume that WSI is the optimum solution for LW so LW remember.\nwas our unregularized loss function right so our new loss function is uh L tilde W is equal to LW.\nplus this Alpha by 2 into W transpose W so this LW as well as was our unregularized loss function.\nso let's assume that the optimal solution for that was given by some W star the sum W star which.\nexists which gave me the lowest value for l w right so that's all we are defining right and there's.\nno problem with that definition foreign now consider a point which is ah say a point w which is in.\nthe neighborhood of w Star right so the way to write that would be W is equal W is equal.\nto W Star Plus U so there's a point w which is in the neighborhood of w star the other.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "the neighborhood of w Star right so the way to write that would be W is equal W is equal.\nto W Star Plus U so there's a point w which is in the neighborhood of w star the other.\nway of saying that is that let's consider a jump such that you have W minus W Star right so.\nuh you could think of it this way this is more natural that we are looking at a point w.\nwhich is in the neighborhood of w star and from that equation you can derive this equation right so that's.\nthat's the small jump that you have made okay now let's let's we'll now do a bit of reasoning or.\na bit of derivation starting from there right and now let's see what the Taylor series says this is Taylor.\nseries second order approximation so far in this course we have been dealing only with the first error approximation but.\nnow I'm talking about second order approximation and what this says is that the loss function in this neighborhood right.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "now I'm talking about second order approximation and what this says is that the loss function in this neighborhood right.\nso w is the point in the neighborhood is the loss function at w Star Plus the first order term.\nplus the second order right so you already know this from the Taylor series that we have seen earlier and.\nnow the interesting thing here is that this term is zero why is that zero because that's the optimal for.\nthe loss LW right and if it's the optimal for the loss LW then the derivative at that point is.\ngoing to be zero right it's going to be some Minima at the Minima the derivative is going to be.\n0 right so that's why this quantity is going to be zero I know I'm just going to expand this.\nright so now what I have done here is this quantity has disappeared and this U I have written as.\nW minus W star so wherever I see do I have substituted as W minus W star nothing great again.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "right so now what I have done here is this quantity has disappeared and this U I have written as.\nW minus W star so wherever I see do I have substituted as W minus W star nothing great again.\nabout that and this Delta WL this quantity has disappeared so I'm just left with the first term and this.\nlast term here okay that's what my LW looks like where LW uh yeah it's the unregularized loss now let's.\njust look at a few other quantities yeah so now uh this is the uh if I compute the derivative.\nof this right so this is what my LW was now if I compute the derivative of this that's the.\nsame as the derivative on the uh of the quantity on the left hand side so I can take the.\nderivative of this quantity and I can just open up the brackets I have derivative of L W Star Plus.\nH into W minus W Star right so this again you can think of this as uh uh x h.\nx right so you are kind of taking the derivative of a quantity which has x square right and the.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "H into W minus W Star right so this again you can think of this as uh uh x h.\nx right so you are kind of taking the derivative of a quantity which has x square right and the.\nderivative of that one of the X's would disappear and just left with X and in this case X is.\nW minus W star and then we get 2 coming out which will get canceled with this right so that's.\nhow you can think of this uh derivation that I have written here so this is what I get right.\nso the derivative of the loss function with respect to W turns out to be H into W minus W.\nstar and we had seen earlier that this H is actually the Hessian which is a matrix containing the second.\norder uh partial derivative so we had already already seen this in a earlier lecture so I'll repeat that okay.\nso that's that's nothing great here right I've just applied Taylor series and done some simplifications I mean just taken.\nsome derivatives so I have just a set set of steps which you can uh which one step follows from.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "some derivatives so I have just a set set of steps which you can uh which one step follows from.\nthe previous and you could just go back and look at this if you have any doubts but there's nothing.\nconceptually difficult that I've done here okay now uh from here let's uh now look at this loss right this.\nis our regularized loss and the derivative of the regularized loss as we had derived on the previous slide was.\nthis plus Alpha W right this we are derived so remember LW is equal to L W plus Alpha by.\n2 W transpose W now if you take the derivative of this with respect to W so here this derivative.\nall of these are derivatives with respect to W it's obvious from the context so I have not suffixed it.\nexplicitly but it's derivative with respect to w uh and so this would just be derivative of this quantity plus.\nthe derivative of this quantity and the derivative of this quantity as we saw on the previous slide was just.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "the derivative of this quantity and the derivative of this quantity as we saw on the previous slide was just.\nAlpha W right so again nothing new that I have written here but now what I'm going to do is.\nI'm going to try to use this value instead of this quantity here right that's the main change I am.\ngoing to do as I go ahead okay now let there be another W which is the optimal solution for.\nthe regularized loss right so now if there's such a w tilde exist then the derivative of the loss function.\nat that point would be zero right and now we have seen that the derivative of the loss function at.\nany point was given by this formula we just saw this on the previous slide so I've just substituted that.\nformula instead of this formula and instead of the generic W I have substituted W tilde right that's the only.\nchange that I have done here right yeah again I'm just doing a lot of steps but there's nothing much.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "change that I have done here right yeah again I'm just doing a lot of steps but there's nothing much.\ngoing on here right just like I'm defining some quantities and then using those quantities and at points I'm at.\nsome points I'm using the properties that the derivative is zero right so again uh there's nothing to uh really.\nsay much about here right at this point once we reach uh the final answer that we're looking for then.\nI'll have significant things to say now this I can rewrite it as I've just opened up the brackets here.\nso this is h w tilde minus h W Star Plus sorry W's plus Alpha W tilde equal to 0.\nso I have taken all the W tilde terms on one side right so it's h w tilde minus the.\nplus sorry plus ah Alpha W tilde is equal to H W star right and this Alpha I can write.\nit as Alpha I it does not make a difference it's just multiplying by the identity Vector so I into.\nany Vector is the same as that Vector right so it does not make a difference so that's all I'm.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "it as Alpha I it does not make a difference it's just multiplying by the identity Vector so I into.\nany Vector is the same as that Vector right so it does not make a difference so that's all I'm.\ndoing here and then just regrouping the terms this is how I arrived here now now I get a formula.\nfor w tilde and why do I know that this inverse exists because I've gone ahead and straight away taken.\nthe inverse how do I know that this inverse exists because H is the Hessian matrix so H is a.\npositive positive semi division it Matrix positive semi definite Matrix and hence the inverse exists say so that's why I.\ncan write it as this quantity here okay ah so continuing in my journey now as Alpha tends to zero.\nright if I don't use a regularization right if this quantity is 0 then this term disappears then I just.\nget H inverse h w star so in that case my w tilde is actually equal to W star and.\nthat should indeed be the case right because if Alpha is 0 that means my second term was not this.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "get H inverse h w star so in that case my w tilde is actually equal to W star and.\nthat should indeed be the case right because if Alpha is 0 that means my second term was not this.\nif the second term was not there then I'm just left with my original loss function and for the original.\nloss function I had already defined that the optimal is W star so if I'm going to get rid of.\nalpha if Alpha is equal to 0 then even for the regularized loss which is just equal to the original.\nloss my w tilde equal to W Star right so simple uh observation but just trying to make the point.\nclear okay now we are of course not interested in the case of alpha not equal to zero we are.\ninterested in the case when Alpha is not equal to zero right because we are using regularization and in that.\ncase we want to see what is the solution between W tilde and W star okay so let's go ahead.\nso since H is a symmetric positive semi-definite Matrix we know that its eigenvalue decomposition will exist not just that.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "so since H is a symmetric positive semi-definite Matrix we know that its eigenvalue decomposition will exist not just that.\nwe know that its eigenvalue decomposition the eigenvectors are going to be orthogonal so I can write it as Q.\nLambda Q transpose so uh I'm sure you have done this in a course on linear algebra if not there.\nare these other videos that I have on linear algebra you can go ahead and look at that I do.\neigenvalue decomposition in quite a bit of detail there right so this I would assume you know that any Matrix.\nyou can write the eigen value decomposition and Q is orthogonal here so q q transpose equal to Q transpose.\nQ is equal to I right we know that so now on the previous slide whatever I had I'm just.\ngoing to bring that back okay this pen work is fairly cumbersome I need to get rid of okay and.\nnow instead of H so this is what we had on the previous slide right so this is what we.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "going to bring that back okay this pen work is fairly cumbersome I need to get rid of okay and.\nnow instead of H so this is what we had on the previous slide right so this is what we.\nhad derived so far and I'm just bringing that back and wherever I see H I'm going to substitute the.\neigenvalue decomposition of H again all of this is nothing great happening here it is all just a series of.\nsteps that I am doing I am trying to reach a point after which I can make some conceptual comments.\nwhere I can get some insights into what is happening into L2 regularization right so right now I'm just going.\nthrough the drill now I am just going to do a few more simple things yeah so again instead of.\nI here I have written it as q i q transpose so Q into I is of course equal to.\nq and then q q transpose is again equal to I so there is no harm in writing this this.\nis uh correct right and now once I have done that I can now regroup some terms and I can.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "q and then q q transpose is again equal to I so there is no harm in writing this this.\nis uh correct right and now once I have done that I can now regroup some terms and I can.\nwrite it as the product of three matrices here right so I have taken the middle part as common which.\nwas Alpha the Triangular Matrix Lambda which is the Matrix of eigen values of H then Alpha I and then.\nI have q here and Q transpose here right so I have just regrouped the terms now this becomes like.\nthe product of three matrices a b c and I am trying to find the inverse of this product then.\nthe inverse is going to be C inverse into B inverse into a inverse right so that's what it's going.\nto be so I can just do that's where I'm headed now right so I'll just do that yeah so.\nI can write it as Q transpose inverse then Lambda plus Alpha inverse and Q inverse and this Q inverse.\nand Q will multiply out and give me I and Q transpose inverse is of course Q because Q is.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "I can write it as Q transpose inverse then Lambda plus Alpha inverse and Q inverse and this Q inverse.\nand Q will multiply out and give me I and Q transpose inverse is of course Q because Q is.\nan orthogonal Matrix right so a few things have got simplified for me now so this is where I end.\nup with right and here again I would notice that if Alpha is 0 right then this term disappears I.\nhave left with Lambda I inverse into Lambda so that will also disappear that will become I then I am.\nq q transpose into W transpose q q transpose will again become I so I'll just be left with W.\nstar which means in the absence of regularization my w or when I said Alpha to 0 my w tilde.\nis equal to W star which is the same as saying that if I do not ah use a regularizer.\nmy optimal solution is W Star right so I am just saying this again to make sure that in the.\nsteps that we have done we have not made any mistake uh things are still looking meaningful in whatever we.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "steps that we have done we have not made any mistake uh things are still looking meaningful in whatever we.\nhave done okay ah yeah so let's just go ahead a bit now yeah so now I can write it.\nas a W tilde is q d q transpose where D is this Matrix so this is this is a.\ndiagonal matrix this is also a diagonal matrix so the sum of two diagonal matrix is going to be a.\ndiagonal matrix and then you are again multiplying it by a diagonal matrix so that's also going to be a.\ndiagonal matrix hence I am calling it d d as the diagonal matrix so now W tilde is equal to.\nthis quantity right so now I have reached somewhere that I wanted to reach and from here on I can.\nstart hopefully start making some observations right so let's just work with this foreign so this is where we are.\nwe had of derivation for w tilde and we have now been able to express W tilde as a function.\nof w star with these Matrix Q D and Q transpose right so what exactly is happening here right so.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "we had of derivation for w tilde and we have now been able to express W tilde as a function.\nof w star with these Matrix Q D and Q transpose right so what exactly is happening here right so.\nuh W star was my optimal solution for the unregularized case now when I am using the regularization what is.\nhappening W star is first getting rotated by this Matrix Q transpose okay so this will give me some let's.\ncall that quantity as Z right so Q transpose W star is again going to be a vector so Q.\nhere would be an N cross n Matrix W is an N cross one vector so the result is again.\ngoing to be n cross 1. right and now that is getting multiplied by again an N cross n diagonal.\nmatrix right so now multiply by multiplying by a diagonal matrix if you only have the diagonal terms and everything.\nelse is 0 and if multiply the vector by such a matrix then it's the same as taking every entry.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "else is 0 and if multiply the vector by such a matrix then it's the same as taking every entry.\nof the vector and scaling it by the corresponding diagonal element so there are n diagonal elements and N entries.\nin the vector so this multiplication the result that you get is simply uh every entry of the product is.\nactually this corresponding entry multiplied or scaled by the corresponding diagonal element right so that's what is going to happen.\nhere and so on right so that's what the diagonal multiplication looks like so first you rotated it then you.\nscaled it by D and then again whatever you got so let's call that Z1 right so this was my.\nz uh now I got a Z1 and again I'm taking Z and rotating it by Q again right so.\nthat that's what I'm doing here so this I've made this observation now what does this mean and so on.\nit's still that's something that we need to understand but this is what is happening here from a linear algebra.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "it's still that's something that we need to understand but this is what is happening here from a linear algebra.\nperspective right so w style first gets rotated by Q transpose to give Q transpose W star however if Alpha.\nis equal to 0 right if Alpha was 0 then this just becomes this whole thing here just becomes I.\nright if Alpha is equal to 0 and then Q transpose rotated W star then Q rotated it back in.\nits original position and we got uh Q right because q and Q transpose are inverses of each other hence.\nuh because Q is a orthogonal matrix so there was no effect of alpha is zero again just saying this.\nfor the sake of showing that whatever we are doing is sounding correct right ah but if Alpha is not.\nequal to 0 then what is happening is the question right then what does D look like so first D.\nhas two components so first component is this so let us look at what that component looks like right ah.\nby this I mean the whole of it right so first Lambda plus Alpha I so you have so you.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "has two components so first component is this so let us look at what that component looks like right ah.\nby this I mean the whole of it right so first Lambda plus Alpha I so you have so you.\nhave a matrix which contains the eigen values on the diagonal everything else is 0 and you're adding the Alpha.\nI Matrix so it has Alpha s all the way on the diagonal and there are n of these Alphas.\nso adding these two matrices so we'll just get Lambda 1 plus Alpha Lambda 2 plus Alpha and so on.\non the diagonal right now the inverse of a diagonal matrix is just the address is just another diagonal matrix.\ncontaining the reciprocals of the diagonal elements right so this would be what the inverse would look like every element.\non the diagonal would be the inverse of it right so this is what is happening here so each of.\nthese quantities is just going to be the inverse of that quantity now D is actually this further multiplied by.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "these quantities is just going to be the inverse of that quantity now D is actually this further multiplied by.\nLambda right so this is what D is so now let me again try to multiply it by Lambda yeah.\nso this is what I'll get right so I had this Matrix and now I am multiplying that Matrix but.\nanother diagonal matrix and it has lambdas on the diagonal so if you do this multiplication you will just get.\nthe product of these diagonal elements on the product Matrix right so that's all you will get okay so this.\nis what this middle guy here looks like okay and if Alpha was 0 then the middle guy would just.\nbecome I or this would all disappear so we'll just get I and there is no change happening W tilde.\nis equal to W star but now if Alpha is not equal to 0 then what exactly is helping Happening.\nHere Right foreign transpose W Star right so this is the first multiplication that is happening now each element of.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "Here Right foreign transpose W Star right so this is the first multiplication that is happening now each element of.\nthat Vector is getting scaled scaled by this quantity right and after it gets scaled by that quantity it is.\nrotated back by q that is what is happening right the rotations performed by q r transpose and Q R.\ninverses of each other but in between this scaling is happening right and now if the eigen values is very.\nlarge if it's greater than Alpha right then this quantity that you have on the diagonal is actually going to.\nbe equal to 1 right so a lot that means for those weights there will be no scaling which will.\nhappen they will remain the same as their original value right so remember this W star is a vector of.\nweights and we are talking about every element of that Vector getting scaled but we're just seeing that if the.\neigen value is large then those weights will not get Scaled let's suppose Lambda 1 was very large then nothing.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "eigen value is large then those weights will not get Scaled let's suppose Lambda 1 was very large then nothing.\nno change will happen to this guy similarly if Lambda 2 was very large then nothing would happen to this.\nguy because Lambda by Lambda plus Alpha would almost be equal to 1 right now on the other hand if.\nyeah if Lambda is very less than Alpha then this quantity almost becomes zero that means whatever elements were there.\nif this is what your W star was and say this is the ith element if the corresponding ith eigen.\nvalue was very small then it is almost like this weight will go to zero and this is actually what.\nwe wanted what the effect that we were hoping for was that some of the weights should shrink because if.\nall the weights are large then your L2 regularization term which is W transpose W is going to be high.\nand now what we are seeing is that if the eigen values are small right then those weights are going.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "and now what we are seeing is that if the eigen values are small right then those weights are going.\nto shrink a lot and that's exactly what we were hoping for and the other weights will remain the same.\nas what they were earlier right they will not grow but they would remain the same as what they would.\nhave been in the absence of regularization right so now you are seeing the effect that certain weights are shrinking.\nhere and that's a proxy for controlling the model complexity because now you are not allowing the model to do.\nwhat it wants you're not giving it you're given it capacity but then you have not given it the freedom.\nto fully utilize this capacity the full capacity would be I should be able to take the weight W1 and.\nset any value between minus infinity to Infinity 80 but now I am not allowing you to do that I.\nhave given you as many weights you wanted but on each weight I've now drawn some boundary and if you.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "set any value between minus infinity to Infinity 80 but now I am not allowing you to do that I.\nhave given you as many weights you wanted but on each weight I've now drawn some boundary and if you.\ngo out that boundary then W transpose W is going to increase and this regularization term is going to ensure.\nthat some of those weights actually decrease actually actually shrink right now still we need to look at a few.\nmore things here right so effectively what will happen the the significant directions where there are larger eigen values those.\nwill be retained because those are not getting scaled but the directions where the eigen values are small those will.\nshrink and those weights will be heavily penalized they might almost go to zero and now if you look at.\nthe effective number of parameters that your network has then this is the effective number of parameters right because your.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "the effective number of parameters that your network has then this is the effective number of parameters right because your.\ntotal number of parameters was n now many of those parameters are getting scaled down significantly by this Factor right.\nnow the depending on how many are getting scaled down if you take this summation this would be less than.\nn so effectively you have you have ended up reducing the complexity of the model right because you started off.\nwith n parameters but now because of the scaling not all of your n parameters are fully effective and hence.\nthe effective number of parameters that you have is actually much less than M right so that's what is happening.\nthat's how it is controlling the model uh complexity okay now let's just look at a bit more of geometric.\ninterpretation also of this so now what I'm showing you here are two loss Contours so and we are using.\nthe same uh oops so this is my uh LW so this is what my unregularized loss looks like this.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "interpretation also of this so now what I'm showing you here are two loss Contours so and we are using.\nthe same uh oops so this is my uh LW so this is what my unregularized loss looks like this.\nis my Alpha W transpose sorry not Alpha Dot this is what my w transpose W looks like so this.\nis the regularization loss and you can see that that loss has this uh if you look at this this.\nContour here this is the Contour of a bowel which is the Contour of a square function and indeed it.\nshould be the Contour of a square function because this is a square function right so this is the place.\nwhere W transfer transpose W is equal to say one W transpose W equal to two three and so on.\nright so these are the Contours that you have here and these are the Contours of w right now let's.\nmake some observation about these plots right and what is w tilde W tilde is the place where these two.\nplots are intersecting right of course the I could have drawn some more uh this Contours will go on right.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "plots are intersecting right of course the I could have drawn some more uh this Contours will go on right.\nand similarly I could have expanded this Contours also so there are many places where these two plots will interact.\nintersect right it's not that this is the only uh space a place that they will intersect but this is.\nwhere my final solution came out for this problem this is the W tilde that I ended up with so.\nw tilde was the optimum point for L tilde where L tilde was L theta plus Omega Theta right and.\nthis is L Theta and this is my Omega R Theta okay so that is what is happening here now.\nI want you to look at this plot right so I will now now we want to look at this.\nplot and try to understand how this relates to the geometry and how this relates to the idea that certain.\nweights are actually shrinking what exactly is happening here right I want us to pay attention to all of these.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "weights are actually shrinking what exactly is happening here right I want us to pay attention to all of these.\nso I'll clear these drawings because we understand what each of these plots corresponds to right now in the absence.\nof regularization this is what your W was W star so that means it has taken this value along the.\nW1 axis and a ticket taken this value along the W to axis okay so you can see that these.\nW's are relatively large right now in the presence of regularization this is what my solution looks like let me.\njust change the color so this is my value for W2 and this is my value for W1 so indeed.\nmy weights have shrunk right not just that my weights have not shrunk uniformly right it's not that W1 has.\nbecome half its original value and W2 has also become half its original value you can see that W1 has.\ndecreased much more as compared to W1 right so first observation yes indeed my weights Shrunk the second observation that.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "decreased much more as compared to W1 right so first observation yes indeed my weights Shrunk the second observation that.\nthe weights have not shrunk uniformly they have shrunk by certain different factors and that is expected because those factors.\ndepend on the eigen values so those factors are Lambda 2 Lambda 2 by S Alpha right now the question.\nis why did W1 shrink more than W2 right and to understand that you need to look at this loss.\nsurface here now that if you look at that you should be able to understand why if W2s shrunk more.\nthen that means W2 is not so important as compared to W1 so if you look at this loss surface.\nthe solid loss surface does it give you that impression that's the question I'm trying to ask and indeed it.\ndoes right so I'll tell you why so if you look at this direction which is the W2 Direction then.\nhere you can see that the distance between the contour lines is very smaller as compared to the distance between.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "here you can see that the distance between the contour lines is very smaller as compared to the distance between.\nthe contour lines along this direction so this is the W1 Direction and this is the W2 Direction what does.\nthat tell us that along this w n axis even if I change the weight a lot I'm not losing.\nmuch in terms of my loss right so I have more flexibility here if I want to shrink the weights.\nthen I have a better chance of shrinking this weight because whatever is the optimum value for this weight even.\nif I shrink it my loss is going to shrink a bit more slowly because there's a higher gap between.\nthe contour lines here so the loss is not rapidly changing along this direction but along the W2 Direction you.\ncan see that the loss is rapidly changing so now if I ever to aggressively shrink W2 that even for.\nsmall shrinks in W2 my loss will go increase right and that is not not acceptable to me because I.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "can see that the loss is rapidly changing so now if I ever to aggressively shrink W2 that even for.\nsmall shrinks in W2 my loss will go increase right and that is not not acceptable to me because I.\nwant to shrink it but I also want to maintain this trade-off that this law should also be small I.\nwant my W's to be small but I also want this loss to be small hence I have ended up.\nwith this trade-off that the weights which are more significant right which is W1 in this case I am shrinking.\nit less because if I shrink it a bit more if I had brought it down to this value then.\nmy loss would have been even higher right and that is not something that I want right whereas when I'm.\nshrinking W1 I have more leeway because along this axis my loss is less sensitive to changes in W because.\nby a small change in W my loss is also decreasing slowly that's what is you can indicated by the.\nslope right so this Gap if you remember indicates the slope the slope here is smaller that means a small.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "slope right so this Gap if you remember indicates the slope the slope here is smaller that means a small.\nchange in W does not cause a large change in uh the loss right whereas along this axis the slope.\nis steeper because this Gap is small so small change in W causes a large change in the law side.\nso this figure kind of summarizes everything that we wanted to understand about L2 loss it tells us what does.\nthe shape of L2 loss look like it tells us what the optimum is in the absence of L2 loss.\nand then it tells us how there is a compromise being made where you are shrinking the weights but you.\nare not shrinking all the weights uniformly you are shrinking them according to different proportions and that's exactly what is.\nhappening here and these proportions actually decide are decided based on which directions are important and which directions are not.\nimportant because the eigen values these are remember the eigen values of the Hessian and the eigen values of the.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "important because the eigen values these are remember the eigen values of the Hessian and the eigen values of the.\nHaitian tell us which directions are more sensitive and which directions are less sensitive right so I am not proving.\nthat but you can go back and look at that the eigen values of the Hessian matrix these were the.\neigen values of uh yeah of the Haitian Matrix so these tell us or which directions are more important and.\nwhich directions are less important right so all of this has now fallen in place and this picture kind of.\nsummarizes what happens with LD regulations so that's all I had about L2 regularization I'll end this video here.", "metadata": {"video_title": "L2 Regularization"}}
{"text": "foreign [Music] let's go ahead with our original goal right which was to find that Delta Theta which is good.\nnow what do we mean by good is something that we discover now right so for ease of notation let's.\njust call that Delta Theta by U I don't want to write this Delta Theta every time so then from.\nTaylor series we have the following right that we have uh L of theta plus and it's this is where.\nthat Epsilon comes into play right I was saying that we want the difference to be small right so I.\nam at some point Theta and this is some point Theta Nu right this is what this point is and.\nI'm saying that it's just a small distance away and this is where this ETA is important right it makes.\nsure that the distance is small because we know that the approximations are good in the small neighborhood okay now.\nthis is L Theta now what are these other terms here right this ETA is fine right that's our small.", "metadata": {"video_title": "Learning Parameters Gradient Descent"}}
{"text": "this is L Theta now what are these other terms here right this ETA is fine right that's our small.\nuh that multiplication will keep happening everywhere now this here is that seems to be the dot product uh between.\ntwo quantities right now you I have already defined here right U is the change that we want to make.\nso that's nothing much to explain there but what is this other quantity that you see here and let me.\njust underline it with a different color so this quantity so we'll try to figure out what that quantity is.\nso let me clear up the slide now what that is is the gradient right and that's where the name.\ngradient descent comes from but let's try to understand what that gradient is right so now if you have a.\nfunction right let's say x square okay I have y is equal to x square then I know that d.\ny d x this is called the derivative all of you know this from high school which is 2x right.", "metadata": {"video_title": "Learning Parameters Gradient Descent"}}
{"text": "function right let's say x square okay I have y is equal to x square then I know that d.\ny d x this is called the derivative all of you know this from high school which is 2x right.\nso this is fine this everyone knows right but now suppose I have a function y is equal to W.\nsquare plus b square right now this is a function of two variables so now I can say and node.\nis a change in notation right it's not D now it's do is 2W and dou Y by dou B.\nit is 2 b and what have I done here this is known as the partial derivative so I have.\ntaken the partial derivative of this function once with respect to W the other time with respect to B because.\nthese are the two variables on which the function depends right so these are the partial derivatives so this is.\nderivative when you have a single variable function these are partial derivatives when you have a multi variable function and.", "metadata": {"video_title": "Learning Parameters Gradient Descent"}}
{"text": "derivative when you have a single variable function these are partial derivatives when you have a multi variable function and.\nyou're just taking the derivative with one variable treating the other as constant right that's why the derivative here disappears.\nbecause you are taking the derivative of the b square part with w and it does not depend on W.\nso hence the derivative will be so that is the partial derivative now the question is what is the gradient.\nthen the gradient is nothing but just the collection of the partial derivatives right so that gradient in this case.\nwould be 2W 2B right so it's the collection of all the partial derivatives is called the gradient right and.\nin this case I have only two variables so I'll just have two partial derivatives so I'll collect them and.\nI'll get the gradient vector and the gradient Vector is a two dimensional Vector in this case right so that's.", "metadata": {"video_title": "Learning Parameters Gradient Descent"}}
{"text": "I'll get the gradient vector and the gradient Vector is a two dimensional Vector in this case right so that's.\nwhat this quantity is you have the loss function Theta and you are taking the gradient of theta of the.\nloss function with respect to Theta and we know that Theta is nothing but W comma B and this Theta.\nis also W comma B so this is just a vectorial way of writing it that you're taking the partial.\nderivatives with respect to W and B you put those partial derivatives in a vector that Vector is called the.\ngradient and this is the quantity that we are using to denote that gradient and now it makes sense because.\nyou're taking the gradient of a function which is a function of a vector with respect to that Vector right.\nthat's what that's how you should read right so this is what this quantity is right now for this discussion.\nI think I can stop here because this is where I am going to draw a line I am going.", "metadata": {"video_title": "Learning Parameters Gradient Descent"}}
{"text": "that's what that's how you should read right so this is what this quantity is right now for this discussion.\nI think I can stop here because this is where I am going to draw a line I am going.\nto cut the formula at the linear approximation but let me not do that let me just go a bit.\nahead and also try to tell you what this quantity is Right which is right now here if I ask.\nyou what's the second order derivative that means I'm going to take the first derivative and again take the derivative.\nwith respect to X again and that's going to be uh 2 right so this is the second order derivator.\nnow what this is is the gradient of the gradient right that's a bit uh confusing to understand so this.\nis what your gradient looks like it's a vector containing the partial derivatives now again you are taking a derivative.\nof this Vector with respect to your parameters which is nothing but taking the gradient with respect to Theta right.", "metadata": {"video_title": "Learning Parameters Gradient Descent"}}
{"text": "of this Vector with respect to your parameters which is nothing but taking the gradient with respect to Theta right.\nso what would this look like so you have ah two elements in this Vector right so for each element.\nyou will take a derivative with respect to W you will take a valuator with respect to B for the.\nsecond element also you will take a derivative with respect to W and you'll also take a derivative with respect.\nto B so what you will get is dou Square l by dou W square right so you had the.\nfirst partial derivative you are taking the derivative of that again with respect to W so that's what you will.\nget now the same partial derivative you will be taking a derivative with respect to B now so you'll have.\ndou dou b dou w and similarly you could fill the remaining two entries here right so this would be.\ndou b square and this would be dou w dou b right so the way you should remember this is.", "metadata": {"video_title": "Learning Parameters Gradient Descent"}}
{"text": "dou dou b dou w and similarly you could fill the remaining two entries here right so this would be.\ndou b square and this would be dou w dou b right so the way you should remember this is.\nthe following right so when you are taking the derivative of a vector okay with respect to some another vector.\nokay what are you trying to do you're trying to say that if this value changes how much does this.\nvalue change how much does this value change right and if this value changes how much does this value change.\nhow much does this value change right so the derivative of a vector with respect to a vector would thus.\nbe a matrix right and what would the dimension of the Matrix be if this was M dimensional and this.\nwas n dimensional then this would be an M cross n Matrix right so if you had for example three.\nelements in this vector okay and you're taking the derivative of that Vector with respect to two elements so what.", "metadata": {"video_title": "Learning Parameters Gradient Descent"}}
{"text": "elements in this vector okay and you're taking the derivative of that Vector with respect to two elements so what.\nwould happen is that each of these elements suppose these uh two elements that you have here are your parameters.\nW and B right so each of these elements would be a function of w comma B and hence you.\ncan take that derivative so when you're changing W you want to see how much does this value change because.\nit's a value function of w comma B so how much does this value change how much does this value.\nchange how much does this value change right so you're asking for three calculating these three changes and then we.\nchange the value of B again you want to change how much does this change how much does this change.\nhow much does this change right so there are six values that you're trying to compute so then set what.\nwe are three cross two or two cross three depending on how your uh uh seeing it right so it.", "metadata": {"video_title": "Learning Parameters Gradient Descent"}}
{"text": "how much does this change right so there are six values that you're trying to compute so then set what.\nwe are three cross two or two cross three depending on how your uh uh seeing it right so it.\ncould uh so it that's what it would be right so that's what and this second order derivative is called.\nthe Hessian uh uh Matrix and then similarly you could imagine what would be here there would be a third.\norder uh derivative here and you can again imagine that you have a matrix and now you're taking the derivative.\nof every element in that Matrix with respect to a vector and so on it but that that I know.\nI won't go that far I'll just stop here although even this we don't need for the current lecture okay.\nso I hope that is clear so uh partial derivatives derivatives sorry derivatives partial derivatives collection of partial derivatives is.\nthe gradient the gradient of the gradient is the Hessian right and here are a few things to note now.", "metadata": {"video_title": "Learning Parameters Gradient Descent"}}
{"text": "the gradient the gradient of the gradient is the Hessian right and here are a few things to note now.\nright so if you have a scalar quantity right so if I have a function which is Y is equal.\nto x square okay then the derivative would be 2x so if I were to compute this at a particular.\nvalue of x then I'll get a square right so derivative of a scalar with respect to a single variable.\nis a scalar but now if you have a vector or if you have a quantity uh x square okay.\nall this is x square plus Z square right then I can take the derivative with respect to X and.\nI can also take the derivative with respect to Z so then I'll get a vector right so the derivative.\nof this would be a vector which will be basically the gradient right so that's that's what uh we are.\nuh understanding now and this is where I'm going to draw the line right so my argument for that would.\nbe that ETA is small so ETA square is going to be even smaller hence I can ignore this ETA.", "metadata": {"video_title": "Learning Parameters Gradient Descent"}}
{"text": "uh understanding now and this is where I'm going to draw the line right so my argument for that would.\nbe that ETA is small so ETA square is going to be even smaller hence I can ignore this ETA.\nQ would be even smaller so all of this I can ignore right so since ETA is small I'm just.\ngoing to cut the derivative or cut the formula here and I'll just use the linear approximation and we have.\njust seen previously that when ETA is small a linear approximation is good enough right so I have reason at.\nleast I have shown you geometrically that I can do that okay so this is what I'm going to do.\nokay now ah this move right this is the move that I have made I was comfortably at some value.\nof theta there was a certain loss there and I decided to make this move now this move would be.\nfavorable only if what is the condition that I would want I have moved and I would call the move.\nto be favorable only if so only if this value is less than this value right otherwise it's not favorable.", "metadata": {"video_title": "Learning Parameters Gradient Descent"}}
{"text": "to be favorable only if so only if this value is less than this value right otherwise it's not favorable.\nI was at certain loss now I have moved and if the loss increases then that's not a favorable move.\nright why I should have why would a better State there right so I would like uh you only if.\nuh the new loss is less than the previous loss right so that's the condition that I want so what.\nI want is that if I do the subtraction If I subtract the new loss from the old loss then.\nI should get a value less than 0 which is a simpler I mean it's just another way of saying.\nthat the new loss is less than the Old Law so that's all I'm seeing right and now if I.\njust look at this equation here what is this quantity so I just take L Theta to the other side.\nand I am left with this right so what I am saying is that this quantity should be less than.\nzero that's what I'm effectively saying right so I'll just delete some stuff so this is the quantity that I.", "metadata": {"video_title": "Learning Parameters Gradient Descent"}}
{"text": "zero that's what I'm effectively saying right so I'll just delete some stuff so this is the quantity that I.\nwant to be less than 0 if I take L Theta this side then that is the quantity that I.\nget and that is just equal to this quantity right so if I want this quantity to be less than.\nequal to 0 that means essentially I want this quantity to be less than zero right and I have omitted.\nthe ETA here because ETA is a positive scale right that's uh you we don't take a negative scalar there.\nso ETA is positive so that will not affect so I can ignore ETA here and this quantity should be.\nless than zero okay so that is what I have arrived at from the uh Taylor series and why is.\nthis why am I interested in this because this is a condition which depends on you right so now I'm.\nwhat is being told to me that you have to select a u such that U transpose the dot product.\nof U with the gradient Vector would be less than zero only then your U would be good so this.", "metadata": {"video_title": "Learning Parameters Gradient Descent"}}
{"text": "what is being told to me that you have to select a u such that U transpose the dot product.\nof U with the gradient Vector would be less than zero only then your U would be good so this.\nis a condition that has been imposed on my U Now using this condition I want to find a good.\nU which not only satisfies this condition but satisfies this condition in the best possible way right and I'll tell.\nyou what best possible way it means right so what is the range of this right so this is the.\ndot product uh between uh two vectors I think this should not be there this is the dot product so.\nwhat is the range of this right so let's try to understand that so let beta be the angle between.\nU transpose and the gradient vector then we know this right this is the COS of the angle this is.\njust the formula for the COS of the angle and this quantity which I was interested in luckily shows up.\nin this formula hence I am interested in it right so this is what uh cos beta is and the.", "metadata": {"video_title": "Learning Parameters Gradient Descent"}}
{"text": "just the formula for the COS of the angle and this quantity which I was interested in luckily shows up.\nin this formula hence I am interested in it right so this is what uh cos beta is and the.\ngood thing is that cos beta is bounded right so that means this quantity is also bounded okay good so.\nif I multiply throughout by this the denominator that you have here I'm just calling it K so that I.\ndon't have to write this large quantity every time so this is what I get right so the quantity that.\nI am interested in I know that it lies between minus K and K right and I don't care about.\nuh the positive values of this quantity right I want this quality quantity to be less than 0 right so.\nI just care about the negative side because I want my condition was that U transpose into this gradient should.\nbe less than 0 that's the condition that I have right so I am interested in the values which are.", "metadata": {"video_title": "Learning Parameters Gradient Descent"}}
{"text": "be less than 0 that's the condition that I have right so I am interested in the values which are.\nless than 0. so now if I'm interested in values less than 0 I should ask this question right how.\nless can I be and the more or less the better right because what is what is this quantity this.\nquantity is the difference between the new loss and the current loss right and I the more negative this difference.\nthe better right that means my new loss is as low as it can be from the current loss right.\nand when would this be when would this difference be maximum when uh this quantity is as low as it.\ncan be right because this quantity can be as negative as it can be and this is that quantity it.\ncan be as this is how negative it can be and when will be that negative when the angle or.\nthe cosine of the angle is minus 1 because this condition is directly been derived from here right so if.\nthe cosine of the angle is minus 1 that's when I will get the most negative value for this quantity.", "metadata": {"video_title": "Learning Parameters Gradient Descent"}}
{"text": "the cosine of the angle is minus 1 that's when I will get the most negative value for this quantity.\nwhich is my quantity of Interest right and when would the cosine be minus 1 when the angle between the.\ntwo vectors is 180 degrees which two vectors the vector U and the gradient vector so that's where I get.\nmy U now I want you to be at 180 degrees with the gradient Vector that means I Want U.\nto be opposite to the gradient Vector hence in gradient descent and this is something you would have heard a.\nhundred times I move in the direction oppose it to the gradient right so this is how that formula or.\nhow that rule comes we started off with the Taylor series that gave us a certain condition on what a.\ngood use should be then we've pushed that condition to be to its limit right we wanted it to be.\nless than zero but we said let it be as less as newer as it can be and we found.\nthat that happens when this happens okay and that happens when beta is equal to 180 degrees and beta is.", "metadata": {"video_title": "Learning Parameters Gradient Descent"}}
{"text": "less than zero but we said let it be as less as newer as it can be and we found.\nthat that happens when this happens okay and that happens when beta is equal to 180 degrees and beta is.\nthe angle between these two vectors so that happens when U is exactly opposite to the gradient Vector right so.\nthat's why you move in the direction oppose it to the gradient vector let me just delete this is more.\nnegative is the most negative right not just more negative it's the most negative when cos beta is equal to.\nminus 1 that is when beta is equal to 180 degrees.", "metadata": {"video_title": "Learning Parameters Gradient Descent"}}
{"text": "[Music] so welcome back uh so in the previous module we looked at this typical machine learning supervised machine learning.\nsetup and the key there was to now come up with a algorithm for learning the parameters right so what.\nwe'll do is now start with an infeasible algorithm right which will not really work in practice but it will.\ngive us intuitions about how what should we do to make it i mean go towards the more practical approach.\nright so let's start so this is the supervised machine learning setup that i have i have n inputs okay.\nand i have some weights and then i have the output now this is the model right this is my.\napproximation of the relation between y and f y and x right so this is f hat actually this is.\nthe model and now i want to know an algorithm okay which can be used to learn the parameters of.\nthis model given the data that is would be given to me and using some objective function which makes sense.", "metadata": {"video_title": "Learning Parameters Infeasible guess work"}}
{"text": "this model given the data that is would be given to me and using some objective function which makes sense.\nright so that's what my quest is okay now then this is what my f hat of x is i.\ngenerally use therefore fake side i mean i've explained the difference between f of x and f dot x but.\ni'll just call it f of x and from the context it should be clear that i'm talking about my.\napproximation because the true function of course is not known right so i cannot even define that right and i'll.\nuse sigma here which is written here is actually a short form or the notation for the sigmoid function okay.\nso now we'll consider a very simplified version of this model right for all that explanation that there's only one.\ninput which is connected by the weight w and then there's this constantly on input which was connected by minus.\nw naught but now to be consistent with the literature going forward we are going to call it as b.", "metadata": {"video_title": "Learning Parameters Infeasible guess work"}}
{"text": "w naught but now to be consistent with the literature going forward we are going to call it as b.\nright b stands for bias so instead of w naught we are going to use b and our function would.\nthen become minus of w x plus b earlier it was minus of w x plus w naught right so.\nyou're just changing that notation and i'll stick to this notation for the rest of the course now right yeah.\nand lastly right so so far what we have been doing is we have been trying to talk about the.\nproblem that you are given one input and you are trying to predict the value of the uh zero or.\none right whether this movie are going to like it or not but now i'm going to make it uh.\nmake it a problem where you're trying to predict a real value instead of a boolean value and you're trying.\nto predict the imdb rating on a scale of zero to one right so it could take values like zero.\npoint zero five point one point two whatever right it could take any of these values so it's a real.", "metadata": {"video_title": "Learning Parameters Infeasible guess work"}}
{"text": "to predict the imdb rating on a scale of zero to one right so it could take values like zero.\npoint zero five point one point two whatever right it could take any of these values so it's a real.\ninput a real output right so that's it's easier for me to explain a few things in this setup but.\nit does not change anything that i'm going to talk about even if you had boolean inputs we'll see later.\non how to adjust the same explanation to boolean inputs but for now i'll assume we have a real output.\nright sorry not input output so we have a real valued output okay so that's the setup that i'm going.\nto work with right and now the first thing is that i need some training data right so i've been.\ngiven some training data so as i said that there are capital n pairs of now this is a good.\nnotation capital n for the number of training pairs and small n for the number of inputs that i have.\nthe small n is equal to 1 in this case right so this is the training data that i have.", "metadata": {"video_title": "Learning Parameters Infeasible guess work"}}
{"text": "notation capital n for the number of training pairs and small n for the number of inputs that i have.\nthe small n is equal to 1 in this case right so this is the training data that i have.\nnow my training objective as i said is going to be that when i make predictions and these are the.\ntrue values and these are the prediction values i want the difference between them to be as small as possible.\nso this is my difference function and this i'm going to calculate over all the training points take the average.\nand i'm going to define that as my loss function and my objective is to find w and b right.\nsuch that my loss function is minimized right so that's an english explanation of this entire expression written here minimize.\nwith respect to w and b which means find the values of w and b which minimizes this quantity and.\nthis is how this quantity is defined it's the average difference of the predictions from the two values for all.", "metadata": {"video_title": "Learning Parameters Infeasible guess work"}}
{"text": "this is how this quantity is defined it's the average difference of the predictions from the two values for all.\nthe training points right so that's a clear notation right now suppose i was given only two training points okay.\nthese are the two training points given to me these are the x's and these are the y's so this.\nis one x one comma y one pair and this is the other x2 comma y2 pair i'm just given.\ntwo training examples okay and these are the values also right i mean these are actual points on the graph.\nso i can see that one point is 0.5 comma 0.2 right so this is the point 0.5 comma 0.2.\nas you can see and the other point is 2.5 comma 0.9 right so these are the points which are.\ngiven to me okay this is my input training data and using this data i want to learn w and.\nb and that's the setup that i have okay now um what is happening yeah so at the end of.\ntraining we've we expect to find the optimal values of w and p right what are the optimal values though.", "metadata": {"video_title": "Learning Parameters Infeasible guess work"}}
{"text": "b and that's the setup that i have okay now um what is happening yeah so at the end of.\ntraining we've we expect to find the optimal values of w and p right what are the optimal values though.\nthese are those values which minim minimize the loss function that we had defined in the previous slide right so.\nthose are the values that we want to find so now suppose so now what do you expect right so.\nsuppose at the end of training i told you that i found w star b star what is it that.\nyou expect what would happen i'll ask a more specific question if i plug in the value 0.5 here if.\ni pass 0.5 as the input and if you have learned w and b well what do you expect to.\nhappen the output should be the output should be close to 0.2 similarly if i plug in the value 2.5.\ni would want the output to be close to 0.9 right this is what i expect to be and this.\nis what i've written here right so if i plug in f of 0.5 then it should be as close.", "metadata": {"video_title": "Learning Parameters Infeasible guess work"}}
{"text": "i would want the output to be close to 0.9 right this is what i expect to be and this.\nis what i've written here right so if i plug in f of 0.5 then it should be as close.\nto 0.2 right tending to 0.2 and f of 2.5 should be as close to 0.9 right now let's try.\nto relate this algebra to the geometry right so now geometrically i know that i am looking for a sigmoid.\nfunction which is a s-shaped function right now algebraically what i am telling you is that when i plug in.\nthese values they should be as close to 0.2 and as close to 0.9 so now can you tell me.\nwhat is the geometric interpretation of that i'm going to get some function because this actually defines a sigmoid function.\nright and now i'm found on w and b so i'm going to plug in those values right and then.\ni can draw that function so what should that function look like or what should be a characteristic of that.\nfunction of course it will be a sigmoid function of course it will have s shaped but there is respect.", "metadata": {"video_title": "Learning Parameters Infeasible guess work"}}
{"text": "function of course it will be a sigmoid function of course it will have s shaped but there is respect.\nto the training data what should happen the two points that i have shown here should lie on the function.\nright so what what that means is let me just draw it apply or close to that function right so.\ni'm assuming it's perfectly done so this is what should happen right so whatever w comma b values i have.\nwhen i plug them here i know that when i plug in the value 0.5 i want something which is.\nclose to 0.2 right i'm drawing the best case here that i should get a sigmoid function such that these.\ntwo points lie on that and that's what it means right that when i plug in the value of x.\ni get this y when i plug in the value x2 i get the other way right so this is.\nwhat we are looking for it so this connects uh i would say the math to the geometry right so.\nthe algebra to the geometry that uh when you plug in these values you get some output and this is.", "metadata": {"video_title": "Learning Parameters Infeasible guess work"}}
{"text": "what we are looking for it so this connects uh i would say the math to the geometry right so.\nthe algebra to the geometry that uh when you plug in these values you get some output and this is.\nwhat it would mean geometrically right okay so now let us see this in more detail right so as usual.\ni never know what the w's and b's are right i've given these two points i'm going to start with.\nsome random w's and b's so here i have started with uh if you can't see it let me just.\nsee if it's on the file ah so w equal to three and b is equal to minus one 1..\nthat's what i have started with okay and now this is the sigmoid function that we get i know that.\nintuitively i know that this is not correct why is this not correct both the points are not on the.\nfunction right this point is also not on the function right so both the points are not on the function.\nand just on the previous slide we saw that when the network is trained or when i have learned the.", "metadata": {"video_title": "Learning Parameters Infeasible guess work"}}
{"text": "and just on the previous slide we saw that when the network is trained or when i have learned the.\nvalues of wnb i would expect these points to lie on the sigmoid function but right now it's clearly not.\nthe case so this is bad there is no argument about that but how bad is it can you quantify.\nthat all of you are saying that it's bad right that's a qualitative answer can you quantify that yeah right.\nso the answer given is that we could look at the loss function right so we now what we will.\ndo is that we look at the loss function which depends on w and b and we'll see how to.\ncalculate the loss function and then we'll be able to say exactly how bad this is right so let's do.\nthat right so my values of w and b i know are 3 and minus 1. so i know that.\nthis is equal to 1 over 1 plus e raised to minus of 3x plus b and b i have.\nchosen s minus 1 so this is what my function looks like now in this function i'll plug in all.", "metadata": {"video_title": "Learning Parameters Infeasible guess work"}}
{"text": "this is equal to 1 over 1 plus e raised to minus of 3x plus b and b i have.\nchosen s minus 1 so this is what my function looks like now in this function i'll plug in all.\nthe points that i have how many points do i have only two points so for one point the value.\nwould be the x is 0.5 and the other x is 2.5 so i'll plug in these values and i'll.\nbe able to compute this quantity and i already know what the true y should be right so i can.\ncalculate this so i'm just going to go ahead and calculate this right so this is what it looks like.\nokay i just expanded the sum now i've going to calculate those values and this is what the values turn.\nout to be now i'm just going to do the math and this is the value right so this is.\n0.099 that's the loss if it was perfect the loss would have been zero right but this is 0.099 so.\nthat's how bad this is now i am able to quantify my answer using the loss value if you give.", "metadata": {"video_title": "Learning Parameters Infeasible guess work"}}
{"text": "0.099 that's the loss if it was perfect the loss would have been zero right but this is 0.099 so.\nthat's how bad this is now i am able to quantify my answer using the loss value if you give.\nme a w comma b i can tell you how bad that is right that's right what i am able.\nto do right now we want the loss to be as close to zero as possible ideally zero but as.\ni said i would settle for approximately zero also right so that's what we want to do so let us.\ntry some other values of w comma b right and we'll change we'll use the slider to change it okay.\nso first i'll try uh let me just see if i can do that so first i'm going to try.\nw is equal to 0.5 okay okay this is 0.5 and b is equal to 0 right so let me.\njust oh for some reason i'm not able to change it okay so suppose i do w is equal to.\n0.5 and b is equal to minus 1 right this is also bad and how bad is it it's saying.\nit's 0.121 right so earlier my loss was 0.09 now my loss is 0.121 right so it's even worse right.", "metadata": {"video_title": "Learning Parameters Infeasible guess work"}}
{"text": "0.5 and b is equal to minus 1 right this is also bad and how bad is it it's saying.\nit's 0.121 right so earlier my loss was 0.09 now my loss is 0.121 right so it's even worse right.\nand you can see from the figure also that's even worse right so let me try something else i'll try.\nminus 0.1 now okay like some of these values are not here but let's say i try this value right.\nagain i see that my loss is increasing so then i keep changing right so i keep trying different values.\nand then i tried 0.94 and minus 0.94 and i saw that the loss is 0.0214 okay so this is.\ngood okay this seems to be good so let's let's see what my rational was it it started with 0.50.\ni got a loss of 0.07 then i randomly jumped to some other point the value increased okay so then.\ni concluded that i made the w negative maybe that's a problem right this is what my thinking was this.\nis how i have come up with these numbers so let me interest increase the w so i increased the.", "metadata": {"video_title": "Learning Parameters Infeasible guess work"}}
{"text": "i concluded that i made the w negative maybe that's a problem right this is what my thinking was this.\nis how i have come up with these numbers so let me interest increase the w so i increased the.\nw and i made b a bit negative and the loss decreased okay so then i assumed okay increasing the.\nw further and decreasing would be further might help and that actually helped and i kept continuing that way and.\nit had right i'm doing random guesswork but it's slightly not so random because i'm looking at the loss values.\nand slowly trying to adjust the weights right and all these values that i tried these are all the different.\nfunctions that i got right so you can see what had happened for w equal to 0.50 you can go.\nback and look at all these values so i was getting different uh functions and then as i kept changing.\nit i somehow found the right function right so this is what my guesswork was and you can all imagine.", "metadata": {"video_title": "Learning Parameters Infeasible guess work"}}
{"text": "it i somehow found the right function right so this is what my guesswork was and you can all imagine.\nthat this guesswork could have possibly never ended these are very specific values and i chose them because i knew.\nwhat the right value was and i was just trying to get that i mean i have reverse engineer these.\nwasn't if i was completely randomly trying this this would not really converge that i would not end up anywhere.\nright so now let us look at uh something better than this guesswork algorithm right so this is slightly better.\nthan the guesswork algorithm so what i have done here is again i had seen we had seen this earlier.\nso i've just plotted so my this is my uh oops this is my w axis this is my b.\naxis okay and the vertical axis is the error axis right so for all possible values of w and b.\ni have just computed the loss i can do that i can do that programmatically and i have just plotted.", "metadata": {"video_title": "Learning Parameters Infeasible guess work"}}
{"text": "axis okay and the vertical axis is the error axis right so for all possible values of w and b.\ni have just computed the loss i can do that i can do that programmatically and i have just plotted.\nit right and this is what the plot looks like okay and you can see that there is this region.\nhere where the loss is low or minimum right and there are certain regions where the loss is very high.\nright so i could just plot this and then i could just pick up some value from this good region.\nhere and that would what my answer be right and we had seen a similar argument when we're doing the.\nperceptron algorithm and we need the know the fallacy of this argument also that this is okay for this toy.\nexample uh but it will become intractable as the number of data points increasing right so as the number of.\ndata points increases just computing this loss would be very difficult for all the infinite values of w and b.", "metadata": {"video_title": "Learning Parameters Infeasible guess work"}}
{"text": "data points increases just computing this loss would be very difficult for all the infinite values of w and b.\nright and here although i'm saying infinite i've actually plotted this only from minus 6 to plus 6. right those.\nare the only values i'm not really plot i mean i cannot plot for minus infinity so what if there.\nwas a value here right going forward this side for which the loss was even lower right so what would.\nwhat why i could have chosen that also right so this kind of just plotting the error surface and looking.\nat it and figuring out the value that does not work in practice right even although this is a good.\nway of seeing what is happening it does not you cannot do this in practice right you won't be able.\nto get anywhere okay so now let us with that plot in mind and my guesswork in mind let us.\ntry to look at the geometric interpretation of what was happening in my guesswork algorithm right so we will try.", "metadata": {"video_title": "Learning Parameters Infeasible guess work"}}
{"text": "try to look at the geometric interpretation of what was happening in my guesswork algorithm right so we will try.\nto do that okay so what had i done i had initially chosen 3 and one as my values right.\nand then uh three and one or was it three n minus one i guess right so three n minus.\none is what i had chosen right so this is where i was on the lost surface this is my.\nthree comma minus one point and my loss was very high right and then from there i randomly jumped to.\nthis point uh 0.5 uh comma 0. and before i click enter just see that when the value of x.\nw1 wnb is 3 comma minus 1 this is what the sigmoid function looks like and it's obvious that the.\nerror is high because it's nowhere close to fitting the two points that i have right so then i tried.\n0.5 comma 0 and let's see what happened now if you look at the sigmoid function it looks a bit.\nmore favorable right and you can see that the loss has also decreased right but since i was randomly guessing.", "metadata": {"video_title": "Learning Parameters Infeasible guess work"}}
{"text": "more favorable right and you can see that the loss has also decreased right but since i was randomly guessing.\ni did not have this error surface in front of me so i just continued randomly right and then i.\nwent to the point minus 0.1 comma 0 okay and i goofed up you can see that i actually went.\nup on the error surfaces i was somewhere um yeah it was somewhere here and i climbed up on the.\nerror surface right and then i changed it further so what i did is oops yeah then i changed it.\nto a 0.94 and minus 0.94 and then the loss decreased right and i can see from the sigmoid function.\nalso that i am going in the right direction and then i kept in continuing in this direction but you.\ncan see that my movements if you look at the wb plane they're very random that is jumping from one.\npoint to another i'm just getting lucky somewhere and i once i hit this point 0.94 minus 0.94 then i.\nget a sense of what is required i need to increase the w's and decrease the b's and then i.", "metadata": {"video_title": "Learning Parameters Infeasible guess work"}}
{"text": "point to another i'm just getting lucky somewhere and i once i hit this point 0.94 minus 0.94 then i.\nget a sense of what is required i need to increase the w's and decrease the b's and then i.\ncontinued that way and then i went finally reach to the point 1.78 comma minus 2.27 where my error was.\nclose to zero and you can see that the sigmoid function is in the way it should be and you.\ncan see on the error surface also i am in that low error region in fact the error is actually.\nzero right so that's that's what i was doing i was actually uh uh subconsciously or like without really knowing.\nit actually navigating the error surface but i was navigating it randomly which is problematic because then i went down.\nthen suddenly i came up then again went down came up and i want to avoid that i want to.\nhave a more principled way of navigating this error surface right so that's what my quest is and that is.\nwhat we will do in the next module okay.", "metadata": {"video_title": "Learning Parameters Infeasible guess work"}}
{"text": "[Music] okay so now let's talk about some intuition behind the intuition behind uh learning parameters for a feed forward.\nneural network right and how do we connect it to what we already know right we have already seen how.\nto learn parameters of a very simple network i mean it's not even a network but for a single neuron.\nwhich had a w and a b and a single input and there was a y hat we had seen.\nhow to learn the parameters of this network using gradient descent now this thing that we know and understand well.\ncan we somehow stretch it and extend it to help us learn the parameters of all the parameters of a.\nfeed forward neural network right so that's what we'll try to focus on okay uh so the story so far.\nis that we have introduced feed forward neural networks and now we are interested in finding an algorithm for learning.\nthe parameter right so this is what our feed forward neural network looks like now let's just quickly recall our.", "metadata": {"video_title": "Learning parameters Intuition"}}
{"text": "the parameter right so this is what our feed forward neural network looks like now let's just quickly recall our.\ngradient descent algorithm and make some commentary on that right so this is what our gradient descent algorithm was we.\nhad initialized the weights and then at every step we were updating the weights right and now i can think.\nof writing this a bit more compactly right in fact we had looked at it already i know that this.\ni could write it as a vector theta t plus 1 similarly this i could write it as theta t.\nand this i could write it as a vector gradient right the gradient and this also i could write it.\nas a vector theta naught right so i am going now going to change this equation and write it more.\ncompactly where i am going to replace the collection of w and b by theta right that's the only change.\ni am going to do and that's fair enough this is how i'm going to write it more compactly so.", "metadata": {"video_title": "Learning parameters Intuition"}}
{"text": "compactly where i am going to replace the collection of w and b by theta right that's the only change.\ni am going to do and that's fair enough this is how i'm going to write it more compactly so.\ntheta naught is the collection of w naught b naught and once you understand that this falls in place right.\nso theta t plus 1 is just now w t plus one comma b t plus one and you're just.\ndoing vector uh operations now instead of like individual uh element wise operations right and that's perfectly fine as we.\nsaw in the previous slide where i had annotated the vectors right so there's nothing wrong here and where the.\ngradient right so when i say grad delta theta right i am going to use this notation right the more.\nappropriate elaborate notation would be gradient of the loss function with theta evaluated at time t right but i'm just.\ngoing to use this shortcut notation and hence i am elaborating what i mean by that it means just the.", "metadata": {"video_title": "Learning parameters Intuition"}}
{"text": "going to use this shortcut notation and hence i am elaborating what i mean by that it means just the.\ncollection of the partial derivatives right so i've taken the partial derivative with this of the loss function with respect.\nto wt the partial derivative of the loss function with respect to b it's not bt or wt it's w.\nand b and then evaluated at the current values right which is at time step t and we have seen.\nthis what that means in the previous lecture so this is what my this notation here means right i am.\njust clarifying that is just a collection of the partial derivatives that means it is the gradient right now this.\nwas all uh good right now in this feed forward neutral network instead of theta equal to wb right which.\nwas just a collection of two vectors now my theta is a collection of many more elements right it's all.\nthe elements of w1 which is n square elements all the elements of w2 w3 which are again n square.", "metadata": {"video_title": "Learning parameters Intuition"}}
{"text": "the elements of w1 which is n square elements all the elements of w2 w3 which are again n square.\nelements and then elements of wl which is n into k then all the biases there were n biases in.\nlayer one n biases in layer two and then k in layer three right all of this collected together so.\ni have a large army of parameters now instead of just two parameters right but if i'm going to write.\ntheta as a collection of all the parameters i can still do that i'm just going to say that theta.\nis a collection of all the parameters earlier my vectors were of size 2 now my vectors are very large.\nright there n square plus n square plus n into k plus n plus n plus k right in this.\nspecific example so it's a very large vector so what it's still just a vector and all these operations still.\ncan hold it just as i can add two dimensional vectors or subtract one vector from another i can do.", "metadata": {"video_title": "Learning parameters Intuition"}}
{"text": "can hold it just as i can add two dimensional vectors or subtract one vector from another i can do.\nthe same for these very large dimensional vectors also right so you can still use the same algorithm for learning.\nthe parameters of our model except that now earlier remember there were only these two quantities and we still had.\nto derive this side we still had derived painfully what is the equation or what is the expression for the.\nderivative of the loss function with respect to w what is the expression for the derivative of the loss function.\nwith respect to b and then substituted values in that right so we still have to do that computation right.\nso it's just that our gradient of del theta now looks like a very very big vector and we should.\nknow how to compute every quantity in this vector right and we did this for those two simple values w.\nand b and that itself was quite a bit of a derivation so our quest would be somehow come up.", "metadata": {"video_title": "Learning parameters Intuition"}}
{"text": "know how to compute every quantity in this vector right and we did this for those two simple values w.\nand b and that itself was quite a bit of a derivation so our quest would be somehow come up.\nwith a formula which allows us to compute all of this at one go right without painfully deriving that in.\nfact we'll derive it but we derive it in such a way that we could compute an entire matrix of.\ntheir partial derivatives at one go instead of computing each of those n square values one by one right so.\nthat's what one of the quests of this lecture is going to be but that's all for later for now.\ni want you to focus on this graduation from theta naught being a collection of two elements to theta naught.\nbeing a collection of many elements but as long as i can tell you what these are the same algorithm.\nstill applies right because you just want to compute the derivative of the loss function with respect to each parameter.", "metadata": {"video_title": "Learning parameters Intuition"}}
{"text": "still applies right because you just want to compute the derivative of the loss function with respect to each parameter.\nand update the parameter accordingly right so insects now our delta theta looks much more complex so we have delta.\nt the loss derivative of the loss partial derivative of the loss function with respect to w 1 1 1.\nthat is the first weight in w1 matrix all the way up to the n square weights that you have.\nin the w1 matrix similarly the n square weights that you have in the w2 matrix similarly the n into.\nk weights that you had in the last layer similarly the n biases that you had in each layer and.\nthe k biases that you had in the last layer right so this is not like a something cross n.\nmatrix right because this last row has only k right so it will not be like uh i just put.\neverything together in one collection i'm not saying that this is a matrix right this is just a collection of.", "metadata": {"video_title": "Learning parameters Intuition"}}
{"text": "everything together in one collection i'm not saying that this is a matrix right this is just a collection of.\npartial derivatives because each layer might have different so i just assume everything is n square here but we saw.\nthat other example where it could be n cross m here then m cross p here then some n here.\nm here and then k here right for the biases right so it's all going to be different across different.\nlayers so this is not like a well-formed matrix it's just a collection i have put together just for illustrative.\npurpose right so these are all the partial derivatives that i need to collect and i should be able to.\ndo this fairly uh conveniently and not like painfully go over every element and have to write down the formula.\nfor that and compute right i should be able to at least take one entire matrix of weights and compute.\nthe partial derivatives of all the elements at one goal right so that's what my quest is going to be.", "metadata": {"video_title": "Learning parameters Intuition"}}
{"text": "the partial derivatives of all the elements at one goal right so that's what my quest is going to be.\nbut if i can do that then i am done right so this is where we are this is what.\nthe intuition is you can use the gradient descent algorithm as it is provided you have these quantities so we.\nneed to answer two questions how to choose the loss function why do we need to answer this question because.\nwe need to compute the partial derivatives of the loss function with respect to the weights right so unless i.\nknow what the loss function is i can't even start writing down what that formula is going to be so.\ni need to know how to choose the loss function and once we choose the loss function i need to.\ncompute every element of the gradient vector right which is the partial derivatives with respect to all the weights that.\ni had in the network right so if i know these two then i'll just come back to my gradient.", "metadata": {"video_title": "Learning parameters Intuition"}}
{"text": "i had in the network right so if i know these two then i'll just come back to my gradient.\ndescent algorithm and i can find the i can learn the parameters right so that's the intuition this idea should.\nbe clear that while we have graduated from that two parameter case to like a very large number of parameters.\ncase the basic idea still remains the same i just need to be able to compute the partial derivatives of.\nthe loss function with respect to the weights if i can do that i can run gradient descent and to.\ncompute that i need to know the loss function and i need to know how to compute the partial derivative.\nso that's what we are going to do uh in today's lecture right so this is going to be a.\nvery long lecture but this is what we are going to do okay.", "metadata": {"video_title": "Learning parameters Intuition"}}
{"text": "foreign [Music] with this idea that we want to estimate the parameters and we reintroduce the concept of error and.\nwhat we understood is that in our random guess algorithm or the guesswork algorithm right we were actually navigating the.\nerror surface although we were not realizing we're just moving from here and there and at times we are making.\na mistake where we went up the error surface right or went to a higher error and we don't want.\nthat to happen right we want a journey which is smooth that you keep going down as far as the.\nerror is concerned and then settle to a point where the error is minimum right if not zero so we.\nneed a more principled approach and that's what we're going to do in this module right so this is the.\ngoal as I just stated that find a way of traversing this error surface so that we can reach the.\nminimum quickly and without a lot of back tracing or like going back or not back racing actually but random.", "metadata": {"video_title": "Learning Parameters Taylor series approximation"}}
{"text": "minimum quickly and without a lot of back tracing or like going back or not back racing actually but random.\njumps here in there which don't necessarily decrease the error at each iteration right and we don't want to do.\nthis brute for search either right so that is not possible right we cannot plug in all the values of.\nthe parameters and then see that value which gives us the minimum value that would be the Brute Forces we.\ncannot do that and we have argued why that is infeasible there are infinite values there okay so let's look.\nat the setup now right so we have our parameters in the toy Network where W comma B and I'm.\njust going to put them in a array or a vector and call that Vector as Theta right so now.\nTheta is my parameter of vectors and now I'm looking for changing this value right so just as is always.\nthe case I don't know what the correct Theta is so I'm going to start off with some random value.", "metadata": {"video_title": "Learning Parameters Taylor series approximation"}}
{"text": "the case I don't know what the correct Theta is so I'm going to start off with some random value.\nfor Theta and when I start off with this random value of theta I want to know how much to.\nchange it so that at this Step at least I'm looking at it one step at a time at this.\nStep at least the error should decrease right so I'm looking for that change in Theta which is nothing but.\na change in W and a change in B so that I get a new value of theta and then.\nmy loss is hopefully decreased I'm not sure of that yet but that's what the goal is right so this.\nis my Theta some Vector Delta Theta is also a vector it is also a vector in R2 this is.\na vector in R2 and similarly this is also a vector in R2 right now these are two vectors now.\nwhat I'm going to do is my change is going to be just given as Theta Nu is equal to.\nwherever I am currently I'll move by this change right so Theta Nu is equal to theta plus Delta Theta.", "metadata": {"video_title": "Learning Parameters Taylor series approximation"}}
{"text": "what I'm going to do is my change is going to be just given as Theta Nu is equal to.\nwherever I am currently I'll move by this change right so Theta Nu is equal to theta plus Delta Theta.\nokay so what is happening here is that I was here and I mean sorry this this is where I.\nwas and I then added this vector and what has happened effectively that I moved in the direction of Delta.\nTheta right I've come closer to Delta Theta and we know this from the perceptron Rolling algorithm also when we.\nadd it it comes closer to it right so now instead of uh moving fully so let me just delete.\nthis right so instead of moving fully uh in the direction of Delta Theta what I'm going to do is.\nI'm going to be a bit conservative and whatever change you tell me to do I'm not going to do.\nthat full change I'm just going to multiply that change by a very small value which is ETA so essentially.\nwhat that does is the following right so this is my Delta Theta Vector the black one I am going.", "metadata": {"video_title": "Learning Parameters Taylor series approximation"}}
{"text": "what that does is the following right so this is my Delta Theta Vector the black one I am going.\nto multiply by a small scalar so essentially the magnitude of the vector will decrease the direction does not change.\nso I'm still going to move in that direction but I'm just going to be a bit conservative and take.\na small step right one small step at a time right so that gives me my update rule which is.\nif you tell me what is Delta Theta then I'm going to move by a small value in that direction.\nright that's that's what my update rule is going to be now the question of course is what is the.\nright Delta Theta to use right I was earlier what was my algorithm in the random guess I was just.\ncoming up with some Delta Theta just moving there oh and then realizing oh the error has decreased so let.\nme just go back and then try to find some new Delta Theta change that and I mean change that.\nmuch and then find a new value I don't want to do that someone has to give me the right.", "metadata": {"video_title": "Learning Parameters Taylor series approximation"}}
{"text": "me just go back and then try to find some new Delta Theta change that and I mean change that.\nmuch and then find a new value I don't want to do that someone has to give me the right.\nDelta Data to use and it turns out that what is the right Delta Theta to use the answer from.\nthis comes from the Taylor series right so let's see first what is Taylor series I'm sure some of you.\nhave seen this in high school well let's quickly refresh this right so Taylor series as I'm just going to.\nread out the definition here is a way of approximating any continuously differentiable function right and this is important for.\nus because whatever loss function we are dealing with is a differentiable function because we have the sigmoid neuron which.\nis differentiable and then on top of that a loss function currently is the squared error loss which is again.\ndifferentiable right so this is a differentiable function that we are dealing with and this differentiable function may have any.", "metadata": {"video_title": "Learning Parameters Taylor series approximation"}}
{"text": "differentiable right so this is a differentiable function that we are dealing with and this differentiable function may have any.\nshape right it could have a shape like this it could have any shape it just smooth and differentiable right.\nand what Taylor series does is it gives us a way of approximating this using polynomials of degree n right.\nand the higher the degree the better the approximation now this part may not be clear right what do I.\nmean by approximate using polynomials of degree N I should have circled n also that part is not clear and.\nwe'll fix that right let's try to understand what we mean by that okay so I'll delete some of of.\nthis okay so here's a function right so let's assume this is my w I have a single parameter for.\na use of explanation and this is my loss function right this is L of w suppose okay and what.\nit's saying is that as W increases L seems to increase right so that's that's okay whatever this function is.", "metadata": {"video_title": "Learning Parameters Taylor series approximation"}}
{"text": "it's saying is that as W increases L seems to increase right so that's that's okay whatever this function is.\nI don't care about it but this is how the function looks like okay now Suppose there is a point.\nuh W naught okay at which I know the loss okay and where is this coming from how does this.\nrelate to our original R uh uh our goal right which is to find uh a new value of uh.\nW says that the loss is less right so that will soon get related but for now I'll give you.\nsome intuition right so Suppose there was only one parameter and I timed step 0 I started with the value.\nW naught and I could compute the loss I don't know the full loss surface but I could compute the.\nloss right now I'm looking to change this value W naught right and I want to decide whether to go.\nhere or here so that my loss decreases that's what the goal is and we'll relate all of this discussion.\nto that goal uh soon right in the figure it's clear that I should move to the left because there.", "metadata": {"video_title": "Learning Parameters Taylor series approximation"}}
{"text": "to that goal uh soon right in the figure it's clear that I should move to the left because there.\nis where the loss is decreasing right but that's that's what we are trying to do so now if I.\nknow this W naught what the Taylor series does is that if I know a point w okay which is.\nW naught plus sum Delta okay if that is the point right so then Delta is equal to W minus.\nW naught right and you see that W minus W naught quantity everywhere here right so if I know the.\nvalue of the loss at a certain point then I can approximate the value of the function in a neighborhood.\naround that what do I mean by that this w if this Delta is very solid then W lies in.\nthe neighborhood of w naught right so in that neighborhood I can approximate it by using this formula right and.\nthe interesting thing about this formula is that it has this uh first order term it has a second order.\nterm third order term and so on and I can cut off the formula wherever I want if I cut.", "metadata": {"video_title": "Learning Parameters Taylor series approximation"}}
{"text": "the interesting thing about this formula is that it has this uh first order term it has a second order.\nterm third order term and so on and I can cut off the formula wherever I want if I cut.\noff the formula here I am doing a second order approximation if I cut off the formula here I'm doing.\na first order or a linear approximation so let's see what happens when I do a linear approximation okay so.\nagain uh delete stuff on the slide so this is what the formula looks like I've just cut off the.\nformula at uh the first order terms right so I just have this much okay and now I want to.\nfind the value of w and I'm saying that I already know this right I already know what LW naught.\nis right that is given to me now I'm saying that LW is just going to be this oh sorry.\nLW is just going to be this which is whatever was my current value plus some quantity okay and what.\nis this quantity this is L Dash W naught L Dash W naught is nothing but the first order derivative.", "metadata": {"video_title": "Learning Parameters Taylor series approximation"}}
{"text": "is this quantity this is L Dash W naught L Dash W naught is nothing but the first order derivative.\nof the loss function W minus W naught is actually the Delta between that right now what is happening here.\nall this looks very confusing to me so let's just try to draw a line here so what am I.\ndoing here right I am the actual function has a curve right it's some it's definitely not a linear function.\nright but I'm approximating that function by a linear function I am saying that in this small neighborhood here right.\nsay from this is the small neighborhood in this small neighborhood I'm going to assume that the function is linear.\nright now if the function is linear that means I have assumed that the function is of the form Y.\nis equal to MX plus C okay and I already know y naught which is MX naught plus C right.\nI already know that right and what is uh m m is simply the slope and the slope is just.", "metadata": {"video_title": "Learning Parameters Taylor series approximation"}}
{"text": "is equal to MX plus C okay and I already know y naught which is MX naught plus C right.\nI already know that right and what is uh m m is simply the slope and the slope is just.\nthe derivative right so this is L Dash W right so this is what uh the slope is right now.\nI want to find the value of y1 right and I know that say uh y1 lets you see right.\nso y1 would be M into x 1 plus C right now if I subtract these two If I subtract.\nthe SEC first equation from the second equation I get y1 minus y naught okay is equal to the C's.\nwill cancel and you will have M into X1 minus X naught okay and if I rewrite this I'll have.\ny1 is equal to Y naught okay plus M into x 1 minus X naught right and that is exactly.\nwhat this formula is saying right so y naught is nothing but l w naught m is the slope and.\nW minus W naught is the same as x 1 minus X naught right so that's what it's saying that.\nnow you can find y1 by assuming that this is a linear function okay and then you know the slope.", "metadata": {"video_title": "Learning Parameters Taylor series approximation"}}
{"text": "W minus W naught is the same as x 1 minus X naught right so that's what it's saying that.\nnow you can find y1 by assuming that this is a linear function okay and then you know the slope.\nbecause you can compute the derivative at that point and now I can get a new value for y I.\ncan get the value for y1 where y1 is the value of the function at the point X1 and there.\nis a small Delta between X1 minus X naught that means x 1 lies in the neighborhood of X naught.\nokay and it will soon become clear why this is important okay so everything that needs to be explained has.\nnot been explained on the slide yet I still have some visuals on the next two slides which will make.\nthings more clear right but let's just go ahead with what we have now right let's just try to understand.\nwhat it means to have a linear approximation you're just drawing a line at that point right and you're saying.\nthat if I just move along this line right so instead of moving along this curve which is what you.", "metadata": {"video_title": "Learning Parameters Taylor series approximation"}}
{"text": "that if I just move along this line right so instead of moving along this curve which is what you.\nshould have done you are moving along that line and whatever value you get on that line that is the.\nvalue you are assuming for y1 right so that's an approximation and you can already see right that this value.\nhere let me just mark it with a different color right so this blue value here is what you would.\nget right that's the point on the line it's slightly above the red curve right so it's an approximation you.\nwill get some error there because you have done a linear approximation right now let me just get rid of.\nall this content and now let's see if I had done a quadratic approximation that means if I had cut.\nthe formula here right so what happens in that case this is what the formula looks like right and just.\nas the first one was of the form Y is equal to MX plus C right and here you have.\na quadratic term so this is nothing but Y is equal to ax Square plus BX plus C right so.", "metadata": {"video_title": "Learning Parameters Taylor series approximation"}}
{"text": "as the first one was of the form Y is equal to MX plus C right and here you have.\na quadratic term so this is nothing but Y is equal to ax Square plus BX plus C right so.\nnow I'm approximating the function as a quadratic function right and these coefficients are coming from the derivatives as was.\nthe case in the first case the first first order derivative the second order derivative right so what does that.\nmean it means that now I'm approximating the red curve by this uh c n or blue colored curve right.\nand you can see that in the small neighborhood now right this neighborhood now the second order approximation is actually.\nbetter than the first other approximation right it's more closer the blue curve in that small neighborhood is more closer.\nto the red curve than the black curve is to the red curve right so this is you can just.\nkeep cutting the formula wherever you want and that's what the approximation gives how this relates to what we are.", "metadata": {"video_title": "Learning Parameters Taylor series approximation"}}
{"text": "keep cutting the formula wherever you want and that's what the approximation gives how this relates to what we are.\ntrying to do all that is not clear right now right now we just want to understand what Taylor series.\nis Taylor series just gives a way of approximating uh the function and you can use any order of approximation.\nn equal to 1 n equal to n equal to three the higher the order the better the approximation okay.\nso now let's uh kind of move ahead from here and see way to go yeah so now here's the.\nsame plot right and now I'll do some uh stuff here so this Epsilon actually defines the neighborhood oops right.\nso the smaller the value of Epsilon you can see that smaller is the neighborhood that I am considering here.\nokay and I'll tell you why that is important okay uh now uh this function is log of w okay.\nand now I know the value of the function at this point here right this point here I am going.", "metadata": {"video_title": "Learning Parameters Taylor series approximation"}}
{"text": "okay and I'll tell you why that is important okay uh now uh this function is log of w okay.\nand now I know the value of the function at this point here right this point here I am going.\nto approximate its value in a small neighborhood around it right and the value would be whatever is on the.\nOrange Line because that's the linear approximation I have made that's the line that I have drawn and I'm trying.\nto approximate the blue curve by this line now when the Epsilon is small that means when I am looking.\nat a very small neighborhood this Orange Line acts as a good approximation of the blue curve right in that.\nsmall neighborhood that you see between these two points here that is the orange line is very close to the.\nBlue Line but as the neighborhood increases if you come go very far from the point that you were and.\nnow you're going to try it so this is your X naught right this this point here is your X.", "metadata": {"video_title": "Learning Parameters Taylor series approximation"}}
{"text": "Blue Line but as the neighborhood increases if you come go very far from the point that you were and.\nnow you're going to try it so this is your X naught right this this point here is your X.\nnaught right and let me consider some x one here right so now what is sorry this should have been.\nX1 so now what it is telling me is that if I try to compute y1 is equal to Y.\nnaught right so this point here on the line is y naught okay if I try to compute y1 is.\nequal to Y naught plus the formula that I had at M into X1 minus X naught right then the.\nanswer that I'll get is this and you can see that it's way off right it's not not really a.\ngood approximation even if uh my X1 I was here right and then what I'll get if I substitute in.\nthis formula I'll get the y1 as this point and you can see that it's actually very far off from.\nthe true value of y1 right so hence this as the neighborhood goes this approximation becomes bad so that is.", "metadata": {"video_title": "Learning Parameters Taylor series approximation"}}
{"text": "the true value of y1 right so hence this as the neighborhood goes this approximation becomes bad so that is.\na key thing that you can do this only in a small neighborhood that means the difference between X1 and.\nX naught should be very small that is one okay now let's see something else also the other thing is.\nthat as I increase uh W naught right I mean I'm just changing the value of w naught and you.\ncan see that the orange curve is a very good approximation let me just increase the Delta but let me.\nreset this yeah uh now I'm just going to increase uh the neighborhood right okay now even with this increase.\nin neighborhood the orange line still seems to be a very good approximation of the blue curve and why is.\nthat happening because the slope in that region right is quite less so my Delta or my M right it's.\nvery less and that's helping right but now if I change the value of w a bit right if I.", "metadata": {"video_title": "Learning Parameters Taylor series approximation"}}
{"text": "that happening because the slope in that region right is quite less so my Delta or my M right it's.\nvery less and that's helping right but now if I change the value of w a bit right if I.\ngo here now here my slope is very high as you can see right the gradient is very high and.\nnow in that case my approximation is becoming very bad right so this is what the same window I have.\nnot changed the window right but uh now if you can see this is the approximation and this is the.\ntrue value and the Gap is very high right whereas with the same window if I'm in a region where.\nthe slope is small right then still the approximation is quite good you can see that the points on the.\nOrange Line are very close to the points on the blue line because this is a region where the slope.\nis very low right whereas this was the reason where the slope was very high right that's just some observation.", "metadata": {"video_title": "Learning Parameters Taylor series approximation"}}
{"text": "is very low right whereas this was the reason where the slope was very high right that's just some observation.\nso now you can see what is actually happening in the Taylor series approximation wherever you are you are drawing.\na tangent and you are saying that this line given by the tangent is my approximation for the original function.\nand in small windows that approximation works very well right even when the gradient is large right so if my.\nwindow is small oops then even as my gradient changes right my approximation is still quite good right so in.\nthe small neighborhoods my approximation always remains quite good even for high values of the gradient but if my uh.\nmy window is large then the approximation becomes quite bad as you can see here right so that's important that.\nTaylor series is only used in small uh Windows okay now that was one now let me just reset this.\nokay and I change the function okay I want to make it the sine function okay and now this is.", "metadata": {"video_title": "Learning Parameters Taylor series approximation"}}
{"text": "Taylor series is only used in small uh Windows okay now that was one now let me just reset this.\nokay and I change the function okay I want to make it the sine function okay and now this is.\nwhat my approximation looks like now I'm going to increase the degree so this is a linear approximation right now.\nright because I've just cut off the formula at the first order term now this is a quadratic approximation you.\ncan see that it looks better right the orange curve is closer to the blue curve right and now what.\nI'm trying to prove actually is the uh utility of the Taylor series or uh how good the approximations get.\nas you keep increasing the degree so this is a degree to approximation where I cut the formula at the.\nsecond order terms if I cut the formula the third order terms this is what I get right and now.\nyou can see that in a very larger neighborhood also right the approximation is quite good right I mean the.", "metadata": {"video_title": "Learning Parameters Taylor series approximation"}}
{"text": "you can see that in a very larger neighborhood also right the approximation is quite good right I mean the.\nthe true curve is very close uh to the orange curve right let's increase it further further further I think.\nI'm at degree 7 or degree 8 now and it's almost perfectly approximating right so as I keep increasing the.\ndegree the approximation becomes better than that and if I go till the very large terms then I'm all I'm.\nconstructing the entire function in by as a sum of its parts right and that's what the Taylor series actually.\nsays that if I use the full formula then you get the function back right that's the beauty of the.\nTaylor series approximation if I keep decreasing then my approximations become bad and in the linear case my approximations are.\nbad but they're very good in a small neighborhood around the point right that's the main in the small neighborhood.", "metadata": {"video_title": "Learning Parameters Taylor series approximation"}}
{"text": "bad but they're very good in a small neighborhood around the point right that's the main in the small neighborhood.\nyeah my approximations are quite good right so that's the main thing you need to understand about the Taylor series.\nokay so now let's go ahead uh and see what happens if you have a 2d function right so I.\nhad earlier now I was just dealing with L of uh w now what if I have L of w.\ncomma B right and in this case instead of w comma B I just have X comma y right for.\nconvenience uh those are the two axes that I have taken okay but I just the main point is if.\nyou want to move to two variables right so now yeah so this is what the curve looks like okay.\nand let me just delete my annotations yeah so this is what the curve looks like now again if I.\ndo a linear approximation as you can see here I'm doing a linear approximation then what I'll get is a.\nline in 2D it would just be a plane right so I've just approximated the function by a plane and.", "metadata": {"video_title": "Learning Parameters Taylor series approximation"}}
{"text": "line in 2D it would just be a plane right so I've just approximated the function by a plane and.\nyou can see that in this small neighborhood my approximation is still good right let me just get the right.\nangle so that you can see it properly yeah see now I can see that the my plane is very.\nclose uh to the curve but now if I increase this ETA then as was the case earlier my approximation.\nbecomes bad right so at this point my approximation of my original point w naught was here then if my.\nnew point is very far out of the neighborhood then the approximation would be bad but even in a small.\nneighborhood the linear approximation Works quite well right and again I could just change the value of x and if.\nI go to a region where the slope is changing then the approximation accordingly will become good or bad but.\nthat doesn't matter if you keep the ETA small then your approximations would be good right and now if I.", "metadata": {"video_title": "Learning Parameters Taylor series approximation"}}
{"text": "that doesn't matter if you keep the ETA small then your approximations would be good right and now if I.\nmake a quadratic approximation right I'll just remove the linear one and just keep the quadratic approximation uh I think.\nyou're not able to see it yeah so here's the quadratic approximation right and now even if I increase the.\nW the window a bit you can see that my approximation is still quite good right so that's the idea.\nhere in fact the original function itself is uh quadratic in various parts right so that's why my approximation is.\nquite good right so that's uh that's that's the idea here so the original function is of course 0.5 sine.\nX Plus 0.25 sine y but what I meant it in some parts it's Contra I mean it's looking like.\nquadratic function right so if you do a quadratic approximation it still would be very uh good right so you.\ncan do this whether you have a one variable function or a two variable function of course three variable functions.", "metadata": {"video_title": "Learning Parameters Taylor series approximation"}}
{"text": "can do this whether you have a one variable function or a two variable function of course three variable functions.\nI cannot I'm just trying to get the angle right yeah this is what it is and you can see.\nthat the approximation is good so this is what the Taylor series does and you can do this approximation in.\nmultiple Dimensions also right so it need not be a function of a single variable it can also be a.\nfunction of multiple variables okay um second take a pause here I'll let you reflect on this you could also.\ngo back to the lecture slides right the slides are also available on my web page and you can just.\ngo and try to play around with this digest this idea and then come back and watch the next video.\nokay I'll just take a pause here thank you.", "metadata": {"video_title": "Learning Parameters Taylor series approximation"}}
{"text": "foreign [Music] schedulers and so this is the chart for learning rate schemes right so you have what we saw.\nearlier was based on epochs where we had step Decay so after one Epoch if something goes wrong then just.\ndecade or for every Epoch just exponentially keep decaying the learning rate uh then we have based on validation where.\nwe had line search right so we were doing trying multiple learning rates and then based on which one gives.\nthe smallest loss we were trying to use that as the learning rate it could also be a log search.\nsimilar to a line search where you would search for learning rates on a log scale and then based on.\ngradients right these were the Adaptive learning rates that we had so we had adagrad RMS Prof ATA Delta Adam.\natomax we did an Adam also uh we did not do AMS grad and we did not do Adam W.\nuh so these are all the things that we have seen so far but there are a few more learning.", "metadata": {"video_title": "Learning rate schemes"}}
{"text": "atomax we did an Adam also uh we did not do AMS grad and we did not do Adam W.\nuh so these are all the things that we have seen so far but there are a few more learning.\nrate schedules that we'll see now which are based on Epoch so one is cyclical epochs or the number of.\niterations right the other is cosine annealing bomb restart so these three are what we're going to see now right.\nuh so we'll first talk about this cyclical learning rate so uh suppose the loss surface looks like this and.\nfor some of you have exposure to uh optimization uh you would know that this loss function is lost surface.\nis what is known as a saddle so what happens in a saddle uh shape loss surface that if I.\nlook it from here right then this looks like a Minima right this point here looks like a Minima because.\nit's at the end of the valley right but that's clearly not where I want the algorithm to stop because.\nif I look at it this way then I would could have gone down from here I could have gone.", "metadata": {"video_title": "Learning rate schemes"}}
{"text": "it's at the end of the valley right but that's clearly not where I want the algorithm to stop because.\nif I look at it this way then I would could have gone down from here I could have gone.\nfurther down from here right my Minima would have been somewhere down right but it's because of this saddle shape.\nwhere in One Direction you are seeing that okay I've reached the Minima but there is another Direction across which.\nyou could have found a better Minima right and many times because the Deep learning uh the loss surfaces that.\nyou typically encounter in deep learning are not convex loss functions it has been observed and this has been reported.\nin multiple papers that often when your training is getting stuck or it's low or something it's because you're stuck.\nin some saddle point right where there is a Minima down the valley but since you are already in this.\nValley here you're not able to go further down right so suppose we had we initialize the weights to this.", "metadata": {"video_title": "Learning rate schemes"}}
{"text": "Valley here you're not able to go further down right so suppose we had we initialize the weights to this.\nyellow point right this is where you had initialized now you're coming down nicely and you'll get stuck in that.\nValley you'll get stuck somewhere at the right point that I have marked where my mouse is currently and you.\nwill not be able to go down right because your learning rate is decree amazing exponentially over iterations now uh.\nnow this since the learning rate has decreased so you're you're going down and you are going on a fast.\nloop and in most algorithms the learning rate will start decaying and by the time you reach the valley the.\nlearning rate would have become very small and now you won't be able to come out of that you'll be.\nstuck there right so now what if we allow the learning rate to increase after some iterations right at least.\nthere's a chance that it might escape the saddle point right and that's what the idea is that you try.", "metadata": {"video_title": "Learning rate schemes"}}
{"text": "there's a chance that it might escape the saddle point right and that's what the idea is that you try.\nto have this adaptive learning rate which will change but that so that is often expensive right because you're doing.\nthese computations WT the history all of that you are maintaining then you're doing ETA T ETA are divided by.\nthis history so you're creating a lot of keeping a lot of variables and making things a bit complex right.\nand uh this often happens right that the minimizing the loss arises because of the saddle points so therefore it.\nis beneficial to have these learning rate schemes which could increase the learning rate near the saddle point right and.\nadaptive learning rate schemes that we have seen are actually trying to do that right they are trying to do.\nthat but as I said it comes with an additional computational cost instead what if we'd have a simple uh.", "metadata": {"video_title": "Learning rate schemes"}}
{"text": "that but as I said it comes with an additional computational cost instead what if we'd have a simple uh.\nlearning rate scheme where we alternatively vary the learning right so we start with small go to high again make.\nit small again go high again make it small just alternately slightly keep doing it so now we are not.\nlooking at history or anything right we are gradually increasing it then again decreasing so if it's in a high.\nregion it just has to wait out that cycle and then again the learning rate will start decreasing right if.\nit's in a low region it just has to wait out the cycle and the learning rate will increase and.\nthen push it out again right this is a very simple schedule that you could use and this is one.\nsuch cyclic learning rate which is the Triangular schedule that you could use and this here mu which is the.\nuh period right so you could mu is 20 here so that that is the number of time steps it.", "metadata": {"video_title": "Learning rate schemes"}}
{"text": "such cyclic learning rate which is the Triangular schedule that you could use and this here mu which is the.\nuh period right so you could mu is 20 here so that that is the number of time steps it.\ntakes from going to the minimum value to the maximum value and once it reaches the maximum value it again.\ngoes down right so on the x axis you have the learning rate and this is the maximum value which.\nis in this case is 0.5 the minimum value is 0.01 and the value at any time step can be.\ngiven by this uh complex equation but it just looks complex if you break it down it's very simple so.\nlet's look at it right so if T is equal to 20 I'll just substitute the value t equal to.\n20 in this equation let me just get rid of the annotations yeah so let me just substitute the value.\nt equal to 20. now uh I'm going to substitute it here first right so when I'm Computing this quantity.\nit's turning out to be T right so everything inside the bracket is T oops and now I'm looking at.", "metadata": {"video_title": "Learning rate schemes"}}
{"text": "it's turning out to be T right so everything inside the bracket is T oops and now I'm looking at.\nthis quantity which I had already computed everything inside the bracket which came out to be two I'm in fact.\nlooking at this quantity right so then this plus 1 from here and this T by mu which is 20.\nby 20. so that quantity comes out to 0 in this case then I'm going to look at Max of.\n0 so now I'm looking at this quantity which is Max of 0 comma what I had computed earlier so.\nthis is what I had computed earlier so that is going to be 1 right so then I have ETA.\nmin Plus ETA Max minus ETA min and this entire thing has evaluated to 1 so this is going to.\nbe 1 right so then it's going to be 0.01 plus 0.5 minus 0.01 which is just going to be.\n0.5 right and that's exactly what I get at time step t equal to 20. similarly you could substitute the.\nvalue 30 here and you will get uh this as the value right somewhere here as the value right so.", "metadata": {"video_title": "Learning rate schemes"}}
{"text": "0.5 right and that's exactly what I get at time step t equal to 20. similarly you could substitute the.\nvalue 30 here and you will get uh this as the value right somewhere here as the value right so.\nthat's that's how you compute this cyclically running rate the increase keep increasing the again decrease keep increasing then again.\ndecrease right so this is uh it was almost like having like a very uh simplistic idea right you are.\njust saying that at some points I want the learning rate to increase at some points I want the learning.\nrate to decrease and to make this happen you we had all these complex algorithms which were adapting the learning.\nAid according to when it should increase when it decreased now this is like a very uh Baseline idea right.\nyou wanted to increase you wanted to decrease just make it cyclically and if you're in a bad part you.\njust have to wait out that cycle and then again you might if you want a smaller learning rate it.", "metadata": {"video_title": "Learning rate schemes"}}
{"text": "just have to wait out that cycle and then again you might if you want a smaller learning rate it.\nis going to come in sometime steps if you want a larger learning rate it's going to come into some.\ntime steps you just have to wait out that cycle right so that's that's the simple idea that you have.\nhere okay uh so now let's see whether this actually solves the problem that we started off right so we.\nhave this saddle point and we started with that initialization and looks like we will get stuck in the valley.\nright so I have this exponentially decaying learning rate which is the black guy and that comes into the valley.\nand gets stuck there but this blue guy has a chance because the exponentially decayed learning rate has become very.\nsmall but now the blue guy because it can increase the learning rate at some point so even though the.\ngradients there are small when the learning it just waits till the learning rate increases and then it gets per.", "metadata": {"video_title": "Learning rate schemes"}}
{"text": "gradients there are small when the learning it just waits till the learning rate increases and then it gets per.\npush and then it comes down into the valley right so with this cyclic learning rate you are able to.\nget out of this saddle point and saddle points are a big pain as reported in multiple papers when you're.\ndealing with deep neural networks right so this kind of takes care of such bad uh learning rate studies right.\num yeah so now again we are going to use another cyclic learning rate which is the cosine learning rate.\nand uh here instead of having a fixed learning rate which is one we are going to use this cyclic.\none and you can see that this algorithm has converged much faster right because it still has some oscillations here.\nbut you could just take care of that by using some kind of an early stopping or something right so.\nnow let's see what this uh cosine early annealing look likes and I just see that so this is what.", "metadata": {"video_title": "Learning rate schemes"}}
{"text": "but you could just take care of that by using some kind of an early stopping or something right so.\nnow let's see what this uh cosine early annealing look likes and I just see that so this is what.\ncosine annealing look like it has a different formula again this is just based on a cosine function so if.\nyou substitute the time step T the value of T here right and your capital T would just be the.\nperiod of the uh uh uh of the wave that you have uh and if you substitute these values you.\nwill get the effective learning rate right and it's better to just show this as the graph and that you.\nyou'll understand what is happening here so ETA Max is the maximum value for the learning rate ETA mean is.\nthe minimum value T is the current Epoch and capital t is the restart interval which is until what time.\nyou increase and then again restart right so that's what capital t is and so let's see what happens so.", "metadata": {"video_title": "Learning rate schemes"}}
{"text": "you increase and then again restart right so that's what capital t is and so let's see what happens so.\nthis is what it looks like right so it you have is you start with a high value and then.\nit keeps decreasing and when it reaches this interval this interval here is 50. so when it reaches the 50.\nand you can substitute these values that you can substitute 50 here and you substitute 50 and substitute here so.\nyou if you compute these values right so T mod 50 plus 1 would be T mod sorry 50 mod.\n51 which is going to be uh 50 and then divided by 50 so this quantity will just become 1.\nso we'll have COS of Pi which is going to be a minus 1 or sorry cos of Pi is.\ngoing to be uh minus 1 and then you have 1 minus 1 so this will become 0 so we'll.\njust have the minimum value here right and that's exactly what is happening here right okay and this is called.\na warm restart because from uh once you come to the minimum point you quickly jump to the maximum Point.", "metadata": {"video_title": "Learning rate schemes"}}
{"text": "a warm restart because from uh once you come to the minimum point you quickly jump to the maximum Point.\nagain as opposed to the Triangular learning rate where you are gradually increasing gradually decreasing here from the start from.\na high point come down and then quickly go to the maximum value and then again come down so this.\nis called a warm restart because when you're restarting you're not like gradually restarting but you are starting from a.\nwarm point which is a high learning rate and then coming down from there right so this uh warm uh.\nstart warm restart is a popular term that you'll hear in many of the modern papers right so this is.\nin Max and Min t uh and the update change after T which is why it is called the bomb.\nresta right and there's also warm start which is quite popular in the uh uh Transformer literature and most of.\nthe current literature is on Transformers I mean they're popularly used in many applications so what happens is that in.", "metadata": {"video_title": "Learning rate schemes"}}
{"text": "the current literature is on Transformers I mean they're popularly used in many applications so what happens is that in.\nwarm start you initially let the learning rate increase right in all the other algorithms you start whatever adaptive algorithms.\nwe had seen in you start with some learning rate and then it keeps decaying right and even the earlier.\nlearning rate schedules that we had said step DK exponential decay you start with some value and there's always a.\nDK there right incident warm start what you do is you let the learning rate gradually increase to the maximum.\npoint and from that point onwards you start the Decay right so initially when you don't know much right the.\nparameters are all randomly initialized you're not sure of your gradients then slowly slowly have the learning rate very small.\nso that you are not making very large updates and then as you gain confidence you keep increasing the learning.", "metadata": {"video_title": "Learning rate schemes"}}
{"text": "so that you are not making very large updates and then as you gain confidence you keep increasing the learning.\nrate to a high value and then once you reach a high value from there on you start decaying at.\nexplanation right so this is the default learning rate schedule used in uh a Transformer based architectures and this is.\nwhat uh it looks like right so as you keep and if you use different so the warm-up step is.\nthe number of steps for which you will let the learning rate to climb so in one case I have.\nthe warm-up step as uh 2000 and in the other case I have the warm-up step is four thousand so.\nthe 2000 curve of course Rises rapidly and then Falls and the 4000 step Rises more uh smoothly and then.\nit uh Falls right so starts ticking exponentially from there okay so that's that's all we had we covered the.\ndifferent learning rate schedules that we had uh and this you could use in conjunction with the optimization algorithms that.", "metadata": {"video_title": "Learning rate schemes"}}
{"text": "different learning rate schedules that we had uh and this you could use in conjunction with the optimization algorithms that.\nwe have used also right so both can be uh done so that's all for this lecture and we'll see.\nyou next week again thank you.", "metadata": {"video_title": "Learning rate schemes"}}
{"text": "[Music] so in this module now we'll focus a bit more on this uh boolean functions and when are they.\nlinearly separable and not linear separate the reason that we want to delve into this is that we just saw.\nthat the perceptron model or the even the perceptron learning algorithm only has guarantees when the data is linearly separable.\nso now what are these linearly separable boolean functions or rather what are boolean functions which are not linearly separable.\nso that's something that we want to understand right so let us see one such simple example and this is.\na famous xor function that all of you know about so this is what the xor function looks like and.\nnow let's see what happens right so if i want to implement this using a perceptron then these are the.\nfour inequalities that my weights should satisfy right so for 0 0 inputs when i plug in this formula the.\noutput should be less than 0 for 0 1 0 it should be greater than equal to 0 for 0.", "metadata": {"video_title": "Linearly separable functions"}}
{"text": "four inequalities that my weights should satisfy right so for 0 0 inputs when i plug in this formula the.\noutput should be less than 0 for 0 1 0 it should be greater than equal to 0 for 0.\n1 greater than equal to 0 and for 1 1 again less than 0 right so let's just expand this.\nso this is what the first condition says right i'll just expand the first one and not do it for.\nthe others w naught plus w 1 into 0 because x 1 is 0 plus w two into zero because.\nx two is zero should be less than zero and that leaves us with the condition that w naught should.\nbe less than zero similarly i'll expand the other condition and leaves me with the condition that w two should.\nbe greater than equal to minus w naught w one should be greater than equal to minus w naught and.\nthen the last condition is w one plus w two should be less than minus w naught right and you.\ncan see that you cannot satisfy these four inequalities right there is one quantity which is greater than minus w.", "metadata": {"video_title": "Linearly separable functions"}}
{"text": "can see that you cannot satisfy these four inequalities right there is one quantity which is greater than minus w.\nnaught another quantity which is greater than minus w two naught and i have added these two quantities and i.\nwant it to be less than w naught that cannot happen right so if i just add the two eq.\ninequalities i will get w one plus w two is greater than equal to minus w naught right so i.\ncannot simultaneously satisfy all these four inequalities right so that means i cannot find a set of w i's such.\nthat all my positive points will be on one side and my negative points will be on the other side.\nthat means this is not a linearly separable function right okay and we can see this from the diagram right.\nso here is these four points right so you have this two negative points sitting at 0 1 and 1.\n0 and 2 positive points sitting at 0 0 and 1 1 right and now you can go home and.\npractice this you can try to adjust these weights w1 w2 as much as you want right but you will.", "metadata": {"video_title": "Linearly separable functions"}}
{"text": "0 and 2 positive points sitting at 0 0 and 1 1 right and now you can go home and.\npractice this you can try to adjust these weights w1 w2 as much as you want right but you will.\nnot be able to draw a line such that the two positive points are on one side and so many.\nof you will guess from the figure it's not possible that you cannot have a line such that only the.\npositive points are on one side and only the negative points are on the other side you cannot draw such.\na line so it's clear from the figure it's actually clear from the inequalities the set of inequalities that you.\nsee there right so this is not going to be possible so this is an example of a function a.\nsimple boolean function which is not linearly separable right now the reason we're talking about uh not linearly separable functions.\nbecause most real world data is going to be not linearly separable right so here suppose you have two classes.", "metadata": {"video_title": "Linearly separable functions"}}
{"text": "because most real world data is going to be not linearly separable right so here suppose you have two classes.\nof people right people who like machine learning and people who don't like machine learning right now it's going to.\nbe the case that there are some people here maybe the blue guys here right okay which satisfy all the.\ncharacteristics of what the guys who like machine learning satisfy but they still don't like it right and same way.\nthe other way around also right so you could think of it a more easier example would be that if.\nyou have a certain locality right and you might assume that all people on that locality speak a particular language.\nright but there might be a few people there who don't speak that language or speak another language right and.\nso there would always be this outliers right so its data will never be linearly separable right and it need.\nnot just be outliers you could have other things also now this is a very good example there is no.", "metadata": {"video_title": "Linearly separable functions"}}
{"text": "not just be outliers you could have other things also now this is a very good example there is no.\noutlier here the people in the inner circle maybe behave a certain way people in the outer circle behave a.\ncertain way there's no outlier here but i can't draw a linear decision boundary to separate the yellow points from.\nthe purple points right so this is very much common in many real world situations right so this constraint that.\ni can only deal with data which is linearly separable is something which is not acceptable in the long run.\nright so we'll have to move beyond this perceptron model because the perceptron model can clearly handle only linearly separable.\ndata right okay now this can handle is again something which is to be defined what do i mean by.\ncan handle we'll come back to this in a later lecture but for now i guess you have a rough.\nidea of what i mean right it cannot find a line which separates the positive points from the negative points.", "metadata": {"video_title": "Linearly separable functions"}}
{"text": "idea of what i mean right it cannot find a line which separates the positive points from the negative points.\nokay so now with a single perceptron we cannot do this right now what i am going to show is.\nthat with a network of perceptrons you can do that with the network of perceptrons whatever a network means i.\nhave not defined what a network means we will define that with that you can separate positive points from negative.\npoints even if the data is not linearly separable to begin with right but before we see that so you.\nstart with this question right so how many boolean functions can you design from two inputs so let me start.\nwith some easy ones and then we'll try to calculate the number of functions and i once i show you.\na few patterns you should be able to guess the number of functions so these are two inputs given to.\nthe u and here's one function that you know which is the always off function similarly the always on function.", "metadata": {"video_title": "Linearly separable functions"}}
{"text": "the u and here's one function that you know which is the always off function similarly the always on function.\nand you can see that from the numbering that they're going to be 16 such functions possible right and it.\nsimply follows that you have these four outputs right and each output could take 0 or 1 value so you'll.\nhave 2 raised to 4 which is 16 which is actually 2 raised to 2 uh raised to 2 right.\nso this is the or function and this is the and function and then similarly you can construct other functions.\nright so in uh of these how many are linearly separable so you can go and work this out on.\nyour own and turns out that all except xor and the not xor function are linearly separable you can go.\nback and verify this so it's out of 16 there are two functions which are not linearly separable so in.\ngeneral how many boolean functions can you have from n inputs it is 2 raised to 2 raised to n.\nright so that's what happened here it was n was 2 so you had 2 raised to 2 raised to.", "metadata": {"video_title": "Linearly separable functions"}}
{"text": "general how many boolean functions can you have from n inputs it is 2 raised to 2 raised to n.\nright so that's what happened here it was n was 2 so you had 2 raised to 2 raised to.\n2 functions you can go back and convince yourself that if n is equal i mean if you have a.\ngeneral n number of inputs then the number of boolean functions would be 2 raised to 2 raised to m.\nokay now of these how many are not linearly separable so when you had 16 functions two are not linearly.\nseparable so if you have two raised to two raised to n functions how many would be not linearly separable.\nso many of you would try to say that the answer is n that's not the correct answer in fact.\nthis is an unsolved problem so you can go back and try looking for an answer for this right but.\nthat's that's not the point the point is that there are going to be some functions which are not linearly.\nseparable right and if that is the case with the simple boolean functions and i said in real world we.", "metadata": {"video_title": "Linearly separable functions"}}
{"text": "separable right and if that is the case with the simple boolean functions and i said in real world we.\nhave more complex cases where the data will not be linearly separable right so we need to have a way.\nof dealing with not linearly separable functions we need to have a way of dealing with them in the generic.\ncase when your inputs and outputs are not boolean at all but for now in this lecture at least try.\nto see if you can have a solution for the case where your inputs and outputs are a boolean so.\nthat's what we'll do for now okay so i'll end this module here and in the next module we'll try.\nto come up with a network of perceptrons which can handle boolean functions which are not linearly separable even though.\nwe know that a single perceptron cannot handle boolean functions which are not linearly.", "metadata": {"video_title": "Linearly separable functions"}}
{"text": "[Music] okay so now continuing on in our journey right so we have seen quite a few things uh we.\nhave seen about the revival we have seen about more advances like optimization activation functions etc which made this revival.\npop possible and then generated in interest in solving real-world problems and winning challenges in image tasks uh sequence labeling.\ntasks and uh even games right now just again kind of re-looking at this period right from 2010 onwards and.\nthat has been absolute madness in terms of about the euphoria around ai right and the interest in ai so.\nthis is a report published by stanford ai index report and you can see right the number of publications in.\nai has dramatically increased and this has a lot of repercussions in terms of the speed which with the field.\nis moving the pressure on young graduate students nowadays and the quality of conference reviews and so on right those.", "metadata": {"video_title": "Madness and the rise of transformers"}}
{"text": "is moving the pressure on young graduate students nowadays and the quality of conference reviews and so on right those.\nare discussions for a separate day perhaps but you can see how rapidly it is increasing and you can see.\nalmost close to exponential growth in the past few years not quite exponential but very rapid growth in the past.\nfew years right ah this is also no matter how you slice indices whether you look at a number of.\npublications in journals or conferences or book chapters all of this is rapidly increasing in the last few years right.\nand similarly if you look at it across different fields whether it be machine learning computer vision nlp of course.\nall of these fields in their own small ways the number of research the amount of research output that is.\ncoming out has significantly increased not such just research right there's also a lot of startups and interest in big.", "metadata": {"video_title": "Madness and the rise of transformers"}}
{"text": "coming out has significantly increased not such just research right there's also a lot of startups and interest in big.\ntech companies uh in the area of ai and this has resulted in exponential growth in terms of the number.\nof patents that are getting filed in ai right so this i call this a period of absolute madness where.\nai has kind of taken over everyone's imagination and now it is the field that you want to work in.\nit is the field that you want to start have a startup in or have a phd in and so.\non right so they're living in that uh era which is very different from the ai winter which i had.\nsaid right it's quite on the opposite side now a lot of funding for research in ai a lot of.\nfunding for startups in ai and so on right so just kind of uh recapping or seeing the same period.\nfrom a different lens of how the research and development output has increased in ai in the past few years.", "metadata": {"video_title": "Madness and the rise of transformers"}}
{"text": "from a different lens of how the research and development output has increased in ai in the past few years.\nokay now again coming back to uh the technology side of things right so we talked about uh convolutional neural.\nnetworks recurrent neural networks and so on becoming very popular and solve becoming these state of the art de facto.\nmodels for speech and nlp right around this time while this was happening right ah in a very short span.\nof time while we were seeing all this success uh there came what is known as transformers and which is.\nagain revolutionize the field of ai most models that you see today are transformer-based models so that's what we are.\ngoing to talk about now and you'll again go back in time and talk about machine translation so transformers are.\nfirst introduced in the context of nlp and machine translation so we'll go back to machine translation where the first.", "metadata": {"video_title": "Madness and the rise of transformers"}}
{"text": "first introduced in the context of nlp and machine translation so we'll go back to machine translation where the first.\neffort started way back in 1954 and touched upon this earlier there was interest in translating between some politically important.\nlanguages such as english russian due to the political environment at that time right a lot of promises were made.\nright that within a few years we'll be able to have perfect translation machine translation between these languages right but.\nthen research went on and nothing much really happened right in fact there was this very uh critical report which.\ncame out in 1966 which said that hey the initial hype was just too much and people have not lived.\nup to those expectations and we're not really seeing much progress in this field and this was around the same.\ntime as the ai winter right so this also contributed to that saying that hey all this hype is not.", "metadata": {"video_title": "Madness and the rise of transformers"}}
{"text": "time as the ai winter right so this also contributed to that saying that hey all this hype is not.\nreally meaningful and we should look at other ways of solving this problem or maybe the time is not even.\nright for looking at these problems right but research in machine translation continued this era was largely the yellow era.\nthat i have shown here on the timeline was largely the rule-based era of rule-based systems then around 1990s came.\nwhat are known as the ibm models for machine translation which were probabilistic models right so they cast this entire.\nproblem of translation as the probability of finding the french sequence given an english sequence and the way i'm saying.\nit you can you can understand that this is like a conditional distribution that you're trying to model and they.\ncame up with a model of how to how to do this right and also there were a lot of.", "metadata": {"video_title": "Madness and the rise of transformers"}}
{"text": "it you can you can understand that this is like a conditional distribution that you're trying to model and they.\ncame up with a model of how to how to do this right and also there were a lot of.\ninference challenges in the process and they proposed various techniques for doing that and this model right which is quite.\na revolution at that time uh dominated nlp for the next 20 years right 1993 to almost 2012 13 till.\ndeep learning uh eventually took over right and so the first uh i mean of course deep learning in small.\nways was there before there were these word vectors and other things ah but the sequence to sequence model right.\nwhich was introduced sequence to sequence with attention right and if i may i think attention was perhaps the idea.\nof the decay right and it led to a paradigm shift in nlp uh which i showed in a era.\nof like very big models very data hungry models and also pushed the boundary in terms of the performance of.", "metadata": {"video_title": "Madness and the rise of transformers"}}
{"text": "of the decay right and it led to a paradigm shift in nlp uh which i showed in a era.\nof like very big models very data hungry models and also pushed the boundary in terms of the performance of.\nthese models so within a couple of years of being introduced in 2014 people almost gave up on this phase-based.\nstatistical machine translation work which had taken 20 years which is 20 years in the making right in 20 years.\nit dominated nlp and within a couple of years of these neural machine translation being introduced people even stopped comparing.\nwith those models and right now i don't think any paper actually even talks about these uh phase-based statistical machine.\ntranslation that's the kind of impact uh that this has had right i mean just imagine like completely wiping away.\n20 years of i won't say wiping i think that's a bit too harsh but kind of moving like making.\na significant paradigm shift right in terms of how things are done right and not just that right i mean.", "metadata": {"video_title": "Madness and the rise of transformers"}}
{"text": "20 years of i won't say wiping i think that's a bit too harsh but kind of moving like making.\na significant paradigm shift right in terms of how things are done right and not just that right i mean.\nthis is again i find it remarkable right so i said like one major revolution happened in 2014 and within.\nthree years right in 2017 this another model which is a transformer model came out which is very different from.\nthe uh recurrent neural network based model which was proposed in 2014 and it also takes the idea of kind.\nof attention and puts it on steroids that means there's multi-headed attention and so on and that led to the.\nsecond revolution after 2017 within a couple of years now people are no longer talking about the recurrent neural network.\nbased models that were so popular for machine translation right now most modern machine translation models are transformer based model.\ntwo very big revolutions in a very short span of time and a key characteristic again was that we are.", "metadata": {"video_title": "Madness and the rise of transformers"}}
{"text": "two very big revolutions in a very short span of time and a key characteristic again was that we are.\nmoving in the era of bigger and bigger models and trained on really large obscene amounts of data right that's.\nthat's the story or that's the uh that's the story of today all right that's the current era in which.\nwe are living uh big models data hungry models right and uh then this journey continued so 2017 was when.\nit got introduced in the context of machine translation and in 2018 came out this burt model right which the.\nkey idea here was that uh just as humans learn language and then specialize on certain tasks right so when.\nwhen you're when you're growing up you learn about languages and then when you're at a certain age if i.\nask you to do sentiment analysis i can just show you a few examples and you will quickly get the.\nessence of the task and then you'll be able to mark sentences as positive sentence or negative sentiment because you.", "metadata": {"video_title": "Madness and the rise of transformers"}}
{"text": "essence of the task and then you'll be able to mark sentences as positive sentence or negative sentiment because you.\nalready know so much about language right so lovely going by the same analogy the idea in birth is that.\nyou could pre-train models using unlabeled data just as we learn in a very unstructured manner when we are growing.\nup about languages you just train on unlabeled corpora and then when you are dealing with specific tasks maybe small.\namounts of label data would be enough right and this was like kind of reintroducing some sort of transfer learning.\nagain right and that idea has really again been the dominant paradigm today where you do this unsupervised pre-training and.\nthen train uh fine-tune on smaller amounts of data and the advantage of this is that with because you have.\naccess to a large amount of unlabeled corporate so if you think of a language like english you have the.", "metadata": {"video_title": "Madness and the rise of transformers"}}
{"text": "access to a large amount of unlabeled corporate so if you think of a language like english you have the.\nentire wikipedia you have so many news articles you have tons of data available online which you can escape from.\nthe web even for languages indian languages some of the indian languages like hindi marathi and so on you could.\nget a lot of data on the web and you could just train using this data right this is not.\nlabeled data these are just sentences so this allows you to train really large uh models and then fine-tune them.\nusing small amounts of data right and then came gpt which is a lot all of you might be aware.\nof this this is like a massive 175 parameter billion parameter model and it just takes a prompt and generates.\ninteresting things so there are exam there are entire websites uh dedicated to this i'm forgetting the name thinking just.\ngoogle for uh examples from gp33 or something and like you could just type a prompt right you could type.", "metadata": {"video_title": "Madness and the rise of transformers"}}
{"text": "google for uh examples from gp33 or something and like you could just type a prompt right you could type.\na prompt of a famous personality i want to have a chat with you about the first computer right and.\nit will generate a very coherent conversation based on this prompt or you just give it a newspaper headline and.\nit will generate a very coherent one or two paragraphs of articles which is of an article which is in.\nline with the headlines right so we've come a long way in terms of our generation capabilities if you look.\nat where we were in 1954 where we had a simple system which had a few grammar rules and could.\ntranslate a few sentences between english and russian now we have these really large uh billion parameter models and we.\nhave even moved past the billion parameter era right so this is the billion parameter club where as you can.\nsee from uh 2018 onwards 2018 to 2020 right i mean just look at elmo at one end which is.", "metadata": {"video_title": "Madness and the rise of transformers"}}
{"text": "have even moved past the billion parameter era right so this is the billion parameter club where as you can.\nsee from uh 2018 onwards 2018 to 2020 right i mean just look at elmo at one end which is.\nthe number of parameters in billions is what i'm showing on the y-axis uh to what we came to tnl.\nuh sorry the tnlg model right uh the yellow bar that you see like a really huge number of parameters.\nand then you have gpt3 which is not shown here this is only gpt 2 which is 175 billion parameters.\nso that wouldn't even fit in the scale that i've used on the y axis right so that's that's how.\nfar we have come and that's how rapidly these models are increasing in terms of one the amount of training.\ndata they need second the amount of parameters that they have and third is the performance that you get with.\nthese models right all of these on all of these axis we are really pushing the frontier uh quite a.", "metadata": {"video_title": "Madness and the rise of transformers"}}
{"text": "these models right all of these on all of these axis we are really pushing the frontier uh quite a.\nbit and then came the trillion parameter club so this is an interesting animation so you have these number of.\nsynapses right so uh i remember at some point uh or maybe i'll be talking about that later or i.\ndon't know whether i already said that so human brain i think has 10 is to 10 or maybe 10.\nbillion neurons right so 10 billion neurons and or maybe i'm like off by one order and then there are.\nconnections between these neurons those connections are called synapses and there are like 10 raise to 15 synapses in a.\nhuman brain right so for a fruit fly it's greater than 10 raised to six for a honey bee it's.\naround 10 raised to nine for mouse it is around 10 to 12 for cats 10 raised to 13 and.\nfor humans brains it has 10 to 15 synapses right on these synapses i would roughly equate them to the.\nnumber of weights that are there in a neural network right if i used to if i were to take.", "metadata": {"video_title": "Madness and the rise of transformers"}}
{"text": "for humans brains it has 10 to 15 synapses right on these synapses i would roughly equate them to the.\nnumber of weights that are there in a neural network right if i used to if i were to take.\nthe brain analogy of neurons and how the neurons are connected and then apply it to the neural network analogy.\nof artificial neurons and weights between neurons or weights connecting neurons then then synapses would correspond to the number of.\nparameters that a model has right so we had this transformer model the first model which came out 2017 it.\nhad roughly 400 mil 400 million parameters so that's where it would fall on the axis just to the left.\nof honeybee then came the 1.5 billion parameter model gpt 2 then you had the 10 billion megatron uh model.\nthen you have the 175 billion gpd3 model and you have the 1.1 trillion g shard model right which is.\nfor translation and then you have this recent 1.6 trillion parameter model uh which is the switch model right which.", "metadata": {"video_title": "Madness and the rise of transformers"}}
{"text": "for translation and then you have this recent 1.6 trillion parameter model uh which is the switch model right which.\nis like slightly the to the to the right of ah mouse now so in terms of of course these.\nare just for the sake of formulative it doesn't i mean the analogy only holds from the point of view.\nof drawing a diagram of brains of living organisms are quite more complex and capable of doing much more things.\nbut if i was just to uh have fun with this then this is where we are in terms of.\nevolution of neural networks right and this model was strained on 2048 tpus it is an insane amount of compute.\nthat is being used and insane amount of model size that has been used okay so this this was to.\nkind of tell you about the era of transformers that we have if we have entered since 2017 where models.\nare getting bigger and bigger and bigger and of course it's also reflecting in the performance of these models is.", "metadata": {"video_title": "Madness and the rise of transformers"}}
{"text": "are getting bigger and bigger and bigger and of course it's also reflecting in the performance of these models is.\nnot just like a race of building the biggest model but also show performance improvements by doing that right and.\nthen while these were happening in language transformers have also entered vision and here on the dark yellow axis i'm.\ngoing to show you the evolution of image classification so you had this 2012 you had lx net which was.\nfirst participated in the image net and won that competition although it was still around 16 percent or so error.\nrate and then we had multiple convolutional neural network based models around the same time in object detection there was.\nthis move from the traditional object detection methods to a cnn based method rcnn and then there was an entire.\nfamily of models of our cnn which came out and in parallel and image classification we had rest network so.", "metadata": {"video_title": "Madness and the rise of transformers"}}
{"text": "family of models of our cnn which came out and in parallel and image classification we had rest network so.\n2015 all of this is convolutional neural network based models and we also had convolutional neural network based models for.\nobject detection right and it continued for a while then in 2017 transformers came in nlp right so that's it's.\njust like slightly you have to look at different timelines here but in 2017 17 is when transformers came and.\nthen by 2019 we had transformers enter uh the image classification scene and then they have entered the object detection.\nscene also so now even the image classification object detection uh models uh the state-of-the-art models are based on uh.\ntransformers now right and this is kind of again replaced the convolutional neural network based models largely right i mean.\nof course it's not saying that they're completely out but since which have been around since 1980s right so we.", "metadata": {"video_title": "Madness and the rise of transformers"}}
{"text": "of course it's not saying that they're completely out but since which have been around since 1980s right so we.\nare seeing like uh massive paradigm shifts right in the way things are happening in these fields and now transformers.\nare the talk of the uh town as far as the state-of-the-art models for uh image speech and nlp are.\nconcerned today right so yeah okay so now ah we were talking about uh the rise of the transformers now.\nthis perhaps should have been a separate section but i just have it here so in parallel while we were.\ntalking about image classification and object detection which are largely regression and classification problems there's also interested in generation right.\nyou should be able to generate images automatically right you are able to synthesize an image and there was a.\nlot of interest in that over the past few years the first models to do this were variation auto encoders.", "metadata": {"video_title": "Madness and the rise of transformers"}}
{"text": "lot of interest in that over the past few years the first models to do this were variation auto encoders.\nand quite astonishingly right i mean if i tell you this at this point if you do not know much.\nabout variational autoencoders or any of these generation models you'd be astonished that they just take a noise vector that.\nmeans they take a random vector as input and then generate real world images right so the images that you.\nsee here are generated by these models and none of these faces or none of these people actually exist in.\nthe real world right and you can see as time is passing by we're generating very very high quality real.\nlooking images right and this is all uh this these are innovations which are happening in what are known as.\ngenerative models gans being the most popular of those i can see over the years starting in 2014 when gans.\nwere first introduced which is generative adversarial networks there have been a lot of innovations and improvements on that for.", "metadata": {"video_title": "Madness and the rise of transformers"}}
{"text": "were first introduced which is generative adversarial networks there have been a lot of innovations and improvements on that for.\nthe past uh for the six seven years after that right and now uh again here uh there has been.\nlike a paradigm shift right so in terms of what has now come out are what are known as diffusion.\nbased models right which try to overcome several drawbacks of gan based models and you some of you might have.\nbeen so actually if you look at my twitter feed it's full of examples of hey i tried this generating.\nthis with dali too and this is just fabulous hey i tried generating this with the stable diffusion model and.\nit's just fabulous right a lot of this kind of animations that you see on the right hand side of.\nthe screen which is about generating images and daily what it actually does right it just takes a prompt and.\ngenerates very realistic images right so the example here is that it was given a prompt which says an arm.", "metadata": {"video_title": "Madness and the rise of transformers"}}
{"text": "generates very realistic images right so the example here is that it was given a prompt which says an arm.\nchair in the shape of an avocado right and it generates such beautiful images this is not a retrieval mind.\nyou this is not that it has gone on the internet and retrieved images which were of avocado chairs these.\nare images that it has generated starting with a noise vector and based on this prompt so think of it.\nthat you have told an artist that i want the image of a chair in the shape of an avocado.\nand this is the artist's imagination right so dally is the artist here and an illustration of a baby daikon.\nradish in a tutu walking a dog right a brilliant rendering in terms of faithfulness to the prompt that it.\nwas given right it was trained on many such text image pairs so that at test time when you're giving.\nit a text it is able to generate an appropriate image corresponding to that text right and dali was a.", "metadata": {"video_title": "Madness and the rise of transformers"}}
{"text": "it a text it is able to generate an appropriate image corresponding to that text right and dali was a.\ntransformer based model it was not a diffusion based model and now dalit 2 has come which is a diffusion.\nbased model and it has exceeded i would say expectations as i said it is very recent 2022 and improvements.\non top of that which is the stable diffusion model and a few other computing models are are quite the.\nrage now right people are coming up with these prompts and feeding them these to these models and trying to.\ngenerate very realistic images there's an astronaut riding a horse in a photorealistic style right i mean this of course.\nthis image perhaps wouldn't exist anywhere right but the but the model is able to imagine and generate these kind.\nof image so it's quite exciting times for what are known as generative models and we won't cover uh in.\nthis first part of the course right i may do a subsequent course but in the first course unfortunately we.", "metadata": {"video_title": "Madness and the rise of transformers"}}
{"text": "this first part of the course right i may do a subsequent course but in the first course unfortunately we.\nwon't have the time to cover any of these generative models but i just thought it would be good to.\ngive some perspective on the advances happening in the generative field right i mean the field of generative models okay.", "metadata": {"video_title": "Madness and the rise of transformers"}}
{"text": "foreign [Music] so welcome back so we were talking about activation functions and we looked at a few popular ones.\nstarting with the logistic function then the tan H function uh then the relu and a few of its parameters.\nlike the Leaky relu the parametric uh relu and then the exponential uh a linear unit right and in all.\nof these cases we made some comments about how do the gradients behave and then what can we uh do.\nto uh how these activation functions evolved to take care of certain drawbacks of the earlier activation functions right so.\ntan H was preferred because sigmoid had certain problems it was not zero centered so moments only in few directions.\nwere possible and relu is better than tanh and sigmoid because it's simple to compute and it does not saturate.\nin the positive region then the variance of relu are better because they allow some gradients to power flow even.\nwhen the in the negative region right and so on so we did all that and now we look at.", "metadata": {"video_title": "MaxOut"}}
{"text": "when the in the negative region right and so on so we did all that and now we look at.\ncontinue our journey on a more activation functions we'll do around three to four more uh starting with the max.\nout activation function right so we'll motivate that uh starting with Dropout and then try to connect it to how.\nthere is some uh kind of I wouldn't say relationship but some analogy that you could draw from Dropout okay.\nso we had discussed about this model averaging where you have a data set and you create sub data sets.\nfrom here by sampling with replacement what do I mean by that so I have say a thousand data points.\nokay I want to create another data set of thousand uh data points uh as how I will do that.\nis I'll pick one training example from the original data set put it in this bucket right and then keep.\nthe training example here also it's not being removed so it's not that now you have 999 samples here you.", "metadata": {"video_title": "MaxOut"}}
{"text": "the training example here also it's not being removed so it's not that now you have 999 samples here you.\nhave the entire thousand samples I took one from there sample randomly one from there created a copy and put.\nit in this bucket and I still have thousand here again I send them sample one randomly and put it.\nin this bucket and I keep doing this till this bucket has thousand entries right so I'm create I'm sampling.\nwith replacement and I'm creating multiple such uh sub multiple such training sets from the original training set and each.\nof these training sets will then be used to train a model and those models would have their own parameters.\nlike W1 to WK so here W just think of it as T direct it's a collection of all the.\nparameters in the models I'm calling it W uh here all the parameters in the model and here W1 W2.\ncorrespond to the parameters of these K different models each trained on a different version of the training set and.", "metadata": {"video_title": "MaxOut"}}
{"text": "correspond to the parameters of these K different models each trained on a different version of the training set and.\neach training set has the same size but it contains uh sampling with replacement right so what would happen in.\nthat case is that it is shown that if you do this sampling with replacement right so this bucket that.\nyou had the first bucket that you created that was used to train the model f hat1x which had a.\ncollection of parameters W1 so that bucket around 46 percent of the samples there would be duplicated and does make.\nsense right because you're sampling with replacement so all roughly half the samples would get duplicated in this set so.\nit won't be that all these thousand would be unique that should be obvious and there's also some analysis that.\nroughly half of the samples are actually duplicates so we always expected duplicates there's a study which says exactly how.", "metadata": {"video_title": "MaxOut"}}
{"text": "roughly half of the samples are actually duplicates so we always expected duplicates there's a study which says exactly how.\nmany duplicates you can expect right so now you have 1000 data points but there are duplicates in that so.\nnow the model has a better chance on overfitting on that right because now you have fewer samples than the.\noriginal training data and you're seeing the same samples again and again some other model gets a chance to adjust.\non the same sample more times now right and hence in these models uh each of these models F1 hat.\nto F2 had would like really better overfit to the bucket of training data on which it was a trained.\nbecause that bucket is containing duplicates and it's each model is seeing the same training examples multiple times and then.\nwhen you do inference you do some kind of model averaging it could be arithritic mean or geometric mean uh.\nany of these are possible that we have seen it so we take the prediction from all the K models.", "metadata": {"video_title": "MaxOut"}}
{"text": "any of these are possible that we have seen it so we take the prediction from all the K models.\nand then we take the average is 1 possibility or you could take the max that is another possibility you.\ncould even take voting all of those possibilities exist right so this is what happens in uh model averaging and.\nthen in deep neural networks the way we had done that was using uh Dropout right so we had uh.\nwe knew that it's difficult to train multiple models so Dropout was a substitute for uh uh for getting the.\neffect of model average okay now uh the same thing I'll just repeat right so the model weights get optimized.\nfor a specific set of sample each of those buckets corresponding to one of the models the model is over.\ntrained on that bucket so it overfits but in dropouts we don't have a similar luxury right so in dropouts.\nwhat is happening is that you have these multiple sub models each sub model is getting sampled rarely right because.", "metadata": {"video_title": "MaxOut"}}
{"text": "what is happening is that you have these multiple sub models each sub model is getting sampled rarely right because.\nthere are an exponentially High number of sub models so each sub model is getting sampled rarely so each sub.\nmodel is not getting the chance to overfit on specific portions of the data because hardly it will see the.\ndata once I mean forget about seeing it twice it may not even see it once right so this kind.\nof uh phenomenon which happens in model averaging or in bagging where the each model gets each sub model or.\neach different model gets a chance to overfit on the training data Dropout does not really get that chance right.\nso now uh in the case of bagging the model is getting to update the weights again and again on.\nthe same training sample but in the case of Dropout it's not getting that chance so whenever it sees a.\nsample we should try to do a larger update right because this if this data point is sensitive we should.", "metadata": {"video_title": "MaxOut"}}
{"text": "sample we should try to do a larger update right because this if this data point is sensitive we should.\ntry to do a larger update but then how do you do a larger update maybe you could increase the.\nlearning rate but if you do that there could be problems right we have seen several problems in with increasing.\nlearning rate and the case of dropouts since these parameters are shared across all these sub models now you increase.\nthe learning rate it could have effects on the other sub models right so that's the situation where we are.\nin we drop we motivated dropouts from model averaging but now we are observing that something which happens in model.\naveraging where every model sub model gets to overfit on the specific training data that effect does not really happen.\nin Dropout and in Dropout still we do model averaging right so now the way of model averaging was again.\nthat each of these neurons you consider their out port and you multiply it by a fraction P which is.", "metadata": {"video_title": "MaxOut"}}
{"text": "that each of these neurons you consider their out port and you multiply it by a fraction P which is.\nthe number of times this neuron was on right so we are just assuming that all neurons were equally participating.\nand equally sensitive to all the training examples and hence they were all on only a fraction of times P.\nso we'll take all of their weight as P assign a weight P to all of them and then do.\nthe model averaging right but this Gap is still not being addressed that in bagging actually the model overfits on.\nspecific samples whereas in Dropout the concept of overfitting is not really happening on any specific training examples because each.\nsub model is rarely seeing the training data right so now how do we close this Gap can we do.\nsomething so that's where MaxOut gets motivated from so this is what Dropout does right so drop out at every.\ntime step now you have four neurons in the hidden layer H one one H one two H one three.", "metadata": {"video_title": "MaxOut"}}
{"text": "something so that's where MaxOut gets motivated from so this is what Dropout does right so drop out at every.\ntime step now you have four neurons in the hidden layer H one one H one two H one three.\nh one four so we'll apply a mask right with probability P so some of these would be on some.\nof these would be off the ones which are off will not participate in the computation so in particular if.\nI am Computing a to 1 only h11 and h14 will participate with the corresponding weights w11 and W one.\nfour if I am Computing a22 then again only H 1 1 and H2 h14 will participate with the corresponding.\nweights w21 and W 2 4 right uh and now what would happen uh yeah so now now let's consider.\na scenario right where uh uh these two outputs a21 a22 have the following relationship they're both greater than zero.\nbecause I'm taking them greater than zero because it's a relu activation function so I want both of them to.", "metadata": {"video_title": "MaxOut"}}
{"text": "because I'm taking them greater than zero because it's a relu activation function so I want both of them to.\nbe uh active and not dead so say both of them are greater than zero but a two one is.\nless than a to 2 right so what does that actually mean uh let's try to understand that in terms.\nof how neurons react to training samples right so if a21 is actually uh less than a to 2 it.\nmeans that 8 to 2 is reacting more positively to this training sample right it's firing for this training sample.\neven better because its output is high right so now if it related to our earlier discussions on MP neurons.\nand so on we had the situation that if the inputs are of a certain configuration then this neuron fires.\nright now this neuron is firing More Than This neuron which is firing weakly right so now that means this.\nneuron is more uh sensitive to this training data and maybe I should respect that right so when I'm updating.", "metadata": {"video_title": "MaxOut"}}
{"text": "neuron is more uh sensitive to this training data and maybe I should respect that right so when I'm updating.\ncan I update this better as opposed to updating this right because this is reacting weekly right so can I.\ndo that so one option of doing that is to take the yeah so this is the effect that we.\nwant right that we want to make these we want to acknowledge the fact that these neurons are sensitive to.\nTurning certain training samples one neuron is more sensitive than the other and hence I should perhaps try to get.\nthis effect which I had in bagging where the neurons were adapting to the specific bucket of training data and.\nnow since this neuron has shown such some preference for this training data can I give it more preference can.\nI make its updates better so as opposed to the relative updates of the other neuron right I don't want.\nthat other neuron to specialize for this training sample because it has not shown interest in this training sample right.", "metadata": {"video_title": "MaxOut"}}
{"text": "that other neuron to specialize for this training sample because it has not shown interest in this training sample right.\nso one way of doing that is now I could take the max of these two so max would turn.\nout to be a22 right and then when the gradients flow back right so now what is uh h21 it's.\nthe max of a to 2 and a21 which is equal to a 2 2 right so now if I.\ntake the derivative of h21 with respect to a21 and the derivative of h21 with respect to a22 then this.\nwould be 0 and this would be 1 so the gradient will only flow to this part it will not.\nflow to this part right and that's what you wanted to achieve that you wanted to make this neuron specialized.\nby giving it the signal and ignore this neuron because it was not really showing much enthusiasm for this input.\nso let it not fire and I'll kind of ignore it and just try to make this other neuron specialize.\non this training sample and if you do that then in some sense your different neurons are uh kind of.", "metadata": {"video_title": "MaxOut"}}
{"text": "so let it not fire and I'll kind of ignore it and just try to make this other neuron specialize.\non this training sample and if you do that then in some sense your different neurons are uh kind of.\ngetting adapted to specific training samples and you are getting the same uh effect as you expect in model averaging.\nor in bagging right where certain neurons are certain sub models are specializing for certain training samples right so that's.\nthe effect that you're trying to achieve and hence you do this Max now what was a21 a21 was actually.\nthis and a22 was this so you can think of these as there are two linear Transformations happening here and.\nyou're selecting the max of those linear Transformations and by restricted 2 right so you could even have K linear.\nTransformations and you could select the Max from that right so let's let's just delve a bit more into this.\nover the next few slides to get this picture uh clear of what I mean by K linear Transformations right.", "metadata": {"video_title": "MaxOut"}}
{"text": "over the next few slides to get this picture uh clear of what I mean by K linear Transformations right.\nso this is what happens in Dropout right we're still trying to compare Dropout and see what was happening there.\nso in Dropout what happens is uh I can divide this nodes in this hidden layer right uh this was.\nthe hidden layer I am dividing nodes into three parts one is the black nodes which have been dropped out.\nso they are completely not participating in the uh uh in the output right they are not participating in the.\ncomputation at all and then there are two types of nodes one are strongly responding nodes and the others are.\nweakly responding notes and now in the case of Dropout irrespective whether you're strongly responding or weakly responding the gradients.\ndo flow back to you right and the gradients will of course flow back in proportion but you're still all.\nof you are being responsible for the output and the certain nodes may not maybe get spatialized for the specific.", "metadata": {"video_title": "MaxOut"}}
{"text": "of you are being responsible for the output and the certain nodes may not maybe get spatialized for the specific.\ninput right so that's what is happening in the case of Dropout now if you look at the sub model.\nthat we have in the case of max out what is happening here is the following right so this is.\nwhat is happening in max out now I have considered a max out layer and because I can't show many.\nuh neurons here this is One max out neuron okay this max out neuron in turn has three nodes each.\nof this has some uh linear transformation right so this is W1 transpose X this is W2 transpose X this.\nis W3 transpose X and then I'm only taking the max of these guys and allowing it to pass forward.\nright so in essence these two nodes have been dropped out because of the max operation so you are getting.\nthe same effect as Dropout similarly you have these two nodes here three this max out neuron here which has.", "metadata": {"video_title": "MaxOut"}}
{"text": "the same effect as Dropout similarly you have these two nodes here three this max out neuron here which has.\ninside three linear Transformations happening okay let me just call them as W1 maybe call it w tilde transpose x.\nw 2 tilde transpose X and W 3 tilde transpose X see here again I had three linear Transformations then.\nI just selected the max of those and dropped out these neurons right so now as opposed to Dropout where.\nafter dropping out all all the neurons participate and even during inference we take equal weightage to all the participating.\nneurons now we are ensuring that the neurons which participate Only The Strongest Ones of those are actually active and.\nwe are further dropping out the weak one okay so this is uh what max out looks like so you.\nhave Max of multiple affine Transformations I've been saying linear but they would be a fine because there's a plus.", "metadata": {"video_title": "MaxOut"}}
{"text": "have Max of multiple affine Transformations I've been saying linear but they would be a fine because there's a plus.\nb also possible there right yeah so these are in multiple uh neuron multiple uh affine Transformations inside I have.\nshown n of those and then you're just selecting the max of those two right and uh you you could.\neven think of max out as a generalization of the relu function right so now you could think of uh.\na relu function uh any of the relu functions right relu or leaky value or parametric relu as a generalization.\nof uh or as a special case of the max out neuron uh such that there are two max out.\nI mean two neurons inside the max out neuron and for one of them the weight and the bias is.\nzero and then for the other one you have W2 X plus b now in the case of uh relu.\nnow W2 is again just equal to 1 and B is equal to zero so then you're just left with.\nMax of uh this quantity is 0 and the other quantity is just X right now in the case of.", "metadata": {"video_title": "MaxOut"}}
{"text": "now W2 is again just equal to 1 and B is equal to zero so then you're just left with.\nMax of uh this quantity is 0 and the other quantity is just X right now in the case of.\nthe earlier leaky value that we had seen this was 0.1 x so you could think of right as again.\nMax of uh zero comma 0.1 x plus the bias again is zero right so this is also a special.\ncase of the max out neuron so you can think of max out neuron as generalizing relu and all its.\nvariance and uh again it gives you more uh Power more non-linearity so this is what if you have these.\ntwo max out neurons right so if one of them learns the the value of W1 as 0.5 x and.\nthe other one learns the value of W2 is minus 0.5 x then effectively you are getting this v-shaped a.\nnon-linearity right which is uh the same as the absolute function and then you can also show there are certain.\nconfigurations possible so suppose you have uh four uh affine Transformations within a max out neuron right so I'm talking.", "metadata": {"video_title": "MaxOut"}}
{"text": "configurations possible so suppose you have uh four uh affine Transformations within a max out neuron right so I'm talking.\nabout a max out neuron this is what a max out neuron looks like it has four affine Transformations and.\nI'm going to take the max of those right and if those are fine Transformations happen to be these these.\nare the four refined Transformations if I take the max of this then I almost can approximate the x square.\nfunction right so it's uh it's giving me a higher degree of a non-linearity and of course it has the.\nother benefits there is uh no saturation no death now because it will flow through the max path and the.\nmax at least one of the neurons would be great as long as it is greater than zero you would.\nhave some gradients flowing and for multiple affine Transformations the more you have there is a proportional increase in the.\nnumber of parameters you could think of that as a disadvantage but there is a proof which also shows that.", "metadata": {"video_title": "MaxOut"}}
{"text": "number of parameters you could think of that as a disadvantage but there is a proof which also shows that.\ntwo max out neurons with sufficient number of affine Transformations right so just have two of these but you have.\na large number of affine Transformations inside them right then these can act as a universal approximate so you just.\nneed like two max out neurons to approximate any function so that's a uh kind of a testimony for the.\nuh non-linearity or the representation power that max out neurons brings in August makes out neurons uh despite whatever I.\nhave explained are still not very popular I think their special case which is relu or any of its variants.\nthey are still more popular but it's good to know about them it's good to see the analogy with Dropout.\nit's also good to see how they generalize relu and other functions is also good to know that just two.\nmax out neurons are uh can act as a universal approximately so this all helps you build a better understanding.", "metadata": {"video_title": "MaxOut"}}
{"text": "max out neurons are uh can act as a universal approximately so this all helps you build a better understanding.\nof some of the concepts in deep learning right so so that's all I had to say about the max.\nout neuron um I'll end this video here and then I'll come back and talk about some other activation functions.", "metadata": {"video_title": "MaxOut"}}
{"text": "foreign [Music] descent and we'll see whether it tries to fix the problem so what were our observations about gradient.\ndescent that it takes a lot of time to navigate regions having a gentle slope right so wherever we have.\ngentle slope and now again connecting it to the discussion on Contours gender slope means if you have two successive.\nRings the distance between them would be very large and in those gentle slope regions it takes a long time.\nand the intuition is clear because on those slopes the gradients are small and if the gradients are small your.\nupdates are small so at every Point you're moving like a very very very small step so it'll take a.\nlot of steps to get out of that flat surface right and this is what is explained here so can.\nwe do something better so let's take a look at momentum based gradient descent okay uh so there's the intuition.\nright and uh I'll repeat the intuition which I always give for this right so uh this would be relevant.", "metadata": {"video_title": "Momneum based Gradient descent"}}
{"text": "right and uh I'll repeat the intuition which I always give for this right so uh this would be relevant.\nto the audience here at IIT Madras so now suppose you're standing at the velachery gate right and you're asking.\ninstructions you ask the uh some person there about how do I go to Phoenix Market City Mall right and.\nthat person would say that you take a right uh from the gate and you will reach the ball right.\nso now you have asked someone but you are a bit conservative so you take a small step towards in.\nthat direction you take small small steps and say after say 10 steps or 20 steps or say about 100.\nsteps you ask someone else hey where is uh the mall and that person again says yeah go in this.\ndirection right go towards the right so now your confidence would increase right because now repeatedly you are being told.\nto go in that direction someone said go to the right again someone said go to the right so now.", "metadata": {"video_title": "Momneum based Gradient descent"}}
{"text": "to go in that direction someone said go to the right again someone said go to the right so now.\nmaybe you'll start want to increase your speed a bit right because you have two uh validations for uh this.\ndirection that this is the right direction right then you again walk a few steps and then you ask someone.\nwhere to go again that person points you in that direction so then you again start your confidence increases and.\nyou gain more momentum right so what we are trying to say here is that if you're repeatedly being asked.\nto go in the same direction then you should probably gain confidence and start taking bigger steps in that direction.\nright so at this step when you are Computing the gradient maybe the gradient is small because you're on a.\ngentle surface but you have been moving in this direction for a long time right so this is what your.\nuh surface looked like okay this is very gentle slope here so here every step is small right but you.", "metadata": {"video_title": "Momneum based Gradient descent"}}
{"text": "uh surface looked like okay this is very gentle slope here so here every step is small right but you.\nhave been moving along this direction for a long time so what if you accumulate all this and then start.\nmoving very fast so that you can come out of this uh this gentle region very quickly right so that's.\nthe idea of momentum gaze gradient descent and we now need to take this uh intuition and convert it into.\nan equation right so let's see and this is the same as a ball gains momentum while rolling down a.\nslope right so it's slowly moving when it's on the gentle part here right but that it's moving it gains.\nmomentum and then it starts moving very fast and because one way of looking at it is that it's constantly.\nbeing pushed in that direction so you're also constantly being asked to go in that direction the slope is small.\nbut the direction is constantly this way you're saying okay go here at every point you are saying go here.", "metadata": {"video_title": "Momneum based Gradient descent"}}
{"text": "but the direction is constantly this way you're saying okay go here at every point you are saying go here.\ngo here go here so if so many times you have been asked to go here can you go a.\nbit faster right so how do you capture this intuition into a set of equations right so this is what.\nwe will do so this is the update rule for momentum based gradient descent okay so this is what I'll.\ncall as the history vector fine and uh you are giving some importance to the past history plus the current.\nupdate right so this is your derivative of the loss function with respect to the gradient oh sorry derivative of.\nthe loss function with respect to the parameter w at the current time step right so now the difference is.\nthe following right so in uh foreign descent your update rule was WT minus ETA times Delta w t that.\nmeans you are only listening to the current instruction you are not really considering the fact that I've been constantly.", "metadata": {"video_title": "Momneum based Gradient descent"}}
{"text": "means you are only listening to the current instruction you are not really considering the fact that I've been constantly.\nbeing asked to go in the direction okay people might have asked me to go in this direction but where.\nam I asked where are people asking me to go in the current step in this direction okay I'll just.\nfollow that right but this entire history which was there that you were constantly being asked to go in that.\ndirection that is not being captured right so now instead of just moving according to the current uh derivative or.\nthe current gradient you are actually also considering the entire history right and this is a recursive equation and we'll.\njust open this recursive equation soon but this is akin to kind of taking all your past gradients and giving.\nthem some importance and then moving in the cumulative direction right so that's what you are trying to do here.\nand this would become clear on the next slide so let me just do that and then come back uh.", "metadata": {"video_title": "Momneum based Gradient descent"}}
{"text": "and this would become clear on the next slide so let me just do that and then come back uh.\nto this slide right so what is is happening here so this is what my equation is right that UT.\nand I was saying that UT is my history Vector so let's see how I'm constructing this UT so of.\ncourse at time step 0 because my U minus 1 is not defined right so if I want to compute.\nu0 I need beta times U minus 1 and there's no minus one step so I'm just going to set.\nit to 0. so at time step equal to uh 0 this is how this will turn out right and.\nthis quantity is 0 so we'll just have uh U 0 is equal to the derivative of the loss function.\nwith respect to the parameter at time step 0. now let's see what U1 would be so U1 will be.\nbeta times u0 plus the current derivative so now if I substitute the value of U 0 then it would.\nbe beta times Delta the previous derivative plus W1 right so this is what I mean by the history now.", "metadata": {"video_title": "Momneum based Gradient descent"}}
{"text": "beta times u0 plus the current derivative so now if I substitute the value of U 0 then it would.\nbe beta times Delta the previous derivative plus W1 right so this is what I mean by the history now.\nyou are not only considering the current derivative but you are also giving some importance to the past history right.\nyou are saying oh in the past also I was asked to move in this some Direction so I'll take.\nthat also into account right and Now Beta U2 would be beta Square into the derivative at time step 0.\nbeta into the derivative is time Step 1 and then the current derivative right so your updates remember are WT.\nis equal to W T minus 1 minus ETA into u t right now this u t is essentially this.\nright so it's not only the current derivative so at time Step 2 you're not just taking the current derivative.\nbut you are also taking all the history into account right so that's what is happening in momentum based gradient.", "metadata": {"video_title": "Momneum based Gradient descent"}}
{"text": "but you are also taking all the history into account right so that's what is happening in momentum based gradient.\ndescent and this beta is typically less than one right so typical value for beta would be 0.9 so what.\ndoes that mean you are giving weightage of 1 to the current gradient that's the maximum weightage you are giving.\ngradient of point weightage of 0.9 to the previous gradient weightage of 0.9 Square which would be 0.81 to the.\nprevious gradient so we are giving decreasing importance to your uh previous and previous gradients right as you are farther.\nfrom the current step your weightage of that gradient is less but you are still accumulating all of that and.\nhence collectively this sum should tell you right because instead of just moving with this now collectively you will be.\nmoving by a larger amount and that's exactly what you want on the flat surfaces because you're moving slow slow.", "metadata": {"video_title": "Momneum based Gradient descent"}}
{"text": "moving by a larger amount and that's exactly what you want on the flat surfaces because you're moving slow slow.\nslow slow at every time step your movement is slow but if you just accumulate this history then your movement.\nwould become fast right so that's what is happening here so your UT is just going to be the sum.\nof this right so this was at time step two this was just uh you could write this as beta.\n2 minus 0 into W 0 then beta 2 minus 1 into W1 plus beta 2 minus 2 into W.\n2 right sorry Delta should have been here so that is what this equation is capturing right so this minus.\n0 is essentially minus Tau so you're going from time step 0 to time step Tau and at every stage.\nyour weightage for that gradient is given by Beta T minus beta raised to T minus Tau right so that's.\nwhat u 2 is it's a collection of all the gradients that you have seen so far and this exponentially.\nweighted average of current and all pass gradients right as I was saying that if you are currently at time.", "metadata": {"video_title": "Momneum based Gradient descent"}}
{"text": "weighted average of current and all pass gradients right as I was saying that if you are currently at time.\nstep 9 right then the weightage for the current gradient would be one right because this would be 9 minus.\n9 and that would be 0 if you are at time step if you are looking at the gradient at.\ntime step 8 then that would be 9 minus 8 so it would be given the weightage of beta and.\nas I said beta is typically 0.9 so this would keep decreasing exponentially so gradients which were taken very long.\nback will have very less say in this cumulative history right so that's what the idea behind momentum is and.\nnow with this idea in mind let me go back to the previous slide I'll again try to look at.\nthe equations right so this is what the equations look like now we understand that this quantity here actually captures.\nthe entire history okay the weights are initialized randomly your U minus 1 is 0 and beta is a constant.", "metadata": {"video_title": "Momneum based Gradient descent"}}
{"text": "the entire history okay the weights are initialized randomly your U minus 1 is 0 and beta is a constant.\nquantity between 0 to 1. so what you're doing is taking an exponentially weighted average of all the gradients and.\nnow it should be clear that instead of moving just by the current gradient which on a flat surface would.\nbe very small you are moving by this current you this cumulative history right and that cumulative history would of.\ncourse be larger than the current gradient because it also includes the current gradient right so hence you'll be able.\nto move by larger amounts right so let's see what happens now uh when we run this algorithm foreign okay.\nso this is how I have written momentum based gradient descent so it's very similar to the gradient descent algorithm.\nthat I had written let me just point out things here uh so this is my initialization so I have.\ninitialized W and B to some random values for the sake of convenience minus 2 minus 2 e tabs has.", "metadata": {"video_title": "Momneum based Gradient descent"}}
{"text": "that I had written let me just point out things here uh so this is my initialization so I have.\ninitialized W and B to some random values for the sake of convenience minus 2 minus 2 e tabs has.\ntaken one I'm not planning playing around with the learning rate and I have this uh for the u t.\nright which was storing the history so I've initialized the history for w as well as B to be equal.\nto 0 right and I have taken beta is 0.10 as I said 0.9 is the typical value now this.\npart is the same as green gradient descent for every point in my training data I am Computing the derivative.\nand just summing up the derivative so that's the derivative of the loss function and now I'm maintaining this cumulative.\nhistory in VW and VB and then updating according to the cumulative history as opposed to the current uh or.\nas as opposed to only using the current grade unit so that's what the code is doing it's in line.", "metadata": {"video_title": "Momneum based Gradient descent"}}
{"text": "as as opposed to only using the current grade unit so that's what the code is doing it's in line.\nwith the uh set of equations that you had if you want you can pause at this point and look.\nat the code more carefully um but it's pretty uh straight forward okay so now let's try to run uh.\nmomentum based gradient okay and now you will see that right earlier it was very slow now it started moving.\nvery fast and it has gone like somewhere and it's coming back now whether that's a problem or not is.\nsomething that we'll discuss soon but what is clear it's that definitely moving much faster than the gradient descent algorithm.\nright and later on we'll have some uh comment on both gradient descent and momentum starting together and you will.\nsee the difference in one moving faster than the other moving slower okay but here it's clear that the momentum.\nbase algorithm is moving faster and we understand why that is also happening right let's look at the other view.", "metadata": {"video_title": "Momneum based Gradient descent"}}
{"text": "base algorithm is moving faster and we understand why that is also happening right let's look at the other view.\nnow so this is the contour map you understand what the contour map is so this region here is a.\nflat region so this region here is a flat region and then here that the slope is very Steep and.\nthen again here the slope is very gentle this is again the slope is steep these are again flat regions.\nright so you understand what that is so this is just the contour map corresponding to the 3D plot that.\nwe had and now we'll try to run uh gradient descent on this contour map it's nothing different from what.\nI have shown here but now I can see the other view of what is happening to the sigmoid function.\nand you will see something interesting here right so let's run it of course it's moving very fast and it's.\nalmost reached there but then again it has moved far and then again come to something which is some things.", "metadata": {"video_title": "Momneum based Gradient descent"}}
{"text": "almost reached there but then again it has moved far and then again come to something which is some things.\nare happening here which many of you might be realizing that it's oscillating around the solution and we'll see why.\nthat is happening and how to fix that right what is clear for now is that it's moving faster than.\ngradient descent right so some observations and questions even in the regions having gentle slopes momentum based gradientation is able.\nto take large steps because the momentum carries it along right because it is having the entire history behind it.\nit just goes faster now but is moving fast always so good right would there be a situation where momentum.\nwould cause us to run past our goal right and again I'll go back to my analogy so you're going.\ntowards the Phoenix Market City a lot of momentum has been built many people have asked you to go in.\nthe right right right right and now suppose imagine you are on a scooter because now you are fast and.", "metadata": {"video_title": "Momneum based Gradient descent"}}
{"text": "the right right right right and now suppose imagine you are on a scooter because now you are fast and.\nyou go very fast okay people have said Gora so you just zoom and go now what will happen you.\nwill cross the mall and go ahead right and then you'll have to take a U-turn and come back and.\nyou're taking a U-turn people will say okay go in the left direction or go this way and again you.\nwill say Okay many people are saying that so let me go fast then again you might overshoot and then.\nagain come back take a U-turn come back and so on right so going fast is not always good as.\nyou'd say I mean you could imagine in any navigation problem you just keep going fast without checking whether okay.\neverything is fine you might overshoot and then I have to take a U-turn and come back right in fact.\nand this is what the u-turns are what we saw on the momentum gradient descent right so it actually overshot.", "metadata": {"video_title": "Momneum based Gradient descent"}}
{"text": "and this is what the u-turns are what we saw on the momentum gradient descent right so it actually overshot.\nright so it come here so it actually if you look at yeah this is what it looked like right.\nso now uh so it went down in the valley now it should have gone towards its goal but it.\novershadowed the goal right it went further ahead then it took a U-turn and came back now again it should.\nhave gone somewhere here but its momentum carried it here in this direction and then again it had to take.\na U-turn and come back here right so that is what we saw so clearly it's moving fast but it's.\nit's kind of moving a bit out of control right so we need to see whether if we can control.\nthe way it is moving right and so make let's see this in a more you know one more setting.\nright so now we have this kind of an input and we have a very weird looking 3D surface here.\nright and this is what the contour map looks like and now I'm just going to focus on the contour.", "metadata": {"video_title": "Momneum based Gradient descent"}}
{"text": "right so now we have this kind of an input and we have a very weird looking 3D surface here.\nright and this is what the contour map looks like and now I'm just going to focus on the contour.\nmap because the contour map tells me everything this is a flat surface and from this flat surface I am.\nquickly moving into a value rate so there's a steep slope which takes me to a valley similarly here I.\nhave a flat surface and I am going into a Valley from a steep slope so this is a Surface.\nwhere you have like flat surfaces here and then you quickly move into a Valley from both sides right so.\nfrom all sides you have flat surfaces and then you quickly move into a valley so this is what it.\nlooks like is it's a very steep valley and now you can imagine right so if I put a ball.\nhere it will roll down it will not stop here it'll go up then again come back go up then.\nagain come back and it will keep oscillating it so let's see if this effect is what we see in.", "metadata": {"video_title": "Momneum based Gradient descent"}}
{"text": "here it will roll down it will not stop here it'll go up then again come back go up then.\nagain come back and it will keep oscillating it so let's see if this effect is what we see in.\nthe momentum based gradient descent algorithms oh my God so now you see gradient descent is moving very slow but.\nit's decidedly going towards its goal okay I'll have to play this video a few times so this gradient isn't.\njust slowly converging now you can see that it's found the configuration where both the points lie on the sigmoid.\nfunction right so this is actually a sigmoid function like this right so I'm just showing you a slice so.\nit's a sigmoid function like this and I'm just showing you this portion of the sigm point functions it's not.\na line it's still a sigmoid function it's just that it's a very uh gentle slope sigmoid function so it's.\ngoing to go like this and at this point it's going to go like this okay but let's focus on.", "metadata": {"video_title": "Momneum based Gradient descent"}}
{"text": "a line it's still a sigmoid function it's just that it's a very uh gentle slope sigmoid function so it's.\ngoing to go like this and at this point it's going to go like this okay but let's focus on.\nthe uh movement again so let's see now you can see clearly that momentum based gradient descent is moving much.\nfaster right but then it's also oscillating a lot right so it kind of went uh let me now just.\nminimize this it's better to do it yeah so this is what it looks like and yeah so the momentum.\nbase gradient descent it was clear from the speed it was going faster right but then what was happening is.\nthat it was often overshooting right so it went here it should have gone here but it just went here.\nthen it took a U-turn and again took a U-turn then again took a U-turn right so you can see.\nthat uh you can play the video again and you can see that it's taking quite a few u-turns before.\nit reaches its goal and this is exactly the analogy very good if it's on a scooter or a bike.", "metadata": {"video_title": "Momneum based Gradient descent"}}
{"text": "that uh you can play the video again and you can see that it's taking quite a few u-turns before.\nit reaches its goal and this is exactly the analogy very good if it's on a scooter or a bike.\nor a car and it is going fast an overshoot come back overshoot come back but what is happening in.\nthis case as happens in real life also you will still reach your goal faster than the guy who is.\nwalking right in this case so momentum base gradient descent oscillates in and out of the Minima Valley as the.\nmomentum carries it out of the valley right and then it takes a lot of u-turns before finally converging despite.\nthese used U turns it still converges faster than the gradient vanilla gradient descent right and I'll just quickly play.\nthe video Once More to show this is important right so after 50 iterations momentum based gradient descent is at.\na very low loss whereas gradient descent was a very high loss right and 50 iterations corresponds to two seconds.", "metadata": {"video_title": "Momneum based Gradient descent"}}
{"text": "a very low loss whereas gradient descent was a very high loss right and 50 iterations corresponds to two seconds.\nof the video there are 25 iterations per second here so I'll just play it quickly again one second it's.\nalready much ahead of gradient descent two seconds it's actually reached the Minima it's now just going to keep oscillating.\nthere but gradient descent is still very far and now despite these oscillations it's reached its answer whereas gradient descent.\nis still trying to reach there reach there it's there still not there and now it is there right so.\nyou can see that momentum based gradient doesn't reach there much quicker than gradient descent despite these oscillations so what.\nwe would like to see next now is that can we the oscillations are a part of the life now.\nright given that we are moving fast but can we do something to reduce these oscillations and that's the idea.", "metadata": {"video_title": "Momneum based Gradient descent"}}
{"text": "we would like to see next now is that can we the oscillations are a part of the life now.\nright given that we are moving fast but can we do something to reduce these oscillations and that's the idea.\nbehind nestro based uh gradient nest of accelerated gradient descent so that is what we'll see in the next video.\nthank you.", "metadata": {"video_title": "Momneum based Gradient descent"}}
{"text": "[Music] hi everyone so welcome to lecture two uh in this lecture we are going to talk about uh different.\nneurons mcculloch pits neuron thresholding logic perceptrons and even look at the learning algorithm for perceptrons and see a small.\nproof on why that algorithm would converge and then we'll end with what we call as multi-layer perceptrons or mlps.\nwhich you would have popularly heard of and what is the representation power of such mlps right so let's uh.\nstart with the first module and we'll start with biological neurons right so why biological neurons right so what we.\nare interested in knowing about is what is known as an artificial neuron which is the most fundamental unit in.\nan artificial neural network right but now why the term artificial neuron right so where does this inspiration come from.\nso the inspiration actually comes from biology so i think you remember when we were talking about history we had.", "metadata": {"video_title": "Motivation from Biological Neuron"}}
{"text": "so the inspiration actually comes from biology so i think you remember when we were talking about history we had.\ntalked about these biological neurons and the term neurons getting coined somewhere in 1890s which was the processing unit in.\nthe brain right and the idea is just as we have this neurons in the brain which can do fairly.\ncomplex processing can we have artificial neurons which can also help us do some computational processing right so that's where.\nthe inspiration comes from so let's just look at what a biological neuron looks like so here's a picture i'm.\nshowing two neurons here i'll go through some important parts of the neuron so you have a dendrite right which.\nreceives signals from other neurons these neurons are connected as we had seen in the history again and then there's.\nthe synapse which is the point of connection of the two neurons so dendrite is the part from which one.", "metadata": {"video_title": "Motivation from Biological Neuron"}}
{"text": "the synapse which is the point of connection of the two neurons so dendrite is the part from which one.\nneuron receives signals from the other neurons and the point of connection is called synapse and then you have soma.\nwhich is the kind of you could think of with the central processing unit where all the information gets processed.\nand then you have the axon once the information has been persist the exon is responsible for uh carrying it.\nout to the other neurons right so dendrite to receive signals then synapse is the point of connection between neurons.\nsoma is where the processing happens in the neurons and exons is where you give the output out or transmit.\nthe output to the other neuron so this is what a biological neuron looks like right and let's see like.\na very cartoonish illustration of how this uh works and how a neuron works right so our uh sense organs.\nthey interact with uh the outside world right so we see we hear and so on so now let's assume.", "metadata": {"video_title": "Motivation from Biological Neuron"}}
{"text": "a very cartoonish illustration of how this uh works and how a neuron works right so our uh sense organs.\nthey interact with uh the outside world right so we see we hear and so on so now let's assume.\nyou're watching some uh a cartoon or if you're watching some uh comedy uh movie or serial right and let's.\nin this case it's uh you're watching something from sheldon and then your eyes are seeing it your ears are.\nhearing it let me just get rid of this so they then relay information to the neurons here i am.\njust showing a single neuron on the next slide i will show you that it's not just one neuron but.\na network of neurons but let's just go with this right so the neuron receives signals from the sensory organs.\nit processes it and then it might decide to take an action right so in this case if the neuron.\nis excited enough if this is really something very comical then it might uh get activated and in turn signal.", "metadata": {"video_title": "Motivation from Biological Neuron"}}
{"text": "is excited enough if this is really something very comical then it might uh get activated and in turn signal.\nto the other neurons that hey we need to do something to evoke loft right so that's a very cartoonish.\nillustration and i said in reality it's not just like a single neuron but it's like a network of neurons.\nthat you have and these neurons are arranged in layers so that initial set of neurons at the lowest level.\nwhich interact with the sensory organs and then they get some input and then based on that input some of.\nthem might get excited and pass activate other neurons in other layers and this might continue till finally a response.\nis a physical response is evoked and and in this case that response could be laughter right so the main.\ntakeaway here is that there's like this massive network of interconnected neurons which are interacting with each other arranged in.", "metadata": {"video_title": "Motivation from Biological Neuron"}}
{"text": "takeaway here is that there's like this massive network of interconnected neurons which are interacting with each other arranged in.\nlayers one layer activating the neurons in the other layer and so on and this layered architecture is something that.\nwe'll see frequently through the course right and when i say it's a massively parallel interconnected network i really mean.\nit right because it has the average human brain has around 10 raised to 11 that is 100 billion neurons.\nright and in this massive network there's also natural division of work right so each neuron may not perform all.\nthe tasks that humans perform like each neuron may not be responsible for processing visual information as well as auditory.\ninformation as well as other information right it might just take care of certain information right and one way to.\ndemonstrate uh this so i'll just first again illustrate with the cartoon and then go to a more real example.", "metadata": {"video_title": "Motivation from Biological Neuron"}}
{"text": "demonstrate uh this so i'll just first again illustrate with the cartoon and then go to a more real example.\nis that you might have this neuron which fires only if the visual is funny there might be other neurons.\nwhich fires only if the speech or the sound is funny right and another neuron which might fire if the.\ntext is funny that means whatever is being said is funny and then you might these three neurons might pass.\non the information to a fourth neuron and that neuron might fire if at least two of the three inputs.\nare on right if two of the three inputs are activated then this is enough for it to fire and.\nit in it turn would do some other action right it might just activate other neurons or might directly be.\nresponsible for taking some action right and here i'd like to demonstrate the visual cortex right of the brain right.\nand it has many layers let me just delete this and then do it again yes so here you have.", "metadata": {"video_title": "Motivation from Biological Neuron"}}
{"text": "and it has many layers let me just delete this and then do it again yes so here you have.\nthe retinas as you're saying this interacts with the outside world and it will be like it's the input that.\nyou are receiving right and then this input as you can follow the arrows it will pass through different layers.\nin the brain right and each layer might do some amount of processing and then pass it on to the.\nother layer so you can process the flow and then finally after going through several layers you can follow the.\narrows that you have here it finally generates some action and this goes to the spinal cord and in this.\ncase it might just be to move the hand right you're seeing something in response to which you want to.\nmove your hand right so that's how information gets processed across multiple layers and now i'll focus on these red.\nparts here right which are labeled as v1 v2 uh v4 right and i'll tell you a bit about uh.", "metadata": {"video_title": "Motivation from Biological Neuron"}}
{"text": "parts here right which are labeled as v1 v2 uh v4 right and i'll tell you a bit about uh.\nwhat they do right again a very simplified uh explanation of what they try to do right so here layer.\none might just be responsible for detecting edges and corners so i'm looking at people sitting in front of me.\nand this layer might just tell me okay there are some dots there are some edges and that's all that's.\nall the information it will process i'll pass on this information to the next layer and this next layer might.\nnow start looking at this information in a more organized or grouped manner that there are feature groups or these.\ntwo edges together seem to form a nose these two edges together seem to form eyes and these two sorry.\nthese two edges form a mouth and these two dots were actually eyes and the next layer might again look.\nat bigger objects coming out from these smaller groups here all of these combine to make a face actually right.", "metadata": {"video_title": "Motivation from Biological Neuron"}}
{"text": "at bigger objects coming out from these smaller groups here all of these combine to make a face actually right.\nso this is how each layer is processed doing a different job and each layer is also doing more and.\nmore complex processing or doing more abstraction of the input that was passed right so this is a very touch.\nexplanation of how the human brain works and that's the main takeaways here is that you have a massively parallel.\ninterconnected network of neurons there are many layers there are neurons which might do spatial things and then pass it.\non to other neurons and information flows from one layer to another that's the main takeaways as far as the.\ndeep learning course is concerned or as far as artificial neural networks are concerned right and as you can read.\nthe disclaimers on this on this slide i know i know very little about how the human brain works and.\nwhatever explanation i have given while is not suitable for any biology course it suffices for the purpose of this.", "metadata": {"video_title": "Motivation from Biological Neuron"}}
{"text": "whatever explanation i have given while is not suitable for any biology course it suffices for the purpose of this.\ncourse so with that disclaimer i'll end this video here and i'll come back and talk to you in module.\n2..", "metadata": {"video_title": "Motivation from Biological Neuron"}}
{"text": "foreign [Music] block within the Transformer architecture and we saw that the entire self-attention can be done in parallel for.\nall the capital T tokens and it all happens to these Matrix multiplications right so in effect what is happening.\nis the following right so this is what is known as a scaled dot product self-attention and this is called.\none head and soon we'll see multiple such heads but we'll get there when we get there but for now.\njust remember this is called cell scaled dot product based self attention so what exactly is happening here uh so.\nthis so this purple box here right this is a scale dot product unit right so this is what is.\nlying inside this purple block Here and Now what is the input to the Box let's see uh so you.\nget the key uh query and the value matrices how are these constructed originally remember at the input all you.\nhad were these H1 to HT right so you had these word representations for the keywords and we were calling.", "metadata": {"video_title": "Multi-headed attention"}}
{"text": "had were these H1 to HT right so you had these word representations for the keywords and we were calling.\nthem as H1 H2 and so on right now from the H1 H2 what happens uh you pass them through.\nthe linear projection right through the WQ W K and W V matrices to get the Q K and V.\nmatrices right so this is just multiplying WQ by these vectors and you just stack up these vectors into a.\nmatrix so that you can use this 2W key multiplied by that H Matrix right let's just call it h.\nthen you get the Q Matrix similarly w k multiplied by this gives you the K Matrix and WV multiplied.\nby this gives you the V Matrix right all of this happening in parallel as uh three Matrix operations then.\nyou get the qkv and then this is what happens inside the self-attention head and at the output what do.\nyou get you get a Z1 you get Z1 Z2 all the way up to ZT right so this is.\nwhat we had seen uh when we ended the last lecture now this is called one head so what what.", "metadata": {"video_title": "Multi-headed attention"}}
{"text": "you get you get a Z1 you get Z1 Z2 all the way up to ZT right so this is.\nwhat we had seen uh when we ended the last lecture now this is called one head so what what.\nI mean by a head here so this is a one such unit which takes the inputs H1 to H2.\nand gives you the elements Z1 to Z2 right or the refined representation Z1 to ZT which also take care.\nof the contextual information because they depend on the key they depend on the value and they depend on the.\nquery right so they take care of the contextual information now you could have one more such block right so.\nyou could have the same block repeated so let me just call this z11z21 and z uh one t right.\nso this is the output of the first attention head similarly you could have the same block repeated where you.\nhave another set of parameters right so let me just call these uh W1 right these are the first set.\nof eight matrices you have similarly you could have another's repeated block right and let me just show it on.", "metadata": {"video_title": "Multi-headed attention"}}
{"text": "of eight matrices you have similarly you could have another's repeated block right and let me just show it on.\nthe next slide we just repeat these two blocks right and now you have head one head two and head.\none is giving you representations Z1 and this is giving you representations Z2 Now by several questions here right why.\nwould you have two heads and then if you are getting these two representations which one do you consider right.\nso we'll answer those two questions but first let us motivate right why would you need multiple heads and we.\nhave already seen this in a different context before right so we want to motivate multi multiple heads in attention.\nright so one head is uh one uh one one scale dot product unit which gives you Z one to.\nZ T right the representation Z1 to Z T and I'm making a case for many such heads and so.\nwhy why is why am I doing this right so we've already seen this in the context of convolutional neural.", "metadata": {"video_title": "Multi-headed attention"}}
{"text": "Z T right the representation Z1 to Z T and I'm making a case for many such heads and so.\nwhy why is why am I doing this right so we've already seen this in the context of convolutional neural.\nnetworks right so where we had multiple filters right so these are three different filters uh operating on the same.\nimage right and each filter essentially does the same thing it has parameters say W1 W2 W3 it just goes.\nover the image and gives you an output feature map right and the reason we wanted to capture have multiple.\nfilters is that we were hoping that each filter May capture a different characteristic from the image right some may.\ndetect edges some may detect blurs and so on right so that's why you had multiple filters right and more.\nthe filters the more abstract representations you could compute right the same argument holds here if you have one self.\nattention then it will capture uh we'll learn one way of capturing the contextual information right but there might be.", "metadata": {"video_title": "Multi-headed attention"}}
{"text": "attention then it will capture uh we'll learn one way of capturing the contextual information right but there might be.\nmore than one way of capturing this contextual information right so you might also want to have a situation where.\nit focuses on animal with a very high attention and you might also want it to focus on some verb.\nwith a very high attention right and both of these might be important because one is indicating that it is.\na pronoun for animal and the other might be indicating that it is the object of this verb right and.\nso in both both of these you might want to capture so one head could learn to give more attention.\nto animal the other I had could learn to give more attention to this war right so just as you.\nhad multiple free filters to capture multiple characteristics from the image you could have this multiple attention headset and let's.\nlet's look at it a bit more carefully right so here's an example so the animal didn't cross the street.", "metadata": {"video_title": "Multi-headed attention"}}
{"text": "let's look at it a bit more carefully right so here's an example so the animal didn't cross the street.\nbecause it was too tired right and I have the same copy of the sentence here and now I'm trying.\nto learn a contextual representation for it and now I want to focus more on walks right because I want.\nto know was what is the subject of it right so it is the subject here so I want to.\nknow that so I want to capture that information by paying more attention to Vos right but I also earlier.\nmade a case so what does that mean if I'm learning the alphas right so if I'm learning say let's.\nthis is one two three four five six seven eight nine ten this is the tenth word so if I'm.\nlearning Alpha 10 then this is Alpha 10 1 Alpha 10 2 right and this is Alpha 10 11 right.\nso this is the uh attention that should be paid on the 11th word when you are Computing a refined.\nrepresentation for the tenth word right and I want this to be high right but I had also earlier made.", "metadata": {"video_title": "Multi-headed attention"}}
{"text": "representation for the tenth word right and I want this to be high right but I had also earlier made.\nthe case that I want the attention on animal to be high because animal is the uh I mean it.\nis referring animal animal is the word for which it is the pronoun in this case right so that means.\nif I were to compute again the weights Alpha 10 and so on then I want this to be high.\nso I also have a case for this to be high I also have a case for this to be.\nhigh so one way of dealing with this would we have two separate attention heads right and compute two Alphas.\none from one block of self attention another from the other block of self attention and this block could learn.\nto give more importance to this Alpha and the other block could learn to give more importance to this Alpha.\nright now of course this is slightly make believe right we understand that because we have already visited this in.", "metadata": {"video_title": "Multi-headed attention"}}
{"text": "right now of course this is slightly make believe right we understand that because we have already visited this in.\nthe case of recurrent neural networks now there is no signal right we are not telling the model that hey.\nyou need to pay more attention to was or hey you need to pay more attention to animal we just.\nhope that when we are looking at the final loss function and if it indeed is beneficial to focus more.\non Wars right that means have a higher weight for Alpha which in turn will contribute uh accordingly when we.\ntry to compute the refined representation for say the tenth word right and that effectively reduces the loss so it.\nwould then learn to have a higher weight for wash right similarly if it helps so this is z 110.\nsimilarly if you had Z coming out from the other attention head and this will also participate in the fuel.\ncomputation and then participate in the loss so if it helps that now this set of Alphas should be such.", "metadata": {"video_title": "Multi-headed attention"}}
{"text": "computation and then participate in the loss so if it helps that now this set of Alphas should be such.\nthat it should focus more on the word animal and if the loss dictates that then the machine would learn.\nthat or the model would learn that right so that we're not giving it an explicit signal we're just hoping.\nthat by Trying to minimize the loss it has more freedom now in some cases it can learn to put.\na high alpha here in some cases it can put a learn to put a high alpha here right so.\nyou're making a better design Choice by allowing it more uh choices right or allowing it more parameters in terms.\nof the W's the projection matrices which in turn result in different Alphas right so just making it a more.\nflexible model which has more choices or more options to how to adjust the alpha in different cases so it.\nmight choose to have Alpha was high for one case Alpha animal high in the other case and then do.", "metadata": {"video_title": "Multi-headed attention"}}
{"text": "might choose to have Alpha was high for one case Alpha animal high in the other case and then do.\nsomething different in the third head and so on right so that's the motivation for having multiple heads so I.\njust already explained this I'll just skip over this the same thing whatever I explained that in one case it.\nwould want to focus on walls the other case it might want to focus question animal and this is actually.\nfrom a actual train transformer right so we looked at the attention weights there were two heads and we looked.\nat what the attention weights were and we found that in one case it is giving higher in one of.\nthe heads that is giving higher attention to animal and the other head it is giving higher attention to Wars.\nright so it does learn to do such things right so this is what a two-headed attention would look like.\nyou would have the Q uh query uh sorry you'd have the query key and value vectors right which are.", "metadata": {"video_title": "Multi-headed attention"}}
{"text": "you would have the Q uh query uh sorry you'd have the query key and value vectors right which are.\ncoming from your projections say this should have been this is the H Matrix right which is H1 H2 all.\nthe way up to h t and you get out here is q k v okay so this is again.\nthe H Matrix and what you get out here is q k v right so you just have these two.\ncopies now this as I said would release Z1 Z2 all the way up to Z T and I'll just.\ncall it Z 1 1 and so on and this would give you Z One Z two although we have.\nto Z T and I'll call it z2s right now what do I do with this I have two representations.\nnow computed for this word H1 to h t right so now what do I do with these two Z's.\nsimple I just concatenate them so that's all I'm going to do so you concatenate it so you get a.\nlarger representation and then you'd pass that to a linear transformation right so we'll see that uh soon and then.", "metadata": {"video_title": "Multi-headed attention"}}
{"text": "larger representation and then you'd pass that to a linear transformation right so we'll see that uh soon and then.\nwhat you get is the final output right so what let's just look at this carefully right what is happening.\nhere so again let me just look at some it's important that I get rid of this okay so this.\nis the H here okay and let's just focus on H1 right now H1 suppose H1 was uh five and.\ntwo dimensional vector right so now what I could do is uh I will choose W to be 512 cross.\n256. okay I'm just giving you some example so that means the projection which comes out right my q k.\nv would be 256 dimensional right because it's 5 and 2 multiplied by a 5 and 2 cross 256 Matrix.\nright or rather actually this would be um yeah so you get it it's I'll get I'll just project it.\ndown so this will be 256 dimensional right so at the output again I'll get 256 dimensional Z's in both.\nthe cases now when they concatenate I again get a five and two dimensional output right and this I could.", "metadata": {"video_title": "Multi-headed attention"}}
{"text": "the cases now when they concatenate I again get a five and two dimensional output right and this I could.\nagain pass it through whatever uh transformation I want right for example I could choose a 512 cross 1024 I.\ncould choose a 512 plus 256 or I could choose a 512 cross 512 right and depending on that I.\nwill know what my output Dimension would be if I choose this then my Z final Z is coming out.\nof here would be 5 and 2 dimensional right so let's just understand it correctly so this H1 gave me.\na 256 dimensional Z1 this through this network or through this self-attention it I got another 256 dimensional H1 and.\nthen I got 512 dimensional uh output here which then again I pass it through a linear transformation right so.\nit's because you're going to concatenate it makes sense that you the output of each of these is small right.\nbecause if each of these is five and two dimensional then you concatenate you will keep going larger right and.", "metadata": {"video_title": "Multi-headed attention"}}
{"text": "because if each of these is five and two dimensional then you concatenate you will keep going larger right and.\ntypically you use eight heads so now if each of this is five and two dimensional and then you concat.\nit in them then you'll get a four zero nine six dimensional output here which is two larger because it.\nincreases the size of the parameters that will go through a linear transformation and so on right so typically what.\nyou do is if you want five and two dimensional size here right then you make sure that your each.\nof your eight heads gives you a 64 dimensional output so when you concatenate then you get a five and.\ntwo dimensional output so you start with the five and two dimensional output you adjust these Dimensions such that you.\nget a 64 dimensional output at each of these heads you have eight such heads so when you concatenate them.\nyou'll again get a five and two dimensional output then you do an appropriate linear transformation you could choose this.", "metadata": {"video_title": "Multi-headed attention"}}
{"text": "you'll again get a five and two dimensional output then you do an appropriate linear transformation you could choose this.\none so that your final z's are also five and two Dimension right so that's what you could do right.\nso this is what a two-headed attention would look like and I've already told you how to extend it to.\nmulti heads you will just have the same block repeated as many times as you want and then finally you.\nwould just adjust all these Transformations right so as I said I could uh let's just look at it again.\nif my edges are five and two dimensional these are H's then I pass them through I multiply them by.\na 64 cross 5 and 2 Matrix so I get 64 dimensional outputs here 64 dimensional outputs here same happens.\nin all the eight heads so when I concatenate them I get a five and two dimensional output right so.\nwhatever I started with I get the same so I can just adjust the parameters accordingly and then I do.", "metadata": {"video_title": "Multi-headed attention"}}
{"text": "whatever I started with I get the same so I can just adjust the parameters accordingly and then I do.\na linear transformation to get my final Z1 to ZT right so remember this concatenation is happening per word right.\nthat means uh the Z1 representations coming out of each of these are getting concatenated here then the Z2 representations.\ncoming out of each of these are getting concatenated here right uh so it's per word so the input is.\na set of words you have capital t word embeddings and the output at this layer or at every layer.\nright here here here at all these layers the output is again capital T embeddings right so that's you should.\nremember that so we are done so we have the multi-headed attention okay so we are back to the basic.\nblock that we had so this is what we had we had these uh inputs coming in here right and.\nthen now we have seen this self attention in detail which could be a multi-headed self attention and I gave.", "metadata": {"video_title": "Multi-headed attention"}}
{"text": "block that we had so this is what we had we had these uh inputs coming in here right and.\nthen now we have seen this self attention in detail which could be a multi-headed self attention and I gave.\nit inputs H1 H2 h t and then through all the processing that happens inside I get outputs I think.\nI was calling these as S1 S2 all the way up to s t right so once I have got.\nthis now I need to understand what happens in the feed forward neural network right so let's uh focus on.\nthat now right and this encoder is typically a stacked encoder so you'll have six such blocks here that's why.\nI'm calling this a basic building block this is one layer right so you passed in H1 to h t.\nyou got out Z1 to ZT now this Z1 to ZT becomes input to another such layer and again you.\nget a new set of representations out from your capital T representations out right this way I've seen that the.\noutput of one layer acts as the input to the next layer right so all of this this looks identical.", "metadata": {"video_title": "Multi-headed attention"}}
{"text": "output of one layer acts as the input to the next layer right so all of this this looks identical.\nin all these blocks and there could be 6 8 12 such blocks depending on the transform architecture that you.\nare looking at now let's see what happens in the feed forward Network so now you had uh you so.\nthese are what the final output is of the feed forward network is z this intermediate output coming out of.\nthe self attention I should have called it s and this is the input H1 right so now what exactly.\nhappens in the feed forward neural network right nothing it's quite simple so remember that each of these guys here.\nis a five and two dimensional representation or some D dimensional representation it is going to pass through a feed.\nforward neutral Network and again give you a d dimensional representation at the output right that's all that is happening.\nhere so feed forward neural network is only acting as a projection layer here right so uh so this is.", "metadata": {"video_title": "Multi-headed attention"}}
{"text": "here so feed forward neural network is only acting as a projection layer here right so uh so this is.\nthe input S1 okay there's some intermediate being computed let me just call it uh say m okay right and.\nthen you get Z1 at the output right so this could again be five and two dimensional input one zero.\ntwo four dimensional projection and then again five into dimensional output and of course there would be a non-linearity here.\nyou could use any non-linearity that you want right and typically it is one of the relu based either gelu.\nor one of those normal non-linearities okay and the same set of parameters right so this here would have some.\nparameters right so you'll have some W's here and then some another set of W's here right so let me.\njust call them W Feed forward Network and let me call this W1 because it's layer 1 and W2 Layer.\nTwo right so the same set of parameters will be used everywhere right so each of these s1s or sis.", "metadata": {"video_title": "Multi-headed attention"}}
{"text": "just call them W Feed forward Network and let me call this W1 because it's layer 1 and W2 Layer.\nTwo right so the same set of parameters will be used everywhere right so each of these s1s or sis.\nwill pass to the same transition and give you the corresponding zi right so that's what I'm going to show.\nwith the animation that the same network is essentially being used everywhere right so you get this same output everywhere.\nright so this uh yeah so you have the same network for each position and uh you use this uh.\nas the this is the non-linearity that you're going to uh use right uh so nothing great happening within the.\nfeed forward neural network whatever output the multi-headed attention gives it just projects it and then gives you back a.\nfinal output right so that's all we have done uh with the uh so that's all we are done with.\nthe uh encoder layer right so this is one layer of the encoder and now I could stack many such.", "metadata": {"video_title": "Multi-headed attention"}}
{"text": "final output right so that's all we have done uh with the uh so that's all we are done with.\nthe uh encoder layer right so this is one layer of the encoder and now I could stack many such.\nlayers but each layer the internal working would remain the same just the output of the previous layer will be.\nthe input to this layer so nothing else changes right so we are done with the encoder part of the.\nTransformer so the encoder is composed of n such identical layers and each layer is composed of these two sub.\nlayers one is the multi-headed attention and the other is the feed forward neural network uh and so the computer.\nis computation is paralyzed in the horizontal direction right so what do I mean by that is that you of.\ncourse so if you have these n layers right of course you cannot compute all the layers in parallel right.\nbecause layer 2 will take the output of layer 1 as input right so unless you have done the layer.", "metadata": {"video_title": "Multi-headed attention"}}
{"text": "because layer 2 will take the output of layer 1 as input right so unless you have done the layer.\n1 computation you cannot do the layer to computation right so when I say it's parallelized it's only within each.\nlayer right so for a given uh set of input samples right within that layer all the self attentions all.\nthe alphas all these Z they'll all get computed in parallel right unless I mean earlier when uh again I'll.\njust repeat this because this is important in the case of an RNN when you are given H1 to h.\nt and you had to compute Z1 to ZT right you first had to compute Z1 z2's and ZT and.\nso on right and here we saw that using this large Matrix multiplications we get Z1 to ZT in parallel.\nright you don't have to wait for the previous time step for the next time step to be computed right.\nso this parallelism you see in every layer but of course across layers the computation is still sequential right because.", "metadata": {"video_title": "Multi-headed attention"}}
{"text": "so this parallelism you see in every layer but of course across layers the computation is still sequential right because.\nyou need the previous layers output to do the next layers computation you have the input then the layer one.\noutputs get computed then it feeds to Layer Two and so on till the end and each of these T.\ncross 5 and 2 outputs get computed in parallel right and now this final output of the uh encoder right.\nwhich is the output from the last layer we are going to denote it as e right so we'll refer.\nto it as E1 to E capital T because there are t such tokens in the input and for each.\ntoken you get this final refine representation which is contextual as well as gone through several layers of abstraction or.\nseveral deep layers right.", "metadata": {"video_title": "Multi-headed attention"}}
{"text": "foreign [Music] which looks like a obvious extension that you would do on Adam right so Adam already introduced the.\nmomentum term and then we know that nag is better than momentum based gradient descent right so Adam which uses.\nmomentum can we make it better by adding the nestrob effect to it which is the look ahead effect that.\nwe need to do right so that's we could that is something that we could do and that's exactly what.\nMadame is which is nastrov is the nestro version of Adam so let's see how that works right so this.\nis the update tool for Adam that we had now from here we'll now revisit uh nag The nestro Accelerated.\ngradient descent and then see how to modify the update tools for Adam to get in this natural Factor here.\nokay so recall the equation so now one thing which I have done here is that I have so remember.\nthat you could have beta U T minus 1 plus 1 minus beta times uh Delta WT right so both.", "metadata": {"video_title": "NADAM"}}
{"text": "okay so recall the equation so now one thing which I have done here is that I have so remember.\nthat you could have beta U T minus 1 plus 1 minus beta times uh Delta WT right so both.\nthese forms are popular right so where you have beta and 1 minus beta where of course they sum up.\nto one or you could just have beta and then no weight multiplied by the derivative of WT right so.\nwhen we had originally introduced momentum we had used this equation but from when you used it in atom we.\nwere using this equation so both are popular so when I start off I will stick to this but at.\nsome point I will switch to this equation so just remember that both of these can be used interchangeably right.\nso when I start off I'll just use this because it's easier to convey what I want to convey but.\nat a later point I'll switch back to the this version of momentum okay um now you recall the equation.\nfor Mac the main idea in nag was that instead of looking at the gradient at time step T you.", "metadata": {"video_title": "NADAM"}}
{"text": "at a later point I'll switch back to the this version of momentum okay um now you recall the equation.\nfor Mac the main idea in nag was that instead of looking at the gradient at time step T you.\nfirst did the look ahead and then computed the gradient right that was the main idea in nag and then.\nthe rest of it of course Remains the Same right now what we're going to do is we're going to.\nrewrite nag in a more convenient way and then import it into atom right or that rather than Port it.\nto atom right so this is what we were doing in nag we were Computing a quantity called GT which.\nwas the gradient okay let me just write down the equations all of it first and then that becomes easier.\nfor me too explain okay okay so I'll just now walk you through the slide so we were Computing the.\ngradient at the lookahead value right and once you compute the gradient at the look ahead value then you are.", "metadata": {"video_title": "NADAM"}}
{"text": "gradient at the lookahead value right and once you compute the gradient at the look ahead value then you are.\nComputing uh empty as the history plus the gradient at the look ahead value and then you are updating the.\nnew value as uh uh as this updated history right that's what you were doing now one thing I want.\nyou to notice here is the time steps that you have you have t you have T minus 1 and.\nyou have P plus 1 also right and it looks a bit cumbersome and we're going to try to rewrite.\nthat a bit more compact it that's one challenge the other challenge is what is written here but I'll come.\ncome to that right so let's now see what happens uh maybe I'll just go back a bit yeah so.\nlet's say you started at w0 okay and you computed a derivative at this point so which would be somewhere.\nhere it'll just screw it on the plot so that's where you have computed the derivative then at m0 uh.", "metadata": {"video_title": "NADAM"}}
{"text": "here it'll just screw it on the plot so that's where you have computed the derivative then at m0 uh.\nsince you have remember that we have kept the learning rate as one I have initialized it here in the.\ntop right corner here right uh so ETA is equal to 1 and of course n minus 1 is 0.\nat time step minus 1 you don't have any history so here m0 is going to be beta times 0.\nplus G 0 so it's just going to be G 0 so far so good and then W1 is going.\nto be W 0 minus uh let me just choose the pen so I'll just do this once so there.\nshould have been an ETA here but ETA is set to 1 that's why I'm not using ETA right so.\nremember that ETA is always there in all all the stuff that I'll do in the next two slides is.\njust that the value of ETA is equal to 1 right and then you w 0 minus the derivative computed.\nat uh yep uh yeah so instead of calling it GT GT in this case of course turns out to.\nbe Delta is of 0 itself but actually it would have been as if I follow this equation it would.", "metadata": {"video_title": "NADAM"}}
{"text": "at uh yep uh yeah so instead of calling it GT GT in this case of course turns out to.\nbe Delta is of 0 itself but actually it would have been as if I follow this equation it would.\nhave been Delta W 0 minus ETA into beta into M minus 1 and this quantity just turned out to.\nbe 0 so that's how I am writing it here okay so that's what your W1 is now let's go.\nahead so if so this is where I have reached now now at G1 I am doing this look ahead.\nright so when I'm Computing G1 I am actually looking at the derivative at a slightly further Point I've already.\nmoved by the history and then computed the derivative so that's where the lookup is happening right so that this.\nis the point so now I'm looking ahead and Computing the derivative and now that is where look ahead happens.\nin the original nag equation right and we are going to slightly modify that so now what is happening is.\nthat this yeah so now I have after doing the look ahead I have reached somewhere I have made that.", "metadata": {"video_title": "NADAM"}}
{"text": "that this yeah so now I have after doing the look ahead I have reached somewhere I have made that.\nupdates uh and then I've read somewhere and then I have computed my history Vector again this is my history.\nVector that I have computed again and then I've used that to make my final update right and this is.\nwhat my final update looks like it's w 1 minus M1 right because I'm just substituting uh values in this.\nequation now let me just use the pen so I'm just substituting values in this equation my ETA is equal.\nto 1 and time step is 1 so W1 minus M1 is what I get and now if I substitute.\nthe value of M1 it turns out to be this quantity right it's beta M naught plus Delta W 1.\nminus beta M naught okay so now what is happening here is that if you look at this m naught.\nright which is M of T minus 1 it is used twice right first it is being used to make.\nthis temporary update to the value okay you make that temporary update and then switch back to the original value.", "metadata": {"video_title": "NADAM"}}
{"text": "right which is M of T minus 1 it is used twice right first it is being used to make.\nthis temporary update to the value okay you make that temporary update and then switch back to the original value.\nagain right we because you are not using the temporary value again right so you're switching back to the original.\nvalue so using the B time not here and then again you are using beta I am not here and.\nthen again you are switching back from this temporary computed value that you had to the original value that you.\nhave right so you're making some extra computations here which could be our uh kind of done away with that's.\nthe goal that we want to achieve while giving you the same effect right so now what is happening here.\nis that you look at G1 and it's getting some input which was W1 and then it is subtracting some.\nquantity from it instead if I already had passed it a value which was already equal to this right because.", "metadata": {"video_title": "NADAM"}}
{"text": "quantity from it instead if I already had passed it a value which was already equal to this right because.\nthis I already had the previous time Step at the previous time step I already had M naught so if.\nat the previous time previous time step itself I could have computed an updated value and send it here then.\nI wouldn't have had to do this here right so that's the idea and you can see that what is.\nhappening here is that you have t t minus 1 and t plus 1 uh these are the three time.\nsteps that you see here right and instead can we write everything in terms of t and t plus 1.\nand push the look ahead uh such that I don't have to do these computations again right so that's what.\nwe want to do so let's see what we are going to do about it so the key thing here.\nis that the vector M naught or M minus 1 or M of T minus 1 is being used twice.\nthen you're doing this temporary computation which is W1 minus beta M naught and you compute the gradient there and.", "metadata": {"video_title": "NADAM"}}
{"text": "is that the vector M naught or M minus 1 or M of T minus 1 is being used twice.\nthen you're doing this temporary computation which is W1 minus beta M naught and you compute the gradient there and.\nthen you forget about this temporary computation you never use that again and then again you is in the last.\nequation you Resort back to the value of W1 and then make an update from there right so that's something.\nwhich is we want to get rid of and simplify this right so let's see if we can rewrite the.\nequations so this is what W1 was right so W1 was W naught okay so W1 was W naught minus.\nthis and this is the calculation that I don't like right because I could have done this earlier and sent.\nit a value which would have already taken care of this look ahead right and then I don't need to.\ndo one more look ahead here so that's the thing because we already had beta M naught so this quantity.\nthat you have here is beta M naught and I already had this at the previous iteration right so why.", "metadata": {"video_title": "NADAM"}}
{"text": "that you have here is beta M naught and I already had this at the previous iteration right so why.\nam I doing this in the next iteration why am I coming up and Computing this in the next iteration.\nhere why am I doing it that's the question that we are trying to answer right so can we uh.\nsorry I should have let me just go back sorry I Circle it the wrong so I'll start my expression.\nagain so when I'm Computing G1 I am doing this temporary computation which I don't want to do right and.\nthe what my argument is that at the previous time step I already had this beta m not right because.\nthat was already computed at the previous time step could I have done something at the previous time step and.\ngot an effective value of w which had already encompassed this beta I am not movement and then whatever w.\ni get here I just compute the gradient of that right so I'm doing the liquid in advance and then.\nsending it to the next time step that's the idea that we want to get to is it making sense.", "metadata": {"video_title": "NADAM"}}
{"text": "i get here I just compute the gradient of that right so I'm doing the liquid in advance and then.\nsending it to the next time step that's the idea that we want to get to is it making sense.\nso this is what we will do right so now we had W1 if you look at W1 minus beta.\nM naught right so this is W1 minus beta M naught which is effectively this so you have what was.\nW1 the way you computed uh W1 was you had started with W naught minus ETA times this lookahead gradient.\nthat you had computed and then you did this minus beta M naught right this is what you had done.\nto get the value of uh uh uh so this is what was happening let's say if I do W.\n1 minus beta M naught this is what you will get and this already has this look ahead incorporated into.\nthis right which look ahead I was doing here what if I just do the look ahead in the previous.\niteration itself and when I'm Computing uh W1 what if I just put in this quantity also a over there.", "metadata": {"video_title": "NADAM"}}
{"text": "this right which look ahead I was doing here what if I just do the look ahead in the previous.\niteration itself and when I'm Computing uh W1 what if I just put in this quantity also a over there.\nforeign so now I am rewriting the equations and this is how I'm going to do when I compute GT.\nplus 1 I'm just going to compute the derivative at the current value of w there is no look ahead.\nhappening here then I'm going to just compute the Mt plus 1 using the equations that I have okay and.\nthen I'm going to compute WT plus 1 as WT minus ETA so I have taken the look ahead value.\nhere and then the GT plus one right so now what the effectively what I'm doing is the following foreign.\nI computed the look I had now look at is computed only at the momentum step which is Mt plus.\n1 and then I look at W1 and I'm going to compute the look ahead of the gradient in the.\nnext time step right so I've just changed the time steps a bit now notice in my new equations uh.", "metadata": {"video_title": "NADAM"}}
{"text": "1 and then I look at W1 and I'm going to compute the look ahead of the gradient in the.\nnext time step right so I've just changed the time steps a bit now notice in my new equations uh.\nthere is no WT minus one right everything in terms of t and t plus 1 so an easier way.\nto look at this is that if you have a while loop if you put one step outside the while.\nloop right then all your computations go like one step behind that right so that's that's what you are doing.\nhere you're just doing the look ahead before and then coming into the while loop so then you don't have.\nto do the look ahead here and that is simplifying all your computations because now T minus 1 is nowhere.\nhere at this point you are not Computing a temporary value of w you are just Computing a new value.\nof w at one go in the previous case you had to compute a new value of w twice once.\nfor the temporary look ahead and then one more time when you are updating right in this equation now all.", "metadata": {"video_title": "NADAM"}}
{"text": "of w at one go in the previous case you had to compute a new value of w twice once.\nfor the temporary look ahead and then one more time when you are updating right in this equation now all.\nthis you have income passed into one single equation right so that's why you have Rewritten nag and the reason.\nwe're doing this is now based on this set of equations now these are the set of equations that we.\nwant to bring to Adam right and that then the update rule for Adam will look quite simpler so this.\nis what the update rule for nadum is so you have the same thing as Adam this is also same.\nas Adam this is also same as Adam this is also same as atom so nothing has changed so far.\nthe only thing now you have done is at this step you're doing the look ahead the we are updating.\nbasis on the history as well as the current gradient so for a minute let's just ignore this these quantities.\nright so if you ignore these quantities your update rule is simply of WT minus ETA square root of VT.", "metadata": {"video_title": "NADAM"}}
{"text": "right so if you ignore these quantities your update rule is simply of WT minus ETA square root of VT.\nplus 1 the bias corrected Value Plus Epsilon right into M hat t plus 1 plus Delta WT so here.\nyou have you're just taking the gradient at the current time step and the history now all the other multiplying.\nfactors that you see here those are coming so this is actually uh the uh yeah let me just make.\nit Mt Plus 1. now if this if I ever to bias correct it then I would divide it by.\n1 minus beta raised to t plus 1 and I'll have to do a similar bias correction here also well.\nagain divided by 1 minus beta raised to t plus 1 and then this beta and 1 minus beta are.\nsimply the contributions that I have for the history and the current gradient right so that's how I get it.\nso it's a bit messy uh I I couldn't find a better way of explaining this if you have suggestions.\nI'm open to using them but the main idea here is this the takeaway is this that you add the.", "metadata": {"video_title": "NADAM"}}
{"text": "so it's a bit messy uh I I couldn't find a better way of explaining this if you have suggestions.\nI'm open to using them but the main idea here is this the takeaway is this that you add the.\nnestrov factor into the atom equations which means you do a look ahead and to make this look ahead a.\nbit less Messier in terms of writing and making sure that most of your equations are same as atom and.\nyou just push your head into the last look ahead into the last equation uh we did that rewriting of.\nNag and we just explained that instead of doing something at T minus 1 then t and t plus 1.\nwe had everything in terms of t and t plus one so the look ahead was already done before I.\nwent to the next iteration hence I did not need to do a look ahead again in the next iteration.\nright and once you understand that then this equation is just the just a manifestation of that and the all.\nthe one minus beta beta terms that you see here now let me just clear the all the annotations these.", "metadata": {"video_title": "NADAM"}}
{"text": "the one minus beta beta terms that you see here now let me just clear the all the annotations these.\nare simply because when we are doing nag or when we are discussing whatever we discussed before coming to this.\nslide we did not have this one minus beta terms and this one minus beta term shows up here and.\nthe beta term shows up here and the denominator is simply corresponding to the bias correction that we do right.\nso so that's that's all about madame and now let's see how it works in practice so look at an.\nexample uh yeah so I'll just run it and you can see the same nestro effect here right so the.\nblue curve corresponds to an atom and I'll just run it again for your benefit you can see that the.\nblue curve has smaller oscillations as compared to the black curve and that's the natural effect that you were hoping.\nfor right so that's that's all about madame uh so now which Optimizer to use I think uh uh the.", "metadata": {"video_title": "NADAM"}}
{"text": "for right so that's that's all about madame uh so now which Optimizer to use I think uh uh the.\nmost common I guess the default one that I have seen in most papers is Adam if you don't know.\nanything maybe just go with Adam however there are recent Works which are not so recent also there also three.\nto four years old now which do point out some issues with Adam still by far at least for all.\nthe Practical work that I've seen Adam seems to be a very common choice and if not Adam one of.\nthe variants of Adam right maybe an Adam or uh yeah I think Madame is another good choice right so.\nuh if you don't if you're a new learner just use off-the-shelf optimization optimizers and or Adam is as good.\na choice as any and some of them you could then practice a bit more so people have reported that.\njust using gradient descent and then using or maybe momentum based gradient descent and then really knowing how to set.", "metadata": {"video_title": "NADAM"}}
{"text": "just using gradient descent and then using or maybe momentum based gradient descent and then really knowing how to set.\nthe learning rate initial learning rate how to set the momentum value how to set the learning rate schedule if.\nthey know all of these well then they're able to get as good results as Adam right but it takes.\na while to get that kind of an expertise in that absence of that uh just using Adam simply off.\nthe shelf might be a good choice to use okay so that's all we had on the different uh optimizers.\nto use now I'll just spend some more time on talking about learning rate schedules okay.", "metadata": {"video_title": "NADAM"}}
{"text": "foreign [Music] so first let's see people who try to hint that this is perhaps due to better optimization right.\nso what is optimization problem that we're trying to solve this is the problem optimization problem we are trying to.\nreduce the mean squared error so what is the optimization problem that we're trying to solve this is the problem.\noptimization problem we are trying to reduce the mean square error cross into velocity whatever on the training data right.\nuh is it the case that in the absence of unsupervised pre-training this problem itself is hard you cannot really.\nreduce L Theta to zero but now once you do unsupervised pre-training L Theta is able to reduce to zero.\nhence earlier you had poor optimization but now you have better optimization right so let us see this right so.\nthe error surface as I said for a deep neural network is highly non-connection it has these many plateaus many.", "metadata": {"video_title": "Optimization or Regularization"}}
{"text": "the error surface as I said for a deep neural network is highly non-connection it has these many plateaus many.\nvalleys some very steep points where you go in and then it's very hard to come out and so on.\nright and somewhere there would be multiple Minima then possibly one Global Minima and so on it or multiple equal.\nminimize so it's like a hardly a highly con complex non-convex surface as opposed to a nice convex surface we.\njust have one Minima right so uh given the large capacity of deep neural networks it is still easy to.\nland in one of these zero error regions right so there might be these multiple zero error regions of course.\nin this diagram there is only one visible but there are a few behind which are all at a zero.\nerror level right so given this large capacity it's still possible to land in these zero error regions and why.\nam I saying that which theorem am I using while saying this the universal approximation theorem right it said that.", "metadata": {"video_title": "Optimization or Regularization"}}
{"text": "am I saying that which theorem am I using while saying this the universal approximation theorem right it said that.\nyou could sufficient you could consider construct a sufficiently large neural network to get an arbitrary degree of precision that.\nmeans you could get an arbitrary degree of low error and in particular you could drive the error to Zero.\nby just having a large number of neurons in your deep neural network so I know that despite this non-convex.\nsurface highly complex surface I can still drive the training error to zero right so maybe optimization was not a.\nproblem as long as the Deep neural network has a large capacity right for the universal approximation theorem it talks.\nabout large capacity in the term of really large right I mean you are talking about exponential number of neurons.\nbut maybe even not going to the exponential level with sufficiently large capacity which is reasonable I could still drive.", "metadata": {"video_title": "Optimization or Regularization"}}
{"text": "but maybe even not going to the exponential level with sufficiently large capacity which is reasonable I could still drive.\nthe training error to zero so then someone did experiments to prove that whether this is indeed the case right.\nso what they did is they took a deep neural network and they increased the capacity of the last layer.\nright so just before the prediction they increase the capacity what does that mean you add large number of Weights.\nin the last year how do you do that you add a large number of neurons in the layer before.\nit and now from this large number of neurons you're trying to predict one output so you have many weights.\nhere right so for example I could have say a network which has say some let's say 100 input neurons.\nthen I have a few layers which have 20 20 neurons right then I have the last layer maybe it.\nhas 100 neurons or even more than that right so now I have a lot of Weights here right I.", "metadata": {"video_title": "Optimization or Regularization"}}
{"text": "then I have a few layers which have 20 20 neurons right then I have the last layer maybe it.\nhas 100 neurons or even more than that right so now I have a lot of Weights here right I.\nhave 20 into 100 weights here right so the capacity in the last layer has increased and from this last.\nlayer I'm going to again predict say 10 outputs right so then I again have 100 into 10 weights there.\nnow if I increase this 100 to 1000 then my capacity increases further right so I can just increase the.\nnumber of neurons in the last layer and that would result in an increase in the number of Weights associated.\nwith that layer right and hence your neural network would become large capacitor that's exactly what they did and they.\nshowed that if you do this then you're able to drive the error to zero even without training right so.\nwhat are they saying that that if you have large capacity then you don't need pre-training right but large capacity.", "metadata": {"video_title": "Optimization or Regularization"}}
{"text": "what are they saying that that if you have large capacity then you don't need pre-training right but large capacity.\nof course comes at a cost right your network becomes large and training it takes more time and so on.\nso maybe when you are not able to construct large capacity neural networks in that case pre-training is useful right.\nso that's what they showed that if you don't have large capacity in the neural network then pre-training is still.\nbecoming uh uh useful right so what is it that they're seeing in effect that we always knew that in.\ndeep neural networks you can drive the error to zero because the universal approximation theorem says that but the hidden.\ncatch there was that you're talking about really large neural networks who can do that so what these guys showed.\nthat is indeed the case if you have a large neural network even without pre-training you can write the error.", "metadata": {"video_title": "Optimization or Regularization"}}
{"text": "that is indeed the case if you have a large neural network even without pre-training you can write the error.\nto zero but when you have smaller capacity networks they are not able to drive the error to zero that.\nmeans they find it hard to solve the optimization problem itself and then if you add pre-training then they are.\nable to drive the error to zero that means pre-training is leading to better optimization right so that's the argument.\nthat they made of course this is empirical ah and these experiments maybe were much smaller scale as compared to.\nwhat we see in deep learning today but that's not the point right I mean a lot of these were.\ninitial days and people were still figuring out what to do but it led to this important Insight that perhaps.\nit is helping in optimization and if you can improve the optimization then you can train large neural networks so.", "metadata": {"video_title": "Optimization or Regularization"}}
{"text": "it is helping in optimization and if you can improve the optimization then you can train large neural networks so.\nhey let's focus on designing better and better methods of optimization on supervised pre-training is one such method but what.\nwhat are the other possibilities there now let us look at the other view here right sorry this should have.\nbeen generalization we already saw it should have been regularization right so does it is it because of better regularization.\nsome people try to argue that too and let's see what that argument was right so what does regularization actually.\ndo it constrains the weights to lies within certain regions of the parameter space right we saw this when we.\nwere doing L2 regularization that it constrains the weights to dry in some circle right because you are not allowing.\nthe magnitude to grow beyond that that means only those weight configurations which lie on this circle are possible or.", "metadata": {"video_title": "Optimization or Regularization"}}
{"text": "the magnitude to grow beyond that that means only those weight configurations which lie on this circle are possible or.\nwithin that are possible you cannot go outside that boundary similarly Urban regularization restricts you within this diamond that you.\nsee there right and we even saw with early stopping that is the effect that you don't allow your weights.\nto grow too much right so you are constraining the weights to lie in certain regions of the parameter space.\nright that's one way of saying it right instead of saying lying in a small region or something it constrains.\nit to rise in certain regions right now is the same happening in the case of unsupervised pre-training right indeed.\nunsupervised pre-training is also causing the weights to lie in certain regions of the parameter space what are these weights.\nso these are the weights or these are the regions where the weights are better able to capture capture the.", "metadata": {"video_title": "Optimization or Regularization"}}
{"text": "so these are the weights or these are the regions where the weights are better able to capture capture the.\ncharacteristics of the input data why am I saying that because where did you start with you started with this.\nunsupervised objective you said that forget about my main loss function which depends on y first for every layer I.\nwant to solve this unsupervised objective the moment you do that for every layer you are putting the weights in.\ncertain region of the parameter space where this objective is getting minimized right so they are this is the entire.\nsay weight space now you have gone to some region there where this weights this objective is getting minimized once.\nyou have minimized this Omega Theta then you are throwing in L Theta right but you are starting from the.\nweight initialization here itself now this entire space is not available to you at all right and now once you.", "metadata": {"video_title": "Optimization or Regularization"}}
{"text": "weight initialization here itself now this entire space is not available to you at all right and now once you.\nstart optimizing L Theta from there you will be moving from this region right so again you will be constrained.\nto region around that of course you may move out of that region right but you have done the initialization.\nin a certain region and now from there on if you train you are going to move wherever you move.\nis going to be governed by where you restricted your initial weights to B right so that in that sense.\nis acting like a regularizer and in fact even if you look at the regularization ah the way we studied.\nis that there is an L Theta and an Omega Theta the same thing is happening here right you first.\nhad an Omega Theta you said I want to minimize that and then I will throw in L Theta and.\nthen I'll try to minimize L Theta so in the case of regularization we were seeing that we wanted to.", "metadata": {"video_title": "Optimization or Regularization"}}
{"text": "had an Omega Theta you said I want to minimize that and then I will throw in L Theta and.\nthen I'll try to minimize L Theta so in the case of regularization we were seeing that we wanted to.\nminimize the loss function as L theta plus Omega Theta right this is what our regularized loss function was I.\nam doing something similar here it's just that I am doing it sequentially I am first minimizing Omega Theta and.\nthen I'm minimizing L Theta so in this view it is acting as a regularizer right and it also matches.\nthis other view that you're kind of constraining the weights in a certain region and then that will govern how.\nyour optimization is going right so in L2 you add a certain way of doing that constraint ah constraining the.\nweights to a certain region in early stopping you had a certain way of doing that here also you have.\na certain way that you first train that I want these individual layers to learn well wherever you end up.", "metadata": {"video_title": "Optimization or Regularization"}}
{"text": "a certain way that you first train that I want these individual layers to learn well wherever you end up.\nnow from there start optimizing for the main problem which is L Theta right so that has the same effect.\nso if you look at this view then it's actually acting as a regularizer right so both things could be.\npossible right and some other ah experiments have also shown that retaining is more robust to random initializations what does.\nthat mean so this is what it means right so they trained deep neural networks let's focus on this plot.\nfirst of no maybe on this plot first with different number of layers right so I have a deep neural.\nnetwork which has some W Capital parameters like some large number of parameters and I initialize these parameters randomly and.\ntrain the network once right and then I got certain laws I note what that loss is L1 or the.\nerror which is L1 then I again initialize the weights again train the network again noted the loss right so.", "metadata": {"video_title": "Optimization or Regularization"}}
{"text": "error which is L1 then I again initialize the weights again train the network again noted the loss right so.\nI did this some 100 times and then I am Computing the variance actually I've plotted the box plot so.\nwhich tells me the variance which among other things tells me the variance and they're saying that for shallow networks.\nthis variance is not very large right but when you have a deep neural network every time you do a.\ndifferent regularization a different initialization and then you compute the loss then my loss varies a lot across this different.\ninitializations what does that mean that these networks are not robust to initialization it is very specific to what initialization.\nI have done but now if I throw in unsupervised pre-training and then I do the same thing now again.\nI have first unsupervised pre-trained the weight so I'll just call them Wu right uh and now I am doing.\nthis experiment a hundred times so what am I doing that that I have started with some uh W random.", "metadata": {"video_title": "Optimization or Regularization"}}
{"text": "this experiment a hundred times so what am I doing that that I have started with some uh W random.\ninitialization okay so this is one experiment I end up with some w u after uh pre-training right and then.\nI compute my final loss okay then I took W2 which is a different initialization again got a different W.\nmaybe right and then I computed L2 and this way I did this L 100 times so again I am.\ndoing different weight initializations but I am passing through an intermediate layer of unsupervised pre-training now if I look at.\nthe variance in these quantities then even for deep neural networks is actually quite low right so it's kind of.\nmaking it more robust to random initializations right so now this led to the idea that maybe the whole deep.\nneural networks are sensitive to initialization so now can we come up with better methods of initialization so that sparked.\ninterest in that the conclusions of these experiments are not as important as the research directions that they led to.", "metadata": {"video_title": "Optimization or Regularization"}}
{"text": "interest in that the conclusions of these experiments are not as important as the research directions that they led to.\nright so this now when I look at this I feel hey maybe initialization is an important thing and I.\nshould try to come up with better initialization methods and people did that and came up with better initialization methods.\nand now that started making training deep neural networks even better right so what happened roughly is this right so.\nif I were to summarize this period between ah 2006 to 2009 right uh people thought that okay first was.\nthat unsupervised pre-training works right people did really completely understand why does it work but people started investigating it through.\ndifferent lenses one set of people analyze it to the lens of optimization is it leading to better optimization it.\nled to some conclusive non-conclusive answers but it did seem like maybe it's leading to better optimization similarly people started.", "metadata": {"video_title": "Optimization or Regularization"}}
{"text": "led to some conclusive non-conclusive answers but it did seem like maybe it's leading to better optimization similarly people started.\nlooking at it through the lens of regularization and thought oh looks like regularization is what it is doing people.\nlooked at it from the lens of initialization and thought hey maybe it leads to better initialization right and then.\nthese lens became important that oh all of this seem to be important so why let me focus on better.\ninitialization methods better regulation better optimization methods and so on and that is what has happened right deep learning has.\nevolved since 2009 people came up with better optimization algorithms and we saw a series of those people came up.\nwith better regularization methods and we saw a series of those some was already existing L2 L1 all of that.\nwas existing ah uh early stopping also to an extent but it got popularized in deep learning and then Dropout.", "metadata": {"video_title": "Optimization or Regularization"}}
{"text": "was existing ah uh early stopping also to an extent but it got popularized in deep learning and then Dropout.\nwas something which just specifically came in the context of deep learning right and now today we are going to.\ntalk about activation functions maybe activations if I change then maybe perhaps something could happen and better weight initialization strategies.\nbecause some of these studies also saw that maybe initialization is what it is doing right so that's the context.\nof this lecture or the context of the past two lectures also right where we looked at a series of.\noptimization methods regularization methods so now you know that why we were studying that right because of this spark that.\nhappened in 2006 and that led to some investigations and pointed out in this direction that hey let's focus on.\nthese four areas and then maybe we'll be able to better train the Deep neural networks right so today we.", "metadata": {"video_title": "Optimization or Regularization"}}
{"text": "these four areas and then maybe we'll be able to better train the Deep neural networks right so today we.\nare going to focus on these two parts activation functions and weight initialization strategies so I'll end this video here.\nand we'll then start the discussion on activation functions.", "metadata": {"video_title": "Optimization or Regularization"}}
{"text": "[Music] okay so now let's talk about output functions and loss functions right so this is where we were we.\nwanted to understand how to choose a loss function and that in turn would depend on what your output function.\nis so let's look at that right so the choice of loss function depends on the problem at hand and.\nwhat i'm going to do is that i'm going to talk about two very popular problems side one is regression.\nand this other is classification and in most cases like 90 of the problems that you deal with would fall.\nin one of these two categories right so if you do these then we are largely covered from an application.\nor perspective right even from the understanding perspective right so let's look at what these two examples are so the.\nfirst example is that i have an input about movies and i want to predict the ratings for the movies.\nright and these ratings could be the imdb ratings or the critics rating or the rotten tomatoes rating right now.", "metadata": {"video_title": "Output functions and loss functions"}}
{"text": "right and these ratings could be the imdb ratings or the critics rating or the rotten tomatoes rating right now.\ndeliberately i have moved to a setup where you have an input which is n-dimensional and now you have an.\noutput which is k dimensional which in this case is three dimensional right so just to make sure that you.\nunderstand that it may not be always the case that we're just trying to predict one value using the same.\ninformation about the input so just imagine that i have taken a lot of inputs a lot of movies from.\nthe past right and for each of these movies i have their input representation which is who was a director.\nactor blah blah and then i also have the output and this output is just three dimensional right i have.\nthe imdb rating i have the critics rating and i have the rotten tomatoes it's all three are available to.\nme and now using this input data right so this becomes my x comma y pairs i want to train.", "metadata": {"video_title": "Output functions and loss functions"}}
{"text": "the imdb rating i have the critics rating and i have the rotten tomatoes it's all three are available to.\nme and now using this input data right so this becomes my x comma y pairs i want to train.\na model which takes in as x as input right and gives me these three ratings as output which is.\nthe same as saying that it gives me a three dimensional output right so that's that's what it is now.\nhere i belongs to r3 as i said so now the loss function that we want should be able to.\ndeviate how much does the predicted value should be able to capture how much does the predicted value deviate from.\nthe true value right now here you have a vector as the true vector of true values and vector of.\npredicted values and you want to find the difference between them so the obvious loss function would be these mean.\nsquared error loss function right so you have three values that you're predicting for each of these values you calculate.", "metadata": {"video_title": "Output functions and loss functions"}}
{"text": "squared error loss function right so you have three values that you're predicting for each of these values you calculate.\nthe square of the difference between the true value and the predicted value you sum it over all the n.\ntraining examples that you have and then take the mean or the average so this is the mean squared error.\nthat you would compute so this we have already seen it when you're trying to predict real values it's best.\nto take the squared error loss because it computes the deviation between the true value and the captured value right.\nnow a related question here should be what should the output function be right so you have the output function.\nsitting here right so this dark green guy is the output right and i'm taking a3 and i'm going to.\npass it through some output function and that is going to give me uh the output y hat which is.\nshown as the dark or the shaded green part here right so what should my output function be right so.", "metadata": {"video_title": "Output functions and loss functions"}}
{"text": "pass it through some output function and that is going to give me uh the output y hat which is.\nshown as the dark or the shaded green part here right so what should my output function be right so.\nnow let me ask you a question it can it be the logistic function right so we had seen the.\nlogistic function which takes any input right so a3 would be the input and i am asking whether o can.\nbe the logistic function right and what would that mean it would just take the input and it would compute.\nthe logistic function for that right now can that uh is is that okay is the question right so it.\njust should be it will compute it element wise eight so a three one a three two a three three.\nis what it will compute and that would be y hat one y hat two y hat three right because.\nthe three values that i'm trying to predict so is that a good choice right and as all of you.\nrealize it wouldn't be a good choice because the logistic function i know is going to clamp all my outputs.", "metadata": {"video_title": "Output functions and loss functions"}}
{"text": "realize it wouldn't be a good choice because the logistic function i know is going to clamp all my outputs.\nbetween zero to one right so the logistic function as we had seen is something like this right so it.\nthe output goes just from zero to one right so no matter what the input is my outputs are going.\nto lie between 0 to 1 right now why is that not good here because here i know that the.\nratings are on a scale of 0 to 10 in fact they could also be on a scale of 0.\nto 100 so then if i choose the output as sigmoid function then my network is always going to predict.\nvalues between zero to one whereas my true values are between zero to ten or zero to hundred and then.\nthat doesn't make sense right so i need to predict be able to predict uh unbounded values or values having.\na much higher range than zero to one right so that's what the constraint that i have and in such.\ncases when your output is going to be a real value right and not just bounded between zero to one.", "metadata": {"video_title": "Output functions and loss functions"}}
{"text": "a much higher range than zero to one right so that's what the constraint that i have and in such.\ncases when your output is going to be a real value right and not just bounded between zero to one.\nthen the best thing to use is just use a linear activation function what does that mean so i have.\ndone all the computations up to this point right that means up to this white portion right of the green.\nneuron right and that what i get there is a l a l is of course a l 1 a.\nl 2 a l 3 right in this case it's a collection of 3 elements now i'm going to just.\nmultiply it by a matrix w o okay and what is the dimension of that w going to be it's.\nagain going to be 3 cross 3 right because at the output again i want h right nh is again.\n3 cross 3 right i want 3 values there sorry 3 cross 1 right and which is just the same.\nas y hat 1 y hat 2 y hat 3 right so i want 3 values i have input which.\nis a 3 cross 1 vector so i should multiply it by a 3 cross 3 matrix right so this.", "metadata": {"video_title": "Output functions and loss functions"}}
{"text": "as y hat 1 y hat 2 y hat 3 right so i want 3 values i have input which.\nis a 3 cross 1 vector so i should multiply it by a 3 cross 3 matrix right so this.\nis the transformation that i'm going to do i have some values computed here now i am going to just.\nmultiply it by weight vector w and now plus add the bias right so that's always there i'm going to.\nadd the bias also and i'll get a three dimensional output and those are my values and these are not.\ngoing to be bounded between 0 to 1 because my w is not bounded my a is not bounded my.\nb is not bounded right so that's that's what i'm going to use but now the question that you could.\nask is that if a is not bounded b is not bounded w is not bounded this could take out.\nany values then how am i sure that my uh algorithm will not start or my y hats which i.\nam predicting it which is the output here why would it be like thousand ten thousand 100 and so on.", "metadata": {"video_title": "Output functions and loss functions"}}
{"text": "any values then how am i sure that my uh algorithm will not start or my y hats which i.\nam predicting it which is the output here why would it be like thousand ten thousand 100 and so on.\nwhereas actually i just want values between 0 to 10 right so i want that the output should be 0.\nto 10 right that's why i did not choose the logistic function because i told you that logistic function will.\ngive you between 0 to 1 right then perhaps i should have chosen something which gives me values between 0.\nto 10 i did not do that i chose it choose a linear function which is not bounded it could.\ngive me any values it could give me 10 thousands 1 lakhs and so on so then how how how.\nwould you how would the network deal with the situation right so the answer to this as most of you.\nwould understand is that it's through the loss function right so now if my network starts predicting the value of.", "metadata": {"video_title": "Output functions and loss functions"}}
{"text": "would understand is that it's through the loss function right so now if my network starts predicting the value of.\ny hat one right which was say the imdb rating as thousand whereas actually the value is somewhere between 0.\nto 10 right let's say it was 9.5 then my difference between 9.5 minus 1000 the whole square right this.\nis my loss function this is going to be very high right if this is going to be very high.\nthen my loss is very high that means what is the network uh what is the signal that the network.\nis getting that the current parameter configuration which is giving me these high values of the output is not a.\ngood configuration and it will start pushing it away from those configurations right using the gradient descent algorithm so if.\ni get a large loss that's an indication that i should start moving away from those configurations and the network.\nwill start doing that so hence that is the reason why it will not learn to predict these high values.", "metadata": {"video_title": "Output functions and loss functions"}}
{"text": "will start doing that so hence that is the reason why it will not learn to predict these high values.\nthe only way it can get minimize the loss is by staying within that range the moment it goes very.\nfar from that range it will not uh uh the the loss values would become very high and it will.\nbe pushed away from those configurations and so the loss will make sure that my values stay in that range.\nbut if i chosen the logistic function even if the loss is giving me that signal i can't do much.\nbecause no matter what happens the logistic function is only going to give me values between 0 to 1. it.\ndoesn't matter how i adjust w1 w2 w3 b1 b2 b3 at the end my values are always going to.\nbe 0 right hence that wouldn't have worked out okay okay so that's what you would do in the case.\nof regression now the next thing i'm going to do is talk about classification and to understand the loss function.", "metadata": {"video_title": "Output functions and loss functions"}}
{"text": "of regression now the next thing i'm going to do is talk about classification and to understand the loss function.\nin the classification context you need to know about cross entropy so i have a separate video which is from.\na previous course which will point to so that would be kind of the preparation video for this lecture which.\ntalks about information content cross entropy and so on so you could see that video first before you look at.\nthe next part and we'll just put a link to that video here right okay so assuming that you have.\nseen that video let's go to this discussion so now let us consider another example which is a classification problem.\nso you are given an image and you want to classify it into say one of four categories right and.\nnow again what would uh what would the output you want to predict is you want to predict a probability.\ndistribution right so in the previous video the helper video that i just pointed you to uh we see that.", "metadata": {"video_title": "Output functions and loss functions"}}
{"text": "distribution right so in the previous video the helper video that i just pointed you to uh we see that.\nin the case when the class is known right you could still think of this as y being a random.\nvariable which can take on four values one two three four one for apple two for banana three for mango.\nfour for orange and so on right so it can take on these four values right and what your network.\nwill do is it will predict what is the probability of each of these values right whereas from the true.\nlabel you know what the actual probability distribution is if this is an apple image then you know with 100.\ncertain because that's what the label has already told you so you know with probability one that it's an apple.\nimage and the probability of mango orange and banana is zero whereas your network is going to predict some values.\nright point two point three point four point one right and somehow you want to be able to capture the.", "metadata": {"video_title": "Output functions and loss functions"}}
{"text": "right point two point three point four point one right and somehow you want to be able to capture the.\ndifference between these two values right so there are two problems that we need to solve here one is given.\nthat i know that i'm trying to predict a probability distribution because my true output is a probability distribution right.\nthe sum of the values is one right uh how do i make sure that my output is also a.\nprobability distribution right that means all the values are between zero to one and they sum up to one right.\nso how do i choose an output function such that my output is also a probability distribution if i'm able.\nto do that how do i come up with a loss function right such that it tells me the difference.\nbetween these two now one loss function that you can always choose is the squared error loss function right you.\ncan again treat these as just two vectors and try to find the difference between them but given that you.", "metadata": {"video_title": "Output functions and loss functions"}}
{"text": "can again treat these as just two vectors and try to find the difference between them but given that you.\nknow that these are probabilities what is the special thing that you can do and the video that i pointed.\nout says that you can use the cross entropy loss function so i'm going to rely on that and come.\nback to the discussion on cross country right so this is the intuition that i want you to build that.\nin the case of classification problems you have a true probability distribution which looks like one and all zeros that.\none will be on the class which is the probability mass of one would be on the correct class and.\nzero on all other classes whereas your network will also predict some probability distribution because you will make an appropriate.\nchoice of the output function and now you're looking for a loss function which finds the difference between these two.\nprobability distributions so that's what you want to do in the case of classification so now let's fill in the.", "metadata": {"video_title": "Output functions and loss functions"}}
{"text": "probability distributions so that's what you want to do in the case of classification so now let's fill in the.\nglaps gaps suppose you want to classify an image into one of k classes here again we could use the.\nsquared error loss to capture the deviation but can you think of a better function right so that's the idea.\nthat's the motivation that i have set up now notice that y is a probability distribution where the two y.\nis a probability distribution as i said all the mass is on the correct class and zero on the other.\nclass right that is something that you understand therefore we should ensure that whatever y hat we are going to.\npredict should also be a probability distribution again i've already explained all this just re doing it so now what.\nis the choice of the output function that you should choose right and this is what we have so far.\nright again up to this point we have done all the computations and i'm trying to understand what should be.", "metadata": {"video_title": "Output functions and loss functions"}}
{"text": "right again up to this point we have done all the computations and i'm trying to understand what should be.\nthe output function so that i get this shaded green part right so i've done up to a l and.\nwhat should o be right so the choice of o that we are going to make is what is known.\nas the soft mac function and this is how you write the softmax function and let me now explain what.\nwe are trying to do here right what does this function trying to say right so let's just look at.\nan explanation of that so i can do it i need some space to do that let me just make.\nsome space okay so i can use this so now suppose our a l right this is a l okay.\nnow suppose it had again we had say in this case we were looking at a four class classification problem.\nright so let's see it at a l the all the computations that have happened so far this is what.\nmy al vector looks like right suppose it's minus 10 10 20 30 right so this is what my a.", "metadata": {"video_title": "Output functions and loss functions"}}
{"text": "right so let's see it at a l the all the computations that have happened so far this is what.\nmy al vector looks like right suppose it's minus 10 10 20 30 right so this is what my a.\nl looks like and from that i am not happy with this because this is clearly not a probability distribution.\nit has values greater than one it has also negative values right so this is not what i want from.\nhere i want to go to a y hat which is looking like a probability distribution right and what i.\nhave chosen is the soft max function so what i'm going to do how am i going to compute y.\nhat from here so this is what my y hat is going to be so i'm just going to do.\ne raised to minus 10 okay divided by uh and i'm just going to write it here e raised to.\nminus 10 plus e raised to 10 plus e raised to 20 plus e raised to 30 right so this.\nis what my first output is going to be that is what y hat 1 is going to be what.\nis y hat 2 going to be it's going to be e raised to 10 divided by this same sum.", "metadata": {"video_title": "Output functions and loss functions"}}
{"text": "is what my first output is going to be that is what y hat 1 is going to be what.\nis y hat 2 going to be it's going to be e raised to 10 divided by this same sum.\nthen e raised to 20 divided by the same sum and then e raised to 30 divided by the same.\nsum right and now you can understand that the sum of these values is going to be one right because.\nit's the sum of all the values is the denominator so the sum of the four values that i'm going.\nto compute here is going to be 1 and each of these values is going to be between 0 and.\n1. why is that the case let's try to understand that why is that the case so my original vector.\nmy original vector was say minus 10 10 20 30 right and now each value that i've computed is e.\nraised to the corresponding value divided by the sum of the e raised to all the values right now even.\nif i have a negative value in my original vector e raised to that is still going to be positive.", "metadata": {"video_title": "Output functions and loss functions"}}
{"text": "raised to the corresponding value divided by the sum of the e raised to all the values right now even.\nif i have a negative value in my original vector e raised to that is still going to be positive.\nright so that's one thing for sure that once i compute the y i's there are going to be no.\nnegative values there right the other thing for sure is that the sum of the values is going to be.\none right so two criteria that we have for probability distributions are satisfied since the sum of the values is.\n1 and every value is going to be greater than 0 that means that each of these values is going.\nto be between 0 and 1 right it cannot be greater than 1 because then the sum cannot be 1.\nbecause every value is positive and it cannot be negative because it's all e raised to something and even if.\nthe original value is negative so e raised to minus 10 is going to be 1 over e raised to.\n10 and this is still going to be a positive value right so i've taken the original computations al that.", "metadata": {"video_title": "Output functions and loss functions"}}
{"text": "the original value is negative so e raised to minus 10 is going to be 1 over e raised to.\n10 and this is still going to be a positive value right so i've taken the original computations al that.\ni had have been able to convert that into a probability distribution using the softmax function i've also explained what.\nthe softmax function looks like and just want to talk about uh one more thing here right so you could.\nhave asked me that why did why did you do this e raised to x right i mean why did.\nyou raise every value to the exponent and then did that why couldn't i have just taken every value here.\nright for example for this guy take 10 and then divide it by the sum of the remaining elements would.\nthat have not been enough right and obviously the answer is no because then for this guy which would have.\nbeen minus 10 divided by minus 10 plus 10 plus 20 plus 30 the answer would have been some negative.", "metadata": {"video_title": "Output functions and loss functions"}}
{"text": "been minus 10 divided by minus 10 plus 10 plus 20 plus 30 the answer would have been some negative.\nvalue right and you cannot have the probabilities as negative values right so that's why you use e because once.\nyou raise it to e then all the negative values even the the answer for those will also be positive.\nright so that's why you're using the exponent here and that's why the form of the softmax function right so.\nfor classification functions once you have done this computation you will then pass it through the softmax function to get.\na probability distribution right so now the first part of the problem is solved that my actual outputs were a.\nprobability distribution so now i have chosen an output function which also gives me a a probability distribution right so.\nthis part has been done so now given that both of these are probability distributions can we choose a good.\nloss function right obviously as i said you could use the squared error loss function right but if you know.", "metadata": {"video_title": "Output functions and loss functions"}}
{"text": "loss function right obviously as i said you could use the squared error loss function right but if you know.\nthis is a probability distribution we should use something better from probability theory and that is what i explained in.\nthe video that i pointed to and what you can do is you can use the cross entropy loss function.\nthis is what the cross entropy loss function looks like you have k classes right so you look at the.\ntrue probability for each of those classes and then the predicted probability for each of these classes right so the.\ntrue probability for each of these classes multiplied by the log of the predicted property for probability for each of.\nthese classes right so where does this formula come from this is something that is explained in that video you.\nshould again look at it i'm pointing you to it again and again because that's important right now notice something.\nabout this formula right so that this y c right now y c can be y 1 y 2 y.", "metadata": {"video_title": "Output functions and loss functions"}}
{"text": "should again look at it i'm pointing you to it again and again because that's important right now notice something.\nabout this formula right so that this y c right now y c can be y 1 y 2 y.\n3 all the way up to y k but it will take on the value 1 only for the correct.\nclass right only when c is equal to l that means only for the true cross label in this case.\nonly y one is going to be one and all the other y's are going to be zero right that.\nmeans when you're taking this summation and you have y one multiplied by something y two multiplied by something y.\nthree multiplied by something and so on then only one of these terms is going to survive right the term.\nwhich had a value 1 and all the terms which had value 0 are going to disappear in the summation.\nso although i've written this as a summation it will only have one term which would be y l multiplied.\nby log of y hat l and that y hat yl is again going to be just one right so.", "metadata": {"video_title": "Output functions and loss functions"}}
{"text": "so although i've written this as a summation it will only have one term which would be y l multiplied.\nby log of y hat l and that y hat yl is again going to be just one right so.\nbecause l is the true class and for the true class y l is equal to 1 right so then.\nwhat remains and of course there's a negative sign here is just log of y hat n right so this.\nis what remain it's and let's understand what that quantity is right so it is negative log of the predicted.\nprobability of the true class right negative log predicted probability of the true class and hence this is also called.\nthe negative log likelihood right because likelihood meaning it's telling you the probability of the true class the predicted probability.\nof the true class so cross entropy minimizing the cross entropy is the same as uh maximizing or the log.\nlikelihood of the correct class right so that's what it means okay because you are going to minimize minus of.", "metadata": {"video_title": "Output functions and loss functions"}}
{"text": "likelihood of the correct class right so that's what it means okay because you are going to minimize minus of.\nlog of y hat l i will do this on the next slide anyways so we are minimizing the minus.\nof log of y hat l right that is the same as maximizing the negative of that which is the.\nsame as maximizing the log likelihood right okay so you are minimizing the negative log likelihood which is the same.\nas maximizing the log likelihood and obvious statement so this is what the loss function and the output function looks.\nlike for the classification class right but now one question here this y hat l is it a function of.\nall these parameters because if it's not a function of these parameters then how what am i doing here right.\nis it a function of all these parameters of course it's a function of all these parameters so remember that.\ny hat l is this computed like this right so you have the input x it passes through these series.", "metadata": {"video_title": "Output functions and loss functions"}}
{"text": "y hat l is this computed like this right so you have the input x it passes through these series.\nof transformations at the end you have the output function and then you're looking you get this vector of y's.\nright and then you're looking at the l element of that vector right so all of this is of course.\nconnected all of this is dependent on the parameters hence the loss function depends on the parameters and now once.\nyou have this all you need to do is know how to compute the derivative of the loss function with.\nrespect to the parameters and that's something that we'll do later yeah so this is what y hat l encodes.\nit encodes the probability that x belongs to the ellith class so we want to bring it as close to.\n1 right because that's the probability of the predicted probability of the true class so you want that predicted probability.\nto be as close to 1 because we know that for the true class the probability is 1. okay so.", "metadata": {"video_title": "Output functions and loss functions"}}
{"text": "to be as close to 1 because we know that for the true class the probability is 1. okay so.\nthis is what we have done so far we have looked at two types of problems one problem where the.\noutput values are real values these are the regression problems and the other values where the outputs are probabilities these.\nare the classification problems are two popular family of problems we have looked at for the regression problems we said.\nthat the output function should just be linear because we don't want the output to be bounded between anything and.\nwhatever is the natural range of the output the loss function will make sure that the network predicts something in.\nthat range itself because otherwise the loss will become very large right for the classification problems the output function was.\nthe soft max function and then the loss function for the regression problems for squared error and for the classification.", "metadata": {"video_title": "Output functions and loss functions"}}
{"text": "the soft max function and then the loss function for the regression problems for squared error and for the classification.\nproblems was cross entry right there could be other loss functions but these are the most popular loss functions that.\nyou will encounter in a very large number of problems are greater than 90 of the problems that you will.\nencounter these two loss functions uh should suffice right or some variant of these two loss functions for the rest.\nof this lecture we are going to focus on the case where the output activation is soft max and the.\nloss function is cross entropy everything that we learn for that case is also applicable for this case by just.\none small tweak right so that's i'm just going to focus on the uh softmax and cross entropy part okay.\nso i'll end this module here and then i'll come back and talk about the intuition behind back propagation and.\nthen we'll do the back propagation algorithm in g2.", "metadata": {"video_title": "Output functions and loss functions"}}
{"text": "[Music] so let's begin the next module which is on uh perceptrons right and uh just again talk about the.\nstory uh so far and the story ahead right so so the one question which i asked is like what.\nabout non-boolean inputs right so we have so far been focusing on the case where the inputs are only boolean.\nso we'll have to see what happens if the boolean inputs are not boolean then the second is in this.\ntoy example is the boolean functions that we are looking at so far we were just hand coding the threshold.\ni gave you the example of the and function then asked you what should the threshold be and you are.\nable to quickly calculate uh in your head of course you're doing some calculations in that case it's not hand.\ncoded but it's just like you worked it out by hand and you arrive at a solution but do we.\nalways have to do that is it like won't it become cumbersome in complex cases then the other more i.", "metadata": {"video_title": "Perceptrons"}}
{"text": "coded but it's just like you worked it out by hand and you arrive at a solution but do we.\nalways have to do that is it like won't it become cumbersome in complex cases then the other more i.\nwouldn't say more important but the other slightly different point here is that when we are looking at the uh.\nmccaulick pits neuron right all the inputs were equal right and that is often not the case right when we.\nmake decisions if you are making a decision based on 10 different inputs some of them would be more important.\nthan the others so there's always this one like a few important things based on which you might which might.\nsway your decision one way or the other while there might be many factors on which you are basing your.\ndecision and there is no notion of importance all the things like is it training am i in the mood.\nis the movie is good all of these inputs were some of given equal weights right and if you go.", "metadata": {"video_title": "Perceptrons"}}
{"text": "decision and there is no notion of importance all the things like is it training am i in the mood.\nis the movie is good all of these inputs were some of given equal weights right and if you go.\nback to the biological neuron again there was this connection there and that connection is actually the weight or the.\nstrength of the connection between two neurons so there also it was of all the neurons giving an input to.\none neuron maybe some had more importance than the others right so that notion of weight or importance is not.\nbeing captured and the last is i ended the previous video saying that could there be not non-linearly separable boolean.\nfunctions and we'll try to address that what what are these functions and are what do you do if they're.\nnot linearly separable right so not maybe in this particular module or video but these are the questions that we'll.\ntry to answer over the course of this lecture so so you had this mp neuron or the mecolic bits.", "metadata": {"video_title": "Perceptrons"}}
{"text": "try to answer over the course of this lecture so so you had this mp neuron or the mecolic bits.\nneuron and then came this perceptron right which was proposed by frank rosenblatt again we had seen this in the.\nhistory around 1958 and the main differences were the falling and they are some of them are already obvious in.\nthe diagram itself that earlier the neurons the inputs are only boolean but now the inputs could also be real.\nthat was one difference the other was that now you have these weights right so these weights could tell you.\nwhich inputs are more important than the other and there's also now that you have these weights there's also a.\nmechanism or a learning algorithm to learn these weights right so you might vaguely have an idea of which inputs.\nare important which are not but typically you would want to learn this from data that in the past maybe.\nwhile making decisions i have given more importance to who is the actor in the movie as opposed to who.", "metadata": {"video_title": "Perceptrons"}}
{"text": "while making decisions i have given more importance to who is the actor in the movie as opposed to who.\nis the director in the movie right so maybe these things you want to learn from data so these weights.\nare now learnable right so you have when we're going to look at a learning algorithm for learning these obvious.\nso like three main differences now you have weights the inputs are no longer boolean you could have real input.\nso that was one action item that we had and we have taken care of that in this model and.\nthe third is a learning algorithm so far we were talking about hard coding or like like mentally calculating what.\nthe threshold is but now we look at a good learning algorithm for learning the weights right so what we're.\ngoing to learn in this course is not the original perceptron or the classical perceptron model but more the perceptron.\nmodel which was later refined and analyzed by minsky and pappered again we had referred to this in the history.", "metadata": {"video_title": "Perceptrons"}}
{"text": "model which was later refined and analyzed by minsky and pappered again we had referred to this in the history.\nin 1969 so that's the model that we're going to uh be talking about is just to make it clear.\nthat what we are talking about okay so let's go ahead right so what does the uh perceptron model look.\nlike so it's it's very similar right in form at least so you have y is equal to one if.\nthe weighted sum of the inputs is going to be greater than equal to threshold right so earlier oops in.\nthe uh mcculloch pits neuron or the mp neuron there was no notion of weights so it was just that.\nif the sum of the inputs is greater than equal to threshold but now i have if the weighted sum.\nof the inputs is greater than equal to threshold and that's where i get to assign weights to input so.\nif the input value is small but the weight to the input is large then still the contribution would be.\nhigher right and notice that this index is over 1 to n because there are n inputs so from the.", "metadata": {"video_title": "Perceptrons"}}
{"text": "if the input value is small but the weight to the input is large then still the contribution would be.\nhigher right and notice that this index is over 1 to n because there are n inputs so from the.\nfirst input to the last input right so 1 to n that's where the indices are coming from and it's.\ngoing to be 0 if this weighted sum is less than threshold right so again a very similar form to.\nthe mp neuron now on the rest of the slide i'm just going to do some rewriting to arrive at.\na more neater form right i don't want to keep writing this let's see where i reach there so if.\ni rewrite the above i'm just going to take theta on one side so this minus theta greater than equal.\nto zero nothing wrong with this and minus theta less than zero right so i have just taken theta on.\none side now uh something interesting will happen so now the theta has disappeared looks like the theta has disappeared.\nbut actually it hasn't because i have added this i equal to 0 right so what i am saying now.", "metadata": {"video_title": "Perceptrons"}}
{"text": "but actually it hasn't because i have added this i equal to 0 right so what i am saying now.\nis that my sum earlier was w 1 x 1 plus w 2 x 2 plus blah blah blah up.\nto minus theta greater than equal to 0 right that's what this equation was saying now i am starting from.\nw naught right because it's i equal to 0 so i have w naught x naught plus all of this.\nand then the theta is disappeared there is no theta in this equation so what have i done actually all.\ni am saying is that i am going to have an imaginary input x naught which is equal to 1.\nand then another weight w naught which is actually equal to minus theta so if i substitute these values then.\nthese two inequalities become the same right if i make w naught as minus theta and x naught as 1.\nand i just get minus theta as the first term of this sum of terms and then uh that these.\ntwo inequalities that i have here right so as i said this was same and then greater than equal to.", "metadata": {"video_title": "Perceptrons"}}
{"text": "and i just get minus theta as the first term of this sum of terms and then uh that these.\ntwo inequalities that i have here right so as i said this was same and then greater than equal to.\nzero so these two inequalities become zero so the simple change which i have made quite obvious and what it.\nmeans is that i have this extra input which is x 0 equal to 1 and w naught is actually.\nminus theta and so now the inequality falls in place so now i my summation is from 0 to n.\nwith the understanding that x naught is 1 and w naught is minus theta which is the same as saying.\nsummation 1 to n minus theta that's what my original uh inequality is where i started from so this just.\nmakes it neater in some sense i have no theta now now theta has become w naught or minus w.\nokay so we'll now try to answer the following questions right why are we trying to implement boolean functions right.\nso why am i talking about boolean functions so much i said that we can implement real functions but still.", "metadata": {"video_title": "Perceptrons"}}
{"text": "so why am i talking about boolean functions so much i said that we can implement real functions but still.\nfor the last part of this lecture i'm going to focus on boolean functions i'll tell you why i'm focusing.\non boolean functions the second is why do we need weights and why is this w naught called the bias.\nright so these are some very i would say heterogeneous set of questions that i'm going to try to answer.\nright so first why why are we talking about boolean functions so let's take the simple thing of task of.\npredicting whether we would like to uh go for a movie or not or whether we like a movie or.\nnot right and this decision might depend on several factors like who is the actor of the movie who is.\nthe director of the movie what's the genre of the movie and all of this you could convert to bullion.\ninputs right is actor matt damon his actor christian bale is john a thriller it is johnner comedy is director.", "metadata": {"video_title": "Perceptrons"}}
{"text": "inputs right is actor matt damon his actor christian bale is john a thriller it is johnner comedy is director.\nnolan and so on right so you could just convert it into these series of boolean inputs right so many.\nreal world problems you could just try to convert them to problems where the inputs are boolean and the output.\nis of course bullying right whether the whether you are going to watch the movie or whether you want to.\nwatch the movie whether you're going to like the movie and so on right and now based on the past.\nviewing experience now that this model has weights we could learn the waves from some data how we are going.\nto learn that is not clear yet right and it's going to take a series of lectures to understand that.\nin a much broader context we will do it in the context of perceptron soon but the broader context would.\nget clear over a series of lectures right so this is something that we would like to learn and now.", "metadata": {"video_title": "Perceptrons"}}
{"text": "get clear over a series of lectures right so this is something that we would like to learn and now.\nthis model has these placeholders for weights which could potentially be learned right and these weights could be such that.\nwhat do we want right we want the weighted sum to be greater than some threshold right or the weighted.\nsum including the threshold to be greater than equal to 0 that's what we want right so now you could.\nimagine that there's an input signal which is weak but the weight for that input signal is so high that.\ncollectively it crosses the border right and so it's possible that you only have one input which is on right.\nwhich is the director is nolan and all the other inputs are off but the weight of this input might.\nbe very high based on your data because you have always liked movies directed by christopher nolan so you would.\nhave a very high weight for this and because the weight is high even though there's only one input which.", "metadata": {"video_title": "Perceptrons"}}
{"text": "have a very high weight for this and because the weight is high even though there's only one input which.\nis on the weighted sum might still cross the threshold right and that's that's what the weights help you and.\nthat's the notion of importance encoded in an inequality right so that's where the weights are helping right now coming.\nto the bias why is w naught called the uh why is w naught called the bias so w naught.\nactually refers to the prior or the prejudice so what's the role that it is playing right it is one.\nof the terms in the summation and it's like the blockage right so if w naught or or if theta.\nis a very high value right so then your sum has started off with a very low value it's a.\nhigh negative value that you have started off and now to cross this so that your total weighted sum is.\ngreater than zero all of these inputs will have to contribute a lot right that means many inputs may have.", "metadata": {"video_title": "Perceptrons"}}
{"text": "greater than zero all of these inputs will have to contribute a lot right that means many inputs may have.\nto be on and many high weighted inputs have to be on right so this theta is telling you that.\nwhat is your inertia for crossing the border right so if you are a very niche moviegoer who only watches.\nmovies which have mad demon which are thriller and which are directed by christopher nolan right then your theta let's.\nassume all the weights are one then your theta would be -3 and only if these 3 inputs are on.\nthen you will have minus theta which is minus 3 plus the actor is diamond plus the genre is thriller.\nplus the director is nolan and then this would be greater than equal to zero and only then you will.\ngo and watch the movies if any of these inputs is not on right if either the actor is not.\nwhat you're looking for then you'll not go to watch so this is the bias right this is what it.", "metadata": {"video_title": "Perceptrons"}}
{"text": "go and watch the movies if any of these inputs is not on right if either the actor is not.\nwhat you're looking for then you'll not go to watch so this is the bias right this is what it.\ntells you about that specific user and hence it's called the prior or the bias right but if you are.\nlike a movie buff who does not care about who the director director director is and in that case your.\nthreshold could be zero and even if none of these criteria are matched the actor is not matt damon the.\ndirector is not nolan the honor is not thriller still you will cross the threshold and still you will go.\nand watch the movie because you are just a movie buff so this threshold is trying to encode the initial.\nbias that you have right and it could be adjusted to reflect whether this person is a movie buff or.\nnot right in other situations it could be reflected to uh it could be adjusted to reflect the prior as.", "metadata": {"video_title": "Perceptrons"}}
{"text": "not right in other situations it could be reflected to uh it could be adjusted to reflect the prior as.\nwe say right so what is the without knowing the data without knowing the inputs what is the prior that.\nis about you taking this decision as a positive versus negative right zero versus one so that's why it's called.\nthe prejudice and the same thing i have explained on the slides you can go back and read it later.\nright okay so now coming to the question so this is this was to motivate a few things right that.\nwhy are we interested in boolean functions because many real world problems you can map it to boolean functions then.\nwhy do we have weights because in many real function world problems you want weighted inputs you want certain inputs.\nto be more important than others and what is theta what is the prior so that has also been explained.\nright now let's move on to a different set of questions so what kind of functions can be implemented using.", "metadata": {"video_title": "Perceptrons"}}
{"text": "right now let's move on to a different set of questions so what kind of functions can be implemented using.\nthe perceptron right we know the answer for the mecolic bits neuron can it can only be used to implement.\nboolean functions which are linearly separable that means you can draw a line and all inputs which have a positive.\noutput will lie on one side and all inputs which have a negative output will lie on the other side.\nso is the perceptron any different or what kind of functions can it be used so if you look at.\nthe mathematical form of these two there is only the red part is different right so earlier you had summation.\nx i greater than equal to zero now you have summation w i x i greater than equal to 0..\nso in essence both of these are actually finding linear boundaries right just that now you have certain coefficients for.\nyour inputs right so that's what is happening here it should be clear that even for a mecca for a.", "metadata": {"video_title": "Perceptrons"}}
{"text": "your inputs right so that's what is happening here it should be clear that even for a mecca for a.\nperceptron you're just learning a line and then you will have some points which lie in the positive half space.\nof the line and for those points the output would be one and some points which lie in the negative.\nhalf space of the line and for those points the output would be zero so just in case this is.\nnot very clear we'll look at an example and it will become clear but then the question which already comes.\nup is that if it's not different if it's also learning a decision boundary then what's linear decision boundary then.\nwhat's the difference the difference is that now we have weights and we also have a learning algorithm for learning.\nthose weights right so we are slowly moving in the territory of learning with the perceptron and we'll go deeper.\nin this territory as we go to sigmoid neurons and then eventually to a deep neural networks right so that's.", "metadata": {"video_title": "Perceptrons"}}
{"text": "in this territory as we go to sigmoid neurons and then eventually to a deep neural networks right so that's.\nwhere we're slowly making a transition to right so let us first revisit some boolean functions to convince ourselves that.\nthis is actually drawing a linear decision boundary right so you have the or function i know what the or.\nfunction looks like and now in terms of the perceptron what does it mean this is what my perceptron function.\nis right so i'm going to take a weighted sum of the two inputs plus w naught and i want.\nthat to be less than zero in this case right i want that to be greater than equal to zero.\nin this case greater than equal to zero in this case greater than equal to zero in this case because.\nthat's how my or function is right now let me just expand this right so what is this first guy.\nis telling w naught plus w 1 into 0 plus w 2 into 0 should be less than 0 which.", "metadata": {"video_title": "Perceptrons"}}
{"text": "that's how my or function is right now let me just expand this right so what is this first guy.\nis telling w naught plus w 1 into 0 plus w 2 into 0 should be less than 0 which.\nimplies that w naught should be less than 0. similarly the second equation is saying w naught plus w 1.\ninto 0 plus w 2 into 1 should be greater than equal to 0 which is just saying that w.\n2 should be greater than equal to minus w naught right similarly the third equation is this is what it.\nimplies and similarly this is what the fourth equation implies right so i'll just take a pause for 10 seconds.\nhere so that you could just look at the equations and convince yourself that this is indeed the case okay.\nnow this is not the equations inequalities so these are the four inequalities given to me right and now i.\ncould just look try different values of w one and w two and w naught and try to figure out.\none combination of w one w two w naught which satisfies these inequalities of course in practice i would not.", "metadata": {"video_title": "Perceptrons"}}
{"text": "one combination of w one w two w naught which satisfies these inequalities of course in practice i would not.\nlike to do this randomly but have a learning algorithm which does it and we will go there we will.\ngo to the perceptron learning algorithm but for now it's ok if i just think a bit about it and.\ni have already thought about it and this is a set of values which i am going to give you.\nif w not equal to 1 also not 1 sorry this should have been minus 1 right so this should.\nhave been minus 1 so if w not equal to minus 1 if w 1 equal to 1.1 and w.\nt equal to one point one then all these equations are satisfied you can just plug in these values and.\nsee that all the four inequalities are satisfied right and now i'll look at the geometric interpretation of that so.\nwhat is the geometric interpretation i am saying that minus 1 okay plus 1.1 into x1 plus 1.1 into x2.\ngreater than equal to 0 this is the positive half space less than 0 this is the negative half space.", "metadata": {"video_title": "Perceptrons"}}
{"text": "greater than equal to 0 this is the positive half space less than 0 this is the negative half space.\nso if i eq look at the line equal to 0 that's the separating line which is separating my inputs.\ninto this positive and negative half space right so let's see what that line is looking like so here i.\nhave this line so this is the line minus one plus one point one x one plus one point one.\nx two equal to zero as i said earlier uh this is minus one right so this i already mentioned.\nso that is why this is minus one here ok oh it is already deleted okay so now uh what.\nis happening here right so the green shaded region is actually the positive half space of the line all the.\npoints in this half space will satisfy the equation minus 1 plus one point one x one plus one point.\none x two greater than equal to zero they'll satisfy that inequality hence there is a positive half space and.\nall the points which are in the red zone and in my boolean world there is only one such point.", "metadata": {"video_title": "Perceptrons"}}
{"text": "all the points which are in the red zone and in my boolean world there is only one such point.\nwhich is zero comma zero so that is going to lie in the negative half space and that is exactly.\nwhat we wanted right so this is a correct solution of course this is not the only correct solution so.\ni will go ahead and yeah so i'll go ahead and change this a bit so i've changed this this.\nis also a valid solution right because again my green region has the positive points this is not a valid.\nsolution and you can see here the error is 2 why is the error 2 because this is the one.\npoint which lies in the positive half space and it rightly lies in the positive half space there are three.\npoints which lie in the negative half space of which actually only one should have lied the red point should.\nhave lied in the negative half space but there are these two green points which are also lying in negative.", "metadata": {"video_title": "Perceptrons"}}
{"text": "have lied in the negative half space but there are these two green points which are also lying in negative.\nhalf space so i'm making errors on two inputs so in this configuration my perceptron is not actually representing the.\nor function it's not implementing the or function because it is making errors right so this is not an implementation.\nof the or function but there could be other values this i already showed i could even adjust the threshold.\nright so this w naught actually in the terms of a line tells me what my y intercept is going.\nto be right so i'm going to adjust it and it will intersect or intercept the y axis at different.\npoints right and now so far the error is zero so all these solutions are valid but now if i.\ncome here then i have a problem you can see error is equal to one because one point which red.\npoint which should have been in the negative half space is now lying in the positive half space right so.", "metadata": {"video_title": "Perceptrons"}}
{"text": "point which should have been in the negative half space is now lying in the positive half space right so.\nnow my learning algorithm should be such that you should draw the line in a way that my negative points.\nare on one side and the positive points or the points which give a negative output are on one side.\nand positive output on the other side right so that's what we would want from the perceptron algorithm and this.\nuh same thing right where i have put in these uh try to substitute the values and then come up.\nwith inequalities and then just hand solve for the inequalities the same thing you could have done for the mecolic.\npits neuron there you would have had only w naught right there was no w1 and w2 so you would.\nhave come up with four sets of inequalities and only one variable which is theta or w naught and then.\nyou could have tried to find the value of that data so you can go home and try this try.", "metadata": {"video_title": "Perceptrons"}}
{"text": "have come up with four sets of inequalities and only one variable which is theta or w naught and then.\nyou could have tried to find the value of that data so you can go home and try this try.\nto write the four inequalities for the mcculloch pits neuron remember there will be no w one w two and.\nthen just see if you can come up with the right value of theta so that is also doable right.\nso this the idea of learning is or finding the right value for the parameters is not something that we.\nhave come to suddenly i'm just saying that we had already done it in the case of the mp neuron.\nalso where we had tried to find this value of theta it's just that there it was very obvious what.\nthe value of theta should be as here it could be a bit difficult right and we'll have a learning.\nalgorithm to do that so i'll end this module here and when we come back i am going to talk.\nabout errors and error surface.", "metadata": {"video_title": "Perceptrons"}}
{"text": "[Music] um welcome back so we are in the next module now where we'll be talking about the perceptron learning.\nalgorithm right so far we introduced the perceptron we saw that the decision boundary that the perceptron learns is linear.\nand then we introduce this concept of error where if the weights are not proper or if w1 w2 values.\nare not proper then you will make some errors in dividing the points into the right half spaces right so.\nthat's that's what we have learned so far but we did not talk about how did we get in this.\nw1 wt we were mainly trying to adjust it by hand so now we look at the perceptron learning algorithm.\nwhich allows us to learn these ways right so now before we do that let us try to understand that.\napart from implementing boolean functions right i mean which does not look very interesting right why would you care about.\nboolean functions so what is it that the perceptron can be used for and what is this connection to the.", "metadata": {"video_title": "Perceptron Learning Algorithm"}}
{"text": "boolean functions so what is it that the perceptron can be used for and what is this connection to the.\nbody and functions right so let's uh let's revisit our example right so now we are again talking about the.\ncase of a binary classifier and we want to decide whether to watch a movie or not right and suppose.\nwe are given a list of m movies all the past movies that we had seen along with the class.\nlabel whether we liked this movie or did not like this ok this is based on our past data so.\nthis is just a binary label on all the movies right and each movie could be represented by n features.\nso these features could simply be the inputs which are contributing to the decision so one input is actor daemon.\nthe other input is the genre thriller and so on and these are all boolean inputs that we have and.\nyou could also have some real valued inputs like the imdb rating the critics rating of the movie and so.", "metadata": {"video_title": "Perceptron Learning Algorithm"}}
{"text": "you could also have some real valued inputs like the imdb rating the critics rating of the movie and so.\non right so all of these inputs are feeding into our perceptron and there are also weights associated with them.\nit's just that we don't know what these weights are and we want to learn these weights such that once.\nwe learn these weights then for any given new input it should allow us to make that decision right and.\nwe should be able to learn these weights in such a way that at least for the given data and.\nwe'll assume that the data is linearly separable so whatever m data points that are given to us we will.\nassume that they are linearly separable that means you can draw a line or a plane or a hyper plane.\nsuch that all the points for which the output should be positive are in the one half space and all.\nthe points for which the output should be negative or in the other half space right so we will assume.", "metadata": {"video_title": "Perceptron Learning Algorithm"}}
{"text": "the points for which the output should be negative or in the other half space right so we will assume.\nthat the data is linearly separable and then in this real classification problem right this is not a cooked up.\nproblem this is a real classification problem we are then interested in learning these weights right so that's what we.\nwant to use a perceptron for for handling these kind of classification problems right ah now let's try to look.\nat the perceptron learning algorithm so i'll define some notation so let p be all the inputs for which the.\nlabel is one right so these are all the positive points that we have been calling uh in all our.\ndiscussion earlier right and n is all the inputs which have a label 0 so these are all the negative.\npoints right in our discussion earlier so now i do not know what the weights are so what will i.\ndo i'll just initialize the weights randomly right and while convergence or while not convergence till i reach convergence i'll.", "metadata": {"video_title": "Perceptron Learning Algorithm"}}
{"text": "do i'll just initialize the weights randomly right and while convergence or while not convergence till i reach convergence i'll.\ndo something right now what is the meaning of convergence here so the meaning of convergence here is that if.\ni reach a point right such that my values for w are such that for all the positive points in.\nthe data which is given to me my output is 1 and for all the negative points in the data.\nmy output is 0 right so in other words if for all the positive points summation w i x i.\nis greater than zero and for all the negative points summation w i x is less than zero that means.\nif i have magically reached a configuration where i have learned the w is because excise i don't have a.\ncontrol over it these are the data that have been given to me so for if all the positive points.\nthis condition is satisfied and for all the negative points this condition is satisfied that means i have to been.", "metadata": {"video_title": "Perceptron Learning Algorithm"}}
{"text": "this condition is satisfied and for all the negative points this condition is satisfied that means i have to been.\nable to clearly separate my training data right and that's what i will define as conversions okay so while not.\nconvergence what will i do let's see so i'll pick a random point from the given set of points so.\nthe given set of points are the union of the positive and the negative points so i'll just mix up.\nall this and pick a random point from there i'll call it x which is the notation we have been.\nusing and now if x belongs to p so remember that x in itself is uh x naught x 1.\nall the way up to x n right so in our previous example these are the end decision features that.\ni had is actor matt damon is the genre thriller and so on and x0 was this constant input which.\nhad been set to 1 right so that is what x the bold x looks like it's a vector and.\nsimilarly you have these w naughts to w n right which is the vector uh w okay sorry about my.", "metadata": {"video_title": "Perceptron Learning Algorithm"}}
{"text": "had been set to 1 right so that is what x the bold x looks like it's a vector and.\nsimilarly you have these w naughts to w n right which is the vector uh w okay sorry about my.\nhandwriting but i think it's kind of clear from the context what i am saying so this these are the.\ntwo vectors so i am just going to take the weighted sum of w i x i and if the.\npoint was positive and if this weighted sum is less than 0 then you and i understand something bad has.\nhappened right because if the point was positive i wanted this weighted sum to be greater than zero that is.\nwhat i have said here right so something is going wrong my weights are not the way they should be.\nso i need to do some correction ok and this is the correction i am going to do i am.\njust going to do w is equal to w plus x okay i am not going to say anything more.\nabout this at this point if the point was negative and if my summation was greater than equal to 0.", "metadata": {"video_title": "Perceptron Learning Algorithm"}}
{"text": "just going to do w is equal to w plus x okay i am not going to say anything more.\nabout this at this point if the point was negative and if my summation was greater than equal to 0.\nthen again i know that something wrong has happened because for such a point i wanted the summation to be.\nless than 0 right but that has not happened so again i need to do some correction and my correction.\nis going to be w equal to w minus x again i am not explaining why this magic formula right.\nwhy am i doing plus x in one case and minus x in one case and how does this lead.\nme to where i want to go and where do i want to go i want to go to convergence.\nwhere my weights are such that that these two conditions are satisfied by all the positive and negative points right.\nso how does this ensure that that is what is going to happen sorry okay yeah so why would this.\nwork right that's the question that i'm trying to ask right it's not very clear to me that why should.", "metadata": {"video_title": "Perceptron Learning Algorithm"}}
{"text": "so how does this ensure that that is what is going to happen sorry okay yeah so why would this.\nwork right that's the question that i'm trying to ask right it's not very clear to me that why should.\nthis work so to understand this we'll have to look at a bit of linear algebra and a bit of.\ngeometry right so let's let's go there so we have two vectors w and x and these are what those.\nvectors look like w naught up to w n just as i had written on the previous slide and x.\ns one up to x one x two x n right and now if i consider the dot product between.\nthese two vectors then this is exactly what i get and this is what the expression that you are seeing.\non the previous slide i was taking some decisions based on whether this value was greater than z equal to.\n0 or less than less than 0 right so this is the quantity that we are dealing with and now.\ni'm just telling you which is obvious that this quantity is nothing but the dot product between the weight vector.", "metadata": {"video_title": "Perceptron Learning Algorithm"}}
{"text": "i'm just telling you which is obvious that this quantity is nothing but the dot product between the weight vector.\nand the data vector right the data point that i have so we can just rewrite the perceptron rule as.\nthe following so y is equal to 1 if the dot product is greater than equal to zero and y.\nis equal to zero if the dot product is less than zero nothing magical here the dot product is just.\nequal to this so i have replaced that cumbersome notation of summation and w i x i by this more.\ncompact dot product right now this is where now we need to start digging on if you're looking at the.\ndot product right so now i've gotten hold of some quantity that i can try to explore from a linear.\nalgebra perspective right what does a dot product mean and then try to come up with some explanation for why.\nshould the algorithm work or what does it make why is it making sense right so we are interested in.", "metadata": {"video_title": "Perceptron Learning Algorithm"}}
{"text": "should the algorithm work or what does it make why is it making sense right so we are interested in.\nfinding this line w transpose x equal to zero which divides the input into two halves right that we have.\nalready understood we want to learn these w right so now any a point so which lies on this line.\nit will satisfy this equation i mean that's a trivial statement to make right so if you have the line.\ny is equal to mx plus c right then any point x comma y which lies on this slide will.\nsatisfy this equation that's the same thing i am saying is just that now my way of writing equations is.\nslightly different i am calling this as x 1 i am calling this as x 2 and maybe i am.\ncalling this as w naught right so that is the only notation change that i have so i have w.\n1 x 1 right plus w 2 x 2 plus w naught equal to zero so this is the equation.\nof a line which is analogous to this equation that i have and all i am saying is that any.", "metadata": {"video_title": "Perceptron Learning Algorithm"}}
{"text": "1 x 1 right plus w 2 x 2 plus w naught equal to zero so this is the equation.\nof a line which is analogous to this equation that i have and all i am saying is that any.\npoint which lies on this line satisfies this equation and that's a trivial statement to me now what can you.\ntell us about the angle between any such point right and the vector w so first of all we have.\nto again realize that both x and w are vectors and that's exact precisely what is defined here so now.\ni can ask this question and i am saying that any point which lies on the slice satisfies this equation.\nso then if it satisfies this equation what can you say about the angle between the point and the vector.\nw if it satisfies the equation w transpose equal to zero that means these two vectors are orthogonal we know.\nthat from linear algebra so the angle is going to be 90 degrees right and because we know that cos.\nof the angle is equal to this formula and if w transpose x is 0 so the cos of the.", "metadata": {"video_title": "Perceptron Learning Algorithm"}}
{"text": "that from linear algebra so the angle is going to be 90 degrees right and because we know that cos.\nof the angle is equal to this formula and if w transpose x is 0 so the cos of the.\nangle would be 0 and the angle would be 90 degrees right so this is slowly we have now started.\nmoving into the territory of geometry and linear algebra right so now we know that if the point lies on.\nthe line then the angle is 90 degree with the weight vector okay now let's uh go ahead right so.\nif the point if the vector w right if it's perpendicular to every point on the line then it just.\nsimply means that it's perpendicular right so if this is the line w transpose x equal to 0 then my.\nweight vector is going to be perpendicular to that so nothing great again right now let's see some points which.\nlie in the positive half space of this line right so this is my line and i'm going to look.\nat the points which lie in the positive half space what is the positive half space it means that w.", "metadata": {"video_title": "Perceptron Learning Algorithm"}}
{"text": "lie in the positive half space of this line right so this is my line and i'm going to look.\nat the points which lie in the positive half space what is the positive half space it means that w.\ntranspose x should be greater than 0 right so here are these points now my question is what would be.\nthe angle between these points and the weight vector w without knowing anything just that by looking at the picture.\nyou should be able to tell me that this angle would be less than 90 degrees right it's obvious from.\nthe picture right and why is oh let's let's see if i want to answer this now and now let's.\nlook at some points which lie in the negative half space right which are these red points what would be.\nand by definition if it's in the negative half space and w transpose x is less than zero so what.\nwould be the angle between these points and the weight vector again just by looking at the picture it is.", "metadata": {"video_title": "Perceptron Learning Algorithm"}}
{"text": "would be the angle between these points and the weight vector again just by looking at the picture it is.\nclear that the angle should be greater than 90 degrees right so if i consider this point here this angle.\nis clearly greater than 90 degrees because i mean you know it is greater than 90 degrees okay so that.\nalso is clear now right so now let us try to relate uh the expression that we had with this.\ngeometry right this simply follows from the formula so you had the cos alpha is the formula for that is.\nw transpose x divided by the norm of w and norm of x right so if w transpose x is.\ngreater than zero right that means cos alpha is greater than zero if cos alpha is greater than 0 that.\nmeans alpha is going to be less than 90 degrees right so that is what it follows from because the.\npoints lying in the positive half space satisfy this expression this inequality that's why the angle is going to be.", "metadata": {"video_title": "Perceptron Learning Algorithm"}}
{"text": "points lying in the positive half space satisfy this expression this inequality that's why the angle is going to be.\nless than 90 degrees and because the points lying in the negative half space satisfy this inequality that's why the.\nangle is going to be less than 90 degrees so you just don't know this pictorially now you also know.\nthat it follows from this inequality right so all positive points will have a acute angle all positive negative points.\nwill have an angle which is greater than 90 degrees right so now with this explanation in mind let's revisit.\nthe algorithm and let's try to see what is happening right so what was i doing if i had a.\npositive point on which i was making an error then i was changing the weight vector by adding the point.\nto the weight vector and that's what is happening if i had a positive point on which i was making.\nan error then i was changing the weight vector by adding the point to it right so let's see what.", "metadata": {"video_title": "Perceptron Learning Algorithm"}}
{"text": "to the weight vector and that's what is happening if i had a positive point on which i was making.\nan error then i was changing the weight vector by adding the point to it right so let's see what.\nhappens by that right so if x is belonging to p and if w transpose x is less than 0.\nthat's what this expression means right then it means that the angle between this x and the current w is.\ngreater than 90 degrees right but that is not what i wanted i wanted this angle to be less than.\n90 degrees so now what is it what should my goal be if i am in this situation that my.\nw and x are such that the angle is greater than 90 degrees and i want it to be less.\nthan 90 degrees so i should try to adjust the w at least such that whatever is the current angle.\nthat angle should decrease it may not become less than 90 degrees at one shot but at least it should.\ndecrease from where it is currently because right now i am in a bad situation and that's what i should.", "metadata": {"video_title": "Perceptron Learning Algorithm"}}
{"text": "decrease from where it is currently because right now i am in a bad situation and that's what i should.\ndo so let's see if what happens if we do this update so this is the update that we do.\nour goal was to want it to be less than 90 degrees we somehow ignored that goal and went ahead.\nand said okay let me do this so what i need to convince you is that whether this is in.\nline with my goal right so if my new w is w plus x so let me now check what.\nmy new angle would be right my new angle would be proportional to this right now let me just substitute.\nthe value of w new it's w plus x okay let me just open up the bracket w transpose x.\nplus x transpose x w transpose x is cos alpha right so cos alpha nu is actually greater than cos.\nalpha that means alpha nu would be less than alpha that's what i wanted whatever was my current alpha i.\nwanted my new alpha to be less than that eventually i want to get it to less than 90 degrees.", "metadata": {"video_title": "Perceptron Learning Algorithm"}}
{"text": "alpha that means alpha nu would be less than alpha that's what i wanted whatever was my current alpha i.\nwanted my new alpha to be less than that eventually i want to get it to less than 90 degrees.\nbut i may not be able to do it at one shot so i will slowly try to make it.\nless than right so that's what has happened here okay there is some hidden stuff here right so i've conveniently.\nused this proportional sign and ignored the denominator all which simplifies the calculation for me and it makes it easier.\nfor me to show the intuition if you add the denominator and all you will kind of realize that it's.\na bit more trickier calculation but the idea is just to give you the intuition right so you understand now.\nthat with this update the alpha actually becomes less than what it was currently and that's exactly what i wanted.\nright let's look at the other case if x was a negative point and if i was making an error.", "metadata": {"video_title": "Perceptron Learning Algorithm"}}
{"text": "right let's look at the other case if x was a negative point and if i was making an error.\nright so if w transpose x was greater than equal to 0 that means the current angle is less than.\n90 degrees but what do i want i want it to be greater than 90 degrees right so if i.\nlook at it at one point one step at a time whatever is the current angle i want the new.\nangle to be greater than this angle right so let's see if that happens if i do w nu equal.\nto w minus x the same explanation cos alpha nu is going to be this i'll substitute the value i'll.\nreplace open up the bracket and now you can see that cos alpha nu is actually less than cos alpha.\nright if cos alpha nu is less than cos alpha that means alpha nu is greater than alpha right and.\nthat's what i wanted my current angle was less than 90 degrees which was not acceptable to me i wanted.\nit to be more than 90 degrees so this step has at least ensured that my angle and new angle.", "metadata": {"video_title": "Perceptron Learning Algorithm"}}
{"text": "that's what i wanted my current angle was less than 90 degrees which was not acceptable to me i wanted.\nit to be more than 90 degrees so this step has at least ensured that my angle and new angle.\nis greater than my current angle and i'll maybe slowly keep doing this and move towards convergence slowly slowly i.\nwill keep changing this angle iteratively right because i am going to do this iteratively its not that once i.\npick up a point and i adjust my w i will never revisit that point right i'll keep doing this.\ntill i read convergence that will i keep picking these points again and again cyclically right so at some point.\ni'll keep changing it and then eventually it will cross 90 degree right so that's the intuition that we have.\nhere right and that's why that update makes sense still a bit more to it right i mean some of.\nyou would start should start thinking oh wait when i see a positive point i make an error i change.", "metadata": {"video_title": "Perceptron Learning Algorithm"}}
{"text": "here right and that's why that update makes sense still a bit more to it right i mean some of.\nyou would start should start thinking oh wait when i see a positive point i make an error i change.\nthe w so that this angle becomes something now if the next step if i take a negative point i.\nmake an error i again change the w now how does this ensure that i don't keep toggling like this.\nright for a positive point i change the w so that the angle became less than what it was earlier.\nfor a negative point i changed the w so that the angle became greater than what it was earlier but.\nif i keep doing this then what if i just keep toggling and never reach convergence right so what's the.\nproof of convergence that we have not done that i have just explained you the intuition behind these steps why.\ndo these steps make sense i still need to explain whether this algorithm will converge or not we will get.\nto that later right so now let's try to see this algorithm on a toy data set right so to.", "metadata": {"video_title": "Perceptron Learning Algorithm"}}
{"text": "do these steps make sense i still need to explain whether this algorithm will converge or not we will get.\nto that later right so now let's try to see this algorithm on a toy data set right so to.\nconvince ourselves that the explanation was indeed correct right so we initialize w to a random value uh and so.\nthis is what my w is okay i will just initialize it randomly and you will observe that currently my.\nw is exactly opposite of what it should be right it has a angle less than 90 degrees with all.\nthe negative points and an angle greater than 90 degrees with all the positive points so this is exactly opposite.\nof what i wanted right yes w n suppose x w transpose x equal to 0 is the equation of.\nthe yeah yeah so i'm not happy with the situation because my w is clearly not what i want it.\nto be so let's just run the perceptron algorithm right so what will i do what does the algorithm do.\nit picks up a point randomly from the union of p comma n right so let's say i randomly picked.", "metadata": {"video_title": "Perceptron Learning Algorithm"}}
{"text": "to be so let's just run the perceptron algorithm right so what will i do what does the algorithm do.\nit picks up a point randomly from the union of p comma n right so let's say i randomly picked.\nup a point and the point was p1 okay and my condition is not satisfied so w x w transpose.\nx or the dot product between w and x is less than 0 right whereas i want it to be.\ngreater than equal to 0 so i'll have to apply a correction right so now i'm going to apply the.\ncorrection w is equal to w plus x where x is nothing but the point p 1 okay so if.\ni apply this correction let's see what happens so you can see that my w has changed right and you.\ncan see the as i was expecting the new angle between w and p1 is now less than the older.\nangle right so this greedy step has worked in doing whatever it was said to do right so it has.\nreduced the angle now let me do it again right so now again randomly pick a point and say this.", "metadata": {"video_title": "Perceptron Learning Algorithm"}}
{"text": "angle right so this greedy step has worked in doing whatever it was said to do right so it has.\nreduced the angle now let me do it again right so now again randomly pick a point and say this.\ntime the point that i pick is p2 okay so i'll compute the angle between sorry i'll just compute the.\ndot product w transpose x and again i am seeing that the uh [Music] sorry yeah this is where i.\nwas so i picked the point p 2 and again you can see with this w the angle is still.\ngreater than 0 right sorry greater than 90 degrees so i don't want that so i'll have to apply a.\ncorrection here also what will the correction be i'll add p2 to w and i'll get a new w so.\nthis is what my new w looks like and you can again see that the angle has decreased in fact.\nhere it has already become less than 90 but i don't care about it all i care about is the.\nangle is decreased now let me pick up another point and this time maybe i picked up n1 right and.", "metadata": {"video_title": "Perceptron Learning Algorithm"}}
{"text": "here it has already become less than 90 but i don't care about it all i care about is the.\nangle is decreased now let me pick up another point and this time maybe i picked up n1 right and.\nnow with n1 i again need to apply correction because you can see that the angle is actually quite small.\nright it's forget about greater than 90 is actually quite small so i need to again change it and my.\nadjustment would be w into w is equal to w minus x and when i get do that i get.\nthis new w right and i can again see that the angle has increased okay now let me do again.\npick a random point and say it was n3 no correction is needed because you can see that n3 is.\nalready making an obtuse angle with the weight vector right so no correction is needed because the condition is already.\nsatisfied again i pick up a random point it's n2 so again n2 is already safe because the angle seems.\nto be greater than 90 degrees so i don't need to do any correction now i pick up the point.", "metadata": {"video_title": "Perceptron Learning Algorithm"}}
{"text": "satisfied again i pick up a random point it's n2 so again n2 is already safe because the angle seems.\nto be greater than 90 degrees so i don't need to do any correction now i pick up the point.\np3 and i need to make a correction because with p3 i can see that the angle is greater than.\n90 degrees so i will do the same correction and this is what my new w looks like okay now.\nlet me pick up the point another point randomly and say it is p 2 no correction is needed because.\nyou can see visually that the angle is less than 9 2 again i pick up n1 no correction is.\nneeded the angle is greater than 90 degrees again i pick up n 3 no correction is needed i keep.\ndoing this i keep cycling through the data and i now know that i have reached conversion i cycle through.\nthe entire data once and i will see that no correction is needed that means i have reached conversions and.\nthat is what convergence means i will just cycle through the entire data and if i do not have to.", "metadata": {"video_title": "Perceptron Learning Algorithm"}}
{"text": "that is what convergence means i will just cycle through the entire data and if i do not have to.\nchange my w that means all my positive points have the right angle with w a right angle meaning the.\ncorrect angle with w and all my negative points also have the correct angle with w so i don't need.\nto change anything right so that's how it has changed now the question whether i could have just kept toggling.\nso i picked up n2 i had to make a change if i pick p1 i had to make a.\nchange at the beginning you saw you were making changes so what if these changes were counterproductive right it just.\nmoved somewhere then again it moved somewhere and it ended up changing the angle with the original point which i.\nhad corrected in the last step right so that is not yet clear right whether the algorithm will keep toggling.\nor whether it will converge that is not clear that something that we need to see.", "metadata": {"video_title": "Perceptron Learning Algorithm"}}
{"text": "foreign [Music] okay so we are talking about the problems with gradient descent and as you go along in this.\nlecture we'll try to fix some of those problems but for that discussion it would be useful to understand how.\nto draw contour maps because that would just make our life easier in terms of drawing the plots and so.\non and also Contours as an or contour maps is an important concept to know if you're doing machine learning.\ndeep learning and so on right so the idea here is this side that visualizing things in 3D can sometimes.\nbecome a bit cumbersome right and so uh at least if images are static then you can only view it.\nfrom a certain angle and so on so can we just do a 2d visualization of this traversal along the.\nerror surface and so to do that we need to know what are Contours so let's look at this right.\nso now again we'll start with the 3D plot right so this is like a complex 3D plot that we.", "metadata": {"video_title": "Plotting contours"}}
{"text": "error surface and so to do that we need to know what are Contours so let's look at this right.\nso now again we'll start with the 3D plot right so this is like a complex 3D plot that we.\nhave here okay and now I'll start with this question right that uh if I were to take or let.\nme just draw the contour map and then try to comment this question right so now what I'm going to.\ndo is I'm going to draw something which is called as a contour map corresponding to this 3D plot without.\ntelling you what a contour map is right so I'll just go ahead and draw it and then I'll come.\nback and explain what a contour map is right so you can see what I've done here right so let.\nme just point out a few things that have been done here so what I've done is whatever surface was.\ngiven to me at regular intervals along the z-axis so let me just straighten this so that the z-axis is.\nvertical okay whatever reasons okay this is good yeah maybe this angle is good so at regular intervals along the.", "metadata": {"video_title": "Plotting contours"}}
{"text": "vertical okay whatever reasons okay this is good yeah maybe this angle is good so at regular intervals along the.\nZ axis I have taken slices right so these are the slices that I have taken at regular intervals along.\nthe z-axis and I've also labeled the intervals right so this is minus 0.8 this is the slice at minus.\n0.6 minus 0.4 minus 0.2 and this is at -1 right so these are regular intervals because they are happening.\nat intervals of 0.2 and similarly I keep doing so at every point two distance I have taken a slice.\nnow if I were to see this from the top okay and maybe I should delete yeah so if I.\nwere to um yeah so I've done these slices at regular intervals and I've also labeled the slices now if.\nI were to look at it from the top this is what it would look like right and this is.\nexactly what the contour map looks like which is drawn on the right hand side right so contour map is.", "metadata": {"video_title": "Plotting contours"}}
{"text": "I were to look at it from the top this is what it would look like right and this is.\nexactly what the contour map looks like which is drawn on the right hand side right so contour map is.\nlike the top view of a 3D plot such that you take slices at regular intervals and then imagine what.\nthe slices would look from the top and that's what you draw here right so at any level here right.\nso any of these circles or any of the Rings that you see here they're not all perfect circles but.\nI can call them Rings any of the Rings that you see on this side note that for across that.\nring the value of the z-axis is the same right because that's how I've taken I have taken a slice.\nwhich is parallel to the X Y axis so it's at a particular level on the Z axis so this.\nring for example corresponds to the level one right so this link here is level one ring this ring here.\nis level 0.8 ring point six point four point two and so on right so that's how uh this has.", "metadata": {"video_title": "Plotting contours"}}
{"text": "ring for example corresponds to the level one right so this link here is level one ring this ring here.\nis level 0.8 ring point six point four point two and so on right so that's how uh this has.\nbeen uh drawn it's every ring corresponds to one value along the Z axis in our case or the X.\nY axis would be like the w b axis and the Z axis would be the loss axis so every.\nslice here would correspond to the same value of the loss function because R cross along the ring the value.\nof the loss function is there because we have taken a slice along the vertical axis right so you have.\na parallel plane so the loss would be the same across that axis right and when you look at it.\nfrom the top uh this is how it looks like and that's exactly what the contour map looks like now.\nthere are certain other interesting things to be said about the contour map right and let me just get a.\nhang of this I want to put certain view from which the point that I want to explain becomes easy.", "metadata": {"video_title": "Plotting contours"}}
{"text": "hang of this I want to put certain view from which the point that I want to explain becomes easy.\nokay this is the view that I want now what you see here right uh on this plot here is.\nthat in some cases the distance between the Rings is large right not just that the distance between the Rings.\nis large here but it's very small here right so I'm talking about the same two rings right so let.\nme just highlight those two rings so I'm talking about this ring and this ring okay I'm not circling the.\nfull thing because it will just become a bit untidy so if I look here the distance between the two.\nrings is large but the distance between the same two rings here is a bit small right it's quite smaller.\nthan what it is here so why is that happening right and you can see that it's relates to the.\nidea of slope and that's why I wanted this other view here so if you look at this right here.\nhere the slope is very Steep and here the slope is gentle right so now if I look at the.", "metadata": {"video_title": "Plotting contours"}}
{"text": "idea of slope and that's why I wanted this other view here so if you look at this right here.\nhere the slope is very Steep and here the slope is gentle right so now if I look at the.\ndistance between these two rings horrible so if I look at the distance between these two rings right so let.\nme just delete things yeah now what I was saying is that if I look at the difference between distance.\nbetween these two rings okay and now imagine I am looking for the top then here the slope is gentle.\nwhereas here the slope is steep so wherever the slope is gentle the distance between the two rings would be.\nlarge whereas whether the slope is steep the distance between the two rings would be small right so this the.\nidea of steep and gentle slope was very important for uh the discussion that we were doing right we needed.\nto know in which regions was the slope Steep and in which regions was the gentle and now the contour.", "metadata": {"video_title": "Plotting contours"}}
{"text": "to know in which regions was the slope Steep and in which regions was the gentle and now the contour.\nmaps actually capture that information right so you just need to look at the contour map and if I see.\nthat here the distance is large that means the slope would have been gentle here the distance is small that.\nmeans in that region the slope would have been very steep right so looking at the contour map because the.\nlevel information is also there I can actually guess what the 3D surface looks like and there will be a.\ncontour map corresponding to every 3D surface right so let us look at one more 3D surface the main takeaways.\nhere are this that these Rings correspond to levels on the loss axis and they are equidistant right that means.\nthey have been taken at equal distances on the loss axis the other is the distance between the Rings tells.\nus about the slope in that direction if the distance is large then the slope was gentle and I gave.", "metadata": {"video_title": "Plotting contours"}}
{"text": "us about the slope in that direction if the distance is large then the slope was gentle and I gave.\nyou explain it with the help of this example I have this slope was steep then the distance was small.\nas we saw here right so that's what the contour maps capture so in short they capture all the information.\nthat we are interested in when we are talking about gradient descent based algorithms right so let me just look.\nat one more example right let me take a different function now suppose I take this function okay this is.\nwhat this function looks like right now let me try to guess what the contour map would look like right.\nuh or other let me just draw the contour map right this is what the contour map looks like and.\nhere again you can see that here yeah here the distance is large and does it correspond to my intuition.\nthat the slope would be large in that region so let's see that right indeed the slope was sorry the.", "metadata": {"video_title": "Plotting contours"}}
{"text": "that the slope would be large in that region so let's see that right indeed the slope was sorry the.\nslope would be uh gentle in that region right so whenever the slope is gentle the distance between the Rings.\nis large and whenever the slope is steep for example here it's very steep right so in this region as.\nyou can see this if I look at the plot on the left hand side these here the slope is.\nvery uh yeah here the slope is very steep so the distance between the Rings is small here the slope.\nis gentle so the distance between the Rings is large right so that's that's the idea captured in a contour.\nmap you could take there are many functions given here so I would encourage you to kind of go back.\nand try out different functions right so this is yet another function I think quite uh complex and we can.\ntry to draw the contour map for that it looks quite horrible right but you could just relate it to.", "metadata": {"video_title": "Plotting contours"}}
{"text": "try to draw the contour map for that it looks quite horrible right but you could just relate it to.\nthe intuition that we had right and maybe inverse functions are a bit uh perhaps I should stay away from.\nthat right but this function now now if I try to draw the contour map for this function it again.\nmatches the intuition that we had that in the regions where the slope is uh yeah this is what the.\nfunction looks like so here the slope is throughout very steep right so the distance between the Rings is smaller.\nhere okay so we'll come back to the original function that we had yeah and I'll draw the Contour plot.\nhere and this is what it looks like right so I think if you play around with this function you.\nwill get a lot of insights into what the Contours Maps capture and the main intuition is about the slopes.\nand the distance between the Rings right so now that we understand what contour maps are right let's try to.", "metadata": {"video_title": "Plotting contours"}}
{"text": "and the distance between the Rings right so now that we understand what contour maps are right let's try to.\ndo this we will revisit uh the gradient descent algorithm and now try to explain in terms of the movement.\non the contour map right so this was my uh 3D plot and what you see here and here is.\nthe corresponding contour map right and you can see that in this region which corresponds to this region here the.\nslope is very gentle hence the distance between this contour line and the next contour line which is not even.\nvisible is very large right and here right as I am going into the valley as I'm going into the.\nvalley you can see that all the contour lines are very close to each other because the slope is very.\nsteep there again here as you can see the here the slope is gentle so the distance between the contour.\nlines is high but as I keep coming down the slope becomes sharp and my distance between the contour lines.", "metadata": {"video_title": "Plotting contours"}}
{"text": "lines is high but as I keep coming down the slope becomes sharp and my distance between the contour lines.\ndecreases right and same thing you can see everywhere right wherever the slope is sharp you will see that the.\ndistance between the contour lines is smaller now this entire gradient descent algorithm now I could just visualize it on.\nthe contour map itself right and here you have both in Parable so these are the two views that we.\nhave the 3D surface and its contour map at the bottom then the contour map and what happens actually to.\nthe sigmoid function right so let's try to look at both this views and get comfortable with them right so.\njust as the point was moving on the 3D loss surface you can also show the point moving on the.\n2D surface so now I just have the WB plane and I can now I can see how the loss.\nfunction is imposed on the WB plane right because I can draw the contour map and the contour map gives.", "metadata": {"video_title": "Plotting contours"}}
{"text": "2D surface so now I just have the WB plane and I can now I can see how the loss.\nfunction is imposed on the WB plane right because I can draw the contour map and the contour map gives.\nme all the information right it tells me what the loss levels are right it tells me what the loss.\nlevels are I can see all the loss levels here and also by looking at the difference between the loss.\nlevels I'll know what the slope looks like and now here again I see that in this region here right.\nwhere the slope was sharp there is a sharp movement there is a rapid movement which happens let's play it.\nagain so now the slope is gentle so it's moving very small slowly on the 2D contour map here and.\nthen it as it moves towards the steep region now it will speed up and then again it enters the.\nshallow or the gentle slope region and then again it will move slowly right and now I can just get.\nrid of this 3D view I'll just work with the 2D view where I have the contour map I know.", "metadata": {"video_title": "Plotting contours"}}
{"text": "shallow or the gentle slope region and then again it will move slowly right and now I can just get.\nrid of this 3D view I'll just work with the 2D view where I have the contour map I know.\nwhat the loss is that the loss is high so this is color coded so whether for dark red means.\nHigh loss functions the dark blue means loss low loss regions and there's a gradation from dark red to dark.\nblue red so pink would mean somewhere in between and this is what the pink here is right and now.\nit's moving on this 2D plot and I can see what is happening so now I have reached close to.\nconversion so my points are almost on the sigmoid function and earlier I was moving very very slowly when I.\nwas at this point right and now as I am going to enter the value you will see a sudden.\nchange in my w and B a very large change in my wnb and the entire shape of my sigmoid.\nfunction will change right and now I have entered the value I'm close to convergence and slowly slowly moving so.", "metadata": {"video_title": "Plotting contours"}}
{"text": "function will change right and now I have entered the value I'm close to convergence and slowly slowly moving so.\nthat these two points become completely on the sigmoid curve they are not completely on the curve yet slightly off.\nbut it will slowly move towards that conversion okay so now we know how to see a 3D plot and.\nimagine what its contour map would look like or the other way around that look at a contour map and.\nunderstand what the 3D plot could have looked like right because we know that these were the regions of gentle.\nslope the and the lost levels are marked I have not marked the actual levels but I've just color coded.\nthem so wherever you see dark red you know it's bad and whenever you see dark blue you know it's.\ngood okay fine so we'll end this module here so we know about Contours now and now we'll uh try.\nto go back to our discussion on gradient descent and see whether we can fix the problem that we had.\ndiscussed okay.", "metadata": {"video_title": "Plotting contours"}}
{"text": "[Music] so welcome back uh we're now going to talk about the proof of uh convergence right so the idea.\nis that you have some intuition about how this algorithm works so let's now formalize it right whether it will.\nactually converge or not right so i'm going to first state the theorem and before that i'll define something so.\ntwo sets p and n of points in n-dimensional space are called absolutely linearly separable and i'm just going to.\nread out the definition if n plus 1 real numbers w naught to w 9 exist and you recognize these.\nnumbers or these variables as the weights that i want right so that's what i want to find so of.\ncourse the definition will contain these ways right so if such numbers exist such that for every point right and.\nremember the points are themselves vectors right so this is oops a vector x 1 to x n belonging to.\np so every vector belonging to p satisfies this condition and similarly every vector belonging to n satisfies the other.", "metadata": {"video_title": "Proof of Convergence"}}
{"text": "p so every vector belonging to p satisfies this condition and similarly every vector belonging to n satisfies the other.\ncondition right and this could be greater than equal to zero right okay i have okay i fixed the uh.\ninequalities so uh every point belonging to p should satisfy the greater than equal to w naught condition and every.\npoint belonging to n should satisfy the less than w naught condition right so if that happens then the points.\nare said to be linearly separable that's just a definition now um so now here's the statement of the theorem.\nright so if the sets p and n are finite and linearly separable that just means your data is finite.\nand is linearly separable the perceptron nulling algorithm updates the weight vector a finite number of times right so what.\ndoes it mean so you remember that in the algorithm while convergence while not convergence we are updating the weight.\nvector w is equal to w plus x in some cases w is equal to w minus x in some.", "metadata": {"video_title": "Proof of Convergence"}}
{"text": "does it mean so you remember that in the algorithm while convergence while not convergence we are updating the weight.\nvector w is equal to w plus x in some cases w is equal to w minus x in some.\ncases right and the question was what if this keeps toggling and then we keep updating the vector infinitely or.\nwe never exit the loop like the loop becomes an infinite loop so this theorem is saying that will not.\nhappen right it will update it a finite number of times right so what is it saying that if the.\nvectors in p and n are tested cyclically one after the other and that's what we are doing we're picking.\nup vectors randomly for p and n our weight vector w is found after a finite number of steps t.\nsuch that it can separate the two sets p and n right so that's what this is saying that this.\nwill not go on infinitely if your data is linearly separable the perceptron learning algorithm will find a weight vector.", "metadata": {"video_title": "Proof of Convergence"}}
{"text": "will not go on infinitely if your data is linearly separable the perceptron learning algorithm will find a weight vector.\nw after a finite number of steps right now how do you prove this so let's look at that so.\nlet's look at the proof of this okay so i'll start with the proof uh first i'll talk about the.\nsetup so if x belongs to n right it's an a it's a point for which the output is 0.\nthen minus of x belongs to p this is trivial because if x belongs to n then it means that.\nw transpose x is going to be less than 0 which implies that w transpose minus x is going to.\nbe greater than equal to 0 right so now what this observation allows me to do is that we can.\njust consider a single set p prime which is the union of p and n minus what is n minus.\nn minus is the negation of all the points in n right and now for every point belonging to p.\nprime right which is the union of p and your n minus i want to ensure that w transpose p.", "metadata": {"video_title": "Proof of Convergence"}}
{"text": "n minus is the negation of all the points in n right and now for every point belonging to p.\nprime right which is the union of p and your n minus i want to ensure that w transpose p.\nshould be greater than equal to zero right so this is just sim just changing the notation so earlier if.\ni wanted w transpose p should be greater than equal to 0 right and w transpose n which is an.\nn is a point which belongs to the negative side should be less than 0 right but now what i've.\ndone is from every n i have constructed another p right which is just the negative of n so now.\nevery point in my data set has the output to be plus one right it has the output to be.\np right one sorry uh because i've just take the negative other point so for whatever point for the original.\npoint the output was zero so for the negative of that point the output should be one right so i've.\njust constructed the single set which is a collection of all the points for which the only condition that i.", "metadata": {"video_title": "Proof of Convergence"}}
{"text": "just constructed the single set which is a collection of all the points for which the only condition that i.\nneed to satisfy now is w transpose p should be greater than equal to zero for all the points in.\np prime where p prime is as defined on this slide right so just a simple change nothing uh great.\nhere right this will just uh simplify some of the proof that some of the steps in the proof right.\nokay so i'll start with the algorithm so p is all the inputs with label one n is all the.\ninputs with label zero n is this negative of and minus is the set which contains the negations of all.\npoints in n and now i'm defining p prime as a union of p and n minus right and now.\ni'm going to initialize w randomly again i'll have this convergence condition and what i'm going to do i'm going.\nto pick up a random p a random point belonging to p prime where p prime is the union defined.\nabove so what am i going to do till it does not converge i'm going to pick up a random.", "metadata": {"video_title": "Proof of Convergence"}}
{"text": "to pick up a random p a random point belonging to p prime where p prime is the union defined.\nabove so what am i going to do till it does not converge i'm going to pick up a random.\npoint p belonging to p prime and for that point i'm going to check if the condition is not favorable.\nright this the condition should have been that the summation w i p i should have been greater than equal.\nto 0 but if that is not the case then i'm going to make an update and my update is.\njust going to be w is equal to w eclipse p right so notice that i don't need the other.\nif condition because now i have converted all points to points such that the label has to be 1 that.\nmeans the only condition that i care about is that for all points the dot product between w and p.\nshould be greater than equal to zero if that condition is not satisfied which is what this if loop is.\nsaying then i need to make a correction as i just simplified my algorithm nothing has changed conceptually it's just.", "metadata": {"video_title": "Proof of Convergence"}}
{"text": "saying then i need to make a correction as i just simplified my algorithm nothing has changed conceptually it's just.\nthat i've conveniently changed all the ends to n minus and then that's why all my points in the data.\nbecome positive points that means for all of them the label is one now and then in that case i.\nonly need to worry about one condition right that's what i have done further for convenience what i'm going to.\ndo is that i'm going to normalize all my inputs i can do that right whatever inputs i have i'll.\njust normalize them and that does not change anything because my original point was p and i have normalized it.\nso it has become p divided by the norm of p that's what normalization mean and if w transpose p.\nis greater than equal to zero then this is also greater than equal to zero so it does not change.\nanything with respect to the conditions so it's a simple step which does not change anything conceptually for me so.", "metadata": {"video_title": "Proof of Convergence"}}
{"text": "anything with respect to the conditions so it's a simple step which does not change anything conceptually for me so.\ni'm just going to normalize all the inputs and that ensures that the norm of p is equal to 1.\nright and this will help me in simplifying some steps in the proof okay so and now i'll define one.\nmore quantity which is w star so that let w star be the normalized solution vector what do i mean.\nby that i've started with the assumption the proof says that if the data is linearly separable so if the.\ndata is linearly separable that means there exists some w right and i'm going to call that w as w.\nstar such that that is the value which will linearly separate my positive points for my negative point so some.\nw exists i'm sure about that and i'm going to call that w star it's just that i don't know.\nwhat that w star is but i can assume the existence of such a w star right so that's the.", "metadata": {"video_title": "Proof of Convergence"}}
{"text": "w exists i'm sure about that and i'm going to call that w star it's just that i don't know.\nwhat that w star is but i can assume the existence of such a w star right so that's the.\nw star that i'm going to use right we don't know what it is but it exists right now suppose.\nat some time step t uh we inspected the point p i right what does that mean i have points.\np1 p2 all the way up to such some pm which are belonging to my set p prime right these.\nare all the points and at every point inside that loop i'm going to randomly pick up one point right.\nso that lets that point be pi so i'm at the t iteration of the loop and the point which.\ni have picked up is p i okay and we found that the condition is violated we wanted w transpose.\np to be greater than equal to 0 and suppose it's less than 0 right suppose the condition has been.\nviolated okay so now what will i do we make the correction that my new weight vector is going to.", "metadata": {"video_title": "Proof of Convergence"}}
{"text": "p to be greater than equal to 0 and suppose it's less than 0 right suppose the condition has been.\nviolated okay so now what will i do we make the correction that my new weight vector is going to.\nbe the weight vector the current time step plus pi right that's what the correction that we are going to.\ndo is that correct okay now if i am going to make that correction so now let beta be the.\nangle between w and w t plus 1. okay now what does cos of beta going to be it's going.\nto be this dot product okay and why don't i have a w star here the norm of w star.\nin the denominator because i had already assumed that w star is the normalized wake twitter right that means the.\nnorm is equal to 1 so i don't need to write that in the denominator okay so this is what.\ncost beta is now let's just try to expand this right so i'll now look at the numerator alone uh.\nlet me just get rid of some stuff here so the numerator is uh w star the dot product between.", "metadata": {"video_title": "Proof of Convergence"}}
{"text": "cost beta is now let's just try to expand this right so i'll now look at the numerator alone uh.\nlet me just get rid of some stuff here so the numerator is uh w star the dot product between.\nw star and w t plus one remember that w star and w t plus one are weight vectors right.\nthese are vectors so i can talk about the dot product and now i can just substitute the value of.\nw t plus 1 which is nothing but w t plus p i which is again of course a vector.\nit's the addition of two vectors now i'm going to open up these brackets so i get w star the.\ndot product between w star and w t and the dot product between w star and p i okay now.\ni'm going to define a new quantity so let me just explain something before i define that quantity so i.\nhave the points p1 p2 all the way up to some pm right these are all the points which belong.\nto my p prime okay now i can compute the dot product between each of these points and w star.", "metadata": {"video_title": "Proof of Convergence"}}
{"text": "have the points p1 p2 all the way up to some pm right these are all the points which belong.\nto my p prime okay now i can compute the dot product between each of these points and w star.\nokay i don't know what w star is but i can define the dot product i can say that this.\nis the dot product okay and now what is dot product dot product is some scalar value right so for.\neach of these i'll get some value belonging to r right some scalar value i will get now i can.\ndefine the minimum of all these quantities okay right so let me call that minimum as delta so what is.\ndelta i have computed the dot product between w star and each of the points in p prime and whatever.\nis the minimum value i am calling that as delta okay so what does that mean that any of these.\nquantities here are going to be sorry [Music] yeah are going to be greater than equal to delta right because.\ndelta is the minimum value so all these values are going to be greater than equal to delta so that's.", "metadata": {"video_title": "Proof of Convergence"}}
{"text": "delta is the minimum value so all these values are going to be greater than equal to delta so that's.\nhow i'm going to define delta so now with that definition i can say that this quantity here is going.\nto be greater than equal to this quantity this directly follows for my argument that all of these quantities are.\ngoing to be greater than equal to delta so irrespective of which of these pi's i have picked up p1.\nto pm the wstar.pi is going to be greater than equal to delta hence this sum here is going to.\nbe greater than equal to the sum here right follows simply from the definition right where delta is the minimum.\ndot product between w star and all the points that i have okay is this definition clear okay so now.\nlet's proceed so now i can again for w t again replace it by the definition right so wt was.\nagain wt minus 1 plus some pj so at the previous time step i would have taken some pj right.\nand wt minus 1 i would have adjusted by adding some pj so pj is again one of these p.", "metadata": {"video_title": "Proof of Convergence"}}
{"text": "again wt minus 1 plus some pj so at the previous time step i would have taken some pj right.\nand wt minus 1 i would have adjusted by adding some pj so pj is again one of these p.\npoints that you see here so again i'll do that expansion so i'll have i let me just yeah so.\nthis is what the expansion will look like and now again this quantity i know that this quantity is going.\nto be greater than equal to delta the same argument so this quantity is the dot product between w star.\nand one of these points and if delta is the minimum of those dot products so this quantity is going.\nto be greater than equal to delta so i can replace that by delta and now i will have 2.\ndelta is that ok and i can keep going like this unless a till from t i'll have t t.\nminus 1 t minus 2 till i reach w naught right and i'll have some multiplier k here right so.\nnow the question is why is this multiplier k and why is it not t t is the number of.", "metadata": {"video_title": "Proof of Convergence"}}
{"text": "minus 1 t minus 2 till i reach w naught right and i'll have some multiplier k here right so.\nnow the question is why is this multiplier k and why is it not t t is the number of.\nsteps i have taken right but at every step remember i may not have made a correction because for some.\npis that i would have picked the right condition would have been satisfied and i wouldn't have made a correction.\nso my k the number of updates which i make is going to be less than equal to t that's.\nwhy i have k here and not t delta so k is a quantity which is less than equal to.\nt because every time i make an update this bracket opens up and then i replace some quantity there by.\na delta so this bracket will open up that means this uh this in many cases this pj would not.\nhave been there right i would have not changed my weight vector so this bracket will open up only that.\nmany times as many times the condition was violated and the maximum number of times the condition can be violated.", "metadata": {"video_title": "Proof of Convergence"}}
{"text": "many times as many times the condition was violated and the maximum number of times the condition can be violated.\nis t because i have made t update so far so i would have made some k less than t.\nupdates is that okay less than equal to t updates right ah let's see yeah so we do not make.\na correction at every time step so we make a correction only if this condition is satisfied and so at.\ntime step p we would have made k less than equal to t corrections this is exactly what i had.\nsaid and every time we make a correction we get a delta term here right in this equation right because.\nhere i made a correction so because of that i got a delta here here i made a correction so.\nbecause of that i got a delta again here right every time i make a correction i get delta so.\ni will get k such deltas okay so this is what the numerator looks like okay now so so far.\nwe have that w transpose pi is less than equal to zero hence we made the correction right for the.", "metadata": {"video_title": "Proof of Convergence"}}
{"text": "i will get k such deltas okay so this is what the numerator looks like okay now so so far.\nwe have that w transpose pi is less than equal to zero hence we made the correction right for the.\ncurrent point this condition was the unfavorable condition was satisfied hence i had made a correction and cos beta was.\ngiven by this definition of that the numerator is greater than equal to this condition right so the numerator is.\nequal to this and then i derived a series of things that the numerator is greater than equal to zero.\nokay now i'm going to focus on the denominator okay so the denominator is the norm of w t plus.\n1 so the square of the denominator would be the square of the norm okay now again w t plus.\n1 the norm i can first i'm going to expand wt plus 1 which is wt plus pi right so.\nwt plus pi norm square right that is what this is and the norm is just the dot product of.\nthe vector with itself right so wt plus uh the square root of the dot product so now this would.", "metadata": {"video_title": "Proof of Convergence"}}
{"text": "wt plus pi norm square right that is what this is and the norm is just the dot product of.\nthe vector with itself right so wt plus uh the square root of the dot product so now this would.\nbe wt plus pi and dot product with wt plus pi okay now again i'll just open up the brackets.\nokay so i get the square of the norm of wt plus 2 times wt dot pi okay plus pi.\nsquare and i'd assume that the pi's are normalized so the norm of pi is going to be 1 so.\ni'm just going to replace that by 1 okay and i'm going to do something else also so how did.\ni get from here to here because of this condition i know that w t p i is less than.\nequal to 0 that's why i am here right and if that is the case then 2 times w t.\np is also going to be less than equal to 0 that means this sum is going to be less.\nthan or equal to this sum let me just get rid of all the marks so this sum okay has.\na quantity which is negative okay this sum does not have that quantity so it follows that this sum is.", "metadata": {"video_title": "Proof of Convergence"}}
{"text": "than or equal to this sum let me just get rid of all the marks so this sum okay has.\na quantity which is negative okay this sum does not have that quantity so it follows that this sum is.\ngoing to be less than equal to this sum that okay because it's just this sum plus a negative quantity.\nso it's going to be less than equal to okay now the next step of course is to replace p.\ni squared by 1 because p i's are normalized this was the simple assumption i had made now by induction.\nagain the same thing will happen i'm going to now i started with wt plus 1 and i ended up.\nwith this now i'm at wt i'll again do the same step there itself right and what will i get.\nthis is going to be wt minus 1 plus 1 right this will again happen this one which i had.\ncome here again the same thing will come here and i'll get 2 and how many such ones will get.\nadded the same argument as many times i make an update a 1 will be added right where is the.", "metadata": {"video_title": "Proof of Convergence"}}
{"text": "come here again the same thing will come here and i'll get 2 and how many such ones will get.\nadded the same argument as many times i make an update a 1 will be added right where is the.\none coming from the one is coming from this quantity if i made an update then i had added a.\npi if i had added a pi then i'll get this p i squared term and i'll get a 1.\nthere so again here i'll have k right i'll keep simplifying this and i'll get to w naught square plus.\nk by the same observation that we made about delta while dealing with the numerator okay so now we have.\nthe numerator is greater than equal to a certain quantity and the denominator square is less than or equal to.\ncertain quantity so now these two conditions combined with the definition of cos beta will give us the condition this.\ncos beta is greater than equal to this quantity so i have simply substituted the numerator and i have substituted.", "metadata": {"video_title": "Proof of Convergence"}}
{"text": "cos beta is greater than equal to this quantity so i have simply substituted the numerator and i have substituted.\nthe denominator the simplified denominator and this would be greater than equal to because the numerator is greater than equal.\nto this quantity and the denominator is less than equal to this quantity right so the numerator is greater than.\nsomething the denominator is less than something so if i put the numerator over the denominator then the end result.\nwould be that cos beta is greater than equal to this quantity that you see here right so now here.\nyou have a k in the numerator and a square root of k in the denominator so which means i.\ncan roughly say this cos beta grows proportional to square root of k right and what is k k was.\nthe number of times i have updated my weight vector and this is the quantity that i wanted to handle.\non this is the quantity that i wanted to show was finite i wanted to show that k cannot tend.", "metadata": {"video_title": "Proof of Convergence"}}
{"text": "the number of times i have updated my weight vector and this is the quantity that i wanted to handle.\non this is the quantity that i wanted to show was finite i wanted to show that k cannot tend.\nto infinity okay now cos beta is going to be greater than or cos beta is going to grow proportional.\nto square root of k so now as k increases cos beta will increase right now if k becomes arbitrarily.\nlarge if it goes to infinite that means my algorithm is not converging it's it's keeping on updating then what.\nwill happen to cos beta cos beta will keep on increasing but can cos beta keep on increasing no cos.\nbeta is bounded between uh minus one and one right so cos beta has to be less than equal to.\none so cos beta cannot grow infinitely that means k cannot go infinitely k has to be bounded by a.\nmaximum number hence we have shown that k is a finite number so if the data is linearly separable right.", "metadata": {"video_title": "Proof of Convergence"}}
{"text": "maximum number hence we have shown that k is a finite number so if the data is linearly separable right.\nthat means after a finite number of iterations the perceptron rolling algorithm will converge so although in our toy example.\nit does not seem intuitive right that's easy to build this intuition that okay for one point i changed it.\nfor the second point again i changed it why were these updates not counterproductive so now we have a proof.\nwhich says that the number of updates you will make will be finite if the data is linearly separable okay.\nso that's what the perceptron uh this that's what the proof for the perceptron's learning algorithm say that it will.\nconverge okay so now coming back to our questions which we had started out so what about non-boolean inputs yes.\nso real valued inputs are allowed in perceptron do we always need to hand code the threshold no now we.\ncan learn all the parameters not just the threshold w naught but also w1 to wn are all inputs equal.", "metadata": {"video_title": "Proof of Convergence"}}
{"text": "can learn all the parameters not just the threshold w naught but also w1 to wn are all inputs equal.\nno now a perceptron allows weights what about functions which are not linearly separable so this is still not possible.\nwith a single perceptron because we have seen that the boundary is linearly separable the previous proof was also that.\nit will be able to find the weights only when the data is linearly separable right but we'll see how.\nto handle this and that's what this course is about that what if data is not linearly separable okay so.\ni'll end this module here and we'll talk a bit more about linearly separable functions.", "metadata": {"video_title": "Proof of Convergence"}}
{"text": "[Music] okay so we are now at the last uh module for lecture two where we will be introducing a.\nnetwork of perceptrons right so now the word network has come in and will be seeing what a network looks.\nlike and then we will talk about the representation power of such a network of perceptrons right so what we.\nare going to do is we are going to see how do you implement any boolean function irrespective of whether.\nit is linearly separable or not to begin with using a network of perceptrons although we know that we're using.\na single perceptron we cannot build it all right we cannot implement okay so now for this discussion uh this.\nis more like an illustrative proof right so we are going to not have a proof which has like a.\nset of mathematical steps it will just be like we'll prove something by constructing a network itself right so let's.\nfor this discussion right we'll assume that uh true is equal to plus 1 and false equal to minus 1.", "metadata": {"video_title": "Representation power of a network of perceptrons"}}
{"text": "for this discussion right we'll assume that uh true is equal to plus 1 and false equal to minus 1.\nas opposed to 0 and 1 that we have been using so far right and we'll consider the case of.\ntwo inputs okay and we'll have four perceptrons so since i'm going to have a network of perceptrons it is.\nclear that i'm going to have more than one perceptron so now i'm going to start with four perceptrons right.\nand each input is connected to all the four perceptrons okay with specific weights and the red weights are minus.\none and the blue weights are plus one okay why so all that we are not considering right now it's.\njust that you have two inputs four perceptrons every input is connected to every perceptron by specific weights some weights.\nare minus one some weights are plus one right and then the bias or w naught is equal to minus.\ntwo okay so w naught is minus two so what is the perceptron rule w naught plus the weighted sum.", "metadata": {"video_title": "Representation power of a network of perceptrons"}}
{"text": "are minus one some weights are plus one right and then the bias or w naught is equal to minus.\ntwo okay so w naught is minus two so what is the perceptron rule w naught plus the weighted sum.\nof the inputs should be greater than zero for it to be one so what does that mean if w.\nnaught is minus 2 then the perceptron will fire only if the weighted sum of the inputs is greater than.\n2 because then you will have minus 2 plus a quantity which is greater than 2 so that quantity would.\nbe greater than 0 greater than equal to 0 and then the perceptron will fire right so it means that.\nthe weighted sum of the inputs should be greater than equal to two so this is all i have done.\nthis match fixing right there is i've like hard coded everything so far okay then i have one more perceptron.\nwhich is my output perceptron what does that mean that the output of this perceptron will be the output of.\nthe network and if i want to implement a function if i say i want to implement the xor function.", "metadata": {"video_title": "Representation power of a network of perceptrons"}}
{"text": "the network and if i want to implement a function if i say i want to implement the xor function.\nusing this network that means that for a given input 0 0 0 1 1 0 1 1 the output.\nof this green perceptron should match the truth table of the xor function then i would say that i have.\nimplemented the xor function that is what i want and now each of these perceptrons the red perceptrons is connected.\nto the green perceptron using a weight and i'm going to call those weights as w1 w2 w3 w4 okay.\nand then the output is y this is all the setup that i have now why this setup where am.\ni headed all that is not clear but you cannot stop me from having this setup all this is straight.\nforward i have two inputs four perceptrons and then one output perceptron for the first set of red perceptrons i.\nhave hard coded the widths okay now i'm going to introduce some terminology and this terminology is going to stay.", "metadata": {"video_title": "Representation power of a network of perceptrons"}}
{"text": "have hard coded the widths okay now i'm going to introduce some terminology and this terminology is going to stay.\nwith us for the rest of the course right so this network contains three layers the layer containing the inputs.\nis called the input layer so x1 x2 is the input layer then the layer containing the four perceptrons is.\ncalled the hidden layer right because this is between the input and the output so it is not exposed to.\nthe outer world i have something happening here which the outer world does not care about i don't care about.\nwhat is the output of these red perceptrons i care about the output of the y person or the green.\nperceptron this is a hidden layer and the output neuron is what is forming the output layer right so i.\nhave three layers input layer hidden layer and output layer right and the outputs of the four perceptrons in the.\nhidden layer i am denoting them as h1 h2 h3h4 the output of the final layer i am denoting it.", "metadata": {"video_title": "Representation power of a network of perceptrons"}}
{"text": "hidden layer i am denoting them as h1 h2 h3h4 the output of the final layer i am denoting it.\nas y and the red and blue edges are called the layer 1 weights and w1 w2 w3 w4 are.\ncalled layer 2 weights okay so there are two layers one hidden layer one output layer and i'm not counting.\nthe input layer because it's there right so this layer one weights are called the red and blue edges are.\nthe layer 1 weights and w 1 w 2 w 3 4 are the layer 2 panes okay this is.\nall definition all terminology now is the interesting part i'm going to claim that this network can be used to.\nimplement any boolean function whether that function is linearly separable or not what does that mean it means what i'm.\ntrying to say is that i can i have this generic template which has now four weights w1 w2 w3.\nw4 everything else in the network has been hard coded right the only four variables are w1 w2 w34 using.", "metadata": {"video_title": "Representation power of a network of perceptrons"}}
{"text": "trying to say is that i can i have this generic template which has now four weights w1 w2 w3.\nw4 everything else in the network has been hard coded right the only four variables are w1 w2 w34 using.\nthis template i am claiming that i can learn implement any of those 16 boolean functions which i had shown.\nincluding the xor and the naught of xor function right which uh are not linearly separable to billing below begin.\nwith right so what does that mean is that i can learn specific values of w1 w2 w3 w4 for.\neach of these 16 boolean functions and these might be different values for the different functions such that when i.\nplug in those values and now i start feeding 0 0 0 1 1 0 1 1 then my output.\nwould be the desired output as per the truth value of that function that's the claim i am making and.\nthis looks like an astonishing claim right because i know that some of these functions are not linearly separable that's.", "metadata": {"video_title": "Representation power of a network of perceptrons"}}
{"text": "this looks like an astonishing claim right because i know that some of these functions are not linearly separable that's.\none reason why this is an astonishing claim because i'm claiming that even non-linearly separable boolean functions i can start.\nthe other reason why this is an astonishing claim is that this is like a single template i'm not having.\nlike a separate network or separate configuration for different boolean functions i have the same configuration the only thing that.\nis changing is the weights of w 1 w 2 w 3 w 4 if i change those values then.\ni can implement any boolean function right this may look like an astonishing claim but it's not really if you.\nreally understand what is going on so what is going on here let us try to understand that right so.\neach perceptron in the middle layer fires only for a specific input let's see why that happens right so as.\ni said the red weights are minus 1 minus 1 okay now if the input is minus 1 minus 1.", "metadata": {"video_title": "Representation power of a network of perceptrons"}}
{"text": "each perceptron in the middle layer fires only for a specific input let's see why that happens right so as.\ni said the red weights are minus 1 minus 1 okay now if the input is minus 1 minus 1.\nremember that 0 is minus 1 here so these are the 4 inputs that are possible minus 1 1 1.\nminus 1 and 1 1. these are the four inputs that fire now if the input is minus 1 minus.\n1 what will happen minus 1 into minus 1 will be 1 minus 1 into minus 1 will be 1..\nso the sum will be two so the weighted sum would be greater than equal to zero minus two plus.\ntwo hence this perceptron will fire okay and now i will show that this perceptron will not fire for any.\nof the other three inputs right if this is the input then you have minus 1 into minus 1 which.\nis 1 1 into minus 1 which is minus 1 so 1 minus 1 will become 0 and plus the.\nbias will become minus 2 which is less than 0 so this perceptron will not fire similarly you can see.", "metadata": {"video_title": "Representation power of a network of perceptrons"}}
{"text": "is 1 1 into minus 1 which is minus 1 so 1 minus 1 will become 0 and plus the.\nbias will become minus 2 which is less than 0 so this perceptron will not fire similarly you can see.\nthat for these two inputs this perceptron will not fire so this specific perceptron is only going to fire for.\nthis input because that's how i have confided the weights that's why i had this two red edges there minus.\none minus 1 because with that set of weights the only way this perceptron can fire is for the input.\nminus 1 minus is that okay right similarly you can now go back and check right that the second perceptron.\nthe first perceptron will only fire from minus one minus one with the same argument the second perceptron will only.\nfire for minus one one i'll show you how so this is minus one this is one so if your.\ninput is minus one minus 1 so we'll get minus 1 into 1 is 1 1 into 1 is 1.\nso 1 plus 1 is 2 plus the bias is minus 2 so this quantity would be greater than equal.", "metadata": {"video_title": "Representation power of a network of perceptrons"}}
{"text": "input is minus one minus 1 so we'll get minus 1 into 1 is 1 1 into 1 is 1.\nso 1 plus 1 is 2 plus the bias is minus 2 so this quantity would be greater than equal.\nto 0 hence the perceptron will fire and you can check for any of the other 3 inputs it will.\nnot fire similarly the third perceptron will only fire for one minus one the fourth perceptron will only fire for.\none comma one okay so each perceptron is like kind of specializing for a specific input and it will only.\nfire for that input and the question is so what right i mean what what how does this show that.\nit can handle any function right so we need to go back to the uh the table that we had.\nand the algebra the calculations that were happening there right so let's see what is happening here so if you.\nhave the xor function now xor function is the troublesome function because it's a not it's not a linearly separable.\nfunction so let's see what will happen in this network for the xor function so when you have the input.", "metadata": {"video_title": "Representation power of a network of perceptrons"}}
{"text": "function so let's see what will happen in this network for the xor function so when you have the input.\nas 0 0 now when i say 0 0 assume minus 1 minus 1 the xor truth table says that.\nthe output should be 0 right now we know that h1 h2 s3 h4 the h1 will take on the.\nvalue 1 everything else would be 0. so if i look at summation w i hi it will just be.\nw 1 correct right it will be w 1 into h 1 plus w 2 into h 2 and so.\non so h 2 h 3 h 4 are 0 so the last 3 terms will be 0 and the.\nfirst term which is w 1 will remain now if the input is 1 0 then for the xor truth.\ntable i know that the output should be 1. now my h1 h2 s3 h4 will take on these values.\nand my output would be only w2 because my output is the summation of wi-hi now 3 terms will go.\nto zero there only the w2 into h2 term will remain which will just be w2 because h2 is one.\nthat okay now for the next input w3 the next input w4 right now if i go back to my.", "metadata": {"video_title": "Representation power of a network of perceptrons"}}
{"text": "to zero there only the w2 into h2 term will remain which will just be w2 because h2 is one.\nthat okay now for the next input w3 the next input w4 right now if i go back to my.\nconditions what are the conditions that i want so if i want this to behave like the xor function then.\nw one should be less than w naught right in that case it will not fire and hence it will.\nbe the same as the truth table of the xor function let me just clear some things so w 1.\nshould be less than w naught if that is the case then it will not fire and i will get.\nthe output the desired output is that ok so this is minus 1 right uh ah okay okay oh this.\nis the truth table yeah the two table has a problem so this should have been 0 1 and this.\nshould have been 1 0 okay is always fine now ok so now for this to behave so this is.\nthe aggregated that i get right now what is my y if the aggregation is less than w naught then.\nit will be 0 if it is greater than w naught it will be 1 right so now for this.", "metadata": {"video_title": "Representation power of a network of perceptrons"}}
{"text": "the aggregated that i get right now what is my y if the aggregation is less than w naught then.\nit will be 0 if it is greater than w naught it will be 1 right so now for this.\nto behave like the xor function w 1 should be less than w naught w 2 should be greater than.\nequal to w naught w 3 should be greater than equal to w naught w 4 should be less than.\nw 1. there is nothing new i have done here we did the same exercise earlier on right but now.\nwhat is the difference do i have any contradictions here i do not have any contradictions here earlier when i.\nhad the xor function with a single perceptron i had contradictions in these inequalities now i don't have any contradictions.\nbecause i have four weights w one w two w three w four i have four independent conditions that they.\nneed to satisfy there is no condition that is contradicting right and why is this happening because this is how.", "metadata": {"video_title": "Representation power of a network of perceptrons"}}
{"text": "need to satisfy there is no condition that is contradicting right and why is this happening because this is how.\ni have designed it every neuron here every perceptron here is firing for a specific input and hence that is.\nleading to a condition on a specific weight and two weights are not interacting with each other in any condition.\nhence there are no contradictions that are happening here right so the network has been designed in a way such.\nthat you have four possible inputs and you have now these four possible weights so you have these four degrees.\nof freedom so each weight can adjust to satisfy a given input does that make sense right so that's what.\nis happening here i have come up with conditions which are not overlapping or contradicting with each other so hence.\nirrespective of what the boolean function is now you can go and try the same thing for any other boolean.\nfunction you can go and figure out the conditions and the same thing will happen you will have only w.", "metadata": {"video_title": "Representation power of a network of perceptrons"}}
{"text": "function you can go and figure out the conditions and the same thing will happen you will have only w.\none in one condition w two in another condition w three in another condition w four in another condition right.\nand hence you can always satisfy this by adjusting the w one to w four in a particular manner is.\nthat okay fine so that is why since there are no conditions each input is being handled by a specific.\nperceptron hence you can implement any boolean function in this case right and not just the xr function as i.\nsaid you can implement any of the 16 boolean functions so for each boolean function you will get a different.\nset of conditions for w 1 w 2 w 3 w 4 and you can find some values of w.\n1 w 2 w 3 w 4 which will satisfy that particular boolean function right so you can go back.\nand try this out okay now what if we have more than three inputs can you think of the same.\ntemplate now and adapt it for the case of more than three inputs the same idea will repeat you will.", "metadata": {"video_title": "Representation power of a network of perceptrons"}}
{"text": "and try this out okay now what if we have more than three inputs can you think of the same.\ntemplate now and adapt it for the case of more than three inputs the same idea will repeat you will.\nhave x1 x2 x3 now instead of four perceptrons in the middle layer you will have eight why because there.\nare eight possible inputs now zero zero zero up to one one one so each of these perceptrons will adjust.\nthese red and blue weights in a way that for each perceptron only fires for a specific input right the.\nfirst perceptron will fire when it's zero zero zero second will fire when it's zero zero one and so on.\nright so now each perceptron again fires for a specific input each perceptron interacts with the green perceptron by one.\nof the weights w1 to w8 so again when you write down the conditions in the truth table you'll have.\nconditions like w1 has a specific condition w 2 has a specific condition none of these conditions will contradict none.", "metadata": {"video_title": "Representation power of a network of perceptrons"}}
{"text": "conditions like w1 has a specific condition w 2 has a specific condition none of these conditions will contradict none.\nof the weights will interact with each other in any of these conditions so you can find the values of.\nw 1 to w 8 such that all your 8 inequalities are satisfied for any of the boolean functions right.\nso again you will be able to implement all the boolean functions which are possible okay right so that's the.\nsame template that you have used so what if we have uh n inputs right more than three inputs which.\nis n inputs okay so then how many perceptrons will you need in the middle layer 2 to the power.\nn right so when you had two inputs you needed four when you had three inputs you had eight so.\nwhen you have n inputs there are two raised to n possible values that the inputs can take right so.\neach perceptron will handle one of these specific inputs and then you will have two raised to n weights in.", "metadata": {"video_title": "Representation power of a network of perceptrons"}}
{"text": "each perceptron will handle one of these specific inputs and then you will have two raised to n weights in.\nthe output layer and you could adjust them right yeah yeah bias will stay the same minus n right you.\ncould hand code the bias right so now what does this theorem say now any boolean function of n inputs.\ncan be represented exactly by a network of perceptrons containing one hidden layer with two raised to n perceptrons and.\none output layer containing one percent how do you prove this we've already proved it right we proved it by.\nconstruction i gave you the network i gave you the template if you have n inputs you'll have two raised.\nto n perceptrons each perceptron will cater to a specific input then you'll have these two raised to n weights.\nin the output layer each weight will have a specific condition which you can satisfy because there'll be no contradicting.\nconditions right so note that this network which has 2 raised to n plus 1 perceptrons 2 raised to n.", "metadata": {"video_title": "Representation power of a network of perceptrons"}}
{"text": "conditions right so note that this network which has 2 raised to n plus 1 perceptrons 2 raised to n.\nin the middle layer and 1 in the output layer is not necessary but sufficient all this is saying that.\nyou have you if you have 2 raised to n plus 1 you can implement it but we have already.\nseen that the and function can be implemented by a single perceptron so you don't really need 2 raise to.\nn plus 1 perceptrons right and the catch here is of course while this theorem looks interesting is that as.\nn increases even if it becomes 10 20 100 also it becomes unmanageable right because you'll have 2 raised to.\n100 neurons which is obviously going to be very large right so this increases exponentially so we'll have to see.\nour way of coming back from here right coming back from this exponential situation but for now we are happy.\nthat we can even though even if there are boolean functions which are not linearly separable which a single perceptron.", "metadata": {"video_title": "Representation power of a network of perceptrons"}}
{"text": "that we can even though even if there are boolean functions which are not linearly separable which a single perceptron.\ncannot handle we can have a network of perceptrons which can handle that how to make this network a bit.\nmore compact is a question for later but now we know that we can handle with a network of perceptrons.\nso again while we have proved this why do we care about boolean functions right so let's try to relate.\nthis to a real-world example the same example of predicting movies right so this is how the example plays out.\nthat you have certain features as you call it so these were is actor matt damon is the genre thriller.\nis the director christopher nolan and so on okay all these factors and these are the labels that you are.\ngiven and these are all your past movie examples there are some examples which i'm yeah there are some examples.\nwhich i am calling the positive examples because their label was one and some examples which i am calling the.", "metadata": {"video_title": "Representation power of a network of perceptrons"}}
{"text": "which i am calling the positive examples because their label was one and some examples which i am calling the.\nnegative examples which for my labels are zero and as we have argued earlier for most real world examples the.\ndata will not be linearly separable that means you cannot draw a line such that all your positive examples lie.\non one side all your negative examples lie on the other side right so whatever i have shown you here.\nis a boolean function right these are all boolean outputs and i have taken all the boolean all inputs as.\nboolean right so this is a boolean function and in reality in real world examples this will not be linearly.\nseparable and this is a very real example right you have some n boolean decision factors based on which you.\nare taking a boolean decision or a binary decision and now you want to be able to learn that what.\nthis theorem is saying is that you can come up with a network of perceptrons which will exactly implement this.", "metadata": {"video_title": "Representation power of a network of perceptrons"}}
{"text": "this theorem is saying is that you can come up with a network of perceptrons which will exactly implement this.\nfunction what does that mean is that whatever your truth table looks like and this is what your truth table.\nis looking like right you can come up with a perceptron network of perceptron so that it will exactly be.\nable to implement this boolean function so right this theorem says that you can apply this in real world examples.\nthe catch of course is that you will need an exponential number of neurons uh perceptrons and we'll have to.\nget back from that situation right but for now we are happy in knowing that it can be done right.\nnow how to do it more efficiently is a question for later right so that's what this is uh saying.\nsame thing which has been written on the slides you can read it later on okay so what is the.\nstory so far the networks of the form that we just saw are called multi-layer perceptrons or mlps in short.", "metadata": {"video_title": "Representation power of a network of perceptrons"}}
{"text": "story so far the networks of the form that we just saw are called multi-layer perceptrons or mlps in short.\nyou would have heard about this in several contexts the more appropriate terminology is actually multi-layered network of perceptrons why.\nbecause you have perceptrons there's a network of perceptrons and there are many layers so it's a multi-layered network of.\nperceptrons multi-layer perceptron does not make sense and there's a single perceptron it is not multi-layer it's a multi-layered network.\nof perceptrons but more commonly mlp is the common terminology that is used right the theorem we just saw gives.\nus the representation power of a mlp what does that mean what kind of functions can represent it says that.\nit can represent any boolean function so that's the representation power with a single hidden layer right yeah so that's.\nall that we have for today so we have been talking about the perceptrons we went a bit deeper uh.", "metadata": {"video_title": "Representation power of a network of perceptrons"}}
{"text": "all that we have for today so we have been talking about the perceptrons we went a bit deeper uh.\ninto the perceptron learning algorithm we saw the saw that it actually converges then we saw that a perceptron has.\na limitation it cannot deal with functions which are not linearly separable then we made a case that in many.\nreal-world examples functions would be not would not be linearly separable so we need to deal with that situation and.\nwe dealt with that situation with a network of perceptrons we are still not completely happy because we have an.\nexponential number of perceptrons but we are able to map this again to a real-world problem where you are given.\nsome data and that data may not be linearly separable but now given this template you can come up with.\na network of perceptrons and learn the waves w is in your network such that it will give you perfect.\nseparation on the input data right now from here we need to go a bit ahead and see how can.", "metadata": {"video_title": "Representation power of a network of perceptrons"}}
{"text": "a network of perceptrons and learn the waves w is in your network such that it will give you perfect.\nseparation on the input data right now from here we need to go a bit ahead and see how can.\nwe get rid of this exponential number of neurons and then also go a bit ahead and see if we.\ncan do something about this linear decision boundary so these are things that we'll deal with in the next lecture.\nthank you.", "metadata": {"video_title": "Representation power of a network of perceptrons"}}
{"text": "foreign [Music] module I was telling you that the next topic that we'll talk about is the representation power of.\na multi-layer network of Sigma neurons so let's understand what I mean by that right so just a quick recap.\nright so earlier uh when we are finishing perceptrons we had introduced this multi-layer network of perceptrons and we had.\nspoken about the representation power of a multi-layer network of person drones uh analogously now we are going to talk.\nabout the representation power of a multi-year network of sigmoid neurons so it's just the same template being repeated right.\nand there we had uh made an interesting statement right that a multi-layer network of perceptrons with a single hidden.\nlayer and I don't care about the number of neurons in this layer I know that it can be exponentially.\nbut exponential but I don't care about that a single hidden layer can be used to represent any Boolean function.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 1D functions"}}
{"text": "but exponential but I don't care about that a single hidden layer can be used to represent any Boolean function.\nprecisely and the meaning of that was that if you have an N variable input then you would have a.\nBoolean function of n variables and it would have a certain truth table now no matter what the truth table.\nis right and we had seen that for an N variable input you could have 2 raised to 2 raised.\nto n functions so any of these functions you can come up with a network which can precisely represent this.\nfunction that means when you give a certain input configuration the networking produce the same output as is mentioned in.\nthe truth table right and the way we were doing that is by having these exponential number of uh neurons.\nin the hidden layer such that every neuron was like spatializing for or every perceptron was specializing for one specific.\ninput right so that's what the representation power of a multi-layer network of percept transfers now my goal is to.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 1D functions"}}
{"text": "input right so that's what the representation power of a multi-layer network of percept transfers now my goal is to.\nmake a similar statement okay and this of course I had we had proven by construction or we just constructed.\na network and we were convinced that it can actually represent any network irrespective of whether it's linearly separable or.\nnot right now my intention is to make a similar statement for multi-layer network of Sigma neurons right and the.\nstatement is a multi-layer network of neurons with a single hidden layer so far so good it's the same as.\nwhat I see on the left hand side can be used to approximate okay now here are where the differences.\nstart so this was represent now I'm saying approximate right so it's that means it's not going to be exactly.\nequal any continuous function this is good news right because here I was only talking about Boolean functions now any.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 1D functions"}}
{"text": "equal any continuous function this is good news right because here I was only talking about Boolean functions now any.\ncontinuous function it any function f of x which is continuous I can approximate it to any desired precision now.\nwhat does any desired Precision means right so the way you would have seen this in many mathematical proofs right.\nI would design a desire I would Define a threshold Epsilon right and what I'm saying is that you choose.\nwhatever value for Epsilon right it could be 0.0001 it could be 0.100 zeros one right and I can come.\nup with a network such that if my network output right so now this is the situation right I have.\na n dimensional input I'm calling it r x which belongs to r n okay and I'm going to design.\na network I don't know what the network looks like and the network is going to give me a certain.\noutput let me call that g of X so far I've been calling it at F hat of X and.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 1D functions"}}
{"text": "a network I don't know what the network looks like and the network is going to give me a certain.\noutput let me call that g of X so far I've been calling it at F hat of X and.\nis calling in G of X right and I know that there is some true function also such that when.\nI take this input and pass it through the two function I get the output as f of x what.\nI am saying is that what this statement is saying that I can come up with any network I can.\ncome up with a network such that the difference between the true output okay this is how I'm quantifying the.\ndifference I'm just taking the difference between these two values if these are real numbers I'm just talking about the.\nabsolute uh difference uh between those two real numbers right would be less than Epsilon okay so I would be.\nable to approximate it up to the Precision of Epsilon for any of the inputs that are possible I'll pass.\nit through my network I'll get a g of X and I am confident that it will not deviate from.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 1D functions"}}
{"text": "able to approximate it up to the Precision of Epsilon for any of the inputs that are possible I'll pass.\nit through my network I'll get a g of X and I am confident that it will not deviate from.\nf of x more by Epsilon and I can construct such a network you give me an Epsilon I'll give.\nyou a network which will be which will produce an output which is within an Epsilon from the true output.\nright and of course as you can imagine just as we had the case there that we had this exponential.\nnumber of neurons you could imagine and we'll see that in more detail that as the Epsilon becomes smaller and.\nsmaller you will need a more and more powerful Network that means you need more and more neurons in this.\nhidden layer this may not be obvious right away but this is something this idea is something that we will.\ndevelop during this lecture okay so now let me just delete this and let me just put it on the.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 1D functions"}}
{"text": "develop during this lecture okay so now let me just delete this and let me just put it on the.\nslides so in other words I'm just going to read it out as it is in other words there's a.\nguarantee that for any function f of x that's what I was showing that is a function which takes an.\nN variable input and produces an M variable output in my example uh sorry M dimensional output in my example.\nM was just one right I was just producing a single dimensional output we can always find a neural network.\nwith one single hidden layer and many neurons right I don't know how many neurons it would have but I.\ncan find a network with a with a single hidden layer and a large number of neurons such that the.\noutput would always satisfy this condition right now this is a very very powerful theorem right and this is what.\nuh makes deep learning so attractor right so what is it saying right we are take any complex reliable function.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 1D functions"}}
{"text": "uh makes deep learning so attractor right so what is it saying right we are take any complex reliable function.\nthat you have let's return back to our oil mining example where f of x was very complex we did.\nnot even know what f is and we know that it's a very complex function based on your input variables.\nsuch as salinity pressure temperature Marine diversity and all that it's very hard to know what that function is but.\nnow I can come up with a neural network which when takes an input its output would be very close.\nto the desired function right in theory I can do that of course the Practical difficulty is that this such.\na neural network would have a hidden layer with a very large number of neurons and if it becomes exponential.\nthen we know it's not practical right but at least theoretically I can do that and this is only talking.\nabout a single hidden layer right and one of the things that maybe you will see through an assignment or.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 1D functions"}}
{"text": "about a single hidden layer right and one of the things that maybe you will see through an assignment or.\nin a quiz or something is that you if you had one more layer then you could reduce the number.\nof neurons in each layer right so from exponential it will fall down to something which is linear which is.\nmanageable right so that is a separate topic but right now remember we are talking about this constraint case where.\nwe are saying only a single hidden layer okay so that's what we want to be able to uh prove.\nright and this is the famous uh Universal approximation theorem and the actual proof would be a bit difficult but.\nwhat I'm going to show you is an illustrative proof which would be good enough just as we had seen.\napproved by construction it was actually an illustrative proof I just constructed a network and convinced you that this statement.\nis true I'm going to do something similar here right and whatever I'm going to do here is based on.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 1D functions"}}
{"text": "is true I'm going to do something similar here right and whatever I'm going to do here is based on.\nthis excellent illustration right this link that you see in the bottom which is an online textbook one of the.\nvery earlier early online textbooks on deep learning and it has a very good uh Tech stand and accompanying illustration.\nexplaining whatever I'm going to do explain on the slide so all the material that I have presenting here the.\nideas are borrowed from there right so uh just wanted to acknowledge that as well as point you to that.\nokay so so this is what an arbitrary function looks like right so I have my input X I'm dealing.\nwith so this is the x axis and this is my f of x r y axis right and you.\ncan see that I have a very arbitrary looking function right it's maybe I I it's not a sine X.\nit's not a cosine X it's maybe a mixture of some of these functions right uh it's not x square.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 1D functions"}}
{"text": "can see that I have a very arbitrary looking function right it's maybe I I it's not a sine X.\nit's not a cosine X it's maybe a mixture of some of these functions right uh it's not x square.\nX Cube or any of the standard functions that you might know now we are interested in knowing that if.\nthis is what my true relation is can I come up with a network of neurons that can represent this.\narbitrary function to a certain desired degree of precision okay that's what I want to do okay so let's see.\nhow I go about it so what we observe is that such an arbitrary function can be approximated by several.\nTower functions right so these are what I call as taus ok and I have constructed towers of certain widths.\nand it should be obvious that so this this this sum of all these towers right or this the function.\nthat you see here outlined here okay and I can just go on is what my approximation of the original.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 1D functions"}}
{"text": "and it should be obvious that so this this this sum of all these towers right or this the function.\nthat you see here outlined here okay and I can just go on is what my approximation of the original.\ncurve is and you can see that this is a slightly poor approximation right I am close at certain points.\nbut I'm also far from the curve at certain point so the case in point being this right so at.\nthis value of x the actual function tells you what the orange curve tells you what the actual function value.\nshould be but my approximation would predict it as this right let me just change the color right so this.\nis what my approximation would predicted as but the actual value is this right so there's a gap right but.\nnow interestingly right if I I've used a certain number of towers here right and let me just delete this.\nnow if I increase the number of towers okay this as you can read here is a poor approximation as.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 1D functions"}}
{"text": "now if I increase the number of towers okay this as you can read here is a poor approximation as.\nI increase the number of towers right my approximation becomes better and better right now you can see that the.\ntower the sum of the towers that I have right I'm just calling it a sum of towers because I.\nhave many of these individual towers and I've just added them they're just all at displaced locations and when I.\nadd all of them together as you'll see on the next slide I get this function back right and now.\nwhen I'm using a large number of towers where each Tower is very narrow then my approximation is becoming better.\nright so this slide as such has nothing to do with neural networks so far right so don't try to.\nconnect it to what does this mean in respect to sigmoid neurons perceptrons nothing we're just trying to make a.\nbasic observation that any arbitrary function I could I could approximate it by a series of tower functions right and.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 1D functions"}}
{"text": "basic observation that any arbitrary function I could I could approximate it by a series of tower functions right and.\nthe reason I like this is that see all of these whether I look at this Tower or this Tower.\nor this Tower they're all rectangles right only their bits are only their heights are varying and their position on.\nthe x-axis is way right but otherwise they all have the same form which means they are a rectangle right.\nso they are like the same function family so if I know how to construct one Tower I can scale.\nand displace and construct as many towers that I want right so that is something that I like that the.\nbuilding block is very simple it's just a rectangle if I can understand how to make one rectangle then it's.\njust about making smaller bigger rectangles and placing them at different places on the x axis right so that is.\nsomething that I like about this idea here right and we're going to take it further the other thing is.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 1D functions"}}
{"text": "something that I like about this idea here right and we're going to take it further the other thing is.\nthat as I increase the number of towers which eventually will kind of connect back to increasing the number of.\nneurons that you have I can get better and better approximation so that's where the idea that if you give.\nme a particular Epsilon I'll be able to find a network we have not gone there yet right but if.\nyou give me a particular Epsilon you can see that at least in this in this pictorial illustration right I.\ncan get better and better right I can make the approximation as good and I've stopped the slider here but.\nyou can imagine there is still scope of reducing the width of the rectangles in which case my approximation will.\nbecome even better and better right okay so I'll move ahead from here yeah more the number of such star.\nfunctions better the approximation okay so now this is what has happened in the previous slide I had this I.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 1D functions"}}
{"text": "functions better the approximation okay so now this is what has happened in the previous slide I had this I.\nconstructed a tower and I placed it at a certain position on the x-axis and that is where it is.\nI constructed one more Tower it was at a different position one more Tower at a different position so I've.\njust shown you some of the towers in those large number of towers that I had right now I could.\nyou could imagine that I would have one more uh right say one more Tower here which would look something.\nlike this and it should go up and come here like this right so I've not shown you all the.\ntowers that were there on the previous slide but this is what is happening right you are constructing many of.\nthese towers independently then just adding them all up to get the main function that you are trying to approximate.\nright and all of these are similar except for their position on the x-axis and for their Heights or lengths.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 1D functions"}}
{"text": "right and all of these are similar except for their position on the x-axis and for their Heights or lengths.\nas you may see right okay so this is what we have come to so now if I take an.\ninput and if I know I have been given an input X if I know if I have a magical.\nbox called the tower maker okay so I will make a tower I have as many such boxes as I.\nwant right which I'm making these towers okay and then I passed all of these towers so some there's a.\ntower maker right it is taking an input okay and it's just making a tower all of these boxes are.\nidentical in what they do the only difference is the amplitude or the height and the position right but otherwise.\nthey are all Tower makers if I get all these Tower makers and if I add them up then I.\ncan construct my original function plan right that's the idea so what I'm looking for now is this fundamental unit.\ncalled the tower maker if I have that then I have given you a network which can approximate from given.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 1D functions"}}
{"text": "called the tower maker if I have that then I have given you a network which can approximate from given.\nfunction to any arbitrary degree of precision right this is exactly the cartoon version of the network now you need.\nto quantitize it why is this the cartoon version of the network it takes an input X okay and it.\ngives an output which is f of x which is an approximation of the original output the thinner my Towers.\nthe better the approximation would be so I could control that knob now the only thing missing here is what.\nthe tower makers are right so I give the input make the towers add them all up and that's what.\nmy network is looking like Okay so let's see okay so our job now is to figure out what this.\nTower maker black box is okay so we'll figure that out so let's start uh yeah let me just see.\nwhat I want to do on this slide um yeah so I have an input X let me just explain.\nwhat all is obvious here at the input X okay and I have two parameters W1 B1 W2 B2 so.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 1D functions"}}
{"text": "what I want to do on this slide um yeah so I have an input X let me just explain.\nwhat all is obvious here at the input X okay and I have two parameters W1 B1 W2 B2 so.\nyou have been looking at these parameters so these two neurons that I have drawn here are going to be.\nsigmoid neurons so they're going to take an input X and they're going to just simply do 1 over 1.\nplus e raised to minus um W1 X Plus B1 that is what this neuron is going to output and.\nthe other neuron is going to Output 1 over 1 plus e raised to minus W 2x plus B2 right.\nso these are both sigmoid neurons okay now having clarified that now let's see okay now what I want to.\ndo I wanted to make a few changes here let's see so first thing I'll do is I'll just make.\nB1 and B 2 0 okay that will simplify some things for me nothing wrong in doing that okay it's.\na bit hard to do this with the mouse okay so B1 B2 have made it zero okay uh now.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 1D functions"}}
{"text": "B1 and B 2 0 okay that will simplify some things for me nothing wrong in doing that okay it's.\na bit hard to do this with the mouse okay so B1 B2 have made it zero okay uh now.\nyou can see I can show h11 I can show h12 I have show h21 all of these are hidden.\nright now okay so let me just say uh uh show h11 okay so I'll just show h11 so it's.\nlooking like a straight line as I said it's a sigmoid function but surprisingly it's looking like a straight line.\nand it's not hard to see why because oops the formula for the sigmoid function is 1 over 1 plus.\ne raised to minus W 1 X Plus B1 right and now if W1 is 0 right then the X.\ndoes not matter irrespective of what x is it will give the same value and what is that value going.\nto be 1 over 1 plus e raised to minus B and B is also B1 is also 0 in.\nthis case so it's just going to be 0.5 irrespective about the x is hence throughout the x axis the.\nvalue of the Y function is a 0.5 or the value of H1 is 0.5 it makes sense right now.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 1D functions"}}
{"text": "this case so it's just going to be 0.5 irrespective about the x is hence throughout the x axis the.\nvalue of the Y function is a 0.5 or the value of H1 is 0.5 it makes sense right now.\nto make it really a sigmoid meaningful sigmoid function I'll have to change W1 make it non-zero and as I.\nmake it non-zero you can just see right and as I move from 1 it's still a very uh gentle.\nslope right it's not Steep and as I keep increasing W right the slope becomes steeper and steeper to the.\npoint of almost becoming like a step function right as I keep increasing it almost becomes like a step function.\nright so this is how the sigmoid function looks like and if you have a very large the takeaway is.\nthat if you have a very large W then you actually recover the step function right you get the step.\nfunction okay so this is what h11 is looking like now let me take a b two a bit higher.\nso that I can show you the difference between one now let me show h12 again for now it looks.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 1D functions"}}
{"text": "function okay so this is what h11 is looking like now let me take a b two a bit higher.\nso that I can show you the difference between one now let me show h12 again for now it looks.\nlike a flat line because my W2 is 0 so let me increase W2 okay and you can again see.\nthat it will become closer to the step function right and as I adjust B it will move to the.\nleft or the right right that's what the B function does it tells you the point of crossover right from.\nat what point it will start going from 0 to 1 right so that's what the B controls okay and.\nnow I want to do um I've defined h21 as h11 minus h12 right that's how I've defined it so.\nlet me just do this here so I'll show you h21 oops uh maybe not this way yeah okay so.\nnow let me just hide the other ones I'll reset everything and just show you HD one so this is.\nlooking like a tar function now right so this is the kind of tower that I wanted it's not a.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 1D functions"}}
{"text": "now let me just hide the other ones I'll reset everything and just show you HD one so this is.\nlooking like a tar function now right so this is the kind of tower that I wanted it's not a.\nperfect Tower but it's also because my two functions did not become completely step function if I make W's very.\nhigh then you can see that it will become closer and closer to a tower function right it'll start looking.\nlike an Impulse right and I could increase B to increase the width of the track so now it's looking.\nmore and more like a tower function so I've got my tower maker what is my tower maker take one.\nsigmoid neuron take another sigmoid neuron set the W is very high right if you set the W is very.\nhigh then you recover the step function and then what you are essentially doing let me just show that also.\nlet me reset and show this one how did you get the tower because you are doing the red function.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 1D functions"}}
{"text": "let me reset and show this one how did you get the tower because you are doing the red function.\nuh minus the blue function right that's what you were doing red minus blue is what you are doing right.\nso now at this point both are zero here both are zero so all of this will remain zero you.\nare doing 0 minus 0 right at this point both are 1 right so again from this point onwards you.\nwill get 1 minus 1 which is 0 right and in this region the red curve is one but the.\nblue curve is zero so in this region you will get 1 minus 0 which is one so hence you.\nwill get a tower like that right so that's the reason you are getting that topper function right I'll just.\nredraw it right so this is the reason you got the tower function okay so that's why we have set.\nit up it way so now I've got a network which can produce a tower function and not just any.\nnetwork it uses my sigmoid neurons I just have to set the W's to a high value and I can.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 1D functions"}}
{"text": "it up it way so now I've got a network which can produce a tower function and not just any.\nnetwork it uses my sigmoid neurons I just have to set the W's to a high value and I can.\ncontrol the B's to control the uh width of this Tower function right so let me just show you that.\nnow if I make [Music] um okay now if any if I increase the gap between the bees okay then.\nthe width of the Tau function increases if I make it very small of course the length also decreases the.\nheight also decreases but the width also decreases I can use the bees to control the uh width of the.\ntower function right so that's that's what is happening here okay so now I have got my tower maker that.\nI was looking for so let's go ahead right so this was a basic function now let's look at the.\nentire network so you have X you have two sigmoid neurons the first one will give you a step function.\nbecause you set the W to very high the second one will also give you a step function which will.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 1D functions"}}
{"text": "entire network so you have X you have two sigmoid neurons the first one will give you a step function.\nbecause you set the W to very high the second one will also give you a step function which will.\ngive you a w very high then you subtract one from the other right so you have plus one as.\nthe weight and minus 1 as the weight so that's the same as h11 minus h12 and that would give.\nyou a Tau function right so that's I've come up with a neural network now right which will give you.\nthe Tau function and what is h21 what will you use for h21 nothing it's just a linear function right.\nso it's just taking this input it's just taking the uh weighted sum of its input so I'll just call.\nit as w i H 1 I is what it is doing where w i is equal to 1 to.\n2 and W 1 is equal to 1 and W2 is equal to -1 right so that's all that h21.\nis doing right just taking the weighted sum of its input and the weights have been hand coded right so.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 1D functions"}}
{"text": "2 and W 1 is equal to 1 and W2 is equal to -1 right so that's all that h21.\nis doing right just taking the weighted sum of its input and the weights have been hand coded right so.\nnow this network has been constructed which can give you a tar function which I can have a neural network.\nwhich gives me a tar function and now now I'm happy right because that is the tower block is what.\nI was looking for I have my tower Maker Now.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 1D functions"}}
{"text": "[Music] now what would happen if you have more than one input right so now this was all for a.\nsingle input right i just had the x axis and the y axis so x was input and y was.\nthe output but now i have a situation where i have two inputs right and what is happening here is.\ninteresting right so this is my uh the orient oops i know intention of doing this so now here again.\nthis now let me be careful and do it well yeah so this is this orange plane that you see.\nthat is my x1 x2 plane right and i have points on this plane right which you can see are.\nsome the blue points and the orange points right and what is this i've just taken my oil mining example.\nagain and suppose this is the salinity axis and this is the pressure axis right so a blue point here.\nis simply the x1 comma x2 point on this planet where the salinity is equal to x1 and the pressure.\nis equal to x2 and same the all the points here are just the x1 x2 coordinates where x1 is.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 2D functions"}}
{"text": "is simply the x1 comma x2 point on this planet where the salinity is equal to x1 and the pressure.\nis equal to x2 and same the all the points here are just the x1 x2 coordinates where x1 is.\nsilent and pressure x2 is pressure right now it so happens that some of these points you have found oil.\nwhich are the orange points and some of these points you have not found oil which is the blue points.\nright so now what is my decision boundary should look like i cannot have a decision boundary like this right.\nbecause it does not separate the blue points from the orange points you can see that i cannot no matter.\nwhat i try i cannot separate the orange points on the blue points by drawing a line right because this.\nis not linearly separable but if i draw i have a function which looks like exactly what has been shown.\nhere and let me just delete and use a better color you know not black maybe the function is 0.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 2D functions"}}
{"text": "is not linearly separable but if i draw i have a function which looks like exactly what has been shown.\nhere and let me just delete and use a better color you know not black maybe the function is 0.\neverywhere here and then 1 here right and then again 1 here and 0 everywhere here 0 everywhere here 0.\neverywhere here right so for all the blue points now it is predicting 0. you can see that it's also.\nobvious in the 2d version of the figure and for all the orange points it is predicting 1 and that's.\nexactly what i want because for the orange points i found oil and for the blue points i did not.\nfind out right so even in two dimensions you could have this case where you have an arbitrary function now.\nthis is the shape of the function the entire orange surface that you see is the shape of the true.\nfunction and that's what the true function looks like and now i want to approximate it right and now again.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 2D functions"}}
{"text": "function and that's what the true function looks like and now i want to approximate it right and now again.\nif i had these small two dimensional toggles right so then i could just construct all of them side by.\nside right and get the original figure back right original function back so now i want to have a two.\ndimensional tower so how do i get a two dimensional toggle and once we understand two dimensional we will just.\nsay we can do this in three dimensions four dimensions n dimensions right so once the idea and two dimension.\nbecomes clear okay so now again i i'll just take this function which is a function of two inputs this.\nis a sigmoid neuron function which takes two inputs x1 and x2 each input would have a weight w1 w2.\nand then a bias right so now let's adjust a w1 and see what happens so if i increase w1.\nright again as i keep you can see that as i keep increasing i'll do okay maybe i'll just use.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 2D functions"}}
{"text": "and then a bias right so now let's adjust a w1 and see what happens so if i increase w1.\nright again as i keep you can see that as i keep increasing i'll do okay maybe i'll just use.\nthe yeah this is better so it's flat at zero and as i increase it starts becoming s right now.\nthe slope is very gentle right and as i keep increasing the value of w it becomes steep and steep.\ntill it becomes like a step function right so this w1 controls the uh movement in this direction right the.\nslope in this direction okay now let's see what w2 does and for that i'll again make w one as.\nzero okay and i'll start increasing w two you can see that w two is adjusting the slope in the.\nother orientation right oh coordination so now what w 2 is doing is it's adjusting the slope in this direction.\nright so as i keep increasing w 2 now oops as i keep increasing it it will become flatter and.\nflatter in this direction right and now you can see b again just helps in movement along the direction right.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 2D functions"}}
{"text": "flatter in this direction right and now you can see b again just helps in movement along the direction right.\nso i can change the point at which the crossover happens by changing b so you can go back and.\nplay around with this figure and animation and try to understand how changing the values of w1 and w2 affect.\nthese uh this sigmoid function if i increase both something interesting happens right so you can just play with this.\nright so the point is again clear if i set the w1 very high i could again get the step.\nfunction and the orientation and all i can control and if i have b as i move b the step.\nfunction will move right at the threshold at which the crossover happens right so again the same idea as in.\n2d i can adjust the w's and b's to get something which looks like a step function right so now.\nlet's try to construct a tower in 3d okay now i didn't want to show you this okay right so.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 2D functions"}}
{"text": "2d i can adjust the w's and b's to get something which looks like a step function right so now.\nlet's try to construct a tower in 3d okay now i didn't want to show you this okay right so.\nfirst let's see h11 right so i have taken one neuron again again the same idea i have a network.\nokay where x is my input and i have one neuron which is h11 another neuron which is h12 and.\nthen both these outputs will be get added which i'm going to call as h21 right added meaning plus one.\nand minus one right so now i'm showing you but it just that both of these are now this x.\nis x 1 comma x 2 right so the functions are 1 over 1 plus e raised to minus w.\n1 x 1. plus w 2 x 2 plus b right so that's why you have w 1 w 2.\nb right i'm going to uh just use the same parameters for both h 1 1 and h 2 to.\nexplain this okay so now this is what h11 is looking like i have set the w very high as.\nyou can see here oops yeah i've set the w to a very high value which is 60 and hence.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 2D functions"}}
{"text": "explain this okay so now this is what h11 is looking like i have set the w very high as.\nyou can see here oops yeah i've set the w to a very high value which is 60 and hence.\nit has become a step function and now if i change b it will move along here right i can.\nmake it move wherever i want right now let's see what h12 is so i have h12 which are parameters.\nw3 w4 and b now again i have set w3 to be very high right so it has become like.\na step function okay don't think that it became uh yeah sorry i shouldn't have changed this yeah so in.\nboth cases one of the w's i have kept as zero and just changed the other w so now this.\nhas become again like a step function okay and now if i were to subtract them right which is h11.\nminus h2 and if i were to hide the other things then i'll get a tower function right again the.\nsame logic that i'll not show you this right so here both are oops yeah maybe this is fine yeah.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 2D functions"}}
{"text": "minus h2 and if i were to hide the other things then i'll get a tower function right again the.\nsame logic that i'll not show you this right so here both are oops yeah maybe this is fine yeah.\nthis looks better yeah so here both are 0 so the output will remain 0. here both are 1 so.\n1 minus 1 is going to become 0 so it will become flat and here this is 1 and this.\nis 0 so 1 minus 0 so you'll get a tower like this right so that is now if i.\nhide h 1 1 and h 1 2 and only show you h 1 1 minus h 1 2 then.\nit's going to look like this tower right and let me just delete some of the okay right so this.\nis looking like a tower so i have been able to construct a tower but this is still not complete.\nright because this tower is like open from two sides i don't want a tower which is open from two.\nsides i want a closed tower right just as that 2d thing where you had a tower like this i'd.\nwant a tower which is closed from all the four sides right now right now it's open from two sides.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 2D functions"}}
{"text": "sides i want a closed tower right just as that 2d thing where you had a tower like this i'd.\nwant a tower which is closed from all the four sides right now right now it's open from two sides.\nand that's where i had this discussion on what happens with the orientation right so you could have you could.\nsee that one of the weights controls the movement in one direction and the other weight controls the movement in.\nthe other direction right so right now is adjusting one of the weights right you can see here that w1.\nand w3 is what i have played with i have kept w2 and w4 as zero right and w2 and.\nw4 would help me move in this direction so it will probably form a block which would be in this.\ndirection so it will bring in the remaining two walls i already have two walls for the tower i'm looking.\nfor two more walls and all of this will become clear when i show you the illustration on the next.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 2D functions"}}
{"text": "for two more walls and all of this will become clear when i show you the illustration on the next.\nslide okay so this is what my network is right it's the same thing i have two sigmoid neurons and.\nthen one addition and i'm getting f of x one but at this i'm getting only open tower right i'm.\nnot still getting the fully uh closed tower right so now let's see if you can get a come up.\nwith a network for the entire thing okay let me just do some adjustments here first i guess this should.\nbe fine uh i think i've already pre-adjusted the values you can play with them later on so i already.\nshowed you h11 and h12 okay and when i do h11 minus h12 i get a tower so this is.\nsomething that you are convinced about so i will not show you these two right now i'll take h13 where.\ni am going to so if you look at this right so w11 was set to 200 right so that.\nis and w one two are set to zero that's why it's a step in a particular direction but now.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 2D functions"}}
{"text": "i am going to so if you look at this right so w11 was set to 200 right so that.\nis and w one two are set to zero that's why it's a step in a particular direction but now.\nfor the remaining one right i have set w three one to zero and i've said w three to two.\nhundred so what would happen in that case i would get a step function in this orientation right and similarly.\nfor h14 i have set w41 to 0 and w4 to 200 so again i will get a step function.\nin this orientation right and now if i subtract these two i get a tower in this orientation right so.\nnow let me just show you the towers only so i have one tower in this orientation i have another.\ntower in the other orientation right so now you can see that the walls have closed here if i just.\nmove it around you can see that it's the walls have closed completely right so you can see that the.\nwalls there is no opening anywhere right you cannot see a white surface anywhere because on all four sides the.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 2D functions"}}
{"text": "walls there is no opening anywhere right you cannot see a white surface anywhere because on all four sides the.\nwall has closed right but still i'm not there right it is not like a tower yet i mean there's.\na tower somewhere inside this middle portion this portion here right this okay maybe i should not have drawn it.\nlike that okay so this portion underneath this block here right that is the tower but i'm getting a lot.\nof extra things also along with it right so there's some some things coming out and i don't want those.\nfor now so i'll see how to get rid of those right but for now you get what happens one.\nthe main thing to note is that now the walls have closed as i've shown you at no point in.\nthat region you cannot see uh you can see a blue wall then you can see the cyan wall here.\nthen again the blue or the purple wall and then again the cyan wall right so it's it's you have.\na tower sitting somewhere inside that so now let me look at the sum of these two functions right so.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 2D functions"}}
{"text": "then again the blue or the purple wall and then again the cyan wall right so it's it's you have.\na tower sitting somewhere inside that so now let me look at the sum of these two functions right so.\nnow i have what i've shown you is h11 minus h12 and h13 minus h14 one of them is the.\npurple colored structure and the other is the cyan colored structure now let me take a sum of these two.\nand see what happens right this looks quite weird let's see why this is happening right so let's just try.\nto explain that again so now it's very simple actually nothing much to explain the same argument that i had.\nearlier so in this region both are 0 right so if i add them again you will get 0 right.\nsimilarly in this region both are 0 if you add them you will get 0. similarly if i just maybe.\nmove it around yeah in this in this region again both are 0 so if you're going to add them.\nyou'll again get 0 and then let me just move all the way back again in this region both are.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 2D functions"}}
{"text": "move it around yeah in this in this region again both are 0 so if you're going to add them.\nyou'll again get 0 and then let me just move all the way back again in this region both are.\n0 so if you add them up you'll get 0. so you have understood what is happening at the flat.\nsurface is it you can just again go back and play around with this you can just move it around.\nand you can see it right now let's look at the other regions right so where there's not going to.\nbe zero output if you look at this region okay so then you can see that the let me just.\norient it properly okay i'll just delete all my marks oops okay i'm really having trouble in adjusting to this.\nokay um yeah this is a good view so now if i look in this region right so this purple.\nguy was 1 but the cn guy was 0. so if i add 1 plus 0 i'll again get 1.\nhere right same argument for this region where the cn guy was 1 but the purple guy is 0 so.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 2D functions"}}
{"text": "guy was 1 but the cn guy was 0. so if i add 1 plus 0 i'll again get 1.\nhere right same argument for this region where the cn guy was 1 but the purple guy is 0 so.\ni'll add them i'll get 0. same argument for this region and same argument for this region does that make.\nsense now for this region in the middle both the cyan and the purple are one so if i add.\nthem i'll get a 2 which i'll get an elevation there and that's exactly what is happening when i show.\nyou the sum of these two right so in all these regions you can now see it yeah i think.\nthis is a good view uh i'll just stop it so in these regions it's zero in these regions it's.\none in these four regions and in this region it's two right so i've got my tower actually my tower.\nis sitting here on the top of the structure this is the tower that i'm interested in but i have.\nthese some extra scaffoldings here and there which i want to get rid of right so now how do i.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 2D functions"}}
{"text": "is sitting here on the top of the structure this is the tower that i'm interested in but i have.\nthese some extra scaffoldings here and there which i want to get rid of right so now how do i.\nget rid of that right so now if i look at this entire structure right i'll just get rid of.\nthe annotations and also get rid of the individual h11 and h12 yeah yeah so this is what my structure.\nlooks like and you can see that at the bottom while it's open yeah the structure sitting at the top.\nis closed from all sides so this structure sitting at the top is closed from all sides right now there.\nare three levels of output here there's a zero level okay which is here then there is a level one.\nand then there's a level two right and i only want to retain things which are greater than one right.\nso that if i only retain things which are greater than one suppose i could have a way of filtering.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 2D functions"}}
{"text": "so that if i only retain things which are greater than one suppose i could have a way of filtering.\nso that i could only retain things which are greater than one then only this part would remain right everything.\nelse is either one so this is all one okay and this is all zero right so if i have.\na thresholding such that i retain everything which is greater than one then i'll get my tower out of it.\ndoes that make sense right so if i do a thresholding so how will i do that i can again.\nintroduce a sigmoid function there with a threshold of 1 so that the output is 0 till it reaches there.\nand then it climbs to one right so if i have a sigmoid function which is a threshold of one.\nor the switch over point set to one then when i take this output right so this is the output.\nof some function right now let me just call it as h now when i take this edge so this.\nis h and i'm going to pass it through another sigmoid okay such that it switches over at one then.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 2D functions"}}
{"text": "of some function right now let me just call it as h now when i take this edge so this.\nis h and i'm going to pass it through another sigmoid okay such that it switches over at one then.\neverything which is less than one all of this will disappear and only these things at the top right which.\nare greater than one will remain and what the result end result would be that i'll get my tower function.\nout so if i delete this and i just show you the tower so this is what will remain right.\nso this is just the part of the that entire structure which was above level one right and only that.\npart will remain and i get my tower function which is now indeed close from all sides at which we.\nwere already convinced about right so you can go back play around with these visuals and convince yourself that you.\nhave got a 2d tower i am convinced that we have got a 2d tower by constructing a network like.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 2D functions"}}
{"text": "have got a 2d tower i am convinced that we have got a 2d tower by constructing a network like.\nthis i took x1 x2 i had these two sigmoids which were oriented in one direction and when i combined.\nthem i got a tower open tower in one direction then i had these two sigmoids which were oriented in.\nthe other direction and when i combined them here i got the tower in the other direction then when i.\nadded both these structures i got this weird structure which had three levels a level zero a level one and.\na level two and then again passed it to a sigmoid such that it allowed only the level one and.\nabove outputs and hence i got the tower which was about one right so that's the tower function that i.\ngot out so i got a 2d tower function from this network you can go back check out the illustration.\ncheck out this network and convince yourself that you can get a 2d function it's the same thing that we.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 2D functions"}}
{"text": "check out this network and convince yourself that you can get a 2d function it's the same thing that we.\ndid for 1d just like one more level of complexity added right and you can see that the number of.\nneurons in my middle layer has now increased right this has become four neurons okay and also our initial goal.\nwas to prove that a single layer neural network should be sufficient i have not proved it for a single.\nlayer right so what you will have is one layer where the number of neurons will keep growing so it.\nwill be like a three layer network which can approximate any arbitrary function right now you could take this idea.\nfurther and prove for the one layer case also but as i said that proof will be a bit complicated.\nbut at least i have shown you that with three layer network you can approximate a 2d function right to.\narbitrary degree of precision right so that's what we have been able to do so now what will happen is.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 2D functions"}}
{"text": "arbitrary degree of precision right so that's what we have been able to do so now what will happen is.\nthe 2d 2d towers that i got out of that tower network i can put many of these together right.\nand construct my uh function as way i desire right and i can adjust for the thickness of the towers.\nto make it more and more a better approximation right so that's the idea so this is what the universal.\napproximation theorem says and the reason it is important is that in real world you will have these complex arbitrary.\nfunctions and now you have a way of coming up with a neural network it still has a large number.\nof neurons right and we'll fix that it has a large number of neurons but if you keep that fact.\naside you can what this theorem says that for any function that you have any problem that you're dealing with.\nwhere there's a true function x and let it be as arbitrary a function as you want right you can.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 2D functions"}}
{"text": "where there's a true function x and let it be as arbitrary a function as you want right you can.\nalways come up with a neural network which will be able to approximate it to a desired degree of precision.\nif you add enough neurons in that neural network right and that's the foundation right that's the reason why deep.\nlearning is so formula popular because it's saying that you could take a deep neural network which could approximate arbitrary.\nfunctions and that's what we do in machine learning right you have an x you have an f of x.\nand you want to come up with an approximation f hat of x and now you have found an f.\nhat of x which is just what i showed you on the previous slide right which is a neural network.\nthat's the function it's just a composite function you took the input took some sigmoid took some out addition then.\nagain added it but it's all a function of x it's a composite function of x you could construct these.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 2D functions"}}
{"text": "again added it but it's all a function of x it's a composite function of x you could construct these.\ncomposite functions such that you could approximate any real function that you are dealing right so that's the basic idea.\nso i think i'm done with this one i just leave you with this thought so for one dimensional input.\nwe needed two neurons for two dimensional input we needed four neurons for n-dimensional inputs how many neurons will you.\nneed and the answer is obvious but i'll let you think about it right so that's where we end uh.\nlecture three and the next lecture we'll start with uh start talking about deep neural networks and then the back.\npropagation acronym thank you.", "metadata": {"video_title": "Representation power of Multilayer Network of Sigmoid Neurons 2D functions"}}
{"text": "foreign [Music] decays a learning rate very aggressively as the denominator Grows Right and we are allowing the denominator to.\ngrow inhibitedly because it's just accumulating the squares right so now uh frequent parameters will start receiving small updates so.\ncan we avoid this and prevent the rapid growth of the development right can we try to scale down the.\ngrowth of the denominator and we know how to do that right and it's a simple uh thing that we.\ncould do is that instead of uh yeah instead of the original equation which was allowing the gradients to get.\nadded we can just take a fraction of the gradients right so now the typical value of beta would be.\nsay 0.9 so at any point you're just taking uh let me just get rid of this yeah so now.\nyou are taking 1 minus beta which is 0.1 times the current gradient and also the history you are repeatedly.", "metadata": {"video_title": "RMSProp"}}
{"text": "say 0.9 so at any point you're just taking uh let me just get rid of this yeah so now.\nyou are taking 1 minus beta which is 0.1 times the current gradient and also the history you are repeatedly.\nexponentially decaying right the history continuously gets multiplied by 0.9 so whatever was my accumulated history that has also become.\n0.9 times the previous history and my current gradient also I am not taking it fully I'm just taking it.\na fraction of it so that's why I'm not allowing my history to grow as rapidly as it was going.\nin the case of adagra right so that's the change I have made in RMS prop uh and then the.\nupdate rule Remains the Same right so now everything else Remains the Same the only thing that I've changed is.\nthis denominator VT which was earlier growing aggressively now I've added like these multipliers beta and one minus beta which.\nare both less than one which means I'm scaling down right I'm not allowing the history to grow at every.", "metadata": {"video_title": "RMSProp"}}
{"text": "are both less than one which means I'm scaling down right I'm not allowing the history to grow at every.\npoint I am multiplying it by a exponentially decaying Factor right so that is what is happening here and that.\nwill become clear as I expand this equation here right so this is what was happening for radagride at VT.\nit was Delta B 0 square plus Delta B1 square plus B2 square and so on but in the case.\nof RMS prop uh you will have this equation right where beta is between 0 to 1. so beta if.\nit's 0.9 then your V 0 is 0.1 times B 0 square B2 is 0.09 times Delta B 0 square.\nplus this and so on and when you come to uh you can already compare these two quantities right so.\nwhere's the pen yeah huh yeah um yeah so if I compare the V2 here with the V2 here it's.\nclear that this is going to be much smaller because my history is now exponentially decaying right here I was.\ntaking the whole of b0 square whereas here I am taking a small fraction again the whole of B1 square.", "metadata": {"video_title": "RMSProp"}}
{"text": "taking the whole of b0 square whereas here I am taking a small fraction again the whole of B1 square.\nbut a small fraction here again the whole of B2 square but even again a small fraction of P2 square.\nright so now I'm controlling the rate at which V2 is going I'm not allowing it to shoot quickly I'm.\ndoing this exponentially decaying average foreign formula uh no I think we have not seen this earlier but you can.\nsee where the formula is coming from uh have you seen we have seen this formula earlier when we have.\nbeen discussing momentum right this kind of a formula so if I just substitute here I have actually substituted the.\nvalue of beta as 0.9 and 1 minus beta is 0.1 but instead if I just kept beta and 1.\nminus beta and solve for that I would have got the formula as they said that at timestamp VT my.\nuh wheat at time step T my v t is going to be given by this formula and what you.\nsee at V2 is just a manifestation of this formula applied at time Step 2 with the value of beta.", "metadata": {"video_title": "RMSProp"}}
{"text": "uh wheat at time step T my v t is going to be given by this formula and what you.\nsee at V2 is just a manifestation of this formula applied at time Step 2 with the value of beta.\nset to 0.9 right and the main takeaway here right this formula you can go back and ponder about but.\nthe main takeaway here is that you're using an exponentially decaying average and hence now your denominator is going to.\ngrow less aggressively and hence now your effective learning rate right and remember this is what I refer to was.\nthe effective learning rate my effective learning rate is going to Decay slowly okay so now let's see what the.\neffect of this is yeah again I'm not going to ponder too much about the code here but you can.\nsee that now instead of only one line has changed right so VW plus DW Square I am doing beta.\ninto VW plus 1 minus beta into DW square right so that's the main change that I've done and now.", "metadata": {"video_title": "RMSProp"}}
{"text": "see that now instead of only one line has changed right so VW plus DW Square I am doing beta.\ninto VW plus 1 minus beta into DW square right so that's the main change that I've done and now.\nlet me just play this okay yeah so you can see that RMS prop is now the yellow curve is.\nRMS prop which is quickly reads the minimum right because its learning rate is not decaying and it's also following.\nthis hypotenuse part as I wanted right it's making updates in the direction of w and B uh proportionately even.\nthough the updates in the direction of w are smaller right but notice something and we'll come back to what.\nyou notice here oops okay just play it in the minimized version here so it's reached there uh quickly but.\nnow it's oscillating a bit around the Minima right so you can see the sigmoid curve it had reached the.\nsolution right all the points were lying on the sigmoid curve but then the yellow curve started oscillating there now.", "metadata": {"video_title": "RMSProp"}}
{"text": "solution right all the points were lying on the sigmoid curve but then the yellow curve started oscillating there now.\nwhy does that happen is not clear but for this Slide the takeaway is that RMS prop converged more quickly.\nthan attack rad because it is being less aggressive in decaying the learning rate right that's the main takeaway why.\nare there oscillations right so that is something that we need to understand right so one of the implications could.\nbe that maybe the learning rate is becoming constant and that is causing of oscillations right why the two questions.\nto be answered here right why is the learning rate becoming constant and if the learning rate becomes constant why.\nshould there be oscillations right so these are the two questions that we need to okay so now recall that.\nin adagrad uh this VT was uh sorry this should have been Square so this Delta WT Square should have.\nbeen there and I when I had shown you the plots I had shown that VT never decreases right it.", "metadata": {"video_title": "RMSProp"}}
{"text": "in adagrad uh this VT was uh sorry this should have been Square so this Delta WT Square should have.\nbeen there and I when I had shown you the plots I had shown that VT never decreases right it.\nkeeps increasing and then it saturates but does not decrease even though the derivative has become zero VT keeps increasing.\nand then it saturates right so it does not ever decrease to it does not start falling down right uh.\nnow what happens in the case of RMS prompt right so let's see what is happening in the case of.\nRMS prop so you have this uh let me just annotate so this is your derivative so derivative increased increase.\nin the negative Direction and then it started decreasing and then it became zero and then you are seeing this.\noscillations right so that let's not go into the oscillations but the main point here is that if you look.\nat your VT it is increasing and then it starts decreasing also right so e v t can decrease in.", "metadata": {"video_title": "RMSProp"}}
{"text": "at your VT it is increasing and then it starts decreasing also right so e v t can decrease in.\nthe case of RMS prop and why does that happen can you think about an answer for that what is.\nit uh no no but it's an accumulation of the history right so the accumulation should keep growing right so.\nhow is the accumulation decreasing in the case of undergrad we said that VT is an accumulator so it's keeping.\nadding terms so it should only increase and in whatever it's adding if it becomes zero then it will become.\nsteady at that so here why is it increasing and then it can decrease also why is that happening same.\nfor uh now we are taking VT Square we are taking Square so direction does not matter see the reason.\nthis is happening is because of the exponentially decaying average right so now uh initially if you look at the.\nsum so say at time step T the sum was some uh so if you look at the derivative at.", "metadata": {"video_title": "RMSProp"}}
{"text": "this is happening is because of the exponentially decaying average right so now uh initially if you look at the.\nsum so say at time step T the sum was some uh so if you look at the derivative at.\ntime step 0 right so let's look at the formula if I have it yeah so here remember that this.\nis like a moving window right so at time step V1 the weightage of beta 2 was 0.09 at time.\nstep V2 the weightage of beta 0 was 0.08 so at each time step we see this Behavior where the.\nblue curve which is the accumulator curve which is the curve for VT it starts decreasing also because now the.\ncurrent sum is less than the previous sum right uh because of the exponential weighting right and now because of.\nthat what is happening is that in the case of ADA grad because your history was always accumulating your denominator.\nwas always growing so hence there was always a decay in the learning rate but now since your history can.", "metadata": {"video_title": "RMSProp"}}
{"text": "was always growing so hence there was always a decay in the learning rate but now since your history can.\ngrow and shrink and it can also become a constant right because now what is happening is as your current.\ngradients are all zero then there all the sums are becoming close to zero right and so then after a.\npoint the history is becoming more or less constant and that the history is becoming constraint from one time step.\nto another ETA divided by that history is again going to remain constant right so as you go ahead your.\nlearning rate is not going to change right so that is why the learning rate becomes constant in the case.\nof RMS could become constant in the case of RMS prop after a certain number of iterations right it can.\nincrease it can decrease or it can remain constant because of the moving average of the gradients in the denominator.\nright and after across 500 iterations I have plotted the uh gradients here and this is what is uh happening.", "metadata": {"video_title": "RMSProp"}}
{"text": "right and after across 500 iterations I have plotted the uh gradients here and this is what is uh happening.\nand you have this oscillation around the Minima so if the learning rate is constant so now we have made.\na case that the learning rate could become constant now if the learning rate becomes constant why should there be.\noscillations right why could there be oscillation so let's try to see an explanation for that right suppose let me.\ntake a simple example this is what my curve is okay and now suppose I was here okay at this.\npoint okay suppose I was at this point uh yeah and I computed the derivative so this slope will give.\nme the derivative and I made the update and after the update so let me just see I'm on the.\nW axis I was here and I added the I mean I moved in the direction oppose it to the.\ngradient and I ended up here right now again I compute the derivative here again I'll move in the direction.", "metadata": {"video_title": "RMSProp"}}
{"text": "W axis I was here and I added the I mean I moved in the direction oppose it to the.\ngradient and I ended up here right now again I compute the derivative here again I'll move in the direction.\nopposite to the gradient with the same learning rate my learning rate has not changed so again I'll end up.\nhere at the same point because I'm moving from one point on the curve to its symmetrically opposite point on.\nthe curve this is possible I'm not saying this will always happen but this could happen right you're moving from.\nhere to here and again from here to here and you keep oscillating between those two points because from every.\nmovement you have the learning rate is constant so if the derivative is also similar you'll just keep moving from.\nhere to here right even if the derivative is slightly different you will keep making these movements from here to.\nhere right instead of directly going to the uh minimum all right so that's what can happen if your learning.", "metadata": {"video_title": "RMSProp"}}
{"text": "here right instead of directly going to the uh minimum all right so that's what can happen if your learning.\nrate becomes constant and that's why you're seeing those oscillations in the case of RMS prop where the learning rate.\nis indeed becoming constant uh after a while because your denominator VT is not changing right so it's accumulated history.\nhas also become like close to a very small number and your current derivatives are also small so now when.\nyou are adding all these terms from one time step to another nothing much is changing there right that's what.\nis happening and that was not the case in Ada grad because you were just adding the gradients so the.\nprevious terms always remain right they are not getting exponentially weighted out right so what is the solution for this.\nsay this learning rate becoming constant and then resulting in oscillations so one solution is to set the learning rate.", "metadata": {"video_title": "RMSProp"}}
{"text": "say this learning rate becoming constant and then resulting in oscillations so one solution is to set the learning rate.\ninitial learning rate appropriately right so this ETA that we have so remember our effective learning rate is ETA divided.\nby that square root of VT so this ETA that we have in the numerator that's the initial learning rate.\nand we could if we set that appropriately then we could solve this oscillation problem right so now for the.\nsame example I will show you the case where we had set the learning rate initial learning rate to be.\n0.05 and now if I uh run this algorithm RMS prop you can see that there are no oscillations here.\nright so how does that happen so if you have this ETA set to a right value then in the.\nexample that I was giving earlier what could happen is that I was showing that this okay maybe let's do.\nit again so I was saying that it was perhaps oscillating from this point to this point and then back.", "metadata": {"video_title": "RMSProp"}}
{"text": "it again so I was saying that it was perhaps oscillating from this point to this point and then back.\nand forth between these two points right and if the ETA was such that uh even though the the denominator.\nhas become more or less constant if the effective learning rate because of the initial learning rated so it depends.\non the initial learning rate if that was such that instead of just moving from the two symmetric points right.\ntwo points on either side of the curve if it was moving like this it came here and then from.\nit here and then from here then it would still converge right so if the learning rate is appropriate says.\nthat it's not uh kind of oscillating from one side to the other but it's gradually becoming uh towards the.\nMinima right so if the learning rate is set appropriately then this could happen now of course this is hard.\nright I mean you don't really know in this case since it's a toy example we experimented with a few.", "metadata": {"video_title": "RMSProp"}}
{"text": "right I mean you don't really know in this case since it's a toy example we experimented with a few.\nlearning rates and we found out that for a particular learning rate it kind of uh stabilizes the point that.\nI'm trying to make here is not that hey you should use RMS prop and then try to set the.\nlearning rate appropriately this may work in some examples but in some examples you may not be able to get.\nthe right learning rate right the point that I'm trying to make is in fact the opposite that RMS prop.\nis indeed sensitive to the initial learning rate and if you don't set it properly then you could see oscillations.\ndespite the other properties of the algorithm which allows for a more smoother DK of the learning rate despite that.\ngood property it might still oscillate if the learning rate is not proper right so the main takeaway is that.\nit is sensitive to the learning rate and setting this learning rate to 0.05 and showing you that it converges.", "metadata": {"video_title": "RMSProp"}}
{"text": "it is sensitive to the learning rate and setting this learning rate to 0.05 and showing you that it converges.\nis not to give the message that hey you could do this by setting the learning rate appropriately it is.\njust to show that different learning initial learning rates could have different effects right so that's the idea here and.\nwe'll have to move to a stage where we kind of get to algorithms which are not too sensitive to.\nthe initial learning rate okay which one ha so VT plus Epsilon so V T is initially 0 so initially.\nit would be ETA divided by square root of 10 raised to minus 4 right okay now let's see again.\njust continuing in that direction right so if we uh were to initialize ETA with different values right so I.\nhave suppose ETA equal to Eta not equal to 0.6 that's what I should say so our effective learning rate.\nis ETA ETA naught divided by square root of VT plus Epsilon and of course this ETA naught remains constant.", "metadata": {"video_title": "RMSProp"}}
{"text": "is ETA ETA naught divided by square root of VT plus Epsilon and of course this ETA naught remains constant.\nthroughout right that's what you have said and only the denominator is change right so now if I change if.\nI use different values for ETA naught what happens I already showed you that if I choose if I had.\nchosen some favorable value for uh ETA naught then my algorithm would have conversation so I'm just going to show.\nyou a few more examples along that direction okay so I'll consider two values ETA not equal to 0.6 and.\n0.1 and let's see what happens to the to uh example right so first let me run 0.6 and here.\nyou can see that there's a wide oscillation right so this is almost like a case where uh the following.\nis happening right it's oscillating from a high loss region to another high loss region and is oscillating between these.\ntwo points as opposed to a slightly better loss oscillation where it's perhaps oscillating between two low loss regions right.", "metadata": {"video_title": "RMSProp"}}
{"text": "two points as opposed to a slightly better loss oscillation where it's perhaps oscillating between two low loss regions right.\nso that's not happening here you can clearly see that it's oscillating between very high loss regions right so from.\none high loss point to another it is going and you can see that because this where it is stable.\nI mean the point of oscillation this is what the sigmoid curve looks like which is nowhere close to fitting.\nthe points that I have right now let's look at the other example here the learning rate is slow so.\nyou can see that the curve is moving slowly right the algorithm is progressing slower to the Minima and now.\nin this case also there are oscillations this is still not good in fact ETA not equal to 0.05 is.\nwhat I had shown on the previous slide was good so even with ETA not equal to 0.1 the learning.\nrate is still high right so the numerator is still high so this is now point one this is still.", "metadata": {"video_title": "RMSProp"}}
{"text": "what I had shown on the previous slide was good so even with ETA not equal to 0.1 the learning.\nrate is still high right so the numerator is still high so this is now point one this is still.\nhigh enough to cause the oscillations it's just that the oscillations are slightly better now they are between these two.\nlow loss regions right and when I change it to 0.05 as I had shown on the previous slide then.\nit was better it was perhaps able to get here and then converge quickly from that point right so that's.\nthat's what the sensitivity to the initial learning rate is right so the point to note here is that everything.\nelse here is remaining the same right the Lost surface is the same the training points are the same the.\nonly thing that has changed is my initial learning rate and that is causing a difference between convergence and we.\nsaw three scenarios one where it converges one where it oscillates around a solution which is acceptable right so this.", "metadata": {"video_title": "RMSProp"}}
{"text": "saw three scenarios one where it converges one where it oscillates around a solution which is acceptable right so this.\nis perhaps not as bad as this one right so it converges or it reaches to an acceptable solution or.\nis like really far off from the solution right all three things could happen depending on how we have chosen.\nthe learning rate so this is in indeed not a favorable situation right so we may want to look at.\nAlternatives and in most cases these initial learning rates for several problems have been kind of discovered through experiments I.\nmean like for example for machine translation there are certain data sets training data sets which are used quite often.\nright across different papers and so many experiments have been done with those data sets that people now have a.\ngood range of what is the right initial learning rate to use and in that case you might not set.\nsuch CSS problems right but if you're dealing with an esoteric problem a new data set which only you are.", "metadata": {"video_title": "RMSProp"}}
{"text": "such CSS problems right but if you're dealing with an esoteric problem a new data set which only you are.\nlooking at for the first time or if not me enough people have looked at then you might not really.\nknow what the right ETA naught should be and then you could land up in any of these situations right.\nso let's just talk about it a bit more so this is what the effective learning rate is this is.\nwhat VT is and which value of ETA naught is a good one right so uh we don't know that.\nand what we really want is that some ETA not might be better some eat or not may not be.\nbetter right and it's also not often that one ETA naught is always better right so let's see what I.\nmean by that right so in this again uh suppose ETA naught was set to 0.6 okay that's the animation.\nthat I'm going to show you now and I'll tell you what all I'm going to show you in this.\nanimation right so this is VT oops yeah so this is VT which is the accumulation right so remember your.", "metadata": {"video_title": "RMSProp"}}
{"text": "that I'm going to show you now and I'll tell you what all I'm going to show you in this.\nanimation right so this is VT oops yeah so this is VT which is the accumulation right so remember your.\nI don't know how many times I have written this formula now yeah so this is VT the blue curve.\nis going to be VT which is not coming yet this green curve is your initial ETA naught in my.\nexample it has been set to 0.6 so that's why the green line does not change across iterations and this.\nis going to be your effective learning rate or I have not taken the square root here but it's going.\nto be proportional to the effective learning rate right so now uh so far so good [Music] let's see what.\nis happening as the iterations go along we are seeing some behavior of the plots right in uh before I.\ncomment on that I just want you to look at what is happening here right so you started off with.", "metadata": {"video_title": "RMSProp"}}
{"text": "is happening as the iterations go along we are seeing some behavior of the plots right in uh before I.\ncomment on that I just want you to look at what is happening here right so you started off with.\nsome Eternal which was 0.6 initially your gradients your history was accumulating right this VT was accumulating and you ended.\nup with some effective learning rate right so remember this orange curve is the effective learning rate as what is.\nhappening in these regions these are the regions where your uh slope was high so your VT was accumulating right.\nand even though this is uh exponentially weighted average you have beta into VT minus 1 plus 1 minus beta.\ninto VT as long as you are in regions uh where not really sorry current gradient as long as you.\nare in regions where the current gradient is high this will keep accumulating right so as you are going down.\nsteep regions your history was also high and your current thing is also high so your VT will keep increasing.", "metadata": {"video_title": "RMSProp"}}
{"text": "steep regions your history was also high and your current thing is also high so your VT will keep increasing.\nright and now when you enter regions where you are in a flat region then over a period of time.\nthis VT will become flat right because now your history is contributing history's importance is gradually decreasing because of the.\nbeta factor and your new stuff is very small to make a difference till it reaches a point that you.\nstart seeing a Tipping Point right now your his this new factor is very small and your history is also.\ndiminished significantly because the initial higher gradients are now multiplied by a very large power of beta right so that.\ninitial history has diminished and your current gradients are very small when you are close to the Minima or to.\nthe flat surface so then as we had said right the is history can then start decreasing right so this.", "metadata": {"video_title": "RMSProp"}}
{"text": "the flat surface so then as we had said right the is history can then start decreasing right so this.\nis the region where you had ah High gradients right this is the region where your gradients started decreasing right.\nI mean this is till this point they were increasing and from here onwards they started decreasing and if you.\nnow look at this and compare it to the behavior of the learning rate in this uh let me just.\nuh change the color yeah so in this region where your gradients were increasing the learning rate is steadily decreasing.\nright and this is exactly what you want when the gradients are high you want the learning rate to decrease.\nand now in this region once your gradient started becoming small the learning rate started increasing because your denominator which.\nwas accumulating the history over a period of time it starts decreasing because now all for the past many time.", "metadata": {"video_title": "RMSProp"}}
{"text": "was accumulating the history over a period of time it starts decreasing because now all for the past many time.\nsteps your history has been smaller gradients so all the high weightage things of R of small values and the.\ninitial High values of gradients are now have diminished because they are being multiplied by a very high power of.\nbeta right so as the VT starts decreasing the learning state starts increasing right so just to repeat till this.\npoint your gradients were high hence your history was increasing after this point the current gradients were very small and.\nhence your history started decreasing because now all all you're in a steep region so all the things that are.\ngetting added from this point onwards they're all small all so your effective sum is decreasing and in these two.\nhalves you see the right behavior of the learning rate initially it decreases and then it increases when the curve.", "metadata": {"video_title": "RMSProp"}}
{"text": "halves you see the right behavior of the learning rate initially it decreases and then it increases when the curve.\nbecomes flat okay so this is what is expected right this is what we like about RMS prop but what.\nwe were talking about its sensitivity to uh learning rate right so the initial learning rate so this is what.\nhappened when I chose the initial learning rate to be 0.6 right now uh so this satisfies our wish list.\nwhatever I just spoke is written on this slide now if we had set it to 0.1 right then let's.\nsee what would have happened maybe I should maximize this right so again let me just Define the things here.\nyou would have already understood but uh yeah so this is my history the blue curve is the history that.\nbehavior does not change the orange curve is for the ETA node of 0.6 which we just discussed in detail.\nand the black curve is for the ETA naught of point Y right and now what is happening is that.", "metadata": {"video_title": "RMSProp"}}
{"text": "behavior does not change the orange curve is for the ETA node of 0.6 which we just discussed in detail.\nand the black curve is for the ETA naught of point Y right and now what is happening is that.\nbecause my initial learning rate is small this behavior of how quickly it changes with steep slopes and gentle slopes.\nis also changing right now you can see that initially this was decreasing rapidly right but now this is not.\ndecreasing rapidly it's decreasing slowly but it's still always lower than the lowest point that I had reached here right.\nso what does that mean that it is still going to be lower than the lowest point that I have.\nreached and then again in the Steep regions when it starts increasing it does not increase that rapidly right because.\nit's still the numerator is still small so it nowhere grows as much as I wanted here right hence in.\nthis case you are seeing very wild oscillations in this case you are seeing mylar oscillations and if I also.", "metadata": {"video_title": "RMSProp"}}
{"text": "this case you are seeing very wild oscillations in this case you are seeing mylar oscillations and if I also.\ndraw the curve for 0.05 then that you would see would have even smaller oscillations such that it allows it.\nto then settle it into the Minima right so this what these curves are trying to show is that how.\nfast or slow the learning rate adopts actually depends to an extent on the initial learning rate right so that's.\nwhat is being shown here so again multiple slides conveying the same point that RMS prop is indeed sensitive to.\nthe initial learning rate right and we might want to get to a situation or to an algorithm where this.\ndependency can be avoided now you might argue right that I might find something which is good right so I.\nmight find ETA not equal to 0.1 and that is perhaps always good or ETA not equal to 0.6 which.\nis always good right now one more thing which I'm going to make an argument for now is that that.", "metadata": {"video_title": "RMSProp"}}
{"text": "might find ETA not equal to 0.1 and that is perhaps always good or ETA not equal to 0.6 which.\nis always good right now one more thing which I'm going to make an argument for now is that that.\nis also not guaranteed so suppose you are in a steep region right so you are in a steep region.\nso your history has been accumulating and suppose your current value of VT is 1.25 then what would your effective.\nlearning rate be it would be 0.48 right if the learning rate was set to 0.6 and if the learning.\nrate was set to 0.1 then it would have been 0.08 right so same situation I'm just trying to make.\na case that depending on the initial learning rate your effective learning rate a Time step would be different right.\nand since I'm in a steep region this is my preferred effective learning rate I want a smaller effective learning.\nrate because I'm in a steep region and that I would have obtained if my initial learning rate was 0.1.", "metadata": {"video_title": "RMSProp"}}
{"text": "rate because I'm in a steep region and that I would have obtained if my initial learning rate was 0.1.\nright but now suppose I am in a uh not in a steep region okay I am in a flat.\nregion and say now because I'm in a flat region my history is accumulating slowly so my VT is 0.1.\nand now if I compute the effective learning rate with 0.6 then it's high but with 0.1 it's going to.\nbe low right and now in this case I might prefer uh this as the learning rate because I'm in.\na flat region so I want the learning rate to be high which means I am preferring a different initial.\nlearning rate right so across along the same loss surface your preference for which would have been a good value.\nof ETA naught that decision might change right so this is yet again uh kind of making a case that.\nyou don't want the sensitivity towards eat or not right because you really want it to adapt and not get.", "metadata": {"video_title": "RMSProp"}}
{"text": "you don't want the sensitivity towards eat or not right because you really want it to adapt and not get.\nstuck with an initial choice that you have made because at the start of the algorithm you make this choice.\nthat Eternal is going to be 0.1 or 0.6 or 0.05 and then you are stuck with that choice right.\ninstead can I get rid of this ETA naught and have like a more adaptive setup right so all of.\nthis is trying to make a case for that okay so that's what is being said on the slide also.\nand therefore we wish the numerator also to change with respect to the gradient or the current slope right so.\nif I'm in a steeper region I want the numerator and denominator to adjust in uh kind of concurrence so.\nthat I get a good effective learning rate and if I'm in a flat region again I want to want.\nthem to adjust in kind of correlation so that I have have a good effective learning rate right right now.", "metadata": {"video_title": "RMSProp"}}
{"text": "that I get a good effective learning rate and if I'm in a flat region again I want to want.\nthem to adjust in kind of correlation so that I have have a good effective learning rate right right now.\nthe only the denominator is adjusting the numerator is not adjusting so let's quickly summarize the drawbacks of adagrad and.\nRMS prop so both are sensitive to the initial learning rate uh then if the initial learning rates are great.\nif the initial gradients are large right if the initial gradients are large my VT would be large if my.\nV 0 was large right so my VT would be uh so V 0 itself would be Delta V 0.\nsquare right so if and then after that it will keep adding so this is at a grad where there.\nis no exponential decay so if my initial gradients are large my VT takes on a large value very early.\non my v0 V1 itself would be large and then my effective learning rate right from the beginning would become.", "metadata": {"video_title": "RMSProp"}}
{"text": "on my v0 V1 itself would be large and then my effective learning rate right from the beginning would become.\nvery small and then there's no way for me to recover from there because my VT has become very large.\nand in other grad it never decreases again right it keeps growing because I don't take an exponential decay right.\nso that's clearly a disadvantage of ADA grad and that we were able to kind of alleviate using RMS prop.\nthen if a gentle curvature is encountered in Ada grad but there is no way to increase the learning rate.\nbecause your history has again accumulated right and now I want uh my history has been accumulated and now I.\nreally want this to be small VT to be small only then my effective learning rate will increase but that.\ncannot happen because once VT starts getting big it never comes back down right so again the same problem in.\norder grad that as you once you start accumulating a lot of history then your learning rate is bound to.", "metadata": {"video_title": "RMSProp"}}
{"text": "order grad that as you once you start accumulating a lot of history then your learning rate is bound to.\nget killed right and that's what we saw in that initial diagram that when I had come close to my.\nMinima my movement had become very slow in the B direction right because my effective learning rate was I cannot.\nincrease it now because I'm not taking an exponential average right so this is also kind of resolved in RMS.\nProf but this sensitivity to the initial learning rate still exists and we would like to get rid of that.\nokay.", "metadata": {"video_title": "RMSProp"}}
{"text": "foreign [Music] descent momentum based gradient nestro vaccinated gradient descent and I know you are waiting for more algorithms Adam.\nmadagrad and so on but in between I took a slight D2 which is I went to the stochastic and.\nminimatch gradient descent mini batch versions of these algorithms and we understood how they operate and before I go to.\nthese Advanced algorithms in today's lecture I want to do two more modules one is on adjusting the learning rate.\nand momentum some tips for doing that and this I'll return back to this at the end of all the.\noptimization algorithms once I finish all the optimized algorithms I'll revisit this part and the second module that I want.\nto cover is on something known as line search right so both these are the two things that I've covered.\ntoday and we'll end this lecture there and then in the next week I'll talk about some of the other.", "metadata": {"video_title": "Scheduling learning rate"}}
{"text": "today and we'll end this lecture there and then in the next week I'll talk about some of the other.\nAdvanced optimization algorithms okay so let's start with this tips for adjusting the learning rate and uh momentum uh yeah.\nright so now we were looking at gradient descent and this is where we had started right this was our.\nfirst loss function where we started on this very flat uh plane right and then we argued that when you.\nare in these flat surfaces the gradients are very small and hence your updates will be very small and you'll.\nget stuck there and that's what the motivation for using momentum and nestro and so on right but you could.\nhave also argued right that instead of using momentum or nestro I could have just used a larger learning rate.\nright so if I'd increase the learning rate and if my gradient is small it the gradient multiplied by the.\nLearning rate would still have been a large quantity and I would still have got the effect of moving faster.", "metadata": {"video_title": "Scheduling learning rate"}}
{"text": "Learning rate would still have been a large quantity and I would still have got the effect of moving faster.\nright and it does make sense that learning rate is how fast you move so maybe that would have helped.\nso now let's see what happens if we had gone by this intuition and set the learning rate to 10.\ninstead of 1 right and it's like 10 times what I had set it otherwise right so let's see what.\nhappens in that case so my learning rate is set to 10 now of course it's moving fast but then.\nagain you see the problem of oscillation right so it kind of uh on this you want it to be.\nfast on the gentle places but you don't want it to be fast once it enters the value right because.\non the Steep surfaces anyways the gradient is large so it's going to move fast but now we have multiplied.\nit by the large learning rate and hence you see that there's this oscillation effect right so we don't really.", "metadata": {"video_title": "Scheduling learning rate"}}
{"text": "it by the large learning rate and hence you see that there's this oscillation effect right so we don't really.\nwant that it's just increasing the learning rate is not really the solution always right and what we want is.\nsomething which kind of adapts to the slope if the slope is small then you want a larger learning rate.\nif the slope is large then maybe you want the learning rate to decrease right and this is exactly what.\nwe'll see in the advanced optimization algorithms which will be have an Adaptive flavor to them right so but for.\nnow I just want to mention that it's not just about increasing the learning rate you can't just set it.\nto be of high value always right so that's not right so that setting the learning rate to a high.\nvalue is not the right thing and then what do you set the learning rate to right so here are.\nsome tips so what we typically try to do is that uh at least so now just let me just.", "metadata": {"video_title": "Scheduling learning rate"}}
{"text": "value is not the right thing and then what do you set the learning rate to right so here are.\nsome tips so what we typically try to do is that uh at least so now just let me just.\nkind of step back and contextualize this right so nowadays like for most uh areas that you would work in.\nright suppose you're working in machine translation or say automatic speech recognition not text to speech you would be building.\nup on work which is already being done right so you would already have these Transformer based models someone has.\ntrained it for many languages and so on so you would have a fair sense of what were the hyper.\nparameters they used and you would try to follow them and just experiment in a small window around it right.\nbut the tips here are for the more generic case where you're looking at something new and you don't really.\nknow no one has actually looked at the kind of data that you're looking at or the kind of application.", "metadata": {"video_title": "Scheduling learning rate"}}
{"text": "know no one has actually looked at the kind of data that you're looking at or the kind of application.\nthat you're looking at now how do you uh set the initial learning rate what is small what is large.\nyou don't know that right so that's the context in which I'm saying this but for most practical applications if.\nyou're working on these standard problems as I said machine translation speech recognition image recognition and so on then you.\nwould have something to refer to in the literature which would give you a ballpark about what the learning rate.\nshould be and you would follow that close right so that's the best thing to do so that's the first.\ntip this tip is more for cases where you don't have anything to go by right so what you would.\ndo is you will try to try you try different learning rates and on a log scale right point zero.\nzero zero one point zero zero one and so on right and then you will run the training for a.", "metadata": {"video_title": "Scheduling learning rate"}}
{"text": "do is you will try to try you try different learning rates and on a log scale right point zero.\nzero zero one point zero zero one and so on right and then you will run the training for a.\nfew epochs with all of these algorithms you'll not do the full training you just run it for a few.\nepochs and you'll observe the loss uh how the loss behaves right and based on observing this loss curve you.\nwill get a sense of which is the best learning rate among these four five values that you have chosen.\non the log scale and now suppose point one turns out to be the best learning rate that means the.\nloss uh the behavior on the loss function so you could plot how the loss is decreasing from one Epoch.\nto another or from One update to another you can keep losing the loss for some uh learning rates you'll.\nsee that the loss will increase right because these are probably very high learning rates and you are quickly overshooting.", "metadata": {"video_title": "Scheduling learning rate"}}
{"text": "see that the loss will increase right because these are probably very high learning rates and you are quickly overshooting.\nthe Minima and then going into high loss regions and for some learning rates it will smoothly decrease right so.\nthose would be the good learning rates and then suppose point one is one such good learning rate then you.\nwill do like a zoom in around this point wanted so maybe you can do a more linear search around.\nthis now so if point one was good maybe try point zero five maybe try point two point three right.\nso all of this is of course just heuristics right there's no clear winner strategy here but what you need.\nto take away from this slide is that it might happen that you just set the learning rate to some.\nvalue and you see that your algorithm is not converging your loss keeps increasing then you will have to kind.\nof experiment a bit around it and this is a good way of experiment you just try a few different.", "metadata": {"video_title": "Scheduling learning rate"}}
{"text": "of experiment a bit around it and this is a good way of experiment you just try a few different.\nvalues and then narrow down on what a good value is and then just maybe zoom in a bit around.\nthat right so that's why uh the other is uh as you initially when you start off maybe you don't.\nknow anything right because the weights are completely random so may you some learning rate even slightly higher might work.\nright but as you have started moving towards the uh Minima you don't want to retain a very high learning.\nrate right because then there's always this chance of overshooting the Minima so they you should do what is known.\nas Anil the learning rate right which means keep reducing the learning rate as the training progresses right so one.\nstrategy there is to use what is known as a step Decay and you could use some fixed number of.\nepochs right after every five epochs or ten epochs I'll keep having the learning array right so as you are.", "metadata": {"video_title": "Scheduling learning rate"}}
{"text": "epochs right after every five epochs or ten epochs I'll keep having the learning array right so as you are.\ntraining is progressing your learning rate is becoming smaller and smaller so you're making more conservative updates especially when you.\nare closer to the Minima you're not making a large update so that you cross the minimum right and your.\napproximation for here I am close to the Minima is just that hey I've been training for five epochs now.\nso maybe let me just reduce the learning rate so you're just using the number of epochs as an approximation.\nfor how close you are to the uh Minima that you want to use right reach another strategy is that.\nyou keep the learning rate uh you had finished one Epoch you compute the loss over your validation data right.\nso you have the training data using which you are training keep some data aside and now after you have.\ndone one Epoch calculate what the validation loss is and let's say the value is some X okay now with.", "metadata": {"video_title": "Scheduling learning rate"}}
{"text": "done one Epoch calculate what the validation loss is and let's say the value is some X okay now with.\nthe same learning rate run the second Epoch right so now you are you have already computed some values of.\nthe weights you have made a lot of updates after one Epoch and you have some value of w just.\nstart from there run the second Epoch and keep the same learning rate and at the end of this Epoch.\nagain look at the validation error so the validation error is also decrease that means things are going fine right.\nyour training error is definitely decreasing and your validation error is also decreasing so maybe this learning rate is not.\nso high you can continue with it but if the validation loss starts increasing right then what would you do.\nin that case uh let me just illustrate this what I mean right so this is a good validation loss.\nright so it from after every Epoch you're lost when you're checking the loss it's decreasing right and suppose you.", "metadata": {"video_title": "Scheduling learning rate"}}
{"text": "right so it from after every Epoch you're lost when you're checking the loss it's decreasing right and suppose you.\nare here now and you so say this was Epoch six and now you ran the seventh Epoch okay and.\nyou calculate the loss again and it was actually increased right so what will you do now you you had.\nyou are of course saving the weights after every Epoch so you will throw away all the updates that you.\nhave done in the last Epoch re-initialize the weights from what they were in the sixth epoch half the learning.\nrate and then run the cpoc again right and then hopefully the loss would decrease it's still possible that the.\nloss still increases that means your learning rate is still high so again throw away all the updates go back.\nto the value of the weights that was there at the end of sixth Epoch half the learning rate again.\nand then continue from there right so this is like more data driven learning rate that okay my validation loss.", "metadata": {"video_title": "Scheduling learning rate"}}
{"text": "and then continue from there right so this is like more data driven learning rate that okay my validation loss.\nis increasing that means my training is not helping me to generalize well so let me just not make a.\naggressive updates because my updates are according to the training data not according to the validation data so let me.\nnot make aggressive updates and one way of ensuring that is to just halve the learning rate and then run.\nokay right and if it keeps increasing after that then maybe you have just conversed then you should stop training.\nat that point right so that's one strategy for annealing the learning rate another way of annealing the learning rate.\nis to use an exponentially decaying learning rate so you have some initial learning rate and then you keep exponentially.\ndecaying it right so what is happening here is that you are having 1 over ETA 0 raised to KT.\nright so at time step 0 suppose your ETA 0 is 1 right so at time step 0 and let's.", "metadata": {"video_title": "Scheduling learning rate"}}
{"text": "decaying it right so what is happening here is that you are having 1 over ETA 0 raised to KT.\nright so at time step 0 suppose your ETA 0 is 1 right so at time step 0 and let's.\nassume that K is also equal to 1. so suppose ETA 0 equal to 1 and K is also equal.\nto 1 so time step 0 this would just be 1 over 1 so your learning rate is 1 and.\nas the time steps keep increasing your learning rate will keep decreasing exponentially right so that's one simple thing to.\ndo but here the issue is that now what is the initial learning rate that becomes a hyper parameter that.\nwas anyways a hyper parameter but in addition this K and this K controls how quickly it will Decay right.\nso if you use a very large value of K it will Decay very quickly right so you would see.\nsomething like this if you use a smaller value of K it would Decay more smoothly right so now what.\nis the right value of K that also needs to be determined so this makes it even more complex right.", "metadata": {"video_title": "Scheduling learning rate"}}
{"text": "something like this if you use a smaller value of K it would Decay more smoothly right so now what.\nis the right value of K that also needs to be determined so this makes it even more complex right.\nso I typically do not uh recommend uh this to be used and there's another way of doing this exponential.\nuh learning rate uh which is to use the 1 by t d k right which is again similarly you.\nhad some initial learning rate and then you divide it by the number of time steps that you have done.\nso far and again K helps you decide how fast the DK will happen right so again you have this.\nK to decide which makes it a bit uh tricky right so again this any kind of this exponential decay.\nwhich introduces this parameter K which controls how fast the DK will happen is again tricky to fix it because.\nyou could have different values of K so my personal choice is always to go by the validation loss and.", "metadata": {"video_title": "Scheduling learning rate"}}
{"text": "you could have different values of K so my personal choice is always to go by the validation loss and.\ndecrease the learning rate if the validation loss is increasing right so that's the first method that we had looked.\nat right now similarly you could have adjust the momentum right and for momentum there was this method suggested in.\nuh this paper and this looks very uh complex equation but it is not really very complex so let's try.\nto decode what it is right so there's a log here so let's and this T here the T stands.\nfor time step so at time step 0 you would have log of 0 Plus 1. and log of 1.\nis 0 so this term would disappear so we'll just have 1 minus 2 raised to minus 1 right which.\nwould just be 0.5 right and this says minimum of 0.5 and beta Max so beta Max is typically one.\nof these values uh so it would be uh or even 0 0 means there is no momentum of course.\nso beta Max would be one of these values so now in this case it would be minimum of say.", "metadata": {"video_title": "Scheduling learning rate"}}
{"text": "of these values uh so it would be uh or even 0 0 means there is no momentum of course.\nso beta Max would be one of these values so now in this case it would be minimum of say.\n0.5 and 0.9 so your momentum would be around 0.5 okay and as you keep increasing now at time step.\nt equal to 250 uh this would evaluate to log of 1 plus 1 which would be log of 2.\nso this would become 1 so you will have 2 raised to minus 1 minus 1 which would be 2.\nraised to minus 2 which would become 0.25 so this would be 1 minus 0.25 which is equal to 0.75.\nso it would be minimum of 0.75 and beta Max so again beta max if it is something in the.\nrange of 0.9 then this would be selected right and now you can keep substituting the values of T so.\nnow if T is equal to 750 then this would be log of uh 3 plus 1 which would be.\nlog of 4 right and that would be 2 so this will become 2 raised to minus 3 which would.\nbe 0.125 and then this would become 0.875 so as you can see you are slowly increasing the momentum uh.", "metadata": {"video_title": "Scheduling learning rate"}}
{"text": "log of 4 right and that would be 2 so this will become 2 raised to minus 3 which would.\nbe 0.125 and then this would become 0.875 so as you can see you are slowly increasing the momentum uh.\nterm the beta term right which is the amount that you should give to the history right beta tells you.\nhow much weight is to give to the history so as you are training is progressing you are planning to.\nrely more and more on the history and less on the current update and that makes sense right because now.\nif you're close to the Minima and then one faulty update will take you away from the Minima right but.\nwhereas if your history is uh pointing in a certain direction you would like to rely on that because over.\nthe large number of updates you have reached in this region so you want to give more weightage to your.\nhistory as opposed to the current update so what this is doing is as your training progresses your beta value.", "metadata": {"video_title": "Scheduling learning rate"}}
{"text": "history as opposed to the current update so what this is doing is as your training progresses your beta value.\nincreases right and as the beta value increases you give more and more weightage to the history right and in.\nthe initial phase you don't want to give lot of weightage to the history because your history is still building.\nup right in your history is as unreliable as the current update right as the training progresses you want to.\ngive more value to the history right so that's the tip for adjusting the momentum so this is uh currently.\nall I have for tips for learning rate and momentum as I said I'll revisit tips for learning rate towards.\nthe end of this lecture again okay so the next thing I want to do is talk about line search.\nis again a simple idea now the whole issue that we have is that if you choose one learning rate.\nand you're kind of married to it either learning rate is good then you would progress well if that learning.", "metadata": {"video_title": "Scheduling learning rate"}}
{"text": "and you're kind of married to it either learning rate is good then you would progress well if that learning.\nrate is too high or too low then either you will keep oscillating if it's too high or if it's.\ntoo low then you will not be making Fast movements right so instead of sticking to one learning rate why.\nnot try a bunch of learning reads right so that's what we are trying to do here that you're trying.\ndifferent learning rates okay now you compute the derivative and your w t plus 1 is equal to WT minus.\nETA times the derivative right now the derivative of course does not change but you can plug in different values.\nof ETA you can plug in the value 0.1 you can plug in the value 0.5 plug in the value.\n1 to 10 all of these values right and for each of these you will get a new value for.\n2 w t plus 1 right so this would be w t plus 1 corresponding to Eta 1 another WT.\nplus 1 corresponding to ETA two and so on so now we have found a new value for weight and.", "metadata": {"video_title": "Scheduling learning rate"}}
{"text": "2 w t plus 1 right so this would be w t plus 1 corresponding to Eta 1 another WT.\nplus 1 corresponding to ETA two and so on so now we have found a new value for weight and.\nnot just one new value you have found a bunch of new values each corresponding to a particular ETA now.\nplug in all these values in the loss function right so all of these values you could plug in into.\nthe loss function all the different W's that you have computed and now whichever W gives you the minimum loss.\nyou pick that up so what you have done effectively is that you have tried different learning rates and you.\nhave made an update so you got a bunch of updated values now you're looking at each of those values.\nand Computing the loss and whichever update to updated value gives the minimum loss you're retaining that and throwing away.\nall of that and then again doing the same in the next iteration so the derivative only gets computed once.", "metadata": {"video_title": "Scheduling learning rate"}}
{"text": "all of that and then again doing the same in the next iteration so the derivative only gets computed once.\nbecause that does not change but you just use different learning rates to come up with different updated values and.\nthen select the best updated value based on the loss right so that's what you do in line search you.\nhave a bunch of learning rates and you don't get married to one learning rate but just try all of.\nthem at every stage of course there's a complexity here involved here that you are trying 10 different learning rates.\nthen at every step you're kind of computing 10 values so you have to do 10 times uh the work.\nand just in that update equation and then compute the loss again and then select the best one right so.\nthat's an additional complexity that you have but as you can imagine this would definitely be good right so in.\nthe flat surfaces you might end up choosing the larger learning rates and in the valley regions where already the.", "metadata": {"video_title": "Scheduling learning rate"}}
{"text": "the flat surfaces you might end up choosing the larger learning rates and in the valley regions where already the.\nslope is steep or when you're entering the valley in those regions if you have a high learning rate you.\nknow that your loss would increase because you would overshoot the Minima so in those cases the smaller learning rate.\nwould get selected right so you're kind of in some sense making it a bit adaptive based on which region.\nyou are in right the flip side of course is that you are doing a lot more computations okay so.\nlet's see what happens when we do line search so you see what happened right so the gradient descent was.\nmoving at a certain speed it's taking more time to converge but the line search you can see that in.\nthe flat regions it was able to move very quickly and it also did not overshoot right it did not.\ngo out of the valley because in the Steep regions a smaller learning rate would have worked and it would.", "metadata": {"video_title": "Scheduling learning rate"}}
{"text": "go out of the valley because in the Steep regions a smaller learning rate would have worked and it would.\nhave not selected the larger learning rate which would have taken it out of the value right so it kind.\nof works very nicely and it of course converges much faster than the gradient descent which was used with a.\nfixed learning rate of one in this case right so I think that's the that advantage of line search is.\nclear so that's all I have for uh today uh we are just going going over the slides the convergence.\nis much faster we see some oscillations but these are not as bad as uh what we had in momentum.\nand nag and overall the line search worked better right so that's where I'll end uh for today and in.\nthe next week we'll see uh some uh variations of gradient descent algorithm which use an Adaptive learning array okay.\nso I'll see you next week thank you.", "metadata": {"video_title": "Scheduling learning rate"}}
{"text": "foreign [Music] so you had this this is what is happening here so you had this one word embedding okay.\nhi let's just call it H1 right if you want H1 H1 you did this linear transformation right so let's.\nsay this was an r t dimensional embedding then you could multiply it by a d cross D dimensional Matrix.\nand again get a d dimensional vector and you could do the same thing at all these three places right.\nso from now one vector you have been able to generate three vectors and you needed these three vectors because.\nthree vectors were participating in your attention equation earlier right so you have these three vectors now now what do.\nyou do with these three vectors so these are they just look at their names again so this was all.\nthe hi's that you had all the edges these were your matrices these were the learnable matrices the transformation matrices.\nWQ for query WV V for value and w k k for key and what you get out is the.", "metadata": {"video_title": "Self-Attention"}}
{"text": "the hi's that you had all the edges these were your matrices these were the learnable matrices the transformation matrices.\nWQ for query WV V for value and w k k for key and what you get out is the.\nquery Vector Q the value vector B and the key Vector okay right so for each word from your perspective.\nfor each word you have now been able to create three vectors from it right now how do you use.\nthese vectors how do you compute the attention is something that will come later on right but for now what.\nwe're doing is something simple right for whatever reasons we are taking one vector and Computing three vectors from it.\nand these three vectors are being Computing with the help of linear Transformations coming from a matrix right so and.\nthese are learnable parameters so you have already introduced some parameters into the X okay so I'm just repeating what.\nI had said on the slide and these are called the respective transformations now so now let's focus on uh.", "metadata": {"video_title": "Self-Attention"}}
{"text": "I had said on the slide and these are called the respective transformations now so now let's focus on uh.\nComputing the output for the first uh guy right so this is the first input right let's see so you.\nhave H1 H2 all the way up to H5 and now let's see how Z1 gets computed right which is.\nthe contextual representation for the word I right and through the self retention layer and what do these key query.\nvalue vectors that I showed how do they play a role in this right so let's see what is going.\nto happen there yeah so just zoom into this I'll just clear the annotations so you had this single embedding.\nwhich was H1 right and from there you can see there are three arrows coming out so you would have.\nrealized that I'm going to compute three different values from this one vector right so let's see what those three.\nvalues are sorry so I'll first do a linear transformation with WQ to get a new Vector which is I'm.", "metadata": {"video_title": "Self-Attention"}}
{"text": "values are sorry so I'll first do a linear transformation with WQ to get a new Vector which is I'm.\ngoing to call as the Q by Q Vector so this is going to be the q1 similarly I'll do.\na linear transformation with K and I'll call it as K1 similarly I'll do a linear transformation with v and.\nI'm going to call it as VR right now how I'm going to use this q1 K1 V1 doesn't matter.\nright but at least pectorially you get to know what is happening here I had one vector I did three.\nlinear transformations to get three different vectors from that I am calling them as q1 K1 V1 I'll continue the.\nsame for all the uh all my words in the input all of this can be done in parallel of.\ncourse so I'll again take the last word for example I'll do this for all the word but I'm just.\nshowing it for the last word again so I'll take uh H Phi in this case pass it through WQ.\nand I'll get my Q Phi similarly I'll get K Phi and similarly I'll get V5 right so if I.", "metadata": {"video_title": "Self-Attention"}}
{"text": "showing it for the last word again so I'll take uh H Phi in this case pass it through WQ.\nand I'll get my Q Phi similarly I'll get K Phi and similarly I'll get V5 right so if I.\nhad t words in the input I have computed this 3 into T vectors using these linear Transformations okay now.\nwhat next what am I going to do next right so now what I want to do is I want.\nto compute earlier I was Computing the score let me just flash the equation and then I can do it.\nso earlier I was Computing the score between St minus 1 and HJ and this was used to give me.\nthe importance of the j8 word and the TX time step right so again now I'll have some score and.\nmy indices would be I and J right the ith word and the jth word but now what does go.\nhere that's the question right so let's see what goes here the function that I'm going to use is to.\ncompute the score between q1 and K J right so now my this is my query Vector so I'm interested.", "metadata": {"video_title": "Self-Attention"}}
{"text": "here that's the question right so let's see what goes here the function that I'm going to use is to.\ncompute the score between q1 and K J right so now my this is my query Vector so I'm interested.\nin knowing the weights of all the other words with respect to the first one that's why that's my query.\nthe query is the first word and I want to understand the importance of all the words with respect to.\nthat word so I'll pass different values of k k 1 k 2 K 3 k 4 K5 through it.\nand compute the score right so I'm going to compute uh five such scores right so earlier this was the.\nSt minus 1 was fixed right uh because that was the state of the decoder time step T minus one.\nthat did not change so that was in some sense my query because for at time step T I was.\ninterested in knowing the weights of all the input words now with respect to my query or by my first.\nword or my word of focus I want to know the weights of all the other words so that word.", "metadata": {"video_title": "Self-Attention"}}
{"text": "interested in knowing the weights of all the input words now with respect to my query or by my first.\nword or my word of focus I want to know the weights of all the other words so that word.\nis going to remain fixed I'm going to have q1 and I'm going to compute capital t such values right.\nso each telling me the importance of the first word second word third word fourth word fifth word right notice.\nthat I'm also Computing the importance of the word itself right that also I'm doing so q1 with respect to.\nK1 K2 K3 K4 K5 right so q1 with respect to K1 K2 K3 K4 K5 so these five scores.\nI'm going to compute and maybe I'll call them as is okay uh now the function that I'm going to.\nuse is just the dot product like what is the scoring function my scoring function is just dot product so.\nmy E's which are the unnormalized attention weights are just going to be the dot product between q1 and K1.\nQ2 and q1 and K2 q1 and K3 all the way up to q1 and K5 right so as I.", "metadata": {"video_title": "Self-Attention"}}
{"text": "my E's which are the unnormalized attention weights are just going to be the dot product between q1 and K1.\nQ2 and q1 and K2 q1 and K3 all the way up to q1 and K5 right so as I.\nmentioned here q1 remains fixed and I just keep changing the key that means the word or the representation with.\nwhich I want to compute the attention right so that keeps changing so I'll get some uh dot products here.\nit should be some real values and the way I'll compute the alphas from that is that I'll just do.\na soft Max on this Vector right so that's what this equation is saying to compute Alpha so this is.\ne 1 1 this is e one two all the way up to E1 Phi so to compute alpha 1.\n2 I'll just take uh okay my I should have chosen my variables carefully so e raised to e Point.\n2 divided by the summation over all E's right that's what I'll do right so I'm just going to take.\nthe soft Max here now once I have taken the soft Max that gives me the alphas but how am.", "metadata": {"video_title": "Self-Attention"}}
{"text": "2 divided by the summation over all E's right that's what I'll do right so I'm just going to take.\nthe soft Max here now once I have taken the soft Max that gives me the alphas but how am.\nI going to compute my Z my Z is going to be a weighted sum of the inputs right and.\nwhat I what are the vectors that I am considering as input I am considering the v's now right so.\nthese are known as the value vectors right so q and K participated in Computing Alphas once I am going.\nto compute Alphas my new representation which is z is going to be a weighted sum of the v's right.\nso you have all the three vectors participating in this computation q and K participate in Computing the alpha and.\nthen there was a v right just as you had a v sitting outside if you remember right you had.\nthis V getting multiplied by everything that was happening inside is here this VA transpose damage into something something and.", "metadata": {"video_title": "Self-Attention"}}
{"text": "this V getting multiplied by everything that was happening inside is here this VA transpose damage into something something and.\nthis is where St minus 1 and H J were so they were participating in Computing this and finally you.\nget a vector and then you multiply it by this right so similarly now the VJs here are participating uh.\nin the final computing station that you have right so Z1 is going to be computed this way now how.\ndo you compute Z2 the same story everything Remains the Same right so now you would have Q2 as the.\nquery right so your e2s would be the dot product between all the Q2 and all the vectors then you.\nwill compute the alphas as the soft Max once you have computed the alphas these are all Alpha twos you.\nwill compute the Z as a weighted sum of your Phi input vectors and the weights would come from the.\nalpha right so very easy to understand now you had one vector you had H1 to H5 from each of.", "metadata": {"video_title": "Self-Attention"}}
{"text": "will compute the Z as a weighted sum of your Phi input vectors and the weights would come from the.\nalpha right so very easy to understand now you had one vector you had H1 to H5 from each of.\nthese H1 h2h5 you computed three vectors using the carry query transformation key transformation and value transformation the query Vector.\nis used to find the importance of all the words with respect to this word right so this is fixed.\nand you're trying to find the importance of all the words so the vector contains the dot product between your.\nquery and all the keys that you have once you have computed the importance now you need to take a.\nweighted sum of all the words so for taking the weighted sum you look at the value Vector right so.\nthat's how you have these three vectors which get used in this computation right so I think it's uh should.\nbe clear from the diagram and the equation how the zis will be computed so you'll just do this Z1.", "metadata": {"video_title": "Self-Attention"}}
{"text": "be clear from the diagram and the equation how the zis will be computed so you'll just do this Z1.\nZ2 for all the cells right now let's see if we can vectorize all of these computations right that means.\ncan we compute the Z1 to ZT in one group right so here I first told you how to compute.\nZ1 then I told you how to compute Z2 and then similarly Z3 up to ZT right but my whole.\npoint was that I don't want to compute these outputs sequentially right that's why I didn't like rnns because the.\noutput of the RN and if this was the RNN block then these outputs were coming one by one and.\nat the end of all this I don't want to do the same thing again right I want all of.\nthese to come out in parallel otherwise it doesn't help me right I might as well have stayed with RNs.\nright so now can I do this in parallel so now let's see how I am Computing uh q1 q1.\nwas The Matrix WQ multiplied by H1 how would I compute Q2 it would be the Matrix WQ multiplied by.", "metadata": {"video_title": "Self-Attention"}}
{"text": "right so now can I do this in parallel so now let's see how I am Computing uh q1 q1.\nwas The Matrix WQ multiplied by H1 how would I compute Q2 it would be the Matrix WQ multiplied by.\nH2 and that would give me uh Q2 how would I compute Q3 it would be the Matrix WQ multiplied.\nby H3 which would give me Q3 all the way up to h capital T so it would be WQ.\nmultiplied by h t which would then give me Q capital T right so now I can just write this.\nas a matrix operation you can just put all of these vectors inside a matrix and if you multiply this.\nmatrix by this Matrix then you get this Matrix as an output right so all the cues you can compute.\nat one go right you don't need to do that sequentially all the query vectors can be computed at one.\ngo by just multiplying these two matrices WQ multiplied by The Matrix containing all these inputs as columns and then.\nyou get the queues at one go similarly now what is the dimension of Q right so let's let's look.", "metadata": {"video_title": "Self-Attention"}}
{"text": "you get the queues at one go similarly now what is the dimension of Q right so let's let's look.\nat that right so let me see how to do that yeah so this is say the input Dimension was.\nd right and for I'll just take d as 64 for the purpose of explanation so this is a 64.\ncross T Matrix right so each of these is 64 Dimension and you have t such entries now this would.\nget multiplied by uh say uh so this would say get multiplied by a 64 cross 64 Matrix right so.\nyou could have just think thought of this as D cross D and this as D cross T so these.\ntwo matrices multiply and I get a d cross D output in my case I have just taken d as.\n64 right so this Cube would also be of the same size as your input representation but you could have.\ndifferent sizes also right so if you had chosen this as D1 then your output would be D1 cross T.\nright and so you could have either the D1 could either be bigger than D or smaller than D based.", "metadata": {"video_title": "Self-Attention"}}
{"text": "different sizes also right so if you had chosen this as D1 then your output would be D1 cross T.\nright and so you could have either the D1 could either be bigger than D or smaller than D based.\non whether you want to generate just project to a smaller space or a higher space but that is not.\nthe main point here right the main point is that you had T inputs and you have t output right.\nthis is not changing right this capital T Remains the Same that means you had T input representations and now.\nyou have got T query representations from those t uh input representations right what do you choose the size of.\nD1 and D2 is up to you in this example I have chosen it as 64 both D1 and D.\nequal to 64. so if my input this was 64 plus T my output would also be 64 cross D.\nthank you what is important as this T that you had T inputs and you got T outputs in parallel.\nright you've got uh you did the entire computation in parallel similarly your value uh your key vectors you can.", "metadata": {"video_title": "Self-Attention"}}
{"text": "right you've got uh you did the entire computation in parallel similarly your value uh your key vectors you can.\ncompute in parallel so you have uh w k multiplied by H1 gives you K1 w k multiplied by H2.\ngives you K2 and so on so you might as well stack them up in a matrix and then you.\ndo these two multiplications so you'll get K1 to KT in parallel again the key thing here is you had.\nT inputs right and you got T outputs in parallel right so your K Matrix is also going to be.\nsomething cross capital T okay that's what is important that you get T outputs and lastly same for the value.\nMatrix also you'll get these T values in parallel right so now you have already parallelized the computation of the.\nk q and a v right so that at least I don't need to do sequentially now can I do.\nthe rest of it also in parallel is the question right so let's see how I can compute the entire.\noutput in power right so this is how I can compute the entire output in parallel so what was I.", "metadata": {"video_title": "Self-Attention"}}
{"text": "the rest of it also in parallel is the question right so let's see how I can compute the entire.\noutput in power right so this is how I can compute the entire output in parallel so what was I.\ndoing earlier right so how did I compute uh Z1 so now can I compute the entire output in parallel.\nthat means can I compute all the Z's in parallel and yes you can so this is what your Z.\nMatrix would look like again it would have these uh capital T outputs and all of them can be computed.\nin parallel by using this equation how does that make sense so what was what what did I wanted if.\nyou remember I had started with this wish list that I had these words I am enjoying blah blah and.\nthen I am enjoying blah blah so this was a t cross T Matrix right so I had to compute.\nthis T cross T attention Matrix is that what is happening here indeed right so if you remember Q was.\n64 cross T So Q transpose would be T cross 64 and K was T cross 64. oh sorry 64.", "metadata": {"video_title": "Self-Attention"}}
{"text": "this T cross T attention Matrix is that what is happening here indeed right so if you remember Q was.\n64 cross T So Q transpose would be T cross 64 and K was T cross 64. oh sorry 64.\ncross T so when you are multiplying these two matrices you are getting a t cross T Matrix which is.\nessentially the attention Matrix and now once you have the attention weights you are multiplying them by the value Matrix.\nright which contains the B1 V2 up to VT right and now if you take the product of these two.\nmatrices then you will again get a t dimensional uh output because this is D cross t uh and this.\nis uh sorry yeah so this is V transpose sorry so this is going to be T cross D and.\nthis is T cross T Matrix so this 2 will multiply to give you a t cross D output so.\nyou'll get a capital t z Each of which is going to be D dimensional right so you can do.\nthe entire computation in parallel and you can make sure right you can go and check this back that if.", "metadata": {"video_title": "Self-Attention"}}
{"text": "you'll get a capital t z Each of which is going to be D dimensional right so you can do.\nthe entire computation in parallel and you can make sure right you can go and check this back that if.\nI were to look at let's see like we can just do it right away um okay so what I.\nwould like to show is that Z2 is indeed if you do this large matrix multiplication Z2 indeed comes out.\nto be the same as what we had seen in the figure right so let's try to uh do that.\nso I'll just uh start from scratch so I had this T cross T Matrix which was Q transpose K.\nright so if I look at the second row of this Matrix right uh so I'm let's see what is.\nhappening here right so this was Q transpose so this was q1 transpose this was Q2 transpose and so on.\nand this was K1 K2 and so on right so if I look at the second row of so any.\nijth value of this entry of this Matrix is just going to be Qi transpose uh k j right and.\nin particular if I were to look at the second row of this Matrix it is going to be Q.", "metadata": {"video_title": "Self-Attention"}}
{"text": "ijth value of this entry of this Matrix is just going to be Qi transpose uh k j right and.\nin particular if I were to look at the second row of this Matrix it is going to be Q.\n2 transpose K1 Q2 transpose K2 and so on up to Q2 transpose K T right that's what the second.\nrow is going to look like and now that this Matrix Q transpose K this is what the Q transpose.\nK Matrix looks like is going to get multiplied by V transpose right so then this would have V1 here.\nV2 here and so on right and if I multiply these two what is the second row going to be.\nthe second row is going to be a linear combination of all the rows of this Matrix where the weights.\nare these right and that's exactly what I wanted I wanted Z2 to be q1 Q2 transpose K1 right which.\nwas Alpha 2 1 multiplied by V1 and then Alpha to 2 multiplied by V2 and so on and that's.\nexactly the computation which is happening here right so that's why this entire thing can be produced as a matrix.", "metadata": {"video_title": "Self-Attention"}}
{"text": "exactly the computation which is happening here right so that's why this entire thing can be produced as a matrix.\nmatrix multiplication and all of this can be done in parallel so just to recap your V's can be computed.\nin parallel your case can be computed in parallel your cues can be computed in parallel once you have that.\nyou get all the Z's in parallel at one shot right so that's what is happening here which are able.\nto compute they are able to paralyze the entire computation of your Z and what actually is z just in.\ncase you have forgotten so you have the input as H1 H2 all the way up to h t and.\nZ was the output of this network right and my main goal was that this output should be computed in.\nparallel as computed as compared to rnns where I was getting 1 then 2 then 3 and then four right.\nbut now I have shown you that all of this can be done in parallel and you just need to.", "metadata": {"video_title": "Self-Attention"}}
{"text": "parallel as computed as compared to rnns where I was getting 1 then 2 then 3 and then four right.\nbut now I have shown you that all of this can be done in parallel and you just need to.\nexecute this matrix multiplication which is which is in turn has many matrix multiplication V itself comes from a matrix.\nmultiplication K comes from a matrix multiplication Q comes from a matrix multiplication once you have that you do these.\nMatrix multiplications and you get the Z at one go right but all of this is parallelizable I don't need.\nto wait uh to compute ZT minus 1 to compute ZT right that's the main takeaway that I have here.\nright and you see them something here which is you're scaling it by the dimension right so this D was.\n64 in the examples that I had done so there is uh some uh there's some justification for why you.\nneed to do that I'll not go into it uh but for all practical purposes you're taking the dot product.", "metadata": {"video_title": "Self-Attention"}}
{"text": "64 in the examples that I had done so there is uh some uh there's some justification for why you.\nneed to do that I'll not go into it uh but for all practical purposes you're taking the dot product.\nand then just scaling it by some value right so this is what the dimensions look like as I had.\nalready explained uh so you get all the capital T outputs and since you're taking the dot product and then.\nscaling it this is known as scaled dot product attention and this is how you should look at the series.\nof operations happening here right so you had Q you had K and B you did a matrix multiplication to.\nget Q transpose K when you did a scaling to get the scaled output then you did a soft Max.\non that and then whatever you got as soft Max which would have been a matrix right this soft Max.\nof this entire thing this is again going to be a t cross T Matrix which is going to get.\nmultiplied by a t cross 64 Matrix and that's the mat mile that I'm talking about here and then finally.", "metadata": {"video_title": "Self-Attention"}}
{"text": "of this entire thing this is again going to be a t cross T Matrix which is going to get.\nmultiplied by a t cross 64 Matrix and that's the mat mile that I'm talking about here and then finally.\nyou will get a t cross uh 64 output right so this is the attention that you get this is.\nthe self attention layer block that you have this is what the full block looks like it starts with these.\nlinear Transformations then the matrix multiplication soft Max and then again matrix multiplication to give you the capital Z at.\nthe output which in turn contains Z1 Z2 all the way up to set T right so we have been.\nable to see the self attention layer which does a parallel computation to give you a contextual representation for the.\ninput vectors that you had provided right so I'll stop here and then uh in the next lecture we'll look.\nat uh multi-headed attention and then I'll talk about a few other components of the Transformer Network thank you.", "metadata": {"video_title": "Self-Attention"}}
{"text": "foreign [Music] neurons gradient descent feed forward neural networks and representation power of feed forward neural networks right so uh.\nbefore I begin just a quick set of acknowledgments so for module 3.4 I have borrowed ideas from the videos.\nof Ryan Harris on visualizing back propagation these are available on YouTube and linked on the slide here and for.\nmodule 3.4 I have borrowed ideas from this excellent book which is set in the see in the footer which.\nis again available online and I'm sure I've taken ideas and been influenced by material elsewhere so I apologize if.\nI have failed to acknowledge them and if you find anything that I should acknowledge please let me know I'll.\nadd it to the slides okay so let's start with the first module uh which is on sigmoid neurons right.\nso where are we right now and what is the story ahead they just get that picture clear and then.", "metadata": {"video_title": "Sigmoid Neuron"}}
{"text": "add it to the slides okay so let's start with the first module uh which is on sigmoid neurons right.\nso where are we right now and what is the story ahead they just get that picture clear and then.\nwe can start uh so frankly enough about Boolean functions right so the last lecture and entire thing was about.\nBoolean functions where we were looking at functions Y is equal to f of x where both X was Boolean.\nand Y was Boolean of course when we were looking at perceptrons we said that real inputs are allowed but.\nfor most of the discussion we were focusing only on Boolean infosets I want to move away from there and.\nwhere do I want to move right so we want to look at arbitrary functions of the form Y is.\nequal to f of x okay where X belongs to r n instead of 0 comma 1 or instead of.\nlike Boolean and Y also belongs to R right so what that means is that both X and Y contain.\nreal numbers right and so now let's let's uh look at an example of this right so what do I.", "metadata": {"video_title": "Sigmoid Neuron"}}
{"text": "like Boolean and Y also belongs to R right so what that means is that both X and Y contain.\nreal numbers right and so now let's let's uh look at an example of this right so what do I.\nmean by I want to move towards these problems so I'll take my favorite example which is the example of.\ntrying to find whether I'll find oil at a particular location or what is the quantity of oil I can.\nfind at that location and that would drive my uh decision on whether I should set up a drilling station.\nthere that's a practical problem and I could imagine that such a decision would depend on several Factor right so.\nI'm looking let's imagine I'm looking at a location inside the ocean right and I want to decide whether this.\nis worthwhile investing in setting up a drilling station there which means I want to know the quantity of oil.\nthat I can mine from here right and I would May is base my decision on several factors so say.", "metadata": {"video_title": "Sigmoid Neuron"}}
{"text": "that I can mine from here right and I would May is base my decision on several factors so say.\nsalinity could be one right then I could look at the density of the water there right sorry this pen.\nis not really working well but I'll just go ahead so salinity density you understand what I'm saying then maybe.\nI can look at the pressure at that location I could look at the temperature at that location I could.\nlook at the Marine diversity at that location right so all of these are real values so it's all of.\nthese belong to R all of these are real values and I might base my decision on N such different.\nfactors right and this we have been discussing for a while that whenever I take decision I consider a lot.\nof inputs and now I'm talking about n inputs each of them is a real number so this Vector that.\nI am looking at here is a vector which belongs to RM right so that's the idea now so that's.\nwhat I meant by this that X belongs to RN and now what is it that I'm trying to predict.", "metadata": {"video_title": "Sigmoid Neuron"}}
{"text": "I am looking at here is a vector which belongs to RM right so that's the idea now so that's.\nwhat I meant by this that X belongs to RN and now what is it that I'm trying to predict.\nin this case I'm trying to predict a y which again belongs to R right so y again belongs to.\nR because Y is again a real number I am trying to predict the quantity of the oil that I.\ncan get from that location right so this is a real world problem and you could think of various such.\nreal world problems right so another example could be if you want to decide what is the interest rate that.\nyou should set for a particular customer right as a bank and this often depends on the past history of.\nthe person whether he'll be able to pay at a certain rate pay back the capital and so on right.\nso you might have factors like what is the salary what is the family size right what was the last.\nloan that that person had taken how many years that those loan was repaid in what was the interest rate.", "metadata": {"video_title": "Sigmoid Neuron"}}
{"text": "loan that that person had taken how many years that those loan was repaid in what was the interest rate.\nfor that loan was there any defaults and all of these you can see are real numbers and then the.\noutput is also a real number which you want to decide an interest rate maybe something between say five to.\ntwenty percent right that's what you want to decide so this is another problem right so these are the kind.\nof problems that we are interested in dealing with where your inputs are real numbers and your outputs are also.\nreal numbers right and we'll take this idea further where we'll start later on in the course talking about images.\nright and even if you look at images what is an image if you look at a 100 cross 100.\nimage these are pixel values which tell you some colors right so these are all again real numbers and based.\non these real numbers which is a very high dimensional Vector because you're looking at a hundred cross hundred Vector.", "metadata": {"video_title": "Sigmoid Neuron"}}
{"text": "on these real numbers which is a very high dimensional Vector because you're looking at a hundred cross hundred Vector.\nnow and I can just flatten it out and I'll have 10 000 values which correspond to the pixel values.\nright so let's assume it's only a gray and white images right so you will have certain pixel values there.\nand this is uh uh 100 dimensional vector and based on this you might want to predict a certain y.\nright so this is again of all of these are cases where your inputs are real and your outputs are.\nalso here and you have a high dimensional input where I'm just calling it as RN which is an N.\ndimensional Vector right so this is the situation that we want to uh deal with now given this situation or.\ngiven these kinds of problems what is our Quest right our Quest would be just as it is on the.\ncase of perceptrons when we are dealing with or in the last lecture when we are dealing with Boolean functions.", "metadata": {"video_title": "Sigmoid Neuron"}}
{"text": "case of perceptrons when we are dealing with or in the last lecture when we are dealing with Boolean functions.\nwe wanted to know if we can have a network which can exactly represent such function right and my definition.\nof exactly represent was very clear in the case of Boolean functions where I was saying that I know what.\nthe truth table of this function is and then when I take an input say 0 comma 0 right and.\nI pass it through the network I should get the same output as is decided by this truth table right.\nthat was my definition of exactly representing the function and then we also moved on to slightly more real worldish.\nwhere we said that I would have been given data where I would have different inputs right and the output.\nfor that input right and if this data was linearly separable then I should have a network such that it.\ntakes any input from this training data and it gives me the same output as is given to me in.", "metadata": {"video_title": "Sigmoid Neuron"}}
{"text": "for that input right and if this data was linearly separable then I should have a network such that it.\ntakes any input from this training data and it gives me the same output as is given to me in.\nthe training date right in the training data I'm given these X comma y pairs right and then I have.\na network I pass this x through the network and it predicts a y I'll just call it y hat.\nso I wanted the Y hat and the Y to be same and that's what it means by this network.\ncan exactly represent the function right now I'm just losing that definition a bit I am saying that can I.\nhave a network which can approximately represent the function right which means what that I want y hat to be.\napproximately equal to y y this approximately y naught exact we'll come to that later all right for now if.\nyou want to assume I want something which is exactly equal that's also fine right but I'm just going to.", "metadata": {"video_title": "Sigmoid Neuron"}}
{"text": "you want to assume I want something which is exactly equal that's also fine right but I'm just going to.\nbe a bit more relaxed in this uh lecture right I would just say approximately is also fine right so.\nthat is what I want can I construct a network such that when I give this real valued inputs right.\nand I get a prediction from the network that should be as close to what is given to me in.\nthe training data right that's what my quest is going to be okay and just to make it more concrete.\nso in the in the oil mining example there are already many uh locations where drilling stations have been set.\nup and oil is being mined for many years right so I know what was the quantity of oil mine.\nfrom those stations and I know what their X characteristics were right so what did the salinity of salinity what.\nwas the pressure what was the density what was the temperature what was the marine diversity all I know right.", "metadata": {"video_title": "Sigmoid Neuron"}}
{"text": "was the pressure what was the density what was the temperature what was the marine diversity all I know right.\nso what I want is a network such that when I feed any of these X's to the network the.\nY predicted by the network should be very close to the Y that I actually have right so that is.\nwhat our Quest is going to be I'm just defining the problem I'm not telling you a solution at all.\nright I'm just just about what the story ahead is going to be okay so let's move ahead from here.\nuh I better delete a few of these things so before answering the above question right we'll first have to.\ngraduate from perceptrons to sigmoid neurons right so that's the first thing that we're going to do so uh recall.\nthat when we are dealing with perceptrons a perceptrons fires when the weighted sum of its input is greater than.\nthe threshold right unless we had this thing that Theta is equal to minus W naught right because we move.", "metadata": {"video_title": "Sigmoid Neuron"}}
{"text": "the threshold right unless we had this thing that Theta is equal to minus W naught right because we move.\nit to the other side of the equation okay so this is what uh perceptron like a single input perceptron.\nlooks like I have an input I have a certain weight associated with the input I have the output and.\nthen I have the weight uh I have the bias or the threshold which is say uh uh the minus.\nof the threshold is 0.5 so minus of w naught is 5.5 right and now this uh neuron this perceptron.\nwill fire if the weighted sum of my input which is just W and X1 in this case is greater.\nthan 0.5 now I'm going to say that this logic used by a perceptron is actually very harsh right now.\nwhat do I mean by harsh harsh is not like a mathematical term so what do I mean by that.\nright so let us return to our example of deciding whether we would like to like or dislike a movie.\nor whether uh yeah whether you like or dislike a movie right now let's say we have basing our decisions.", "metadata": {"video_title": "Sigmoid Neuron"}}
{"text": "or whether uh yeah whether you like or dislike a movie right now let's say we have basing our decisions.\nonly on one input which is the critics rating and the critics rating is something between zero to one right.\non a scale of zero to one a Critic has given some rating and we look at that and decide.\nwhether uh we want to watch the movie or not right now if the threshold is 0.5 and let's assume.\nthat the weight is one because there's only one input so it doesn't matter what the weight is so it.\nwill just give it one and uh what now in this case when the threshold is 0.5 and the weight.\nis one so what about the case when the critics rating was 0.49 the output of the network would be.\ndislike right because 0.49 is less than the threshold so I will not like the movie but now if I.\nhad a movie well it's a Critic rating was 0.51 then my output would be like right and this is.\nwhy I say that this is very harsh right this is not how you would take decisions in real life.", "metadata": {"video_title": "Sigmoid Neuron"}}
{"text": "had a movie well it's a Critic rating was 0.51 then my output would be like right and this is.\nwhy I say that this is very harsh right this is not how you would take decisions in real life.\nthat 0.49 is not like but 0.51 is like there is a very little difference between in these two right.\nwhen you wouldn't base your decisions like that what you're looking for is something more smoother than this right which.\ncalibrates that okay 0.49 is uh good 0.5 minus slightly better right it should have that calibration right so that.\nis missing it's just very harsh that and I'll show you a figure on the next slide which will make.\nthis very clear right and that's the point I want to make that is not a characteristic of the problem.\nthat you have chosen it's not that about movie ratings or it's not about the way I have chosen this.\nthreshold or the examples that I have given it's the nature of the function itself right and I'll tell you.", "metadata": {"video_title": "Sigmoid Neuron"}}
{"text": "threshold or the examples that I have given it's the nature of the function itself right and I'll tell you.\nwhat I mean by that so this is what the perceptron function looks like so uh what I have here.\nthis is my z-axis okay so on this axis I have Z where Z is the weighted sum of the.\ninputs okay so this quantity I'm calling Z which you have seen is the weighted sum of the inputs okay.\nand this is my threshold and I know that when Z crosses the threshold my output is going to be.\n1 and when Z is less than the threshold my output is going to be 0. so there is going.\nto be this sharp increase at 0.5 and that's what was happening in our example still 0.49 I was not.\nso upbeat about the movie the moment it cost 0.5 I said okay like this movie right so this is.\nthe nature of the function itself where if you take this weighted sum and it crosses a threshold you say.\none if not you say zeros this if else nature is what is bringing this discreetness right we're just saying.", "metadata": {"video_title": "Sigmoid Neuron"}}
{"text": "one if not you say zeros this if else nature is what is bringing this discreetness right we're just saying.\nthat okay it will suddenly become one right so it's the nature of the function itself this is a step.\nfunction right and there'll always be the sudden change whenever you have a set a threshold function right and as.\nI said like for most real world applications we don't want this we want the decision to be much smoother.\nwhere the difference between 0.49 and 0.51 is not yes and no right it should be something more smoother than.\nthat right so let's see how do we get there now this looks like a smoother version of the function.\nright so what is happening here again this is my z-axis okay and this is what Z is right and.\nnow I have not explained what this function is I have not given you the formula for the perceptron I.\nhave given you a formula that Y is equal to 1 if something Y is equal to zero something I.", "metadata": {"video_title": "Sigmoid Neuron"}}
{"text": "now I have not explained what this function is I have not given you the formula for the perceptron I.\nhave given you a formula that Y is equal to 1 if something Y is equal to zero something I.\nhave not given your formula but for now we'll just appreciate that this looks more realistic right that it's not.\nsuddenly changing its decision it's gradually moving from zero to one right it's not like query uh sharp as was.\nin the case of the earlier function right so to get this kind of an effect we'll now introduce what.\nis known as the sigmoid neuron and sigma 8 function the function Sigma is like a family of functions and.\nhere's one member from that family which is called the logistic function later on in the course we'll see the.\ntan H function which is also a member of Islam right and let's see what is happening here so this.\nis this is a quantity which is familiar to us right this is the weighted sum of the inputs and.", "metadata": {"video_title": "Sigmoid Neuron"}}
{"text": "tan H function which is also a member of Islam right and let's see what is happening here so this.\nis this is a quantity which is familiar to us right this is the weighted sum of the inputs and.\nthe threshold okay this quantity is familiar to us and we have been writing it as summation I equal to.\n0 to n W naught X naught right it is the same quantity that we have been writing and this.\nquantity we have actually been writing as W transpose X if you remember right where X is the vector from.\nX 0 to X N and W is a vector from W 0 to WN right so this is just.\nthe dot product right so the quantity that I have underlined is just this quantity that we have been dealing.\nwith so far okay now let's look at this function a bit carefully so what happens when w transpose X.\ntends to Infinity okay so this is as I go along this axis this is where W transpose X will.\nstart trending to Infinity rate because Z is what I've defined as double aspects X right so what what would.", "metadata": {"video_title": "Sigmoid Neuron"}}
{"text": "start trending to Infinity rate because Z is what I've defined as double aspects X right so what what would.\nhappen as it goes towards Infinity if I substitute the value Infinity here right so it is 1 over 1.\nplus e raised to minus infinity right that would be 1 over 1 plus 1 raised to e raised to.\n1 1 by E raised to Infinity right should become 1 over 1 plus 1 over e raised to Infinity.\nright that's what this would be and this term would become 0 so you'll just have 1 over 1 so.\nthat would be one and that's exactly what is happening here as the value of this W transpose X is.\nincreasing the function is approaching 1 right and uh what would happen if W transpose X tends to minus infinity.\nthis would become 1 over 1 plus infinity right so in that case the denominator would become Infinity so 1.\nover infinity would be 0 and so as I go in this direction so as I go in uh this.", "metadata": {"video_title": "Sigmoid Neuron"}}
{"text": "this would become 1 over 1 plus infinity right so in that case the denominator would become Infinity so 1.\nover infinity would be 0 and so as I go in this direction so as I go in uh this.\ndirection which is minus infinity my function starts tending to zero right so that's why this behavior is there and.\nnow again let's just do one more thing if W transpose X equal to 0 if I substitute that value.\nthen I get 1 over 1 plus e raised to 0. right and that would be half right so that's.\nwhat you are seeing here okay yeah so that's that's what is happening here uh so for this discussion just.\nlet's assume that W 0 was 0 right so that this uh so that all of this is in sync.\nright so that this is this disappears and all of this Falls in sync if you have a w naught.\nyou can have a similar explanation it's just that the graph will move a bit right so I'll leave it.\nfor you to figure that out but the what is the important thing is how do you get this s.", "metadata": {"video_title": "Sigmoid Neuron"}}
{"text": "you can have a similar explanation it's just that the graph will move a bit right so I'll leave it.\nfor you to figure that out but the what is the important thing is how do you get this s.\nshape at large values it saturates at small values it saturates and it goes smoothly from the small values to.\nthe large values right so that's what is happening here and we no longer see this sharp transition which we.\nare seeing in the uh step function earlier or the perceptron function right let's just delete this also the output.\nY is no longer binary right but it's a real valued output it's not zero or one many values between.\n0 and 1 are possible now the other interesting thing about this function is that since the values are between.\n0 and 1 right so what is the other quantity of Interest which lies between 0 to 1. probability right.\nso probabilities lie between 0 to 1. so now this output of the sigmoid function we could interpret as a.", "metadata": {"video_title": "Sigmoid Neuron"}}
{"text": "so probabilities lie between 0 to 1. so now this output of the sigmoid function we could interpret as a.\nprobability so now when it gives the value 0.49 I can say that there's a 49 chance that I like.\nthe movie earlier I was saying that there is zero chance I not like the movie when it outputs a.\nvalue of 0.51 I can say there's a 51 so one person chance that I like the movie and now.\nthis makes sense right the difference between 49 to 51 percent makes sense right earlier it was 0 and 1.\nwhen it was 49 and 15 so that harshness has disappeared now and more importantly I can now interpret these.\nvalues as probability and this is something very important because in most modeling problems that is what we would want.\nto do that you have to predict a certain binary value and you would like to predict it as a.\nprobability that this not harsh zero and one but this is a 49 chance of finding oil there there's a.", "metadata": {"video_title": "Sigmoid Neuron"}}
{"text": "to do that you have to predict a certain binary value and you would like to predict it as a.\nprobability that this not harsh zero and one but this is a 49 chance of finding oil there there's a.\n49 chance that this person will not return the rule and then as a domain expert you can decide whether.\nyou want to invest in that or not right okay okay so that's that's what it is and this is.\nhow the two functions look like very similar right the inputs are actually similar as you can see the weights.\nare also available you have an output you know that these are all real values you know that the output.\nis real value right in fact in the perceptron also you could have had real values it's just that we.\nwere only looking at Boolean but the main difference is in this function here right you have the sigmoid function.\nhere and you have the step function there and that step function is obvious because of the formula that we.", "metadata": {"video_title": "Sigmoid Neuron"}}
{"text": "here and you have the step function there and that step function is obvious because of the formula that we.\nwere using which is this Y is equal to one if something Y is equal to 0 if something zero.\nthen go to one right whereas now you have this smoother formula which we just saw on the previous uh.\nslide uh which is just one over one plus exponent of something right and that gives you this smooth function.\nright and even if you look at the plots then you see that the uh perceptron function is not smooth.\nand hence not differentiable at this sharp change at the value where it changes whereas the uh sigmoid function is.\nsmooth it is continuous and its differential so now why do we care about continuity why do we care about.\ndifferentiability so for a large part of this course calculus is going to be the hero right so whatever algorithms.\nthat we study today and going forward in the next few lectures everywhere calculus would play an important role in.", "metadata": {"video_title": "Sigmoid Neuron"}}
{"text": "that we study today and going forward in the next few lectures everywhere calculus would play an important role in.\nparticular derivatives would play an important role and hence it's important that the functions that we deal with are differentiable.\nright so that's where I leave it for now I'll not delve further into it and you'll get to know.\nwhy differentiability is important because I'll soon start taking derivatives of something right and if I'm taking derivatives of something.\nin everything in the calculation is not differentiable of all the functions in world are not differentiable then you can't.\ntake the derivatives right so that's why it's important to have differentiable functions and that's why we have it's good.\nto have the sigmoid Neutron okay.", "metadata": {"video_title": "Sigmoid Neuron"}}
{"text": "foreign [Music] okay so now let's talk about stochastic and mini batch gradient descent let me first motivate this with.\nsome intuition and then we will look at how to go about it right so we'll dig us a bit.\nuh actually after nag there are a few other algorithms that we should cover but before that since we are.\nstill in a slightly easier territory gradient descent momentum nag those are easier to understand so in this easier territory.\nI'll talk about the stochastic version of these algorithms and then later on I'll talk about other algorithms like Adam.\nadagrad at adult and so on okay yeah so if you look at the code for uh gradient descent right.\nand the same code I mean with some variations was used for momentum as well as nag then what you're.\ndoing here is that for all the training points so this for Loop is looking at all the training data.\nin this case I had only two training samples but the for Loop is going over all the training samples.", "metadata": {"video_title": "Stochastic vs Batch Gradient"}}
{"text": "in this case I had only two training samples but the for Loop is going over all the training samples.\nComputing the gradient keeps on accumulating it in the sum DW and DB and then comes out of the loop.\nand makes one update right so that's one observation that I want you to make it's obvious from the code.\nthat it's looking over the entire data and then comes out of the loop and makes one update uh and.\nthen keeps going about it right so there are like two for Loops so the inner for Loop runs over.\nthe entire training data okay now the issue with that is that the question is like why are we doing.\nthis right so why are we going over the entire training Data before making one update to the weight parameters.\nright so the reason for that is that because this is a true gradient right we had painfully computed that.\nwhen we were doing that toy Network that to compute the loss function so the loss function is actually a.", "metadata": {"video_title": "Stochastic vs Batch Gradient"}}
{"text": "when we were doing that toy Network that to compute the loss function so the loss function is actually a.\nsum over all the training examples and then whatever formula you use right squared error or cross entropy and then.\nyou take the average of that right so the true value of the loss is this right and it includes.\nall the training examples now if I want to take the derivative of this with respect to the loss function.\nthen the derivative would also be a sum of all the values so it's also going to have a sum.\nover all the N terms and that is exactly what this for Loop is doing here right so this is.\nexactly the formula you just applied the formula and the formula says that you should sum over all the variables.\nand this is the true gradient there's no approximation happening here right so what you're doing is correct but and.\nbecause what you're doing is correct all the theoretical guarantees hold so what does that mean so when we are.", "metadata": {"video_title": "Stochastic vs Batch Gradient"}}
{"text": "because what you're doing is correct all the theoretical guarantees hold so what does that mean so when we are.\nderived gradient descent using the Taylor series approximation we had said that at every step the loss will keep decrease.\nright and that guarantee holds because you are not doing any approximation you have computed the true gradient or the.\ntrue partial derivative and you are moving the direction opposite to this true partial derivative right and it will become.\nK what I mean by true and not true right what true means here is that there is no approximation.\nin the formula whatever formula you derived you're using exactly the same formula hence all theoretical guarantees hold so that's.\nthe good part but what's the flip side what's the bad part the bad part is that suppose you have.\na million points in the training data right then you'll run this for Loop for all the million points okay.", "metadata": {"video_title": "Stochastic vs Batch Gradient"}}
{"text": "a million points in the training data right then you'll run this for Loop for all the million points okay.\nyou will painfully compute this derivatives then come out and make one update right so now if you have to.\nrun the algorithm for 100 steps which is quite small right in terms of modern deep learning then you would.\nbe making like 100 million computations and making 100 steps and 100 steps your weights wouldn't have actually moved much.\nright so you would be nowhere close to conversions and you have done so many computations right so that's the.\nflip side of this to make one update you have to do so many calculations and obviously this is going.\nto be very slow so the question is can we do something better and the answer is that we can.\ndo what is known as stochastic gradient descent so as opposed to uh the Computing the true gradients can we.\njust estimate the gradients using fewer points instead of looking at all the endpoints so that's the idea I will.", "metadata": {"video_title": "Stochastic vs Batch Gradient"}}
{"text": "just estimate the gradients using fewer points instead of looking at all the endpoints so that's the idea I will.\ntalk more about it right so let's not look at the code for now so so now in this stochastic.\nversion of the algor I'm sorry we have to look at the code so now in the stochastic version of.\nthe algorithm what I've done is just a small change right so this this part of the code has been.\nindented and bought into the loop right so what is happening is now for all points in the training data.\nI'm Computing the derivatives and immediately updating the weights right so I'm doing a greedy update so what I'm saying.\nis that my true derivative was actually the average 1 by n into the summation over the derivative for all.\nthe points right so that's what my true derivative was now instead of calculating this sum I'm saying I'll just.\nlook at one point and I'm just making an approximation that the average is as good as one point right.", "metadata": {"video_title": "Stochastic vs Batch Gradient"}}
{"text": "look at one point and I'm just making an approximation that the average is as good as one point right.\nwhatever estimates I make from one point are actually as good as the average that I'll get from a million.\npoints so that's of course a bad approximation but you do that and while it seems bad we'll see under.\nwhat what you could do to not make it so look so bad right foreign so now if you have.\na million data points we'll make a million updates in each Epoch what is one Epoch Epoch is the outer.\nloop right when you are making updates so far uh every Epoch you are going over the entire data and.\nfor every data point you're making an update so if there are million data points then in one Epoch you're.\nmaking one million updates right so one Epoch is one pass over the entire data and one step is equal.\nto one update right now what's the flip side now we have gone into bad territory right you know what.", "metadata": {"video_title": "Stochastic vs Batch Gradient"}}
{"text": "to one update right now what's the flip side now we have gone into bad territory right you know what.\nyou're Computing is an approximate gradient right your true gradient was the average of the derivative computed over all the.\npoints now we are approximated that average by using just one point estimate right and that's definitely bad so you.\nhave an approximate gradient so hence uh the guarantees that you had may be off right because this is called.\nstochastic because we are estimating the total gradient based on a single data point so this is like you asked.\nme to estimate the probability of suppose I give you a biased coin and you ask me to estimate the.\nprobability of heads then you'll just toss it once and you get heads and you say Okay probability of heads.\nis one as opposed to like really tossing it's a hundred or thousand or even more times and then trying.", "metadata": {"video_title": "Stochastic vs Batch Gradient"}}
{"text": "is one as opposed to like really tossing it's a hundred or thousand or even more times and then trying.\nto estimate the probability right so that's what you're doing you're almost like tossing the coin once and estimating the.\nprobability of heads because you're just taking one data point and estimating the derivative from that point instead of computing.\nthe average over a large number of data points right and as you can imagine right as what we do.\nin the case of coin toss also we don't really do like we don't really need to do like a.\nmillion coin tosses that's bad you won't be able to do that similarly just doing one coin toss is also.\nbad because you cannot estimate the probability of Heads Best by doing one coin toss but somewhere in between maybe.\n100 coin tosses would have been okay right and you could get a fair estimate of what the probability of.\nheads is so a thousand may have been okay right similarly what you could do is that waiting for all.", "metadata": {"video_title": "Stochastic vs Batch Gradient"}}
{"text": "heads is so a thousand may have been okay right similarly what you could do is that waiting for all.\nthe million data points to get over before you make update is bad making an update after one data point.\nis also bad but some there in between maybe making an update after looking at 100 data points right so.\ninstead of computing the average as 1 by n summation I equal to 1 by n and then the loss.\nvalue right instead of that maybe instead of using n equal to 1 million or all the training points I.\ncould use n equal to 100 and that might give me a fair approximation right so that's the idea that.\nwe'll head towards so while one approximating from one point really looks bad we need not do that that's only.\nfor illustrating uh the concept right so I'll do that and once you do that whether you approximate from one.\npoint ten point or 100 points as long as you're not using all the data points you are in trouble.", "metadata": {"video_title": "Stochastic vs Batch Gradient"}}
{"text": "point ten point or 100 points as long as you're not using all the data points you are in trouble.\nright because you are not really doing a true estimate of the gradient but you have to make this trade.\noff between what is true which is all the multimedial points taken together versus what is approximate and let's see.\nwhat happens when you do this approximate right so uh now what I'm going to do is I'm going to.\nrun stochastic gradient descent and gradient descent on this loss surface and we'll make some observations about it yeah so.\nthe green guy is gradient descent and the black guy is stochastic gradient descent and as you can see in.\nthe black guy there are a few oscillations whereas the green guy is going finer it's smoothly going down whereas.\nthe black guy is often getting off course and then coming back right and the reason that is happening is.\nbecause you are not making a true estimate of the gradient you're just making like a approximation or a stochastic.", "metadata": {"video_title": "Stochastic vs Batch Gradient"}}
{"text": "because you are not making a true estimate of the gradient you're just making like a approximation or a stochastic.\nupdate here well and at this point right where it was say oscillating right at the points where you see.\nan oscillation let me just use the mouse right so this is one point where you see an oscillation and.\nwhat is happening there is that you approximated the gradient by one point and that point was actually very far.\noff as compared to what the true gradient would have been and hence it led to a bad update and.\nnow again you come back slowly and then you go back towards the minimal right so that's why you see.\nthese oscillations now uh this is when you use k equal to 1 right but uh and we understand why.\nwe see many oscillations because we are making greedy decisions and what is happening is here that each point is.\ntrying to push the parameter so I have a million data points right and I look at one of the.", "metadata": {"video_title": "Stochastic vs Batch Gradient"}}
{"text": "trying to push the parameter so I have a million data points right and I look at one of the.\ntraining uh instances one of the points and I compute the gradient and this point says Hey to decrease the.\nloss with respect to uh my values you need to move here right then the second Point comes I again.\nAsk it hey what should the gradient would be and it again pushes me in Direction which is favorable to.\nit so each point is acting independently and trying to move you in a direction which is most convenient for.\nitself right and that's why you see these all oscillations they're not working together as opposed to in the gradient.\ndescent where all of them I'm asking all of them at one go taking the average of their consensus and.\nthen moving uh in that direction right or the opposite to that the direction of the gradient but here that.\nis not what I am doing and I'm asking every point this point say hey go here go here go.", "metadata": {"video_title": "Stochastic vs Batch Gradient"}}
{"text": "then moving uh in that direction right or the opposite to that the direction of the gradient but here that.\nis not what I am doing and I'm asking every point this point say hey go here go here go.\nhere go here and so on that's why I'm oscillating uh that's why you see the oscillations right uh and.\nas I mean as the case and a parameter which is a favorable for one point may not be a.\nfavorable for the next points that's why this point moved you here and then the next Point said no no.\nno come back here then again go back here go back here and so on and that's why you keep.\noscillating like that right now can we reduce the oscillations by improving our stochastic estimates right so currently our estimate.\nis taken from one point right we have taken n equal to one now instead of one if I take.\n10 if I take 20 then would these oscillations reduce right what this uh I would would I please take.\ncloser to the green curve right I know in the limit if I take n equal to all the points.", "metadata": {"video_title": "Stochastic vs Batch Gradient"}}
{"text": "10 if I take 20 then would these oscillations reduce right what this uh I would would I please take.\ncloser to the green curve right I know in the limit if I take n equal to all the points.\nthen I would be exactly following the green curve right because and there's no surprise there right but if I.\nlook at a mini batch as opposed to the full data and as opposed to just one point if I.\nlook at a mini Point mini batch which is say 25 points 100 points then what my estimates be better.\nright so that's the question uh so let's look at that uh so now we are using a mini batch.\nversion of gradient descent so in stochastic gradient descent you are updating after every point right whereas here you are.\nkeeping track of the number of points that you have seen and if the number of points is equal to.\nsome mini batch size let's say the mini batch size is 100 or 200 so if the number of points.\nthat you have seen so far if it's equal to some mini batch size then you make an update right.", "metadata": {"video_title": "Stochastic vs Batch Gradient"}}
{"text": "some mini batch size let's say the mini batch size is 100 or 200 so if the number of points.\nthat you have seen so far if it's equal to some mini batch size then you make an update right.\nso after every uh batch of say 100 points 200 points only then you make update till that point you.\nkeep accumulating the uh gradients right so instead of accumulating the gradients for all the points you're accumulating the gradients.\nfor say 100 points and then making an update right so that's the only difference in the uh code and.\nnow these stochastic estimates are better because now instead of uh asking instead of flipping the coin once and then.\nestimating the probability of heads maybe you are flipping it 25 times or 50 times and then trying to estimate.\nthe probability offense right so let's see what happens when we have k equal to 25 and we'll compare k.\nequal to 25 versus k equal to 1 right so let's see that okay so we are going to run.", "metadata": {"video_title": "Stochastic vs Batch Gradient"}}
{"text": "the probability offense right so let's see what happens when we have k equal to 25 and we'll compare k.\nequal to 25 versus k equal to 1 right so let's see that okay so we are going to run.\ngradient descent stochastic gradient descent and mini match gradient descent where the batch size was 25 and what I would.\nlike to see is that the mini batch gradient descent is somewhere in between that green curve and the black.\ncurve right in terms of movement and you'll understand what I mean by that so let's start this so you.\ncan see that the blue curve is very close to the green curve whereas the black curve is oscillating quite.\na bit more right of course the blue curve uh which is for mini batch also has oscillations is just.\nthat it's much more smoother than the black guy because the black guy relies on one point and makes an.\nestimate whereas the blue guy is relying on 25 different points and making an estimate okay yeah so the oscillations.", "metadata": {"video_title": "Stochastic vs Batch Gradient"}}
{"text": "estimate whereas the blue guy is relying on 25 different points and making an estimate okay yeah so the oscillations.\nhave reduced to a good extent uh because we now have slightly better estimates and as I said right it's.\nlike estimating the property heads from uh 25 coin tosses as opposed to a single coin toss right and the.\nhigher the value of K the more accurate the estimates are uh there are still oscillations right there's no denying.\nthat it's just that it becomes more and more smooth as you increase the batch size and I again repeat.\nin the limit when you make the batch size equal to the total training points that will just follow gradient.\ndescent but that is not what is desired we'll typically want much smaller batch sizes as compared to the size.\nof the data that we have and also I should mention at this point that uh many of the modern.\nuh algorithms right which use batch updates they are often quite sensitive to this batch size you know when experimenting.", "metadata": {"video_title": "Stochastic vs Batch Gradient"}}
{"text": "uh algorithms right which use batch updates they are often quite sensitive to this batch size you know when experimenting.\nwith Transformers for example we've often seen that a batch size of thousand versus a batch size of 4000 1024.\nthese bat sizes are often in multiples of two or powers of two a batch size of 1024 may give.\nyou very different results from a batch size of four zero nine six right and it's common wisdom is that.\nlarger batch sizes are better right so that's uh while not of course going to the full training data okay.\nso that's where we are and some things to remember so one Epoch is one pass over the entire data.\none step is One update to the parameters n is the total number of data points that you have B.\nis the mini batch size right so now in vanilla uh gradient descent which is also called batch gradient descent.\nso number of steps in one Epoch is just one right because after uh in one Epoch you just keep.", "metadata": {"video_title": "Stochastic vs Batch Gradient"}}
{"text": "so number of steps in one Epoch is just one right because after uh in one Epoch you just keep.\naccumulating all the gradients and then update once in stochastic gradient descent the number of steps in one Epoch is.\nequal to n right so because you are making an update for every data point and mini batch gradient is.\nsaid the number of steps in one Epoch is n by B which is the total number of data points.\ndivided by your batch size so if you have thousand data points and hundred batch size then after every 100.\npoints you'll be making an update so you'll be making a total of 10 updates right so that's what you.\nshould remember okay uh similarly we can have the stochastic versions of momentum based gradient descent and nest of accelerated.\ngradient the same idea applies that you for stochastic momentum based gradient descent you'll just indent the update uh the.\nlines of code corresponding to update because after every data point you will make the update right and similarly for.", "metadata": {"video_title": "Stochastic vs Batch Gradient"}}
{"text": "lines of code corresponding to update because after every data point you will make the update right and similarly for.\nnestro which I'll not show you the code for that I think it's straightforward so now let's look at uh.\non this slide I think this is written as gradient descent and SGD but this is the momentum based versions.\nof both the stochastic momentum based gradient descent and the vanilla momentum based gradient descent okay so let's see how.\nit goes so you can see that in addition to the oscillations that you have right and this was quite.\na complex uh surface right because it had some steep slopes remember from the Contour discussion that when you see.\nthe lines are very close to each other the slope is a bit Steep and we know that there is.\na problem in steep slopes so the green curve was going here and there because uh not is the true.\nmomentum based gradient descendant so you are taking the derivative or you're taking Computing the gradient from all the points.", "metadata": {"video_title": "Stochastic vs Batch Gradient"}}
{"text": "momentum based gradient descendant so you are taking the derivative or you're taking Computing the gradient from all the points.\nbut the reason you still see oscillations is because you have this very steep slopes and you are moving fast.\nso you're over overshooting and then trying to come back whereas the black curve has these oscillations uh because of.\nthe stochastic nature of the estimates right so you're not making perfect estimates so you are going here and there.\nand then coming back right and also see that they diverse right initially they both Go in different directions and.\nthe reason for that is that maybe the first few points that you selected in the stochastic version they were.\nnot a true representation of the total gradient right so they said okay I need to go in this direction.\nand you went in that direction and then you later had to course correct and come back whereas when you're.", "metadata": {"video_title": "Stochastic vs Batch Gradient"}}
{"text": "and you went in that direction and then you later had to course correct and come back whereas when you're.\nlooking at the full uh gradient you're always moving in the right direction that you need to go to but.\nof course you still see oscillations that is there because of the momentum related oscillations you're moving too fast and.\nhence you have to oscillate in the green curve right okay okay so now let's look at uh nag also.\nokay so stochastic version of nag so now I have all three I have nestorov right and you can see.\nthe expected Behavior it's all the three algorithms are the stochastic versions of the algorithms therefore even the vanilla gradient.\ndescent you still see oscillations there those oscillations are because of the stochastic nature of the updates in momentum as.\nwell as nestrov you see some oscillations as we are going again ahead again because of the stochastic version of.", "metadata": {"video_title": "Stochastic vs Batch Gradient"}}
{"text": "well as nestrov you see some oscillations as we are going again ahead again because of the stochastic version of.\nthe algorithms but the relative strengths remain the same right so you can see that the moment of curve which.\nis green it takes a longer U-turn as compared to the Maestra one which is blue it quickly comes back.\nright and that is the expected behavior of nestra versus momentum right so the relative qualities remain the same in.\naddition you see this stochastic related uh oscillations because your estimates are not perfect right so that's the only difference.\nthat you have and I'll make the same commentary on the next like yeah so while the stochastic versions uh.\nretain their relative advantages of Nag over momentum uh there is still this oscillation Behavior which is there and both.\nof them converge much faster than the stochastic gradient descent right so after 500 steps you can see that the.", "metadata": {"video_title": "Stochastic vs Batch Gradient"}}
{"text": "of them converge much faster than the stochastic gradient descent right so after 500 steps you can see that the.\nblack guy is still somewhere on the plateau it has not even entered the valley whereas of both of these.\nalgorithms have conversed despite the stochastic nature of the algorithm and despite their own problems with oscillations they're still much.\nfaster than grain gradient descent which is again nothing new this is what is expected uh where we have discussed.\nthis during uh while discussing the vanilla versions of these algorithms as opposed to the stochastic or not the vanilla.\nthe full batch version of these algorithms as opposed to the stochastic versions okay and of course you could also.\nhave the mini batch versions of momentum and nag so what I showed on the previous slide was uh stochastic.\nbut you can also have mini batch the same thing after a batch you compute the uh derivatives and then.", "metadata": {"video_title": "Stochastic vs Batch Gradient"}}
{"text": "but you can also have mini batch the same thing after a batch you compute the uh derivatives and then.\nmake the update uh and again you will see the same relative advantage that the oscillations due to the stochastic.\nnature would reduce because now your estimates are getting better they are not coming from a single point but from.\na collection of points so that ends our discussion on the stochastic and the mini batch version of these algorithms.\nand the same ideas will apply to all the optimization algorithms that you see and in practice we use the.\nmini batch versions of all these algorithms because you will have a large number of training points so you'll have.\nto use a batch and make updates instead of waiting for all the data to be seen and similarly you'll.\nnot use the stochastic version which is just like one data point so you'll use the mini batch version and.\nthis mini batch size would be a bit dependent on the application that you're using and as I said in.", "metadata": {"video_title": "Stochastic vs Batch Gradient"}}
{"text": "this mini batch size would be a bit dependent on the application that you're using and as I said in.\nNLP when you're using Transformers some popular batch sizes are force uh around 4096 but it again there's nothing to.\nmemorize here or tell you this is a good value as you read papers and look at existing implementations of.\nuh these algorithms you will understand what the right batch size is to use is all I'm saying is that.\nin practice you'll use the mini batch version of these algorithms and there would be slight sensitivity to the bats.\nsize okay yeah so now again before going to uh some of the other algorithms which I want to cover.\nI'll talk a bit about adjusting the learning rate and the moment right uh so uh so let's look at.\nwhat the problem is.", "metadata": {"video_title": "Stochastic vs Batch Gradient"}}
{"text": "foreign [Music] so one thing that people have shown right is and particular this paper which uses skip connections as.\na regularizer and I'll talk about that in a bit that the effect of regularization is to smoothen the Lost.\nlandscape right so typical uh and you can go on to this website right which allows you to explore typical.\nlost Landscapes that you encounter in neural networks and you can see that this is very bumpy right it's not.\nnowhere closer to the very nice loss functions that we have been dealing with which have like a single plateau.\nand then a valley and so on it there are like multiple plateaus rallies here and there are some very.\nsharp slopes here right and what happens is that if you have flat surfaces right and let me show you.\na plot it will become clear right so if you have a flat surface you will get better regularization why.\nwhy is that the case because if you have a flat surface then for a large range you have the.", "metadata": {"video_title": "Summary"}}
{"text": "a plot it will become clear right so if you have a flat surface you will get better regularization why.\nwhy is that the case because if you have a flat surface then for a large range you have the.\nloss as the same right and now your if you cannot over fit like I mean in the sense that.\nyou have figured out a value of the loss you know that for a large neighborhood any of the W's.\nwould have given you a similar loss right but now in this plot where you have these bumps up and.\ndown if you had over fit and selected gone into one Valley around that Valley there are many other Hills.\nright so with a small change you get a very large change in the loss so your model becomes very.\nsensitive right so one effect of regularization that people have shown is that it smoothens the Lost surface right so.\nthey have analyzed the surface of lost surface of neural networks and seen that with regularization and this was in.", "metadata": {"video_title": "Summary"}}
{"text": "they have analyzed the surface of lost surface of neural networks and seen that with regularization and this was in.\nthe context of skip connection being used as a regularization we have not done skipped connections yet we'll do that.\nin the case of convolutional neural networks but they also act as some sort of regularizer and they show that.\nthe loss surface actually smooth inside so that leads to better optimization so now coming to the taxonomy of things.\nright what are the regulation techniques that you have used so we had this loss function which was the empirical.\ntraining error and we added different regularizations to it so one is based on data so we added uh we.\nlooked at data augmentation where we had different rotations of the same image that we had added similarly in speech.\nand text also you can do data augmentation and you also did this noise injection where at the output where.", "metadata": {"video_title": "Summary"}}
{"text": "and text also you can do data augmentation and you also did this noise injection where at the output where.\nthe input we were adding some noise to the inputs right so both at the input and output we had.\nmanipulated the data a bit so that you add some noise and now the model cannot overfit because your data.\nitself has some noise right the other is you could change the architecture right so there we saw Dropout is.\none such change then skip connections which are used in convolutional neural networks is another such thing and weight sharing.\nright which we just briefly touched upon we again looked at it in the case of Dropout but weight sharing.\nis again something which is used in convolutional neural networks and it acts as a good regularizer right and then.\nagain pooling which is used in the context of convolutional neural networks all of these three I am mentioning here.\nfor the sake of completeness but we'll see them in the context of cnns and that that time it will.", "metadata": {"video_title": "Summary"}}
{"text": "for the sake of completeness but we'll see them in the context of cnns and that that time it will.\nbecome clear that these act as regularizers because they help in reducing the number of parameters they help in reducing.\nthe model complexity right at least these two here okay uh then uh it has also been shown that the.\noptimization process itself can act as a regularizer right so some papers have shown that gradient descent has prefers less.\ncomplex Solutions as compared to more complex Solutions and that is a bit more nuanced statement and it's a discussion.\nin itself so I'll not go into the details of it but there's also some kind of implicit regularization happens.\nbecause of these gradient descent based methods which prefer simpler Solutions as opposed to more complex Solutions then early stopping.\nis again a kind of Regulation that we have discussed and then we have looked at penalizing the law cost.", "metadata": {"video_title": "Summary"}}
{"text": "is again a kind of Regulation that we have discussed and then we have looked at penalizing the law cost.\nitself right which is adding this Omega Theta term that we have seen L well regulation is something you can.\nuse we have studied L2 but similarly you could use L1 regularization right so this is one way of grouping.\nthe regularization techniques the other way of grouping them is into explicit regularization and implicit regularization so we have looked.\nat mainly looked at explicit regularization like L2 regulation data augmentation all of these we have looked at and these.\ntwo we will look at when we look at convolutional neural networks in the case of implicit regularization we have.\nmainly looked at early stopping right but but the gradient descent based methods that we have looked at also have.\nan implicit regularization in terms of their preference for like less complex Solutions and then even the initial learning rates.", "metadata": {"video_title": "Summary"}}
{"text": "an implicit regularization in terms of their preference for like less complex Solutions and then even the initial learning rates.\nthat you set up in these methods in the gradient descent the based methods they also kind of act as.\nsome kind of a regularizer right because they also control how your training is going to proceed right so we.\nhave not looked at this in detail we will not cover that also but just wanted to give you a.\npicture of this explicit and implicit regulation that happens right so this is all that I had to say about.\nregularization I'll end this lecture here and in the next lecture we'll talk about activation functions and a few other.\nthings thank you.", "metadata": {"video_title": "Summary"}}
{"text": "foreign [Music] things were not working but now let's see if things start working so there this important contribution by.\nJeff hinton's group around 2016. now what you see here is a deep neural network of course by today's standards.\nmaybe it's a shallow network but at that time it was a deep neural network and even such networks were.\nnot being able to train by back propagation but they propose a simple idea right and we'll see this in.\nthe course where if you do something to initialize the weights of this network properly because earlier you are initializing.\nthe weights randomly and expecting back propagation to learn them but if you do something to initialize the weights properly.\nthen the training with back propagation becomes stable and you're able to train a very deep neural network of course.\nthis idea was again not completely new there were very deep Learners proposed by schmidur in 191 1991 1993. again.", "metadata": {"video_title": "The Deep revival - From cats to ConvNet"}}
{"text": "this idea was again not completely new there were very deep Learners proposed by schmidur in 191 1991 1993. again.\nmany things did not fall in place at that time right around that time internet was not what it was.\nin 2006 and there was not like large tons of data available to you uh even storages at that time.\nwere much smaller and there's not enough compute available right so while this idea was there it did not fructify.\nbecause the other conditions were not conducive right you do not have enough Computing of data to really train very.\ndeep models whereas in 2006 the compute was better The gpus had still not entered the scene at least in.\ndeep learning but still the compute was better and you had more data to train so that's why it worked.\nwell in that era right of course some other Innovations also but there was the basic idea was there earlier.\nand then after this what happened right from the period from 2007 to 2009 once this spark came in hey.", "metadata": {"video_title": "The Deep revival - From cats to ConvNet"}}
{"text": "and then after this what happened right from the period from 2007 to 2009 once this spark came in hey.\nwe always knew that deep learning is good because of the universal approximation theorem the only thing that was lacking.\nwas the ability to be able to train it now we have that so let's investigate this further right and.\nthe next three years a lot of ideas came in a lot of Investigations were done into why unsupervised pre-training.\nworks and that in turn led to insights hey maybe it works better because the optimization problem becomes simpler or.\nhey maybe it works better because it allows it to generalize better so it acts as a regularizer in some.\nsense right and these things led to developments in better optimization algorithms better regularization algorithms which then helped further in.\nimproving the training of these networks and some of these ideas about what these better initializations were better regularizations were.", "metadata": {"video_title": "The Deep revival - From cats to ConvNet"}}
{"text": "improving the training of these networks and some of these ideas about what these better initializations were better regularizations were.\nbetter optimizations work all of this is things that we'll cover in the course right and then now at this.\npoint deep learning started becoming useful where people started winning uh a fairly competitive competitions right on handwriting recognition on.\nmnist data set right then on a speech recognition and then uh visual pattern recognition this was on the traffic.\nsign data so all of this now started materializing that now you just don't have it in theory that okay.\nI'm able to train a deep network but using that I'm able to compete with the best models at that.\ntime and outperformed right so that's what happened around that time and then the image net famous image net challenge.\nwhich came around which is I think around 2008 or 10 when it was first there and then in 2012.", "metadata": {"video_title": "The Deep revival - From cats to ConvNet"}}
{"text": "which came around which is I think around 2008 or 10 when it was first there and then in 2012.\nonwards the winner in the image net challenge which is roughly like you have 1000 different classes of Images cats.\ndogs airplanes trucks and so on and you have a million uh training points and then you have a test.\nset which contains images from this set and you have to classify those images accurately right so in 2012 we.\nwere able to do this with lxnet with a 16 error rate right so 84 percent sometimes the model was.\ncorrect but then after right so then came zfnet which was again an eight layer network but we were able.\nto do 11.2 percent reduce the error till 1.2 percent and then within a few years right which is around.\n2016 or 17. it went down to 3.6 percent at which point it became better than humans right so what.\ndoes that mean is that if I show these test images Suppose there are thousand images in the test set.", "metadata": {"video_title": "The Deep revival - From cats to ConvNet"}}
{"text": "does that mean is that if I show these test images Suppose there are thousand images in the test set.\neven a human makes error on four or five percent of them right because some of those images may not.\nbe clear you may not the face of the dog may not be very clearly visible so you may not.\nknow be able to distinguish between two subclasses within the doc class right two different breeds of toxin and so.\nsimilar errors whereas the model was now able to do it at 3.6 percent and you can see that the.\nnumber of layers has also increased significantly starting with 8 in LX net going all the way up to 152.\nuh in the resnet model right and this is when it was I mean almost uh clear or universally accepted.\nthat now deep learning has arrived and for all image problems we have to migrate to uh what are known.\nas convolutional neural networks or the Deep learning way of doing things right and similar uh inroads also started happening.", "metadata": {"video_title": "The Deep revival - From cats to ConvNet"}}
{"text": "as convolutional neural networks or the Deep learning way of doing things right and similar uh inroads also started happening.\nin the field of NLP and speech right slowly deep neural networks started penetrating and overtaking or rather replacing all.\nthe older methods right so this is like the golden period where everyone of course maybe you could say that.\nthe golden period is now where even just talks about deep learning and nothing else but this is the time.\nwhere the real boom or the shift happened and you could talk of this as a transition uh period 2012.\nto 2016 right so while we are talking about the success on image Network right what image net data set.\nthe key deep Learning Network there was what are known as convolutional neural networks and this section is interestingly titled.\nas from cats to convolutional neural networks so let's see if I use that title so again going back 1959.", "metadata": {"video_title": "The Deep revival - From cats to ConvNet"}}
{"text": "as from cats to convolutional neural networks so let's see if I use that title so again going back 1959.\nright uh uh Hubble and uh weasel did this experiment right what the experiment did was they had a cat.\nwhich had different electrodes connected to different parts of its brain and now there's a screen in front of it.\nwhere you see that stick there right so just imagine some stick pattern appearing on different portions of the image.\nmaybe on the top right corner top left corner Center maybe in the center on the left right and so.\non right and what they observed is depending on where the stick is different parts of the brain of the.\ncat were getting activated that means the cat has certain receptive field and only if things get triggered and those.\nreceptive field to certain sections fire if drinks get triggered in a triggered in a different receptor field that means.\nthe stick is in a different position say the top right corner instead of the top left corner then a.", "metadata": {"video_title": "The Deep revival - From cats to ConvNet"}}
{"text": "the stick is in a different position say the top right corner instead of the top left corner then a.\ndifferent portion of the brain fires right that means as we're talking about this distributed processing so different parts of.\nthe brain are actually looking at different things in the uh scene in front of you right and this is.\nessentially the motivation behind the convolutional neural networks right which is uh okay I think this slide statement is wrong.\nthis is uh from a different place we'll fix it in the slides later on uh this was the neocognitron.\nmodel right which is uh uh which essentially if you look at it right here again the same idea inspired.\nby the cat experiment that different parts in the image are being handled by different parts of the network right.\nso the receptive field as a concept emerged here that for different parts you could use different portions of the.", "metadata": {"video_title": "The Deep revival - From cats to ConvNet"}}
{"text": "so the receptive field as a concept emerged here that for different parts you could use different portions of the.\nnetwork very Loosely speaking of course we'll see this in detail so this title is wrong please note this and.\nwe'll change this later right so the slide number 36 yeah so this new cognitive model was proposed in 1980.\nright so quite uh and quite old and this idea of like using uh you know creating to different parts.\nand having like a water is known as a shallow uh processing right in terms of not having a fully.\nconnected Network all of these are ideas that we'll uh see later in the course right and then of course.\nin 1989 uh Jan likuno is considered as one of the founding fathers of deep learning uh was using convolutional.\nneural networks and there's an interesting video on YouTube office demo from that time uh which was being used for.\nrecognizing handwritten digits right and the motivation at that time was this uh Postal Services where PIN codes and other.", "metadata": {"video_title": "The Deep revival - From cats to ConvNet"}}
{"text": "recognizing handwritten digits right and the motivation at that time was this uh Postal Services where PIN codes and other.\ndigits get written and you want to automatically be able to pass them to sort the letters according to the.\nPIN codes right so that's the application is trying to use convolutional neural networks to take an image of the.\npostcard and like kind of try to extract the handwritten digits from there and just uh and kind of do.\na recognition of what that five digit number or seven digit number is and in 1998 from his initial model.\nin 1989 which was the laneet model uh he proposed several improvements and we had the lane at Phi model.\nin 1998 and this is also around the time when he introduced the now famous amnest data set right so.\nalmost every newbie today uh in the first delve into deep learning that I think the among the first few.\ndata sets that he experiment with is Ms so this data set was proposed in 1998 and this is what.", "metadata": {"video_title": "The Deep revival - From cats to ConvNet"}}
{"text": "data sets that he experiment with is Ms so this data set was proposed in 1998 and this is what.\nhe had tested convolution neural networks on at that time right so the idea of cnns which became popular in.\nimage net in around 2012 was much earlier in existence it's just that it was not applied to very large.\nscale problems like image net and then in 2012 when things were more conducive you had better optimization algorithms better.\ninitialization methods better activation functions better understanding of how to train deep neural networks it started showing success in those.\nreal world or other large-scale problems like image pilot had been in existence for quite a few years right.", "metadata": {"video_title": "The Deep revival - From cats to ConvNet"}}
{"text": "foreign [Music] so now we are going to talk about train error versus test error so let's see ah so.\nnow consider a new point x comma Y which has not been seen during training and so what I mean.\nby that that this is where I was kind of ending in the last video so you are given some.\ntraining data and based on that you have estimated F hat X right and now consider some new points right.\nand one such point x comma Y which was not there in the training data right and now ah you.\nwant to see what is the error that this model makes on this unseen Point why unseen because it was.\nnot seen during training right so that's what the setup is so if you use the model f at X.\nto predict the value of y then the mean square error is given by this quantity right so this is.\nthe expected error and why do we have an expected expectation means like roughly average or mean so what is.\nit that what is it that we are taking the average on so again the situation was I had certain.", "metadata": {"video_title": "Training error vs Test error"}}
{"text": "it that what is it that we are taking the average on so again the situation was I had certain.\ntraining data and then I had certain test data and this test data had many X comma y pairs so.\nthe average error over all these X comma y Pairs and that's what this expectation stands for I want to.\ntake this mean error over all the X comma y Pairs and potentially there could be like a large large.\nnumber of test points right so if I have built an image classifier this is an infinite set of I.\nmean practically infinite a large number of images that I could feed it at test time at which it has.\nnot seen at training time right so every day I don't know how many images get uploaded on Instagram Facebook.\nEtc so if you have trained a model using all the images that were there till today in another one.\nweek some million other images will get generated and now these are all of these images are not seen by.", "metadata": {"video_title": "Training error vs Test error"}}
{"text": "week some million other images will get generated and now these are all of these images are not seen by.\nthe train by the model during training and now all of this if you feed it to this model you.\nare interested in knowing what your expected error would be right which means the average error across all these unseen.\nimages that you have right and average you can represent as expectation right so this is the quantity that you.\nare interested in and if you can show that this quantity is actually equal to bias square plus variance plus.\nSigma square and it's not I mean uh kind of I mean hard to see how you'll go from here.\nto there because this is f of x right and we had seen that in the formula of bias you.\nhad e of f hat of x minus f of x and similarly in the formula of variance you had.\ne of f hat of x minus E of f of x ah sorry the whole Square the expectation of.\nthat right so these terms are there in the bias formula these terms are there here also and there is.", "metadata": {"video_title": "Training error vs Test error"}}
{"text": "e of f hat of x minus E of f of x ah sorry the whole Square the expectation of.\nthat right so these terms are there in the bias formula these terms are there here also and there is.\na square here so if you open up the square by applying the formula for a minus B the whole.\nsquare and then do some rearrangement of terms you will end up with this quantity right because this bias square.\nis again going to be some square of this the variance again has the terms that you have on the.\nleft hand side so if you rearrange all of this you will get this and there is of course a.\nformal derivation of this available online um which we have linked here so if you want to see the full.\nderivation you can see this but what you can show is that the expected error on unseen data is actually.\ndependent on the bias and the variance also so if you have a model which has a high bias like.\nthe simple model then your expected error on the test set is going to be high if you have a.", "metadata": {"video_title": "Training error vs Test error"}}
{"text": "dependent on the bias and the variance also so if you have a model which has a high bias like.\nthe simple model then your expected error on the test set is going to be high if you have a.\nmodel which has a high variance like this High degree polynomial that you had then again your error on the.\ntest set is going to be high so what you need for this quantity to be small you need the.\nbias also to be small and the variance also to be small and the intuition that we just developed is.\nthat these are contradictory right you cannot have both you will either have a high bias and low variance or.\nyou will have a low bias and high variance so somewhere in Middle you have to find the sweet spot.\nwhere you have medium bias and medium variance that would give you the minimum error that you are seeking on.\nthe test data right so that's the connection between bias variance and the expected test error or the mean square.", "metadata": {"video_title": "Training error vs Test error"}}
{"text": "the test data right so that's the connection between bias variance and the expected test error or the mean square.\nerror that you would have on the test data right or the Unseen data yeah so this is the situation.\nthat we have right the parameters of f hat X all the W's that we had they are estimated using.\nthe training data because that's all you have right but now why are we training the model because at test.\ntime you will get new images and you want to classify those images you want to say whether this image.\ncontains a bird or a parrot or does it contain a lion and so on right and that is what.\nyou want to do at test time you are going to work with unseen uh images right unseen meaning images.\nwhich are not seen at training time right so now this gives us to the following two quantities right one.\nis the training error right which is the error that you get on the training data so what does that.", "metadata": {"video_title": "Training error vs Test error"}}
{"text": "is the training error right which is the error that you get on the training data so what does that.\nmean uh I think we'll Define that quantity formally soon and the other is the test error which is the.\nerror that you get on the test data right and you would ideally want both to be zero you want.\nthe training error to also be zero that means once I have trained the model at least for the training.\ndata it should be able to predict everything perfectly right give me close to zero and then of course on.\nthe test data also I want it to be 0 right so typically what happens is these errors exhibit the.\nfollowing behavior that if you have a high model complexity right and in our case we saw this degree 25.\npolynomial if you had increased it further it would have almost perfectly fit the training data that means my training.\nerror would have gone 0 as my model complexity increased right but what would happen is the test error would.", "metadata": {"video_title": "Training error vs Test error"}}
{"text": "error would have gone 0 as my model complexity increased right but what would happen is the test error would.\nbehave like this right if my model is not complex like the like the linear model that I had had.\nwhich is the one which is the one extreme then my test error is going to be high as the.\nred error red curve is I there right but if my model is very complex then also my test error.\nis going to be high for reasons that I mentioned earlier because now I have trained the model to completely.\noverfit on one training data that I had seen and now if I have another point which are not similar.\nto this or which are not like which were not seen during training then I don't know whether it will.\nbe able to perform well because its entire universe was restricted to these points and it did like everything that.\nit could to fit those points properly and in that effort it might have now missed uh considering the other.", "metadata": {"video_title": "Training error vs Test error"}}
{"text": "it could to fit those points properly and in that effort it might have now missed uh considering the other.\npoints which are unseen right and for those points it will make a high error right so this is typically.\nthe behavior that the training error will keep Inc decreasing as the model complexity increasing the test error will decrease.\nup to a certain point but if you make the model extremely complex it is going to overfit on the.\ntraining data and make severe errors on the test data right so what you are looking for is The Sweet.\nSpot somewhere in the center right and on the left hand side you have high bias models like the simple.\nmodels which give you very high error both on the training data as well as test data right because the.\nsimple line model even on the training data it was giving us a high error it was nowhere close to.\nthe two sinusoidal function and on the right hand side you have the high variance models which give a low.", "metadata": {"video_title": "Training error vs Test error"}}
{"text": "the two sinusoidal function and on the right hand side you have the high variance models which give a low.\nerror on the training data but still give you a high error on the test data right and you're looking.\nfor this Sweet Spot somewhere in the middle where you have a good trade-off and you have the ideal model.\ncomplexity right so now let us formally ah Define the train error and the test error right so let there.\nbe n training points and M test points right so these are the N training points that were given to.\nyou then you can Define the training error as this right which is known to you so over all the.\nend points I'm going to take the average mean square error right so average square error or the mean square.\nerror that's what I'm going to do and this is for the N training points and then for the m.\ntest points which is n plus 1 to n plus 1 M points I can similarly Define the test error.", "metadata": {"video_title": "Training error vs Test error"}}
{"text": "error that's what I'm going to do and this is for the N training points and then for the m.\ntest points which is n plus 1 to n plus 1 M points I can similarly Define the test error.\nthe average test error right so this is already known to you intuitively I've just formally defined it yeah so.\nas the model complexity increases the train error becomes overly optimistic right because you saw that the blue curve was.\nshowing you or the training error is zero and you would think okay perfectly I've come from I have found.\nthe perfect relationship between Y and X but that's only for this training data right and as you as soon.\nas you test it on the test data you will find the error is high and so what you actually.\nwant is that the validation error should be low right so the true picture of whether your F hat is.\nreally good comes from the validation data or the test data and not from the training data right you want.", "metadata": {"video_title": "Training error vs Test error"}}
{"text": "really good comes from the validation data or the test data and not from the training data right you want.\nthe test error to be small that means the error on unknown points to be small driving the error to.\nzero on the known points is easy you can just keep increasing the model complexity and it will go to.\nzero right but what you want is to keep track of that sweet spot and to keep track of that.\nsweet spot you need the test error because the moment the test error increases you need to stop you see.\nI don't want more complexity than this because my test error is increasing right now whatever intuition we have developed.\nso far we will try to convert this into a mathematical formulation and then we will try to make a.\ncase for regularization okay so I'll end this video here.", "metadata": {"video_title": "Training error vs Test error"}}
{"text": "foreign [Music] observations right so the case two was when you are going to estimate the so now we are.\nlooking at what happens if we look at the training observations right so again we are interested in this quantity.\nwe cannot estimate it because of this so we have approximated using these three terms and now the first term.\nso now we look at the case when we try to estimate that expectation from the training data right so.\nwhat do I mean by that again making things clear we are interested in this quantity I cannot estimate it.\nbecause of this f of x here so I have expressed it as a sum of these three quantities and.\nnow the first quantity which was an expectation I am going to empirically estimate it using the training data so.\nusing the N training points okay that's what I have put here right so this is the empirical estimation of.\nthe error from the training data this is again a small constant and now we have this quantity here now.", "metadata": {"video_title": "True error vs Model complexity"}}
{"text": "the error from the training data this is again a small constant and now we have this quantity here now.\nearlier we showed that this quantity becomes 0 because we showed that these two are actually independent and Y were.\nthose two independent because Epsilon was y minus f of x right and the other quantity was F of f.\nhat of x minus f of x and we argued that these y's which are actually coming from the test.\ndata did not participate in the estimation of hefat hex hence these two quantities were independent of each other right.\nbut now we cannot make that argument because why which is coming from the training data now has actually participated.\nin the estimation of f hat X so now these two quantities are not independent hence Epsilon and this this.\nquantity is not independent hence I cannot write this as a product of the expected value of Epsilon multiplied by.\nthe expected value of this quantity and then this will not become zero hence the whole term will not become.", "metadata": {"video_title": "True error vs Model complexity"}}
{"text": "the expected value of this quantity and then this will not become zero hence the whole term will not become.\nzero right so now when you are trying to estimate it from the training data this term is not disappearing.\nright so now y minus f of x is not independent of F at x minus f of x because.\nthe training data was used for estimating the parameters so now these two quantities this expectation cannot be written as.\nthis product and hence it will not be zero and hence the empirical train error so now what is the.\nempirical train uh if I had assumed if I had just done this approximation right so if you had asked.\nme hey what is the error of your model and I said okay this is the error I have computed.\nit from the training error I have estimated it empirically from the training data then actually you would have been.\nway off why because this quantity is not zero so if this quantity is high then you are training your.", "metadata": {"video_title": "True error vs Model complexity"}}
{"text": "way off why because this quantity is not zero so if this quantity is high then you are training your.\nestimate would actually have been way off your error would have been the empirical error computed from the training data.\nplus this quantity but what you have reported is only the empirical error computed from the training data you did.\nnot report this quantity you don't even know how to compute this quantity right but there is this quantity sitting.\nhere which you cannot ignore now in the case of test data you are able to ignore know that hence.\nif you had reported this as the test data then your true error your whatever estimate you got would have.\nbeen very close to the true error as we showed at the previous slide but if you are going to.\nestimate this from the training data then that is not the case right so that is what we have learned.\nand we had this intuition that training error as computed from the training data this is the empirical training error.", "metadata": {"video_title": "True error vs Model complexity"}}
{"text": "and we had this intuition that training error as computed from the training data this is the empirical training error.\nis actually very over optimistic because it does not consider this other quantity sitting here right and test error is.\nnot optimistic because in that case this quantity disappears so whatever you get from the test error is actually very.\nclose to the true error that you might get from the expectation but now how is this related to ah.\nmodel complexity right so I said that there is when you are trying to estimate it from the training error.\nthis quantity sits here but then I also said that as your training model complexity increases your training error is.\noverly optimistic what does that mean that as your model complexity increases this quantity is actually very high and hence.\nif you look only at the empirical estimation from the training error then you are being very optimistic because you.", "metadata": {"video_title": "True error vs Model complexity"}}
{"text": "if you look only at the empirical estimation from the training error then you are being very optimistic because you.\nare ignoring a large quantity but now why does this quantity depend on the model complexity right that is something.\nthat we have not seen so that is what we will see next okay so now we will ah try.\nto wrap up the discussion on bias variance by talking about how the true error is actually uh what's the.\nrelation between the error and the model complexity right so this was the quantity that was bothering us right this.\nwas a quantity which did not go to zero in the case when we're trying to estimate these expectations from.\nthe training data and then it ah that meant that if you just compute the empirical training error then you.\nare missing out on things right you are not really ah giving a true picture of the true error right.\nso this was the quantity that was bothering us and now this quantity again I could think of it uh.", "metadata": {"video_title": "True error vs Model complexity"}}
{"text": "are missing out on things right you are not really ah giving a true picture of the true error right.\nso this was the quantity that was bothering us and now this quantity again I could think of it uh.\nof about am of estimating it empirically right so what do I mean by that I could again think of.\nthis as summation I equal to 1 to ah M or n Epsilon i f hat of x i minus.\nf of x i I can think of estimating this quantity empirically okay this is just a comment right just.\nan observation now why this I'm making this point is that uh there's this Lemma called Steins Lemma we can.\nshow that uh yeah this should have been sorry n because we had n training samples and this should have.\nbeen average so what we can show that this quantity which is the empirical estimate of the expectation is actually.\nequal to this quantity ok now this is a joke that I have every year with my students you don't.\nask me what Stein's Lemma is and I will not ask you what Stein's Lemma is but we just for.", "metadata": {"video_title": "True error vs Model complexity"}}
{"text": "equal to this quantity ok now this is a joke that I have every year with my students you don't.\nask me what Stein's Lemma is and I will not ask you what Stein's Lemma is but we just for.\nthis discussion we just need to take it for granted that Stein Slimmer says that this expectation that you are.\ninterested in is actually equal to this quantity right now if you take this on face value if you just.\nassume that time slamma is good is correct then from here on let's see what answers do we get right.\nwhere do we reach if we assume that this is actually correct and this is actually correct because Stein's Lima.\nhas proven this right ah okay so now if this is indeed correct then what is happening here right so.\nwhen will this quantity that you have here this quantity right which is the same as this quantity that you.\nhave when will that be high right this is a derivative so what does that mean that if you have.", "metadata": {"video_title": "True error vs Model complexity"}}
{"text": "have when will that be high right this is a derivative so what does that mean that if you have.\na small change in the observation right if there is a small change ah in the observation then it causes.\na large change in the estimation right so if you are y i right so you are given this x.\ni comma y i pairs right and now if there was a small suppose you are given the x i.\ns ten and the Y I was 20 right and instead of that now if the X I was 10.\nand the Y high was 20.1 this is small change right what this says is that if there is a.\nsmall change in y i there is a large change in the estimated value right so you are now your.\ntraining data has changed instead of this you had this point and now ideally you would expect that whatever F.\nhat X you got right that means whatever parameters you estimated that shouldn't change much because there is a very.\nsmall change in the data right but when would this quantity be high if this quantity is high it means.", "metadata": {"video_title": "True error vs Model complexity"}}
{"text": "small change in the data right but when would this quantity be high if this quantity is high it means.\nthat if you have even a small change in y i you are seeing a very large change in your.\nF hat which means you are changing a very large change in the W's that you have estimated right and.\nthis is not good right this is definitely not good because for this much small change you don't want the.\nmodel to vary a lot right and for what kind of models we saw that this happens that a small.\nchange in the data could lead to a large change in the F hat x that you got this is.\nwhat we saw in the case of when we were discussing the first example of simple and complex models so.\nwhen we had the simple model of Y is equal to W and X plus W naught and then when.\nwe trained it we had a total of 500 training samples and then when we trained it using 30 different.\nsamples this line did not change much right that means my f hat X did not change much right but.", "metadata": {"video_title": "True error vs Model complexity"}}
{"text": "we trained it we had a total of 500 training samples and then when we trained it using 30 different.\nsamples this line did not change much right that means my f hat X did not change much right but.\nthe same experiment then when we repeated it with a complex model right which was X raised to 25 all.\nthe way up to W naught we saw that when we take these 30 different samples which is the same.\nas saying that I am changing the training data right and when I change the training data my estimation was.\nchanging a lot in the case of the complex models right so hence this quantity you can say is going.\nto be high for complex models as compared to simple models right so that's that's what ah we are concluding.\nfrom these two points right that the expected quantity that we were interested in in which does not go to.\n0 in the case of training data is actually equal to this quantity and now we see that this quantity.", "metadata": {"video_title": "True error vs Model complexity"}}
{"text": "0 in the case of training data is actually equal to this quantity and now we see that this quantity.\nhas this sensitivity term which says that if the small change in y i what happens to the change in.\nF hat and we are making this observation that this quantity would be high for complex models because a small.\nchange in y I was causing a lot of changes in the estimated value of f hat X or the.\nestimated values of the parameters whereas for simple models it was not causing a lot of change right so connecting.\nthese observations now we can say that this quantity that you see here is actually going to be high for.\ncomplex models as compared to simple models right ah in complex model would be more changes so just to ah.\nso what does that eventually mean right so on the previous slide we had that true error is equal to.\nempirical train error plus a small constant plus some term and now we are seeing that this term is actually.", "metadata": {"video_title": "True error vs Model complexity"}}
{"text": "empirical train error plus a small constant plus some term and now we are seeing that this term is actually.\nproportional to model complexity or it is a function of model complexity so I am just going to write it.\nas Omega Model complexity right so just to indicate that this is actually dependent on the model complexity so higher.\nthe model complexity larger this term would be that means your true error would be farther away from the empirical.\ntrainer so you would estimate the train error and tell me hey it's almost zero but I will not believe.\nyou because you have not told me anything about this quantity you have only estimated the train error which is.\ngiven by summation I equal to 1 to n y i minus y hat I the whole Square on the.\ntraining data right you have not given me anything about this so even if you tell me that hey your.\ntraining error is 0 I will not believe you because this quantity has not been accounted for and I know.", "metadata": {"video_title": "True error vs Model complexity"}}
{"text": "training error is 0 I will not believe you because this quantity has not been accounted for and I know.\nthat higher the model complexity this value would be high that means more complex your model more farther away would.\nyour empirical estimate of the training error be from the true error right and just to make sure that this.\nindeed happens that complex models are actually more sensitive to changes in the training data as opposed to simple models.\nlet's just take a look at that right let's just try to see that yeah so now what I have.\ntaken this is some training data that was given to me and I estimated the true model I estimated the.\ncomplex model which is the red colored model and the simple model which was the green color model now what.\nI've done is uh I have changed one of these data points only one of the data points right I.\nhave instead of taking X comma y I have taken this x comma y right and now you can see.", "metadata": {"video_title": "True error vs Model complexity"}}
{"text": "I've done is uh I have changed one of these data points only one of the data points right I.\nhave instead of taking X comma y I have taken this x comma y right and now you can see.\nthat my simple model has not changed much right so my f hat of x so I changed y but.\nmy f hat of X did not change much that means this quantity is going to be small right but.\nfor the complex model my my estimation has changed quite a bit right so now when I changed a y.\nby a small quantity my f hat X has also changed dramatically right in this region it's very different uh.\nfrom what it was originally right so hence for complex models small changes in the training data could cause a.\nlarge change in the estimated function right the F at X or the estimated values of the parameters right so.\nthat is just a empirical demonstration or an anecdotal demonstration of this idea that complex models are more sensitive to.", "metadata": {"video_title": "True error vs Model complexity"}}
{"text": "that is just a empirical demonstration or an anecdotal demonstration of this idea that complex models are more sensitive to.\nchanges in the training data and that is the quantity that bothers us so thus quantity uh maybe I should.\ngo to the next slide I'll go to the next slide okay I can just do this slide first right.\nokay so hence while training instead of minimizing the train error we should minimize this error right so now if.\nyou're going to just focus on minimizing the train error okay you might make it zero okay so now what.\nis the what is your true error was the expected value on the right hand side is actually equal to.\nthe train error so this is what my train error is right my train error is I equal to 1.\nto n sorry I keep changing M and M but I assure you understand what I mean this is what.\nL train is and what I was trying to do is I was just trying to minimize this but now.", "metadata": {"video_title": "True error vs Model complexity"}}
{"text": "to n sorry I keep changing M and M but I assure you understand what I mean this is what.\nL train is and what I was trying to do is I was just trying to minimize this but now.\nI've realized that if I try to minimize this and make it to zero there's this another quantity which is.\ndependent on the model complexity which keeps increasing so in effect my expected error is increasing right so I might.\nhave done a great job of reducing this to zero but the cost of increasing this significantly and then the.\nnet effect is as my model is still bad when I am going to pass it test instances which were.\nnot seen during training this expectation is going to be this error is going to be still very high right.\nso that's why you should not try to just minimize the train error but train error plus the model complexity.\nis what you should try to minimize now how do you encode this model complexity drive just Define a generic.", "metadata": {"video_title": "True error vs Model complexity"}}
{"text": "is what you should try to minimize now how do you encode this model complexity drive just Define a generic.\nfunction which says that this captures the model complexity now how do you define model complexity something that we will.\nsee as we go along but the main idea here is that you should not just try to minimize this.\nerror but you should also try to minimize the model complexity and yeah this Omega Theta would be high for.\ncomplex models and small for simple models so if you have complex models this quantity would be high and hence.\nyour the loss that you are trying to minimize is still going to be high so it will not allow.\nyou to make the models very complex right and this acts as an approximation for the quantity that we just.\nsaw right so this is this is the model complexity term that we had seen and now you have to.\ncome up with some functions which act as an approximation of this and when you are trying to reduce this.", "metadata": {"video_title": "True error vs Model complexity"}}
{"text": "saw right so this is this is the model complexity term that we had seen and now you have to.\ncome up with some functions which act as an approximation of this and when you are trying to reduce this.\nit actually decreases the model complexity which means it will decrease this quantity which means it will decrease this sum.\nright so you should come up with omegas which decrease the model complexity because if the model complexity decreases this.\nterm will decrease and hence this term will also decrease right so that's the way you are going to handle.\nthis complex term here because directly handling it make looks difficult but you could Define some omega thetas such that.\nyou are sure that if you minimize that Omega Theta your model complexity is going to be reduced and hence.\nyou will be sure that this term that was bothering you is going to be reduced right so this actually.", "metadata": {"video_title": "True error vs Model complexity"}}
{"text": "you will be sure that this term that was bothering you is going to be reduced right so this actually.\nis the basis for all regularization methods right so this is what you do in regularization instead of just minimizing.\nL train Theta which is the empirical training error you actually add some regularization term to that and then you.\nsee that I am going to minimize the empirical train error as well as the regularization error because that will.\nmake sure that my expected error which is what I was interested in is actually going to be small right.\nand the pictorial view of this this is the same so this is what would happen if I were to.\njust look at the training error right so this is L train which is computed just using those n training.\nsamples then my as I keep increasing the model complexity this will keep decreasing but what will happen is as.\nI increase the model complexity this quantity is going to increase and hence my error is actually going to be.", "metadata": {"video_title": "True error vs Model complexity"}}
{"text": "I increase the model complexity this quantity is going to increase and hence my error is actually going to be.\nhigh right so I need to find this sweet spot and this Omega Theta should ensure that this model model.\ncomplexity is reasonable so that you have low training error but at the same time this this quantity is also.\nnot high and hence your expected error is also not high right that is why we use regularization now why.\ndo we care about regularization in the context of deep learning right why am I teaching you this in the.\ncontext of deep learning so the answer to that is simple right so deep learning we said that regularization is.\nbasically trying to control for model complexity and in deep neural networks you know that these are highly complex models.\nwhy are they highly complex because they have many layers many non-linearities and many parameters so they are actually they.", "metadata": {"video_title": "True error vs Model complexity"}}
{"text": "why are they highly complex because they have many layers many non-linearities and many parameters so they are actually they.\ncan completely overfit right because this we also know ah from the universal approximation theorem that you can completely overfit.\nyour training data right you can get arbitrary close to your true function if you keep adding layers or if.\nyou keep adding neurons right and you don't want to do that right you don't want to become very close.\nto your training data because then your test error would be high and that why that is why you do.\nnot want to overfit because if you leave if you design a very deep neural network with many neurons many.\nparameters then it will be able to drive the training error to zero but while doing so you have increased.\nthe model complexity by adding many parameters and many layers and that is going to give you poor generalization ah.", "metadata": {"video_title": "True error vs Model complexity"}}
{"text": "the model complexity by adding many parameters and many layers and that is going to give you poor generalization ah.\ncapability or poor or a higher test error that's why you add regularization so that you control for model complexity.\nnow what are these different regularization methods that you can use in the context of dipline learning is something that.\nwe'll see in the next lecture thank you so I'll end it here and I'll see you in the next.\nlecture.", "metadata": {"video_title": "True error vs Model complexity"}}
{"text": "foreign [Music] ing so the main question is right what has changed now right what has happened in 2006 which.\nhas allowed us to train these deep neural networks and why has big deep learning become so popular that's the.\nmain question so far right uh so if we look at this seminal work in 2006 which made uh training.\ndeep neural networks are more uh possible Right so the original paper I was actually talks about something known as.\nrbms we have not done ibms yet and we'll not do it in this part of the course uh I.\nwill try to explain it in the context of Auto encoders again we have not done Auto encoders now I've.\nalso Auto encoders will also not be a part of this course because we have replaced it by other more.\nadvanced optimization algorithms as well as discussion on Transformers so we did not have place to put in autoencoders given.\nthe number of lectures that we had so but you could refer to my older lectures and auto encoders if.", "metadata": {"video_title": "Unsupervised Pre-Training"}}
{"text": "the number of lectures that we had so but you could refer to my older lectures and auto encoders if.\nyou want and whatever we need I'll anyways cover in the next few slides right so I'll not have you.\nhave not had a detailed discussion or to encoders but I'll just quickly brush up on whatever I need in.\nthe next few slides right so two points here one is the original paper talks in the context of rbms.\nI am going to talk about this idea in the context of Auto encoders we have not done Auto encoders.\nin this offering of the course but I'll just quickly cover the concepts that I require right so now consider.\nthis deep neural network where there is like a 4 layer Network plus the input and the output layer and.\nas I was saying before 2006 it was hard to train networks which are four five six layers and so.\non it they did not converge well right now let us focus on the first two layers right now what.", "metadata": {"video_title": "Unsupervised Pre-Training"}}
{"text": "as I was saying before 2006 it was hard to train networks which are four five six layers and so.\non it they did not converge well right now let us focus on the first two layers right now what.\nI'm trying to do is I'm going to try to explain the idea of unsupervised pre-training right so now I.\nforget about training the entire network so what is my input that might be some x's that I have right.\nand this x belongs to some r n so it's an N dimensional input and I have taken the simple.\ncase when I'm just trying to predict one output right so my y belongs to R right now I'm not.\nfocusing on this entire and I decided that I want to use like a very complex deep neural network as.\nmy function approximation that this is the relation between Y and ah X and Y right and now I am.\nfocusing on this first layer where I have the input and then the first hidden layer H1 right now what.", "metadata": {"video_title": "Unsupervised Pre-Training"}}
{"text": "my function approximation that this is the relation between Y and ah X and Y right and now I am.\nfocusing on this first layer where I have the input and then the first hidden layer H1 right now what.\nwe do in unsupervised pre-training or at least as it was introduced in this paper now today unsupervised pre-training still.\nstill exist but in a much different form which is conceptually similar but with a lot of moving Parts having.\nchanged right this is in the context of feed forward neural networks today we typically talk about it in the.\ncontext of Transformers and the loss functions that evolved none of that matters at this point but I'm just saying.\nthat when I'm talking about unsupervised pre-tuning right now I'm talking in the context of this work in 2006 right.\nah ok so what we will do is we will take X as the input okay and we will focus.\non a very simple problem first right so this x is the input I have just taken the first hidden.", "metadata": {"video_title": "Unsupervised Pre-Training"}}
{"text": "ah ok so what we will do is we will take X as the input okay and we will focus.\non a very simple problem first right so this x is the input I have just taken the first hidden.\nlayer so I have taken the hidden layer as it is and now instead of feeding this hidden layer to.\nthe next layer I just disconnected the network and what I have done is I am now trying to predict.\nX again right so think of this this way that I had say some 1 0 2 4 dimensional input.\nthen I have a hidden layer in between which is say 256 dimensional and then again an output layer which.\nis one zero two four and my goal is to do the following right first compute this 256 dimensional hidden.\nrepresentation and then again reconstruct the output from it right and I my goal would be that if I call.\nthis x hat and this says X then since I am reconstructing my X hat should be as close to.\nX right and that is what this loss function is calculating so my X was n dimensional so along each.", "metadata": {"video_title": "Unsupervised Pre-Training"}}
{"text": "this x hat and this says X then since I am reconstructing my X hat should be as close to.\nX right and that is what this loss function is calculating so my X was n dimensional so along each.\nof these n Dimensions I want to reduce the error between the prediction and the construction and I want to.\ndo this for all the M training examples and I want to consider the average right so that's the objective.\nfunction this is clear now what what am I trying to do here right what is happening here right so.\nlet us try to understand this conceptually that suppose I'm able to do this suppose I'm able to train a.\nnetwork which is able to reconstruct X hat with zero error that means I am taking an X I am.\ndoing a w x y that is a transformation I do then I pass it through a sigmoid function I.\nam ignoring the biases okay so I get some hidden layer H1 right and then from the hidden layer I.\nam reconstructing X hat as say W Times H1 okay this is what I am doing in this network right.", "metadata": {"video_title": "Unsupervised Pre-Training"}}
{"text": "am ignoring the biases okay so I get some hidden layer H1 right and then from the hidden layer I.\nam reconstructing X hat as say W Times H1 okay this is what I am doing in this network right.\nmy input layer I am doing first a linear transformation followed by a non-linearity and then whatever H1 I get.\nI just reconstructed the X hat from there if you want I can just add the bias also here and.\nI can add the bias here also right so you if you want that's that makes you more comfortable then.\nthat is what it is right now suppose I am able to do this in a way that I am.\nable to reconstruct X hat from X perfectly that means my error is zero what does that tell you about.\nH1 so so it tells you that H1 actually character captures all the information that was there in X right.\nbecause I have done a compression I have gone from one zero to four dimensions to 256. now we know.\nthat in many applications our input has a lot of redundancy right so I would have captured weight height and.", "metadata": {"video_title": "Unsupervised Pre-Training"}}
{"text": "that in many applications our input has a lot of redundancy right so I would have captured weight height and.\nBMI all three are not needed from the weight and height I can know the BMI right similarly I would.\nhave captured several correlated things I would have captured weight and centimeter oh sorry weight in kilograms maybe I would.\nhave captured weight in pounds also or weight in grams also similarly height in centimeters process data is flowing in.\nfrom multiple channels so I do not know even for example I would have calculated the salary I would have.\nsalary as one of the inputs and I might also have the income tax as one of the inputs right.\nnow these two are completely correlated so in that case I did not have both these inputs right so my.\ndata has lot of redundancy so I am doing some sort of a compression where I am reducing it to.\n256 values and then I'm trying to reconstruct the input from these 256 values so if I am able to.", "metadata": {"video_title": "Unsupervised Pre-Training"}}
{"text": "data has lot of redundancy so I am doing some sort of a compression where I am reducing it to.\n256 values and then I'm trying to reconstruct the input from these 256 values so if I am able to.\ndo this perfectly that means this hidden layer that I had is capturing all the important characteristics of my input.\nright and that is enough for me to reconstruct the input right that's what typically happens in compression so that's.\nexactly what I am trying to do here and this is the idea in an auto encoder right I want.\nto learn a more compact representation of the input and the way I do it is that I use this.\nbottleneck layer right and then I try to reconstruct the loss and if I'm able to do this then I.\nhave captured all the important information in this 1024 dimensional data it was by just using 256 Dimensions right and.\nwe can show that under certain conditions this is actually equivalent to principle component analysis which is again a data.", "metadata": {"video_title": "Unsupervised Pre-Training"}}
{"text": "we can show that under certain conditions this is actually equivalent to principle component analysis which is again a data.\ndimensional ID reduction technique right and the same thing you are doing here you're using the dimensions of the input.\ndata right so that's all that we want to know about Auto encoders this is what an auto encoder tries.\nto do and this is what I am trying to do here right so while doing so I am actually.\nComputing ending up Computing a abstract representation of the input in this layer right so my this layer is now.\nComputing a good representation of the input it's capturing all the vital characteristics of the input which are enough to.\nreconstruct the inputs right so that is what is happening here and this is the objective that I am going.\nto work with now notice that this is a single layer network network right so I had a problem I.\nhad difficulties in training a deep neural network so I've ignored that problem I've said okay I'll worry about that.", "metadata": {"video_title": "Unsupervised Pre-Training"}}
{"text": "had difficulties in training a deep neural network so I've ignored that problem I've said okay I'll worry about that.\nlater right now let me just focus on this right so if I am able to train this entire network.\nwhat does that mean that this layer is producing good outputs this layer is producing good outputs this layer is.\nproducing good outputs and this layer is good in producing good outputs and finally the final output is good right.\nso this entire problem was becoming difficult for me I was not able to train the entire network so I'm.\nsaying let me just focus on one layer now and for this layer what is the definition of good that.\nit should capture all the characteristics of the input that is how I have defined goodness and I have designed.\nthis objective function accordingly and I've reduced the problem to training a network containing just one hidden layer so input.\nhidden and output and this is a shallow Network and shallow Network training was not a challenge right so I.", "metadata": {"video_title": "Unsupervised Pre-Training"}}
{"text": "hidden and output and this is a shallow Network and shallow Network training was not a challenge right so I.\ncould train a shallow neural network so I've reduced my problem to training a shallow neural network now how do.\nI go from here again because eventually I want to train the Deep inner Network that is not clear now.\nbut the first step is to just learn layer 1 effectively is that clear no no this could be W1.\nand W2 yeah so you when you compute the loss you are using that right so excited you are Computing.\nthe difference between X hat and X so there you are using that information right what was my original input.\nso loss is defined in terms of what your original input was right so that is getting used there so.\nit will be the same depends upon the input layers right no so this is n dimensional and then you.\nare reconstructing the n-dimensional input yeah right yeah yeah okay so ah so this is what we have done now.", "metadata": {"video_title": "Unsupervised Pre-Training"}}
{"text": "are reconstructing the n-dimensional input yeah right yeah yeah okay so ah so this is what we have done now.\nonce we have understood this we're just going to repeat this process again right and let's see what I mean.\nby repeat this process again yeah this is all written in words whatever I had said and yeah and one.\nimportant thing is where is the word unsupervised coming in here right so remember that in this problem that I.\nhave I'm currently interested in solving there is no y There Is No Label here hence it is in unsupervised.\nI am just having the inputs and my loss function completely depends on the input so I don't care right.\neven if you had not given me y I don't care so there is no label or supervision being used.\nhere and this is whatever I am doing right now is unsupervised later on of course I will move to.\na supervised objective but right now whatever I am doing is unsupervised ok now once I have done this I.", "metadata": {"video_title": "Unsupervised Pre-Training"}}
{"text": "a supervised objective but right now whatever I am doing is unsupervised ok now once I have done this I.\nam going to repeat this process so what I'm going to do now is that I was happy that I.\nhad learned H1 well right from the previous training so I did that round of training and I kept training.\nthat my training loss reduce and my H1 was a good representation now I am happy with H1 right so.\nnow H1 is some say d dimensional input now I'm going to plug in H2 okay and then again try.\nto reconstruct H1 hat right and I'm not going to touch these parameters so you can think of the entire.\ntraining data that was given me the M training samples that were given to me for each of those training.\nsamples I have computed h okay I have done that sorry H one I have done that right now my.\ntraining data becomes H1 I pass it through a network a layer called H2 right so this was maybe D.", "metadata": {"video_title": "Unsupervised Pre-Training"}}
{"text": "samples I have computed h okay I have done that sorry H one I have done that right now my.\ntraining data becomes H1 I pass it through a network a layer called H2 right so this was maybe D.\ndimensional this could be D dimensional or some D1 dimensional and then I again try to reconstruct H1 from there.\nso it's the same training objective right so now remember that the H is while we think of them as.\nhidden layers or outputs they are also the inputs to the net say to the next layer right so just.\nas X is H 0 and my first problem that I had solved was for its 0 now I am.\nsolving the same problem for H1 given an H1 I want to reconstruct it through a single layered Network right.\nso again once again I am training a shallow Network one input layer which is H1 which is fixed I.\nhave computed all the H1s using the x that was given to me and the training that I had done.\nin the first iteration I had computed all the W's I was happy with whatever WS I have computed and.", "metadata": {"video_title": "Unsupervised Pre-Training"}}
{"text": "have computed all the H1s using the x that was given to me and the training that I had done.\nin the first iteration I had computed all the W's I was happy with whatever WS I have computed and.\nusing those W's I have computed H1 now as W times x plus C or B whatever we had used.\ninto that right so this computation I have done now I'm thinking of H I's as H1s as my input.\npassing it through a hidden layer and then reconstructing H1 hat and again using the same objective function with X.\nreplaced by H1 right so nothing new that I am doing so again I'm just training one layer at a.\ntime right this is the only layer that is getting trained which means this is the only layer that I.\nam trying to reconstruct right and now what am I doing I had said that H1 captures all the characteristics.\nof X similarly now H2 is capturing all the important information of H1 so it's yet another level of abstract.", "metadata": {"video_title": "Unsupervised Pre-Training"}}
{"text": "of X similarly now H2 is capturing all the important information of H1 so it's yet another level of abstract.\nrepresentation of the original input right so this was already an abstract representation of the input which had discarded all.\nthe unimportant information and totally everything that you need to know about X and I can fully reconstruct X from.\nhere now H2 is an abstract representation of H1 which kind of captures every information in H1 and it's enough.\nto cap reconstruct H1 from there so H1 is an abstract representation of x and it's to in turn is.\nan abstract representation of H 1 so H2 is like a deeper abstract representation of X right so that is.\nwhat you are doing and that's what the idea and a feed forward neural network is right every layer captures.\na more and more abstract representation of the input right so now again you have focused on layer H2 and.", "metadata": {"video_title": "Unsupervised Pre-Training"}}
{"text": "a more and more abstract representation of the input right so now again you have focused on layer H2 and.\nmade sure that this computes good representations that means you have cap and ensure that these weights that are sitting.\nhere are now some good or well initialized ways right so that's what you have done and now you will.\ncontinue this process in the next iteration now you will freeze H1 and H2 now H2 becomes your input and.\nnow we will throw in H3 right and then you will try to learn these weights such that from H3.\nyou are able to reconstruct H2 perfectly right and you keep doing this till all the L layers that you.\nhave so you are learning one layer at a time and each layer is trying to learn some weights which.\nare an abstract representation which give you allow you to compute an abstract representation of the input right so now.\nthis is what you are going to do so at the end right what have we achieved that all our.", "metadata": {"video_title": "Unsupervised Pre-Training"}}
{"text": "this is what you are going to do so at the end right what have we achieved that all our.\nlayers have been trained to compute a better representation of the input right and now I return back to my.\noriginal problem my original problem was that I was given some X and some y right and I wanted to.\nlearn the relation between them and this was what my f x is now individual components and f x was.\nitself a composite function right I was first Computing this then passing it then Computing this and so on so.\nall these individual components of f of x now I have learned well right so I'll whatever weights I learned.\nin those individual pre-training steps I'll just retain those I'll start from there I'll not start from random initialization I'll.\nstart from those right and I will plug in these weights randomly because these weights I did not learn so.\nI'll plug in these weights randomly and now I'll go back to my original training objective which dependent on y.", "metadata": {"video_title": "Unsupervised Pre-Training"}}
{"text": "I'll plug in these weights randomly and now I'll go back to my original training objective which dependent on y.\nhat and f of x right or yeah sorry not y hat Y and f of x right or this.\nis what my objective function was so now I'll train for this objective function and when I'm doing this I.\nwill of course update these weights I'll do the full back propagation I'll update these weights I'll update all of.\nthese weights right but when I updating these weights remember that I have not randomly initialized them I have started.\nfrom whatever my best configuration was while doing those individual unsupervised pre-training right so this is what I have done.\nnow why does this work what is the implication all of that we will discuss right but we have understood.\nthe procedure that we are training one layer at a time it's unsupervised because we are not using the final.\nlabel once you have done the full unsupervised pre-training we plug in the supervised objective and we train for that.", "metadata": {"video_title": "Unsupervised Pre-Training"}}
{"text": "label once you have done the full unsupervised pre-training we plug in the supervised objective and we train for that.\nsupervised objective so I can think of this as L of Omega right this is what my L of Omega.\nis right this is what was the unsupervised training objective and when they did this they showed that it's possible.\nto Now train the entire Duke neural network effectively a thing which was difficult earlier and they trained quite a.\nfew I mean quite a bit deep neural network I have shown still in modern terms this is still a.\nshallow network but they train quite a bit of a deep neural network right so what have we done right.\nso in effect what we have done is we have initialized the weights of the network using that greedy unsupervised.\nobject right where at F why am I calling it greedy because at every layer I did not care about.\nwhat is happening in the other layers I just wanted to make sure that that layer is trained properly that's.", "metadata": {"video_title": "Unsupervised Pre-Training"}}
{"text": "what is happening in the other layers I just wanted to make sure that that layer is trained properly that's.\nwhy it was greedy and it was unsupervised and whatever was the result of that greedy and supervised training that.\nis the initialization that exists in my network right so now why does this work better is the question and.\nI'll give you two options is it due to better optimization or is it due to better regularization right now.\nthe first thing that I want to ask is whether you understand the difference between these two questions so what.\ndo we mean by optimization optimization deals with Dash data training data or test data training data so when you.\nare doing optimization when you are minimizing the loss function right remember that you are taking the average whatever is.\nyour loss function over your Computing with respect to some um y i and F hat of X I right.\nand this loss function depends on the training data these X I's and Y i's are your training data so.", "metadata": {"video_title": "Unsupervised Pre-Training"}}
{"text": "your loss function over your Computing with respect to some um y i and F hat of X I right.\nand this loss function depends on the training data these X I's and Y i's are your training data so.\noptimization you only have access to training data you don't have access to test it but during regularization what do.\nyou mean by regularization regularization is used for better generalization So when you say better generalization it means that of.\ncourse I know you will do well on the training data you will be able to drive the error to.\nzero but what I care about is generalization which means on test data your performance should be better right so.\nwhat is happening here is it that IFD did not do unsupervised pre-training you are not able to do better.\noptimization itself that means even for the training uh data for which I have been assuming in all our discussions.\nhey training data we can easily drive to zero we just improve the complexity of the model and we can.", "metadata": {"video_title": "Unsupervised Pre-Training"}}
{"text": "hey training data we can easily drive to zero we just improve the complexity of the model and we can.\ndrive it to zero but before 2006 is it that even on the training data I was not able to.\ndrive the error to zero or was it that because of unsupervised pre-training I was able to get better performance.\non the test data right so which of these is the reason for it to work better was it acting.\nas a regularizer or was it acting as a better Optimizer right so now I comment that I want to.\nmake here right so this is I'm talking about the work which happened in the uh period of 2006 to.\n2009 right so this work 2006 happened this unsuper SP training idea came out everybody was excited that now we.\ncan train deep neural networks and then people started inspecting right why is this working is it optimization regularization and.\nso on and not many clear answers emerged right but what got reinforced that hey indeed something is happening and.", "metadata": {"video_title": "Unsupervised Pre-Training"}}
{"text": "so on and not many clear answers emerged right but what got reinforced that hey indeed something is happening and.\nnow it is possible to train the Deep neural networks and it could be because of optimization it could be.\nbecause of regularization so why don't we design better methods of optimization why don't we design method methods of regularization.\nand so on and that's the effect that we saw from 2009 onwards our 2011-12 onwards we saw a series.\nof better optimization rhythms right we saw all of that converging into Adam and then Adam and then again a.\nfew variants of that right so this sparked interest so whatever happened in this period maybe people did not have.\ncorrect answers because these were again early days but these investigations suggested that hey maybe there is uh there is.\nMerit in going after better optimization algorithms hey maybe there is better Merit in going after better regularization methods and.", "metadata": {"video_title": "Unsupervised Pre-Training"}}
{"text": "Merit in going after better optimization algorithms hey maybe there is better Merit in going after better regularization methods and.\nwe saw that one of the regularization methods that came out drop out around 2012 to 2014 that is still.\nvery popular right so because of these investigations people got excited and said okay it could be because of optimization.\nit could be because of regularization if that is the case then maybe unsupervised pre-training is not the only method.\nwhich leads to better optimization I could come up with better optimizers or maybe unsupervised pre-training is not the only.\nmethod which leads to better regularization I could come up with better regularizers then early stopping came drop Dropout came.\nand all of these right so then all of these got used in the context of deep neural networks and.\nthen people also thought maybe there are other things maybe you could initialize the weights better because looks like unsupervised.", "metadata": {"video_title": "Unsupervised Pre-Training"}}
{"text": "then people also thought maybe there are other things maybe you could initialize the weights better because looks like unsupervised.\npre-training is doing some kind of initialization of the baits so why don't I come up with better initialization methods.\nthen people said okay maybe there are something to do with activation functions because we're talking about gradients and maybe.\nsomething happens and if you change the activation functions we might get better gradients and maybe things would improve right.\nso all of that whatever work happened in this 2006 to 2009 period which I'm going to talk about for.\nthe next 15-20 minutes none of this I will be able to give you very conclusive answers hey it was.\ndefinitely due to optimization or definitely due to regulation but that does not matter what matters is it opened up.\nthese possibilities and following this work we saw a lot of other advances half of which we have already seen.", "metadata": {"video_title": "Unsupervised Pre-Training"}}
{"text": "these possibilities and following this work we saw a lot of other advances half of which we have already seen.\nwe have seen regularization methods we have seen optimization methods and today we are going to see the remaining half.\nof this once I go over this period of 2006 to 2009 okay so these are the two possibilities we.\ndon't know what it was and I'll show you some Works which hinted at both of these and then from.\nthere on we'll continue the discussion.", "metadata": {"video_title": "Unsupervised Pre-Training"}}
