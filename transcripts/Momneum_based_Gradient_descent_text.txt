foreign [Music] descent and we'll see whether it tries to fix the problem so what were our observations about gradient descent that it takes a lot of time to navigate regions having a gentle slope right so wherever we have gentle slope and now again connecting it to the discussion on Contours gender slope means if you have two successive Rings the distance between them would be very large and in those gentle slope regions it takes a long time and the intuition is clear because on those slopes the gradients are small and if the gradients are small your updates are small so at every Point you're moving like a very very very small step so it'll take a lot of steps to get out of that flat surface right and this is what is explained here so can we do something better so let's take a look at momentum based gradient descent okay uh so there's the intuition right and uh I'll repeat the intuition which I always give for this right so uh this would be relevant to the audience here at IIT Madras so now suppose you're standing at the velachery gate right and you're asking instructions you ask the uh some person there about how do I go to Phoenix Market City Mall right and that person would say that you take a right uh from the gate and you will reach the ball right so now you have asked someone but you are a bit conservative so you take a small step towards in that direction you take small small steps and say after say 10 steps or 20 steps or say about 100 steps you ask someone else hey where is uh the mall and that person again says yeah go in this direction right go towards the right so now your confidence would increase right because now repeatedly you are being told to go in that direction someone said go to the right again someone said go to the right so now maybe you'll start want to increase your speed a bit right because you have two uh validations for uh this direction that this is the right direction right then you again walk a few steps and then you ask someone where to go again that person points you in that direction so then you again start your confidence increases and you gain more momentum right so what we are trying to say here is that if you're repeatedly being asked to go in the same direction then you should probably gain confidence and start taking bigger steps in that direction right so at this step when you are Computing the gradient maybe the gradient is small because you're on a gentle surface but you have been moving in this direction for a long time right so this is what your uh surface looked like okay this is very gentle slope here so here every step is small right but you have been moving along this direction for a long time so what if you accumulate all this and then start moving very fast so that you can come out of this uh this gentle region very quickly right so that's the idea of momentum gaze gradient descent and we now need to take this uh intuition and convert it into an equation right so let's see and this is the same as a ball gains momentum while rolling down a slope right so it's slowly moving when it's on the gentle part here right but that it's moving it gains momentum and then it starts moving very fast and because one way of looking at it is that it's constantly being pushed in that direction so you're also constantly being asked to go in that direction the slope is small but the direction is constantly this way you're saying okay go here at every point you are saying go here go here go here so if so many times you have been asked to go here can you go a bit faster right so how do you capture this intuition into a set of equations right so this is what we will do so this is the update rule for momentum based gradient descent okay so this is what I'll call as the history vector fine and uh you are giving some importance to the past history plus the current update right so this is your derivative of the loss function with respect to the gradient oh sorry derivative of the loss function with respect to the parameter w at the current time step right so now the difference is the following right so in uh foreign descent your update rule was WT minus ETA times Delta w t that means you are only listening to the current instruction you are not really considering the fact that I've been constantly being asked to go in the direction okay people might have asked me to go in this direction but where am I asked where are people asking me to go in the current step in this direction okay I'll just follow that right but this entire history which was there that you were constantly being asked to go in that direction that is not being captured right so now instead of just moving according to the current uh derivative or the current gradient you are actually also considering the entire history right and this is a recursive equation and we'll just open this recursive equation soon but this is akin to kind of taking all your past gradients and giving them some importance and then moving in the cumulative direction right so that's what you are trying to do here and this would become clear on the next slide so let me just do that and then come back uh to this slide right so what is is happening here so this is what my equation is right that UT and I was saying that UT is my history Vector so let's see how I'm constructing this UT so of course at time step 0 because my U minus 1 is not defined right so if I want to compute u0 I need beta times U minus 1 and there's no minus one step so I'm just going to set it to 0. so at time step equal to uh 0 this is how this will turn out right and this quantity is 0 so we'll just have uh U 0 is equal to the derivative of the loss function with respect to the parameter at time step 0. now let's see what U1 would be so U1 will be beta times u0 plus the current derivative so now if I substitute the value of U 0 then it would be beta times Delta the previous derivative plus W1 right so this is what I mean by the history now you are not only considering the current derivative but you are also giving some importance to the past history right you are saying oh in the past also I was asked to move in this some Direction so I'll take that also into account right and Now Beta U2 would be beta Square into the derivative at time step 0 beta into the derivative is time Step 1 and then the current derivative right so your updates remember are WT is equal to W T minus 1 minus ETA into u t right now this u t is essentially this right so it's not only the current derivative so at time Step 2 you're not just taking the current derivative but you are also taking all the history into account right so that's what is happening in momentum based gradient descent and this beta is typically less than one right so typical value for beta would be 0.9 so what does that mean you are giving weightage of 1 to the current gradient that's the maximum weightage you are giving gradient of point weightage of 0.9 to the previous gradient weightage of 0.9 Square which would be 0.81 to the previous gradient so we are giving decreasing importance to your uh previous and previous gradients right as you are farther from the current step your weightage of that gradient is less but you are still accumulating all of that and hence collectively this sum should tell you right because instead of just moving with this now collectively you will be moving by a larger amount and that's exactly what you want on the flat surfaces because you're moving slow slow slow slow at every time step your movement is slow but if you just accumulate this history then your movement would become fast right so that's what is happening here so your UT is just going to be the sum of this right so this was at time step two this was just uh you could write this as beta 2 minus 0 into W 0 then beta 2 minus 1 into W1 plus beta 2 minus 2 into W 2 right sorry Delta should have been here so that is what this equation is capturing right so this minus 0 is essentially minus Tau so you're going from time step 0 to time step Tau and at every stage your weightage for that gradient is given by Beta T minus beta raised to T minus Tau right so that's what u 2 is it's a collection of all the gradients that you have seen so far and this exponentially weighted average of current and all pass gradients right as I was saying that if you are currently at time step 9 right then the weightage for the current gradient would be one right because this would be 9 minus 9 and that would be 0 if you are at time step if you are looking at the gradient at time step 8 then that would be 9 minus 8 so it would be given the weightage of beta and as I said beta is typically 0.9 so this would keep decreasing exponentially so gradients which were taken very long back will have very less say in this cumulative history right so that's what the idea behind momentum is and now with this idea in mind let me go back to the previous slide I'll again try to look at the equations right so this is what the equations look like now we understand that this quantity here actually captures the entire history okay the weights are initialized randomly your U minus 1 is 0 and beta is a constant quantity between 0 to 1. so what you're doing is taking an exponentially weighted average of all the gradients and now it should be clear that instead of moving just by the current gradient which on a flat surface would be very small you are moving by this current you this cumulative history right and that cumulative history would of course be larger than the current gradient because it also includes the current gradient right so hence you'll be able to move by larger amounts right so let's see what happens now uh when we run this algorithm foreign okay so this is how I have written momentum based gradient descent so it's very similar to the gradient descent algorithm that I had written let me just point out things here uh so this is my initialization so I have initialized W and B to some random values for the sake of convenience minus 2 minus 2 e tabs has taken one I'm not planning playing around with the learning rate and I have this uh for the u t right which was storing the history so I've initialized the history for w as well as B to be equal to 0 right and I have taken beta is 0.10 as I said 0.9 is the typical value now this part is the same as green gradient descent for every point in my training data I am Computing the derivative and just summing up the derivative so that's the derivative of the loss function and now I'm maintaining this cumulative history in VW and VB and then updating according to the cumulative history as opposed to the current uh or as as opposed to only using the current grade unit so that's what the code is doing it's in line with the uh set of equations that you had if you want you can pause at this point and look at the code more carefully um but it's pretty uh straight forward okay so now let's try to run uh momentum based gradient okay and now you will see that right earlier it was very slow now it started moving very fast and it has gone like somewhere and it's coming back now whether that's a problem or not is something that we'll discuss soon but what is clear it's that definitely moving much faster than the gradient descent algorithm right and later on we'll have some uh comment on both gradient descent and momentum starting together and you will see the difference in one moving faster than the other moving slower okay but here it's clear that the momentum base algorithm is moving faster and we understand why that is also happening right let's look at the other view now so this is the contour map you understand what the contour map is so this region here is a flat region so this region here is a flat region and then here that the slope is very Steep and then again here the slope is very gentle this is again the slope is steep these are again flat regions right so you understand what that is so this is just the contour map corresponding to the 3D plot that we had and now we'll try to run uh gradient descent on this contour map it's nothing different from what I have shown here but now I can see the other view of what is happening to the sigmoid function and you will see something interesting here right so let's run it of course it's moving very fast and it's almost reached there but then again it has moved far and then again come to something which is some things are happening here which many of you might be realizing that it's oscillating around the solution and we'll see why that is happening and how to fix that right what is clear for now is that it's moving faster than gradient descent right so some observations and questions even in the regions having gentle slopes momentum based gradientation is able to take large steps because the momentum carries it along right because it is having the entire history behind it it just goes faster now but is moving fast always so good right would there be a situation where momentum would cause us to run past our goal right and again I'll go back to my analogy so you're going towards the Phoenix Market City a lot of momentum has been built many people have asked you to go in the right right right right and now suppose imagine you are on a scooter because now you are fast and you go very fast okay people have said Gora so you just zoom and go now what will happen you will cross the mall and go ahead right and then you'll have to take a U-turn and come back and you're taking a U-turn people will say okay go in the left direction or go this way and again you will say Okay many people are saying that so let me go fast then again you might overshoot and then again come back take a U-turn come back and so on right so going fast is not always good as you'd say I mean you could imagine in any navigation problem you just keep going fast without checking whether okay everything is fine you might overshoot and then I have to take a U-turn and come back right in fact and this is what the u-turns are what we saw on the momentum gradient descent right so it actually overshot right so it come here so it actually if you look at yeah this is what it looked like right so now uh so it went down in the valley now it should have gone towards its goal but it overshadowed the goal right it went further ahead then it took a U-turn and came back now again it should have gone somewhere here but its momentum carried it here in this direction and then again it had to take a U-turn and come back here right so that is what we saw so clearly it's moving fast but it's it's kind of moving a bit out of control right so we need to see whether if we can control the way it is moving right and so make let's see this in a more you know one more setting right so now we have this kind of an input and we have a very weird looking 3D surface here right and this is what the contour map looks like and now I'm just going to focus on the contour map because the contour map tells me everything this is a flat surface and from this flat surface I am quickly moving into a value rate so there's a steep slope which takes me to a valley similarly here I have a flat surface and I am going into a Valley from a steep slope so this is a Surface where you have like flat surfaces here and then you quickly move into a Valley from both sides right so from all sides you have flat surfaces and then you quickly move into a valley so this is what it looks like is it's a very steep valley and now you can imagine right so if I put a ball here it will roll down it will not stop here it'll go up then again come back go up then again come back and it will keep oscillating it so let's see if this effect is what we see in the momentum based gradient descent algorithms oh my God so now you see gradient descent is moving very slow but it's decidedly going towards its goal okay I'll have to play this video a few times so this gradient isn't just slowly converging now you can see that it's found the configuration where both the points lie on the sigmoid function right so this is actually a sigmoid function like this right so I'm just showing you a slice so it's a sigmoid function like this and I'm just showing you this portion of the sigm point functions it's not a line it's still a sigmoid function it's just that it's a very uh gentle slope sigmoid function so it's going to go like this and at this point it's going to go like this okay but let's focus on the uh movement again so let's see now you can see clearly that momentum based gradient descent is moving much faster right but then it's also oscillating a lot right so it kind of went uh let me now just minimize this it's better to do it yeah so this is what it looks like and yeah so the momentum base gradient descent it was clear from the speed it was going faster right but then what was happening is that it was often overshooting right so it went here it should have gone here but it just went here then it took a U-turn and again took a U-turn then again took a U-turn right so you can see that uh you can play the video again and you can see that it's taking quite a few u-turns before it reaches its goal and this is exactly the analogy very good if it's on a scooter or a bike or a car and it is going fast an overshoot come back overshoot come back but what is happening in this case as happens in real life also you will still reach your goal faster than the guy who is walking right in this case so momentum base gradient descent oscillates in and out of the Minima Valley as the momentum carries it out of the valley right and then it takes a lot of u-turns before finally converging despite these used U turns it still converges faster than the gradient vanilla gradient descent right and I'll just quickly play the video Once More to show this is important right so after 50 iterations momentum based gradient descent is at a very low loss whereas gradient descent was a very high loss right and 50 iterations corresponds to two seconds of the video there are 25 iterations per second here so I'll just play it quickly again one second it's already much ahead of gradient descent two seconds it's actually reached the Minima it's now just going to keep oscillating there but gradient descent is still very far and now despite these oscillations it's reached its answer whereas gradient descent is still trying to reach there reach there it's there still not there and now it is there right so you can see that momentum based gradient doesn't reach there much quicker than gradient descent despite these oscillations so what we would like to see next now is that can we the oscillations are a part of the life now right given that we are moving fast but can we do something to reduce these oscillations and that's the idea behind nestro based uh gradient nest of accelerated gradient descent so that is what we'll see in the next video thank you