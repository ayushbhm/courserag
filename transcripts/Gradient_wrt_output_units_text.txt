[Music] okay so we have done the intuition behind back propagation now we want to take this intuition forward and get into the mathematical details of how back propagation or how do you compute these partial derivatives or gradients that you're interested right so this was the outline so we said that we are going to slowly talk to all stakeholders in the neural network and we had divided this into three parts talk to the output layer talk to the hidden layers and talk to the weights so i'll start with the first part which is talk to the output layer right so that's what we're going to do now right so let's see what's what do you what do you mean by talk to the output layer i am interested in computing the derivative of the loss function with respect to the output layer right so this is the output layer right i collectively call it as the output layer although it has two parts activation and pre-activation uh i am going to call uh this layer which is a the a's is what i am going to call the output layer right which i want i am interested in the derivative of the output layer of the loss function with respect to this output activations or output pre-activations okay so that is the quest that i have and this i know would be a collection of some partial derivatives right so derivative partial derivative of the loss function with respect to the first uh output neuron with respect to the second output neuron and so on right and this would be the loss function here similarly the loss function here right so this is the quantity this entire vector which is the gradient vector of the loss function with respect to the output layer is what i am interested in okay now to begin with right so and i know what the loss function is i know that this is minus log of y hat l where l is the true class label okay and what i'm going to do is i'm going to focus first on the upper half right which is the green dark shaded green portion right so what do i have here i have y 1 hat y 2 hat all the way up to y k hat okay and my loss function is minus log y hat l that means one of these the negative of the log of one of these values which value the one which corresponds to the true class label right so that's what the situation is so that's what my loss function is and i'm trying to take its derivative with respect to one of the values it could be y one y two all the way up to y k right so now this is what i am trying to do i am trying to take the derivative of the loss function which has the term y hat l with respect to one of the y i's okay and we know that this log y hat l depends only on one of the elements in this array the one which corresponds to the true loss function right so in particular let's take an example suppose l is equal to 2 that means the second class was the correct class then the loss function would in effect be minus log hat y2 and now i have trying to take the derivatives of this with respect to any one of these right because i have considered a generic i here right so this is an i so any one of those elements so if i'm trying to compute with respect to y1 hat of course the loss would be zero right because this quantity does not depend on y one hat there is no y one hat that shows up here if i try to compute with y two hat then it would be minus 1 over y2 hat because that's the derivative of so if you want to compute the derivative of log x with respect to x that's 1 over x so the same thing apply here instead of x we have y 2 right if i want to compute with any of the other guys y 3 hat y 4 add up to y k hat it's going to be 0 right so that's that's very straight forward so what does it mean it's like an if else condition right so this is going to be equal to minus 1 of 1 over y hat l if i and l tend to be is are the same that means the quantity that i am taking the derivative with is also the quantity that is there inside the log then it would be minus 1 over y hat l otherwise it would be 0 because then there is i'm taking the derivative of one variable with respect to another and they don't depend on each other right so this is very straightforward i have computed the derivative of the loss function with one of the entries here any generic entity here and it would either be zero or one right so how do i write this formally does anyone know that so some of you might have heard of this indicator variable right so what does an indicator variable mean so let me just explain so i am going to write the derivative of l theta with respect to y i and again i repeat i can go from 1 to up to k right is equal to the indicator variable so this one here is the indicator variable i should have written it as this kind of a one okay and what's the condition associated with that i have suffixed something there so it's this is what it looks like i equal to l so what that means is that if i is equal to l then this indicator will take on the value 1 otherwise it will take on the value 0. so that's what an indicator value variable means it just encodes this if else condition right so indicator variable is this is how you write it and it will take on the value one if the condition is satisfied that the condition in the suffix is satisfied otherwise not right so you can now agree that these two are the same right this is just a more compact way of writing that right okay so what do i have so far i have the derivative the partial uh derivative of one uh of the ayat da shaded green guy sorry the partial derivative of the loss function with respect to the ith shaded green guy right but we are all we are not interested in one partial derivative we are interested in the entire collection of partial derivatives so i want to talk about the gradient with respect to the vector y hat not just the partial derivative with respect to one of the elements of the screen right so what do i mean by that so this is i'm going to write the gradient of the loss function with respect to y hat right so this is what my notation is and what does this mean right this is what it means it's just a collection of the partial derivatives of the loss function with respect to all the elements of y hat and what are all the elements of y hat y 1 hat y 2 hat all the way up to y k hat right so it's just a collection of all these guys and i have the formula for any one of them right so this is what the formula looks like the formula was indicator variable l equal to i divided by y hat l and the minus of that right so uh if i want to compute the derivative of the loss function with respect to y 1 hat in that case i is equal to 1 so hence i will write i equal to 1 here and l of course remains the same right l does not change so that 1 by y hat l and the negative of that comes outside and this indicator variable just changes as i go down the vector the condition associated with the indicator variable changes right and since only one of the uh i mean l could take on any one of the k values this vector will have one in only one position and zero everywhere else right for example uh on the previous slide we were discussing that suppose l equal to two that means the correct class is the second class then when i evaluate this indicator variable which is l equal to 1 i'll get the answer as 0 for the next one i'll get the answer as 1 and everywhere else i'll get a 0 right similarly if l is equal to 3 then i'll get 0 0 1 and then all 0 right so any only one of these elements would be one so these kind of vectors are called one hot vector because only one of their entries is going to be one right so let me just clear the slide okay so this is what it looks like i'm going to call this vector as the one hot vector e l where this contains a one in the l position this is a vector containing one in the l position and zero everywhere else and that's just a more compact way of writing this i have not done anything new i've just introduced a new notation for the vector e right okay ah so so far so good right so what have we been able to do so far remember that this was h3 this part was h3 which was the same as y hat sorry i should have written h 3 in the suffix right so i have the derivative of the loss function with respect to y hat but that is not the quantity that i was interested in i was interested in the derivative of the loss function with respect to a 3 right or a l because l equal to 3 in this case right so that i have not done yet but what i have done the reason i did this part was to give you the uh give you like with help of a smaller example right try to explain how this entire discussion is going to go what did we do we first computed the partial derivative with respect to one of the elements so we were actually interested in the partial in the gradient with respect to the entire vector but we didn't go there directly we first computed the partial derivative with respect to one of the elements and then once we had that we were able to write down the formula for the entire vector right uh so this same recipe is going to repeat everywhere we are going to first try to if you have any interest in a derivative of some quantity with respect to vector we'll first try to find out the partial derivative with respect to one element of that vector right and then generalize for the entire vector okay that's what we are going to do so now with that let me just try to find the derivative of the loss function with respect to a l and what i am going to do is first try to find the derivative with respect to one of the elements which is the ith element so again a l is actually a l 1 a l 2 all the way up to a l k right so from with respect to any generic element of that vector i am trying to find the derivative of the loss function what is the loss function minus log y hat l so now what is this going to be i can split it into two parts i can write it as the derivative of the loss function with respect to y hat l and then the derivative of y hat l with respect to a l i right they can split it into these two paths okay and now the question is does y hat l depend on a l i right so what am i asking this is what i'm trying to ask right so this is what my a vector looks like okay and my i here in this is anything from 1 to k right and let us again consider the case when the correct output was uh 2 right so that means my loss function is minus log y hat 2 okay and that's why this i am trying to compute the derivative of y hat 2 with respect to some ith element right it could be a l 1 a l 2 a l 3 anything right so now i'm asking does y hat to depend on all these a's or does it only depend on one of them right because earlier we saw that the loss function depended on only on one of the y hats and not the entire vector for all the other elements the derivative was 0 but now for y hat 2 which is this green guy not the loss function right this shaded green guy does it depend on everything every element of a it does right because how did you compute y hat y hat was computed using the soft max function so y hat 2 in particular was e raised to a l 2 divided by the summation of e raised to all a i's right so it depends on all the ais because all the ai ali's appear in the denominator right so it's a function of all the allies right this is how we had uh computed it right so this this should be clear uh that now we are trying to take the derivative of y hat l with respect to a l i and hence it's a function of all the variables okay so now that we have understood this and we understood what the final formula for y hat l is let's try to find it so let's try to derive this okay okay so this is what we are trying to do right we are trying to compute the derivative of the loss function with respect to one of the elements in the al vector okay now this is going to be and i've already established that y hat l depends on all the a's right so i it's not going to be the case of 1 in some cases and 0 in other cases right that the derivative is 0 in some cases and not 0 in some cases the derivative would always exist now this is a derivative of the form derivative of the form of log of a quantity with respect to a variable and that quantity depends on that variable so this would just be uh 1 by y hat l my 1 by y hat l and then the derivative of y hat l with respect to a li right so this is just the chain rule applied right so this if you have derivative of log of x square with respect to x right then you will first write it as 1 by x square and the derivative of x square with respect to x so the same thing is happening here here now instead of x and x squared which is very straightforward i have a l i and i have y hat and just as x square is a function of x y hat is a function of a l i so the same thing applies here i'll first take the derivative of log of y hat with respect to ali which would be just minus 1 by y hat li and then the derivative moves inside just as what has happened here right this high school calculus i am sure you know this but i just did it in some detail okay now let me just replace y hat l by what exactly is y hat n right so let me explain what this formula means right so what was y hat is actually the soft max of a l what does that mean that a l was a vector right which had these components a l 1 a l 2 all the way up to a l k and then how did i compute y hat from that y hat was computed by applying the soft max function which was e raised to a l 1 divided by the summation e raised to a l 2 divided by the summation and so on right so this y hat is a vector a l is a vector so this soft max of a l is actually a vector and what this is saying is that i am looking at the lth element of that vector which is the same as y hat l right so that's what this formula is saying nothing great here and what is the l element of y hat l it's going to be e raised to a l again suffix with l divided by the summation of all the exponents right so that's what y hat l means that's what softmax off there's a softmax of the vector a l and then taking the small l component of that right so that's what that notation means okay let's go ahead so i'm just going to uh as i said what i just explained that y hat l which is the same as soft max a l l is essentially this guy right so i've just put in the formula for that sorry this should have been i right because here the index is over i uh so i'm summing over all the exponents okay now this is the derivative of the form u by v or i can call it as derivative of the form g x by h x right and we all know the formula for that this is the formula for the derivative of u by v right and you would have again known this formula i will not go into the details of that you can refer to it if you don't know this just maybe quickly brush up some calculus and you will realize that this is the formula okay okay so let's go ahead now uh so i'm just going to supply substitute this formula blindly right so this is g of x for me and this is h of x for me so i'm just going to substitute g of x and h of x so this is derivative of g of x with respect to x so i'll have derivative of this is my g of x with respect to a l i into 1 by h x so i'm just dividing it by h of x right and then uh i'll not explain this entire formula it's like two conversion uh to speak it out right it's not uh difficult but it's just too cumbersome to speak it out this is again g of x as you would as you would notice this is again h of x so this is just what i'm substituting then this is h of x square right so all of this is uh the routine substitutions that i'm making in the original formula once you get that this is g and this is h and you buy this formula the rest of it is just plain substitution right now let's try start computing these derivatives so now i am taking derivative of a l i with respect to uh the exponent of a l l right so let's see what am i trying to do again i had this vector a l which had components a l 1 a l 2 all the way raised to a l k right now i am considering this quantity which is what e raised to a l l okay there's no denominator here right it's just e raised to a l i'm trying to take the derivative of that with respect to one of the uh guys here right and again you can notice that this only depends on uh one of the elements here where corresponding to the l entry here it does not depend on the other guys so this would be zero if i happens to be anything other than l otherwise it would be 1 right and we just saw how to deal with this on the previous slide so i am going to write it as this that it's going to be e raised to x right you understand what i am calling as e raised to x so the derivative of e raised to x with respect to x is e raised to x itself here instead of x we just have a l i right so it's going to be e raised to x if l is equal to i just as i explained here sorry oops okay so it's going to be uh if l is equal to i then the derivative would be e raised to this quantity otherwise it would be 0 hence the indicator variable then i have copied the denominator as it is from the other place right now this quantity i have copied as it is here the denominator i have split into two parts right so it's squares i've just put into two separate parts and now i have this right so let's see what that quantity is so here i have the sum of the exponents so what does that mean i have e raised to a l 1 plus e raised to a l 2 plus all the way e raised to a l k and i'm taking the derivative of this sum with respect to one of the e l i's right and what will happen all the elements in this sum will disappear except the one which corresponds to e a l i hence the derivative would just be e raised to a l i right so this is again straightforward i have a sum of terms only one of them depends on the variable with respect to where i'm com i'm computing the derivative so only that term will rem remain everything else will disappear right so all of this looks uh pretty straightforward don't worry if you're not getting is as i'm explaining it you can just pause the video and look at it it's very mechanical steps right so it's this exp just looks very annoying but it's just e raised to something right and you know the derivative of e raised to something with respect to that same variable is just e raised to x right so that's the only formula that is being applied here right uh okay so now let's get back what is this we just did that do you remember what that is so in the bottom i have the sum of all the exponents right so again let me just do this so i had a l1 al2 up to al k so this the guy the denominator is just the summation of the exponents of all these quantities so that's what this what denominator is and what is the numerator the numerator is the exponent with respect to the ith guy so that means it's just the soft max if you consider the soft max vector which i had said which was y hat then am just looking at the ith entry of that vector so this is just y hat i right that's what it is this is the ith entry of the softmax vector similarly this is the lth entry of the softmax vector similarly this is the ellith sorry this should not have been l equal to i so this should have been l entry right not i dash this should have been the l entry of the vector and now i can just write all this as this guy i can just write it as y hat l this guy i can write it as y hat l and this guy i can write it as y hat i right and now of course the y hat l here and here gets cancelled and so i have i'm just left with this which is uh minus 1 l equal to i minus y hat i right so this is what i have left with again don't fret too much if you did not get it at one go you just pause the video and you will get it at this simple set of steps so this is where we are right so we have the derivative of one of the loss function with respect to one of the elements of my output vector right and this is what it turns out to be and now i want to compute the derivative of the loss function with respect to my entire output vector so this is the quantity that i am interested in and i know that is just a collection of these partial derivatives right so it's the derivative with respect to a l 1 a l 2 all the way up to a l k and i know the formula for each of these guys right because this formula is for i any generic i so i can substitute i equal to 1 2 3 so on right so i can just substitute i as the appropriate value here and this is what i'll get right so minus indicator variable l equal to 1 y minus y 1 hat y 2 all the way up to y k so i have just substituted the i by the appropriate index and just expanded the vector right i've written down the full vector now this here is just my y hat vector and this here is just my one hot vector which in which only the lth entry would be one and everything else would be zero just as we had done on the previous slide so this is just the difference between two vectors okay and the minus sign has been accommodated for appropriately right so i think [Music] yeah okay this minus sign should have been outside right because this was there yeah whichever way you could have thought of this as minus plus minus plus minus plus and then you are left with this right so these signs here are consistent with the signs that you had here right that's what is important right okay so we are done with this so what have we done so far uh we have done the derivative we have done till this far right remember we were talking to every layer and this was our first goal that talked to the output layer so we now know the derivative of the loss function with respect to the output layer that means we know our first gradient right which is the gradient of the loss function with respect to output layer this is a scalar quantity this is a vector so what this tells you is that what happens if i change one value of each of the values of this al vector then what is the change in l theta right that is what each of these elements captures and the collection of all those elements is just the gradient vector the gradient of the loss function with respect to a f right so we are done with the first step that we had which was to talk to the output layer now we are going to talk to the hidden layers and then we are going to talk to the waves okay so we will come back