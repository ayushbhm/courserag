foreign [Music] so this is where we were when we left off yesterday we were talking about how deep learning has evolved and there are these four axes that we were considering and we have already talked in detail about the first two and today we'll start a discussion on better activation functions right so that's where we are so with that let me just go to the next module uh yeah so so before we look at these activation functions right so let's try to uh first motivate why do we need to talk about activation functions and the motivation comes if we try to introspect this question what makes deep neural Nets powerful right and we already saw this uh briefly when we were talking about the universal not briefly actually quite in detail when we are talking about the universal approximation theorems I just want to revisit that and emphasize on uh some a component of the deep neural network which is important in granting it its power right so let's see what that is so now suppose I have this deep neural network right it's a thin network but doesn't matter it's still a deep neural network and suppose all my sigmars which are the sigmoid or the logistic function I just replace them by a simple linear transformation it's earlier I had a any of the A's is equal to W into h plus b and then H was sigmoid of a so I am just saying instead of H equal to sigmoid of a if I just make it h equal to a right so there's only a linear layer and there is no non-linear layer so that's the situation that I am considering uh so this is what it would look like right so Y is a function of X so X first gets transformed linearly by W1 so that is what a one is a one is W one into X but then H1 is just a one so it is again W one into X then this x is the input to the next layer and then you get W 2 is into W1 into X right so w 2 into H1 which is W 2 into W 1 into X which is the same as H2 right because H2 is equal to a two and so on you continue and your output is just a linear uh it's just a multiplication of Weights uh by the input X right and now here this is as good as just having a single linear transformation right so this is just as good as saying that Y is equal to sum w x where W is a product of these four W's right so this is just a linear transformation this is not even though it's a deep Network it does not add any value to me beyond what a linear transformation would act right so the depth of the network is not what ah solely gives the Deep neural network a power of course it plays a role because then you have these composite non-linearities but if you remove the non-linearities then you just have a linear transformation of X and then you are just going to learn linear decision boundaries right because you have just said that Y is equal to w x so your y you are just assuming that the relationship between Y and X is given by a line or a hyperplane in higher Dimensions right so then in particular if you have these complex decision boundaries like that we had here where you really need something long linear right so here you are not learning a line you need something like a circular either a decision boundary which separates the positive points which are inside the brown points from the negative points which are the blue points right I cannot draw any line which suitably separates my uh the brown points from the blue points right without giving me a high error so that is not possible right so uh the reason uh deep neural network can learn such arbitrary boundaries and hence act as a universal approximator right because now we have a very complex function between Y and X which is looking like this right this is the function that we are interested in and as we had seen while looking at the universal approximation theorem and the illustrative proof of it where we are drawn these towers we could do that because we had a lean a non-linear layer it what the theorem said is that if you have even a single sigmoid layer then you can then the then the network can act as a universe Universal approximator so sigmoid was important there right so these non-linearities are what give deep neural networks their power right so there's nothing new that I have said we have already discussed this but I just wanted to quickly refresh it because it's relevant for the discussion so now if the power of deep neural networks comes from such non-linearities where do the nonlinearities come from the non-linearities essentially come from the activation function and hence we need to discuss about these activation functions and what do these bring and are there better activation functions possible is is any non-linear function okay or we want certain better and better properties should be satisfied by these nonlinear functions so that's what we are going to focus on today right so that's the reason why our discussion on activation functions is important right now we look at some of the popular activation functions the quite a bit of this material at least the initial part is taken from the lecture like a few years back uh lecture slides from Andre carpathy's course I'm not sure if that those videos are still available but they were available a few years back and I had taken material from there so Let's uh let's start with the sigmoid function so sigmoid is given by this Sigma of X is equal to 1 over 1 plus e raised to minus X and this is what it looks like it is between 0 and 1. so whatever input you pass it in our case uh the input that we passed to uh the sigmoid function is sum a which in turn is some linear transformation of the previous layer right we pass some Ai and it is some linear transformation of the previous layer right that's what our input is going to be and whatever is the input that applies element wise on this Vector a and it just squishes it or compresses it between 0 to 1 right so we already have seen this in the past uh so now since we are always interested in gradients right because as I said that training deep neural networks is largely about Computing gradients and then based on the gradients either the training would go fast low and so on right so let's see what is the gradient of the sigmoid function right and this again we have computed if you have not you can just take this as an exercise and try to find the gradient of in fact we have but you can just revisit it and try to compute the gradient of this function and you will end up with this formula right which is Sigma X into 1 minus Sigma X right and now let's focus on this formula so what what could happen if we use sigmoid in a deep neural network given that the gradient of the sigmoid is given by this formula right in particular note that at the extremes ah when the sigmoid neuron saturates what do I mean by saturate it takes its maximum value which is 1 or if it takes its minimum value which is 0 right or close to 0 in those cases the gradient vanishes right so if I put Sigma x equal to 1 then I'll get 1 into 1 minus 1 so the gradient will vanish that is in this region or if I put Sigma x equal to 0 which is corresponding to this region then again 0 into anything is 0 right so the gradient actually vanishes and we know that if the gradient vanishes or if it's very smaller so in these regions starting from here the gradient is very small and that is a problem in gradient based learning methods because the updates will not be high right so that's where this discussion is headed ah so let's let's try to go there now okay so this is a deep neural network ah and if I have used a sigmoid non-linearity then in some point in this chain rule right suppose I am trying to find the derivative of this weight with respect to the loss function at some point I am going to encounter this in the chain rule I will encounter derivative of H3 with respect to A3 and you can convince yourself right I mean any uh any even if W1 or W2 or W3 if I'm trying to find the derivative of this with respect to the loss function then this guy is sitting in the path so you will encounter that and that is nothing but the sigmoid of the derivative of the sigmoid function with respect to A3 and that is given by this formula right so that's something that we'll always encounter whenever we are doing a back propagation with and using any gradient based method so now what is the consequence of this right uh so as I said there is this concept of saturated neuron so a neuron is said to be saturated if it is at its peak or lowest value rate minimum or maximum value in the case of sigmoid neurons it's 0 or 1. so whenever it is saturated the gradient is going to be 0 right and if it is 0 then your training will not progress because once the neuron is saturated your gradients are zero now the weights are not getting updated and then it's likely that because of these weights right especially if the weights are high so let's see why would the neurons get saturated right so let's try to understand that why is it that it would get saturated and it's not given right because the function can take values between 0 to 1. so why is it that it would suddenly become one or suddenly become zero right so let's let's try to answer that so this if I were to consider any such neuron in the network right so suppose this is one of the neurons this is not the output neuron but say some neuron in the network and this is what the output of this neuron would be right it would be the sigmoid of the linear combination of the inputs connected to it right so this is what the output would be now suppose I have initialized the weights to a very high value right suppose my weights are say suppose 100 200 and so on and now my inputs are all standardized it'll be anyway standardize the inputs that means we divide by the subtract the mean and then divide by the variance so all our inputs are between 0 to 1. so the inputs are typically standardized that's anyways recommended for using gradient based methods so the inputs will not cause a problem there will be anyway small between zero to one but if the weights are initialized to high value then this sub w i Sigma I is w i x is going to be high and remember see in the plot even for values like 2.5 the sigmoid neuron has already saturated right on the x axis you have 2.5 and the y axis you have almost one right so if your weights are initialized even reasonably High then your neurons your uh the sigma this the quantity pass through the sigmoid neuron is going to be high quantity and then for that the sigma is going to be close to 1 right uh similarly if the weights are initiates to high negative value the same would happen you have a high negative value and the neuron would saturate to zero now of course you could argue that why would the wage be highered I would maybe initialize them to low values but now if there are like a thousand neurons in one layer right then this is a sum of thousand terms and even with very small weights a sum of thousand terms reaching a value of 2 or 3 is not really surprising right so you have to very carefully initialize the weights and that's why this part would be connected to the second part of this lecture where we will talk about weight initialization methods and we will try to see how to initialize the weights correctly so that we don't end up with saturated neurons right so the main point here is that neurons can saturate and if they saturate then the training kind of becomes problematic because the gradients are not flowing through right and if if the neuron is saturating with a value one that means actually these weights were like uh strongly participating in the output right and hence they would have had some impact on the loss function but that signal will not flow back to these weights because the derivatives have died right so that's a serious problem so this could lead to challenges during training since this is not a good the next problem is that sigmoids are not a zero Center that's again obvious from the plot because the plot is from zero to one zero centered would be something like say from example minus one to one so there is a zero in between so it's equally distributed around zero whereas this itself starts from zero and goes to one so the center is around 0.5 right so what if it's not zero center right what could the possible what could the problem be right so now let us try to again see that through an example suppose I have this output layer and before that I have this one sigmoid neuron right and it is connected to two inputs h21 and H22 so I'll have the A3 would be this and then I'll passing it through the sigmoid neuron right now again if I were to update the weights W1 or W2 let's see what chain rule I will use there so this is what my derivative would be right so it would be derivative of the loss function which is sitting here with respect to Y right then the derivative of y with respect to H3 so the output is H3 then the derivative of H3 with respect to A3 right and then the uh oh sorry this and then the derivative of uh A3 with respect to W1 which I have already written as h21 so this is what A3 is if I take the derivative of this with respect to W 1 this quantity disappears because it's a constant and the derivative of W1 h21 is just h21 similarly the derivative of W2 or rather eighth of the loss function with respect to W 2 would have an H22 here right again because derivative of A3 with respect to W2 this quantity would disappear and you will get an H22 right so this is fine this formula you understand now what's with the color coding right what's the red part and what's the blue part so the red part is common for both the derivatives right so this part is actually the same in both the derivatives okay and this part is changing okay this is h21 and H22 now what is h21 h21 is the output of the previous sigmoid neuron right so I know that this is going to be positive right both the blue quantities I know are going to be positive right now if both the blue quantities are going to be positive then the sign of these two gradients is decided by the red quantity right and the red quantity is the same for both the gradients so now both the partial derivatives so now ah either the red quantity is negative in which case both these derivatives are negative or the red quantity is positive in which case again both these derivatives are positive right so I cannot have a situation where I have my gradient Vector which contains all these ah derivatives and either it will be all positives or it will be all negatives right and this would be true even if I had like n neurons here the same argument I could have extended to it so if I had computed those n partial derivatives I would have had this red term common and then this blue term the blue term is always positive because it is the output of a sigmoid neuron so it has no say on the sign and then the sign is then determined by or other I mean it contributes equally I mean whatever it is all of it is positive the sign will just depend on the red part and the red part is same for all these guys so if the red part is negative all of them would be negative it's positive all of them would be positive right so a gradient Vector is either consisting all positive values or all negative values right so that's what is happening and the why am I discussing this in the case of sigmoid Duron because this happened because you had used sigmoid neurons hence the blue parts were always positive right if you had some use some other neuron which had a range between minus one to one then it would have been possible some of these Blues were positive some of these Blues were negative and then accordingly the the vectors the elements of the vector some of them would be positive some of them would be negative but because you are using a sigmoid neuron all W's are positive and hence all the elements of the gradient Vector would have the same sign now what is the problem with that that is I've just told you what the what is happening but we don't know what the problem with that is right so this is just in words whatever I explained so let's see what the problem is right so this is the uh uh say derivative of the partial derivative with respect to W1 partial derivative with respect to W2 so now if I consider the plane right which has uh all possible values of partial derivative of W1 and W2 so now which are the quadrants which are possible either the quadrant where both the values are positive or the quadrant where both the values are negative right so you can never have a vector in this direction your gradient Vector cannot be in this direction nor can be it in any of these directions it can only be in these directions or these directions right that is what this previous explanation is telling you so you can already see that when you're doing gradient descent you're moving in directions now half the directions have been thrown away because of using the sigmoid neuron these directions are not possible at all right so you can only move in certain directions and that already looks like a problem and here's a toy example to show that it is indeed a problem right so suppose you have initialized suppose this this Arrow here this is the optimal value of w right this is where you want to go and suppose this is where you had initialized your uh weight right so now I would want to have a possible movement like this right and reach the optimum but this movement is not possible because it has one value positive one value negative and we know that that is not possible we already saw that all these kind of movements are not possible right so this is not going to be possible so now how will I have to reach the optimum I'll have to just take the movements which are allowed so I will have to perhaps take this movement this is uh okay this is the node yeah this is allowed right so this is where like 1 is 0 and the other is uh negative so that is allowed this this and so on right so I'll have to move like this in a zigzag pattern to kind of reach the optimum right so I'll take more steps to converge and and keeping this toy example aside right the simple thing is this right so I'm asking you to go from place a to place B but I am saying that hey certain types of turns are not allowed if you're not allowed to take certain times of turns and you are restricted only to some types of turns then you are going to take more time right so that's the main idea and that is happening because sigmoids are not zero centered hence we get all values as positive hence this negative positive combination is not possible in the uh directions that you choose to move in right and lastly of course sigmoids are computationally expensive because you have the exponent to compute because there are faster ways of computing it but still it's a computation that you need to do right so that's the other reason so that is all about the sigmoid neurons and why they are not the most convenient activation function so then ah because of these limitations right so at some point uh so now because of these limitations the tan H function is popular because one of the things that you immediately see is that it's zero centered right and then whatever explanation I gave at the end that is kind of taken care of of course tanh is not a new activation function it has been used since 1990s but we are just going over all the activation functions so sigmoid is the default that we start explanations with but now I have shown you certain problems with it and then tanh overcomes some of those problems it is zero centered uh but let's see what is the derivative of this so it's a derivative of this is 1 minus tan H Square X and again at saturation the gradients will vanish right and that you don't need to look at the formula the same problem as the graph itself shows you that if the function value lies here then the derivative is very small or zero same thing here right and if you look at the formula so when Tan x square is 1 your derivative would be uh zero right or when it would be minus 1 whenever it is plus or minus 1 your derivative is going to be 0 right so that's again clear so the problem of the saturating neuron and hence the gradient Vanishing still exist the problem of limited directions to move in has been overcome and also the problem of computational expensive Still Remains it but still it has the E component in it right so tanets the formula is contains the uh Eep part of it right so yeah okay so the next that will look next for Activation function that we look at is the rectified linear unit and this is a very one of the most popular activation functions as we will see at the end of the discussion we'll show you what are the popular activation functions and this is proposed almost a decade back in the context of convolutional neural networks and it's still one of the de facto activation functions to be used across a wide variety of networks right the first question that I have this is what the value relu function looks like right so it is simply Max of 0 comma X right so what does that mean that again in the context of a deep neural network so say this is one of the new translate so this is suppose my H1 this is say x now I have computed the A's right so this guy here lets let me call it say a 1 2 right that is going to be again summation uh w i x i that we know right now that is this a 1 2 is going to be the input to the relu function that is the non-linearity setting here and it will simply look at whether a 1 2 is greater than 0 if so it will just return a 1 2 otherwise it will return 0 right so if you have any negative value being passed to it it clamps it to 0 and if it's a positive value it just lets it let it be as it is right so let's see what are the implications of that right so the first question is that is this really a non-linear function so indeed it is a longer function the max function the way it is shown here is a non-linear function a linear function would just uh behave uh linearly on both sides but this is on the negative side you have uh clamped it and in fact you can show that if we combine two relu units right so these are two relu units that have combined right so this is a relu function and this is another relu function and I have just taken the difference between these two right so you could think of it in terms of neurons I had two neurons both of these were relu neurons right and then I'm subtracting one from the other and I'm getting some output and that output looks almost like the sigmoid function it looks like a linear approximation of the sigmoid function so it is indeed a non-linear function right it's as close to almost like the step function that you have and if you adjust these parameters a bit more you will get even better kind of a step function and so on right so indeed a relu is a non-linear function some of you have not seen it before but might just look at the line and think that it's a linear function but it is not okay uh this specific function that I've drawn is called relu 6 because 6 denotes the maximum value of the relu that you have here okay okay so what are the advantages of relu so it does not saturate in the positive region right what do I mean by that so as long as your A3 right was or a was positive the output would remain positive and it will not saturated there is no saturation here there is no clamping on this side as opposed to what you had in the sigmoid or any of the sigmoid shaped functions right so that is what I mean here but in the negative region it can still saturate right so in the if your contribution is negative right if the a is negative then it will be clamped to zero and of course the derivative would again then be zero right so it does saturate in the negative region so roughly half the problem solved but computational is of course very efficient right there is no exponent here you just look at the value and the simple implementation is if greater than zero pass it right so it's a very simple uh uh function to compute and in practice at least that's what this paper and as I said it's a decade old paper had shown that it converges much faster than sigmoid and tannage and this was in the context of deep convolutional neural networks then but thereafter it has been used in all sorts of networks as I said great feed power networks recurrent networks Transformers also right all the now in Transformers there are there's the Gedo activation function which is perhaps the default but yeah this has been used in multiple contexts right so now uh although this all sounds good right so what all sounds good one is it does not saturate at least half the time so it does not saturate as opposed to sigmoid and tanets which have two saturation regions uh it's of computationally efficient so these two things are taken care of but still there is some challenges here right and we'll see what those are so if you look at the derivative of relu then it is going to be 0 if x is less than 0 and it is going to be 1 if x is greater than 0 right that simply follows from the fact that relu is equal to 0 or X so when it's X the derivative of this with respect to X is just going to be 1 right so this is what the derivative looks like and now if this is what the derivative looks like uh let's see what's the implication that we have right so I've given you some toy example here a toy Network now at some point Suppose there is a large gradient which flows through the network right there is a large gradient which flows and it makes B's value uh a very negative right suppose B takes on a large negative value okay that is what happens and this is not like cooked up this is conceivable that this could happen you have some training example which causes B to update and then B becomes a very large negative value right now if that happens what would the consequence of it be so now your uh A1 right which is the input to the value function right the value function is sitting here would is just W and X1 plus W 2 x 2 plus b now if B is very negative irrespective of what W1 X1 and W 2 x 2 are right this b a very large negative quantity would get added so this is going to be 0. so this is going to be 0 then the derivative is going to be 0. so if the derivative is 0 again you have this problem that when you are trying to update W1 at some point in the chain you have this derivative of H1 with respect to A1 and that derivative is going to be 0 right which means the derivative of the loss function with respect to W 1 W 2 is going to be 0 right now if and also with respect to B is going to be 0. so now if that happens the neuron of course output at zeros or the neurons output was Zero which means it was a dead neuron which is fine if it's dead for this training example I can live with it right but now it's not got updated neither has B got an update right and now from here on what will happen is that the neuron will remain dead for the rest of the training it will never become alive again right so why is that the case so this is all just the explanation that I gave in uh just I was speaking through it this is all on the slides also and this is the main point that I'm talking about now right that the neuron will stay Dead Forever right so why would that happen yeah so weights are never getting updated right so once it became dead W1 W2 and B all three will not get updated right so B Still Remains this large negative value now the next input comes in again the same will happen the B is large negative value so this sum W 1 x 1 plus W 2 x 2 plus b would again be less than zero again the neuron is dead and then again the gradient does not flow through again W and W2 B don't get updated again the next input comes in it's the same story right so once it becomes dead it stays Dead Forever right and in practice people have observed that uh as high as greater than 50 percent of the relu units can die if the learning rate is set to high right why is this related to the running rate if the learning rate is very high then this problem of a large gradient flowing to B and then getting multiplied by a reasonable learning rate can cause B to be very negative right so that's where the learning rate has to be carefully uh satisf carefully set and also when you are using uh relu it is advised to initialize bias to a positive value right and in context of deep learning this is a fairly large positive value 0.01 because you have many elements contributing to the sum so one of them is point zero one right so you should set it properly otherwise many of your neurons would die or you could use some of the other variants of relu that we will see soon so this is one variant which is the Leaky relu right and as the name suggests it's allowed some leakage on the negative side so earlier whenever you had a negative value you were kind of making it zero now you make it small you multiply by a very small constant so it becomes small right so it's close to zero but it does not die completely so it allows some gradients to flow even when your X is negative right so it takes care of neurons not saturating and ah yeah and it will not die because it will ensure that some gradient always flows through and it remains computationally efficient right just making it 0.1 x does not make it any more expensive than the value function right maybe slightly more and also it gives you these close to zero centered outputs because now you have outputs which are negative also and your outputs which are positive also so this problem of gradients only being in One Direction is also taken care of it so multiple advantages of leaky relu and then there are a few such other variants right so one of them is parametric relu so then the question is right that y set this to point One X right so let it be some Alpha and this Alpha could also be learned and it will also get updated during back preparation right so one simple way of thinking is that you set Alpha equal to 0.1 to begin with right and then just as all the weights are getting updated compute this derivative of the loss function with respect to Alpha also and then update Alpha also as Alpha minus ETA times this gradient or whatever gradient doesn't update rule you are using right ah so this is it kind of again makes it uh more careful that why have you selected a certain slope here maybe that slope needs to be adjusted as the training is going right so it gives me more flexibility and again takes care of all the problems that you had with relu of neurons dying and it not being zero center right while these variants have been proposed they are not like so popular I think relu the default version is still the most popular among these variants and one more variant of relu is the exponential linear unit right so yeah so far everything was linear on both sides it was like this but now on the whenever the input is less than zero we are making it exponentially Decay so there's still some gradient flowing on the negative side also it has all the benefits of relu because largely it has a linear uh on the positive side and it's also now again close to zero centered right and some gradient always flows through but it again becomes exponentially uh it becomes expensive right because you have to compute this exponent right so now all of these they were proposed in certain contexts and in those contexts they were like found to be better than relu also some of them theoretically showed that why they are better than relu but at least in practice many of these are not very popular right you just use the default relu and largely that works or you use some of the more recent ones which is angelu and swish right so they are more popular now right so that ends the discussion on three of the most popular or so of the initial activation functions which were there around 2012 2014 which is the logistic activation function then the tanh and then the relu right so I'll end this video here and when you come back we'll discuss a few more uh later activation functions that came out