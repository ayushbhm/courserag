foreign [Music] simple but very effective technique so in data set augmented augmentation this is what you do right so suppose your training data set was about digit classification right and many of you would have experimented with the mnist dataset uh so this these are some images given in the MS data set so this is say one of the images that was given and whose label is of course uh two right so this is the digit two now what you could do is that from this image you could create some other images right so what what are the other images you could create you could Define certain uh transitions right so you could rotate this image by 20 20 degree and get a new image right and that image the label would still be two so you don't need to manually label this you know that even if I rotate the image by a small amount of course for some things like 6 and 9 if you rotate by 180 degrees then you will have a problem but for most digits you will not have this problem right so you could rotate by a small amount you could shift it vertically right so the two being at the center of the image you could bring it down you could take it up and you would get a different image you could shift it horizontally or you could blur the image you could also change some pixels and add some noise right and both doing all of this you have just created more training data and what you're doing here is actually you are exploiting the knowledge of the task right so you know that if you rotate the image if you do any of these augmentations that you have seen and there are many such more augmentations in the possible in case of image classification the label does not change so I can create a lot of data for free from the data that was queued and now how does this help so now you have more training data and now it's no longer enough for the network to kind of uh overfit that okay whenever I see this combination of pixels I will mark it as two because now that will not work for this guy because now the pixels have changed a bit the orientation of the pixels have changed and so on so this this will not uh it cannot easily overfit right it has a harder task so you have given it much more training data that it has to by heart now right it has to memorize now and that will make it uh make it less prone to overfitting right because there's just so much to learn that it cannot hold over fit possibly on all of this of course modern deep neural networks will still over fit but it's found that doing this kind of data augmentation which comes for free it's not additional cost for you then why not just try it and you would be able to get uh a lesser chance of overfitting in this case right that's all there is to this technique there's nothing more to be said here the only intuition here is that you have more training data you have more variants of the same image and all of that you have to be able to classify as two so now this chance of just memorizing and not understanding reduces a bit right that's all that happens and this works well for image classification object recognition it has already also been tried well for speech now it has also been tried for text there are libraries which allow you to do data augmentations but in text it's still a bit harder to come up with very meaningful augmentations uh the main difference is in image and speech you have like continuous signals whereas in text you have discrete uh you have just words whether the speech you have a continuous signal similarly in image you have these pixels and so on so that's the main uh difference but even for text nowadays there are certain annotation techniques available right so that's all to be said about data set augmentation the next technique is about parameter sharing and tying and we will see this a bit more detail when we talk about ah convolutional neural networks so in convolutional neural networks you have if you want to process an image you do what is you use what is known as a convolutional filter and the same filter is applied to different parts of the image right so what I'm trying to show you here and this is of course become much more clear when we do convolutional neural networks is that I have a matrix you can think of there's a matrix of Weights now instead of defining a separate Matrix for every part of the image I am going to use the same Matrix and apply it to different parts of the image right so that's the basic idea I'll not say anything more at this point because you need to understand what convolution is and so on and we will see this in more detail when we reach there right okay and also in the case of Auto encoders which we'll see after a few lectures is also known as something known as weight tying so what you do in weight tying is that if you look at this suppose this is n dimensional and this is D dimensional and this is again n dimensional so then the weight Matrix that you will have here would be n cross T dimensional right and the weight Matrix that you will have here would be D cross n dimensional let's call this p and let's call this Q then what you could do is you could say that I'll just enforce P transpose equal to q that means I'll not have a separate P Matrix here I'll just use Q transpose and Q transpose is indeed D cross n The Matrix that I was looking for so instead of having separate weights in these two layers I am using the same weight in both the layers right so that's called weight tying and this is also a common way of doing regularization it has also been tried in the modern Transformer based models where you have eight different layers and multiple attention heads and you kind of do some parameter tying that means you across all the layers instead of having a separate Matrix of Weights in each of these layers you you just reuse the weights across all the layers or you share or tie the weights across all the layers right so this will again become make more sense when we do either when we do auto encoders or later on when we talk about Transformers but for the sake of completeness I'm just mentioning it here here so you know that these are also regularization techniques right and this here the regularization in both these cases is clear right so here I'm using the same weight Matrix that means I am not using separate weights that means I'm using fewer weights and fewer weights means smaller models smaller model means smaller complexity and the same thing I'm use doing here instead of using 2 times the number of Weights that I should have used I'm just using uh one time that right so I'm just using W instead of using two W's so again I'm reducing the model complexity right so in both these cases it should be clear how this is acting as a regularizer so I'll end this video here thank you