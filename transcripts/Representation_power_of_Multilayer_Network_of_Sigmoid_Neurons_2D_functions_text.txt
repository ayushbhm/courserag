[Music] now what would happen if you have more than one input right so now this was all for a single input right i just had the x axis and the y axis so x was input and y was the output but now i have a situation where i have two inputs right and what is happening here is interesting right so this is my uh the orient oops i know intention of doing this so now here again this now let me be careful and do it well yeah so this is this orange plane that you see that is my x1 x2 plane right and i have points on this plane right which you can see are some the blue points and the orange points right and what is this i've just taken my oil mining example again and suppose this is the salinity axis and this is the pressure axis right so a blue point here is simply the x1 comma x2 point on this planet where the salinity is equal to x1 and the pressure is equal to x2 and same the all the points here are just the x1 x2 coordinates where x1 is silent and pressure x2 is pressure right now it so happens that some of these points you have found oil which are the orange points and some of these points you have not found oil which is the blue points right so now what is my decision boundary should look like i cannot have a decision boundary like this right because it does not separate the blue points from the orange points you can see that i cannot no matter what i try i cannot separate the orange points on the blue points by drawing a line right because this is not linearly separable but if i draw i have a function which looks like exactly what has been shown here and let me just delete and use a better color you know not black maybe the function is 0 everywhere here and then 1 here right and then again 1 here and 0 everywhere here 0 everywhere here 0 everywhere here right so for all the blue points now it is predicting 0. you can see that it's also obvious in the 2d version of the figure and for all the orange points it is predicting 1 and that's exactly what i want because for the orange points i found oil and for the blue points i did not find out right so even in two dimensions you could have this case where you have an arbitrary function now this is the shape of the function the entire orange surface that you see is the shape of the true function and that's what the true function looks like and now i want to approximate it right and now again if i had these small two dimensional toggles right so then i could just construct all of them side by side right and get the original figure back right original function back so now i want to have a two dimensional tower so how do i get a two dimensional toggle and once we understand two dimensional we will just say we can do this in three dimensions four dimensions n dimensions right so once the idea and two dimension becomes clear okay so now again i i'll just take this function which is a function of two inputs this is a sigmoid neuron function which takes two inputs x1 and x2 each input would have a weight w1 w2 and then a bias right so now let's adjust a w1 and see what happens so if i increase w1 right again as i keep you can see that as i keep increasing i'll do okay maybe i'll just use the yeah this is better so it's flat at zero and as i increase it starts becoming s right now the slope is very gentle right and as i keep increasing the value of w it becomes steep and steep till it becomes like a step function right so this w1 controls the uh movement in this direction right the slope in this direction okay now let's see what w2 does and for that i'll again make w one as zero okay and i'll start increasing w two you can see that w two is adjusting the slope in the other orientation right oh coordination so now what w 2 is doing is it's adjusting the slope in this direction right so as i keep increasing w 2 now oops as i keep increasing it it will become flatter and flatter in this direction right and now you can see b again just helps in movement along the direction right so i can change the point at which the crossover happens by changing b so you can go back and play around with this figure and animation and try to understand how changing the values of w1 and w2 affect these uh this sigmoid function if i increase both something interesting happens right so you can just play with this right so the point is again clear if i set the w1 very high i could again get the step function and the orientation and all i can control and if i have b as i move b the step function will move right at the threshold at which the crossover happens right so again the same idea as in 2d i can adjust the w's and b's to get something which looks like a step function right so now let's try to construct a tower in 3d okay now i didn't want to show you this okay right so first let's see h11 right so i have taken one neuron again again the same idea i have a network okay where x is my input and i have one neuron which is h11 another neuron which is h12 and then both these outputs will be get added which i'm going to call as h21 right added meaning plus one and minus one right so now i'm showing you but it just that both of these are now this x is x 1 comma x 2 right so the functions are 1 over 1 plus e raised to minus w 1 x 1. plus w 2 x 2 plus b right so that's why you have w 1 w 2 b right i'm going to uh just use the same parameters for both h 1 1 and h 2 to explain this okay so now this is what h11 is looking like i have set the w very high as you can see here oops yeah i've set the w to a very high value which is 60 and hence it has become a step function and now if i change b it will move along here right i can make it move wherever i want right now let's see what h12 is so i have h12 which are parameters w3 w4 and b now again i have set w3 to be very high right so it has become like a step function okay don't think that it became uh yeah sorry i shouldn't have changed this yeah so in both cases one of the w's i have kept as zero and just changed the other w so now this has become again like a step function okay and now if i were to subtract them right which is h11 minus h2 and if i were to hide the other things then i'll get a tower function right again the same logic that i'll not show you this right so here both are oops yeah maybe this is fine yeah this looks better yeah so here both are 0 so the output will remain 0. here both are 1 so 1 minus 1 is going to become 0 so it will become flat and here this is 1 and this is 0 so 1 minus 0 so you'll get a tower like this right so that is now if i hide h 1 1 and h 1 2 and only show you h 1 1 minus h 1 2 then it's going to look like this tower right and let me just delete some of the okay right so this is looking like a tower so i have been able to construct a tower but this is still not complete right because this tower is like open from two sides i don't want a tower which is open from two sides i want a closed tower right just as that 2d thing where you had a tower like this i'd want a tower which is closed from all the four sides right now right now it's open from two sides and that's where i had this discussion on what happens with the orientation right so you could have you could see that one of the weights controls the movement in one direction and the other weight controls the movement in the other direction right so right now is adjusting one of the weights right you can see here that w1 and w3 is what i have played with i have kept w2 and w4 as zero right and w2 and w4 would help me move in this direction so it will probably form a block which would be in this direction so it will bring in the remaining two walls i already have two walls for the tower i'm looking for two more walls and all of this will become clear when i show you the illustration on the next slide okay so this is what my network is right it's the same thing i have two sigmoid neurons and then one addition and i'm getting f of x one but at this i'm getting only open tower right i'm not still getting the fully uh closed tower right so now let's see if you can get a come up with a network for the entire thing okay let me just do some adjustments here first i guess this should be fine uh i think i've already pre-adjusted the values you can play with them later on so i already showed you h11 and h12 okay and when i do h11 minus h12 i get a tower so this is something that you are convinced about so i will not show you these two right now i'll take h13 where i am going to so if you look at this right so w11 was set to 200 right so that is and w one two are set to zero that's why it's a step in a particular direction but now for the remaining one right i have set w three one to zero and i've said w three to two hundred so what would happen in that case i would get a step function in this orientation right and similarly for h14 i have set w41 to 0 and w4 to 200 so again i will get a step function in this orientation right and now if i subtract these two i get a tower in this orientation right so now let me just show you the towers only so i have one tower in this orientation i have another tower in the other orientation right so now you can see that the walls have closed here if i just move it around you can see that it's the walls have closed completely right so you can see that the walls there is no opening anywhere right you cannot see a white surface anywhere because on all four sides the wall has closed right but still i'm not there right it is not like a tower yet i mean there's a tower somewhere inside this middle portion this portion here right this okay maybe i should not have drawn it like that okay so this portion underneath this block here right that is the tower but i'm getting a lot of extra things also along with it right so there's some some things coming out and i don't want those for now so i'll see how to get rid of those right but for now you get what happens one the main thing to note is that now the walls have closed as i've shown you at no point in that region you cannot see uh you can see a blue wall then you can see the cyan wall here then again the blue or the purple wall and then again the cyan wall right so it's it's you have a tower sitting somewhere inside that so now let me look at the sum of these two functions right so now i have what i've shown you is h11 minus h12 and h13 minus h14 one of them is the purple colored structure and the other is the cyan colored structure now let me take a sum of these two and see what happens right this looks quite weird let's see why this is happening right so let's just try to explain that again so now it's very simple actually nothing much to explain the same argument that i had earlier so in this region both are 0 right so if i add them again you will get 0 right similarly in this region both are 0 if you add them you will get 0. similarly if i just maybe move it around yeah in this in this region again both are 0 so if you're going to add them you'll again get 0 and then let me just move all the way back again in this region both are 0 so if you add them up you'll get 0. so you have understood what is happening at the flat surface is it you can just again go back and play around with this you can just move it around and you can see it right now let's look at the other regions right so where there's not going to be zero output if you look at this region okay so then you can see that the let me just orient it properly okay i'll just delete all my marks oops okay i'm really having trouble in adjusting to this okay um yeah this is a good view so now if i look in this region right so this purple guy was 1 but the cn guy was 0. so if i add 1 plus 0 i'll again get 1 here right same argument for this region where the cn guy was 1 but the purple guy is 0 so i'll add them i'll get 0. same argument for this region and same argument for this region does that make sense now for this region in the middle both the cyan and the purple are one so if i add them i'll get a 2 which i'll get an elevation there and that's exactly what is happening when i show you the sum of these two right so in all these regions you can now see it yeah i think this is a good view uh i'll just stop it so in these regions it's zero in these regions it's one in these four regions and in this region it's two right so i've got my tower actually my tower is sitting here on the top of the structure this is the tower that i'm interested in but i have these some extra scaffoldings here and there which i want to get rid of right so now how do i get rid of that right so now if i look at this entire structure right i'll just get rid of the annotations and also get rid of the individual h11 and h12 yeah yeah so this is what my structure looks like and you can see that at the bottom while it's open yeah the structure sitting at the top is closed from all sides so this structure sitting at the top is closed from all sides right now there are three levels of output here there's a zero level okay which is here then there is a level one and then there's a level two right and i only want to retain things which are greater than one right so that if i only retain things which are greater than one suppose i could have a way of filtering so that i could only retain things which are greater than one then only this part would remain right everything else is either one so this is all one okay and this is all zero right so if i have a thresholding such that i retain everything which is greater than one then i'll get my tower out of it does that make sense right so if i do a thresholding so how will i do that i can again introduce a sigmoid function there with a threshold of 1 so that the output is 0 till it reaches there and then it climbs to one right so if i have a sigmoid function which is a threshold of one or the switch over point set to one then when i take this output right so this is the output of some function right now let me just call it as h now when i take this edge so this is h and i'm going to pass it through another sigmoid okay such that it switches over at one then everything which is less than one all of this will disappear and only these things at the top right which are greater than one will remain and what the result end result would be that i'll get my tower function out so if i delete this and i just show you the tower so this is what will remain right so this is just the part of the that entire structure which was above level one right and only that part will remain and i get my tower function which is now indeed close from all sides at which we were already convinced about right so you can go back play around with these visuals and convince yourself that you have got a 2d tower i am convinced that we have got a 2d tower by constructing a network like this i took x1 x2 i had these two sigmoids which were oriented in one direction and when i combined them i got a tower open tower in one direction then i had these two sigmoids which were oriented in the other direction and when i combined them here i got the tower in the other direction then when i added both these structures i got this weird structure which had three levels a level zero a level one and a level two and then again passed it to a sigmoid such that it allowed only the level one and above outputs and hence i got the tower which was about one right so that's the tower function that i got out so i got a 2d tower function from this network you can go back check out the illustration check out this network and convince yourself that you can get a 2d function it's the same thing that we did for 1d just like one more level of complexity added right and you can see that the number of neurons in my middle layer has now increased right this has become four neurons okay and also our initial goal was to prove that a single layer neural network should be sufficient i have not proved it for a single layer right so what you will have is one layer where the number of neurons will keep growing so it will be like a three layer network which can approximate any arbitrary function right now you could take this idea further and prove for the one layer case also but as i said that proof will be a bit complicated but at least i have shown you that with three layer network you can approximate a 2d function right to arbitrary degree of precision right so that's what we have been able to do so now what will happen is the 2d 2d towers that i got out of that tower network i can put many of these together right and construct my uh function as way i desire right and i can adjust for the thickness of the towers to make it more and more a better approximation right so that's the idea so this is what the universal approximation theorem says and the reason it is important is that in real world you will have these complex arbitrary functions and now you have a way of coming up with a neural network it still has a large number of neurons right and we'll fix that it has a large number of neurons but if you keep that fact aside you can what this theorem says that for any function that you have any problem that you're dealing with where there's a true function x and let it be as arbitrary a function as you want right you can always come up with a neural network which will be able to approximate it to a desired degree of precision if you add enough neurons in that neural network right and that's the foundation right that's the reason why deep learning is so formula popular because it's saying that you could take a deep neural network which could approximate arbitrary functions and that's what we do in machine learning right you have an x you have an f of x and you want to come up with an approximation f hat of x and now you have found an f hat of x which is just what i showed you on the previous slide right which is a neural network that's the function it's just a composite function you took the input took some sigmoid took some out addition then again added it but it's all a function of x it's a composite function of x you could construct these composite functions such that you could approximate any real function that you are dealing right so that's the basic idea so i think i'm done with this one i just leave you with this thought so for one dimensional input we needed two neurons for two dimensional input we needed four neurons for n-dimensional inputs how many neurons will you need and the answer is obvious but i'll let you think about it right so that's where we end uh lecture three and the next lecture we'll start with uh start talking about deep neural networks and then the back propagation acronym thank you