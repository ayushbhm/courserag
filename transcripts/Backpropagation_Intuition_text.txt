[Music] okay so now let's build an intuition for the back propagation algorithm so so far we have answered the first part of the question which was how to choose the loss function and we have taken two popular problems classification and regression and motivated the choice for the loss function for both of them right now what we want to do is that once you know the loss function now we can start talking about the derivative of the loss function with respect to parameters right because that's the quantity that we are interested in if we know the derivative of the loss function with respect to the parameters for all the parameters on our network then we can simply plug it that back into the gradient descent algorithm and we are done right so that's all that we need to do but now in the case of a deep neural network this is i wouldn't say complicated but there is slightly more work involved than what we had in that simple network that we had right so here's our gradient descent algorithm where we had this and we were just updating the weights using the gradients and we want to apply the same algorithm by replacing these by all the weights that i have in the network right the large number of weights and then be able to compute the partial derivatives of all those weights the same algorithm goes through right so now let's focus on one of these weights w112 and to learn this weight using the gradient descent algorithm we need a formula for the derivative of the loss function with respect to this weight right and that needs some work as i said right now what we want to do is first build an intuition for how what would that formula look like and then do some hard work but then come back to a state where we can then say okay once i know how to compute this this red guy can i compute all the weights in that network in one go right can i compute the derivatives uh the partial derivatives for the loss function with respect to all the weights in that layer once i know how to do it for one weight in that layer right then that would not not require us to compute these many formulae but just one formula and then we could generalize it and also not just that once i know it for one layer can i do apply a similar formula for all the layers right then we are coming up with a very generalized formula and the computations will become easier but that's for later for now i want to understand how to compute this right so we'll see how to calculate that but first let us look at a very simple deep neural network which is a very thin network but it's a deep network right that means that every layer i just have one neuron and i have just one weight which connects my input to that and so on right it just keeps computing it's a very simple computation so you can actually write y hat as a function of x very easily in this case right now it is easy to find the derivative by the chain rule so what is the derivative that are interested in i'm interested in the derivative of the loss function with respect to this weight right and this weight is very far away from the loss function right because it's not uh very close right i would say this weight is closer to the loss function and the weight that i am interested is much farther right but i know how to use the chain rule right so i can compute the derivative of the loss function with this guy which is the green guy dark green guy then the derivative of the dark green guy with respect to al1 and then i keep going down right and this is something that you have done right so if i could write y hat is equal to say sine of cosine of square of say tan of e of log of something right then you know how to compute this derivative right and that's that's the chain this is a very long chain and you see a very long chain here too but you know how to compute that right you just go about it one by one you compute the last guy the derivative of y hat with the last guy that you had then the guy previous to that and so on it just keep going uh in a chain that's a chain rule of derivative that all of you know and now i could even compress this chain right so i could even write it as this this is just a compressed form of this chain rule and why i'm doing this i'm just trying to do this to build the intuition that somehow if i know all of this right then to compute this guy i don't need to compute all of these again right i have suppose i have already computed this right suppose i have already computed this which corresponds to that entire box here right so this corresponds to that entire box there so i can just reuse that i don't need to compute the whole of it again and then i just need to compute this red quantity right so that's the idea that we are going to use in back propagation that we'll have these long chains and that would make the task daunting but then we'll argue that some portions of this chain we have already computed and we're just going to recompute reuse them right so it's not as daunting as as it seems right and now some of you could realize that if there were like multiple weights here and if this part of the chain was already computed then maybe for all those weights you could reuse that computation right if it's not clear at this point don't worry this all will become clear as we keep discussing the back propagation algorithm but that's the intuition that i want at least on this slide whatever is there this you should understand that i had this big blue part but if i had already computed that then i can just reuse that when i'm computing this left-hand side here okay okay similarly for w two one one similarly for wl one one okay uh now let us see an intuitive explanation of the back propagation algorithm before we go into the mathematical details right so i've already told you some intuition that there's this large network of many weights and i want to compute the derivative of the loss function with respect to all these weights i took a weight which was like very far away from the loss function and i made a case that if you could have a chain from the loss function to that weight and you have a chain from the loss function to that weight then you could just apply the chain rule right so let's that's that intuition is already there now let's just strengthen it further right so this is what is happening right you had this network okay i gave you an example x as input you did all this computation because you had some current values of w b w 1 w 2 w 3 b 1 b 2 b 3 these are not the final values these are not the final learned values you're somewhere in the training and based on your current understanding current values of w1 w2 w3 you computed y-hat l not just that you computed some loss right after having y hat l you computed the loss function also and you got a non-zero loss so now you're trying to find out what went wrong i gave the network an x okay it produced a certain output i computed the loss based on that output and my loss is non-zero so who is responsible for this so this loss was sitting here so i will ask these guys first these are the dark green guys that i'll ask first right i'll ask them sorry hey you are not producing the desired output right because if you guys were perfect then my loss would have been zero so why are you not producing the desired output you should take responsibility you should do give me better output but these dark the shaded green guys and they'll say hey what can i do right i what responsibility can i take but i am only as good as the hidden layer before me right because how did i get the dark green guys these are the dark green guys how did i get them i had certain values in the previous layer i multiplied them by the weights and the biases and then i get so this y hat will tell me i can't do anything right whatever these guys gave me i just computed soft max on that and gave you some values so if these guys had given me perfect values then i would also have been perfect so please go and ask them you say okay this is fine that sounds interesting sounds correct that if these guys are not doing the job properly then how will the shaded guide do its problem joint property so you go and talk to these three guys who are these guys w's h and b right so the w and uh b so you ask them what is wrong with you right so the w and b say yeah we understand we have made a mistake because we are the weights we are the only things that can be adjusted in the network and maybe we have not really been adjusted very well so far and hence the output is bad right but then hl says that okay i am also part of this computation but i can't do much right because i am again as good as the previous activation layer after all how do you compute hl hl is again a function of the previous guys right so it's again a function of what these weights were and what were the inputs that got passed to those weights right so then again you go and talk to these guys so again the weights will say that hey we are fine right okay we made a mistake maybe we need to get adjusted we take responsibilities but these shaded red guys they'll again say hey what can i do i am a function of the previous guys so maybe you should go and talk to them right and then of course you cannot pass the responsibility to the input input is whatever it is right you cannot say oh change your input if you want the right output right no i have given you a certain input i expect you to uh give me the output so the responsibility never goes to the input but what you realize is that in the entire network the responsibility lies with all the weights all the yellow things that i have shown here right all the weights and all the biases these guys although they are being computed they are just a function of the weights and the biases so their weights and biases are wrong then these guys are going to be wrong right and this argument flows all through the network and then you find out that the responsibility lies between the weights and the biases right and this is what i am trying to do right so i was interested in finding the responsibility of this weight right the first weight here but instead of talking to the weight directly i first spoke to the output layer then i spoke to the previous hidden layer then the previous hidden layer and now i am talking to the weights right so i have constructed this chain rule because directly talking to the weight of interest is hard right because i've given you that function remember sine of cosine of e of log of something something a long chain if i directly try to compute the derivative it's harder but instead if i break it down into this chain rule it's easier right so that's what i'm trying to do here you are interested in the law derivative of the loss function with respect to some weight you talk to these intermediate guys find out each of their responsibility and then find using those computations find the responsibility of the weight that you're interested in right now where did this jump happen right so it's still this point i was only talking english and then suddenly i introduced this math part i suddenly came to partial derivatives right so how did i make that jump why i want to know what is the responsibility of these guys so how did that responsibility became derivative what does the derivative tell us the derivative tells us is that if i change w a bit how much does the loss change right so if changing w a very tiny bit changes the loss of makes a large change to the loss that means w has a very strong influence on the loss w is more responsible for the loss if i change the w a small amount maybe the loss will decrease a lot right hence derivative or partial derivatives is a good way of assigning responsibility to the weights for the loss because it tells me if i change this weight a bit how much will the loss change and that's what i want to know right how much is this guy responsible can i change its value a bit and drastically reduce the loss function then let me do that and that is what the partial derivative tells right so from that english discussion that we had of talking to every guy we came to this mathematical uh kind of realization of that which is that you just need to compute the partial derivatives and if you want to compute the partial derivative then just as we had this uh detective work that we did we went and spoke to every layer and then came to the last guy we just have to do the same thing which just gives us the chain rule of property right so this is what we are going to do in the remaining part of this lecture the quantities of interest that we have is the gradient with respect to the output layer okay the gradients with respect to the hidden units sorry and there can be multiple hidden units and then the gradients with respect to oh sorry i wanted to change the color but yeah the weights and the bias right so these are the three parts to the remaining of to the remainder of this lecture right we'll first see how to compute this then this and then this right and we want to do this in a manner that once i do it for w11 i should somehow be able to do for all the w's all the weights in the network right this formula should not be painfully computed for every weight in the network right so that's what we're going to do in the remaining part of this lecture and our focus is going to be on cross entropy so our loss function is going to be cross entropy which means we are going to deal with classification problems which means we are going to have the output function as soft marks right so i'll end here and we'll come back and do the entire back propagation in its in the gory details of the mathematical details of it in the subsequent lectures thank you