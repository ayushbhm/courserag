foreign [Music] move to a different algorithm and before doing that we are going to revisit the lp Norm right so LP Norm is Gen the general formula for LP Norm is as given here right and now if uh your p is equal to 2 then you get the L2 Norm right so then you get take the squares of the values and then take the square root right so that's the L2 norm and remember when we had introduced Adam I had said that actually we were using uh L to Norm they're the exponentially weighted uh L2 Norm there and that's why I'm revisiting this because now we are going to see the possibility of using something other than the L2 Norm okay uh all of this will become clear soon so this is the uh yeah so we can visualize this so let's let's fix the norm let's take vectors which have Norm one right and now if you try to visualize it now for L one the shape looks like this right so what does that mean that for any point on this surface the norm is going to be 1 right and you can see that so this point for example here is minus 0.5 sorry 0.5 comma minus 0.5 foreign with this right and now if I substitute that in this formula I'll get the answer as 1. right so all the points which lie on this uh uh Square they all are going to have the norm as uh one right and now if you look at L2 Norm you would have seen this oops yeah so if you look at LD Norm now all the points lying on this circle would have are the enormous one and it makes sense right because the equation that you get is the following right so I'm saying that X1 I'll not use the mod here because anyways I'm squaring okay let me just write the mod plus X2 Square equal to 1 right you are looking for All Points which satisfy this equation and of course they will lie on a circle right so that's that's the how the norm works now what happens is that as uh you take larger and larger Norms right greater than 2 3 4 10 hundred then if you have small values of x 1 x 2 right then these computations become numerically unstable right because any small value raised to a large value will become very very small or the other way around right if they are greater than one again if you do 1.1 raise to 100 it will just blow up right not maybe 100 but a slightly higher power than that it might blow up right and then you don't want that right so that's why L2 Norm is the most preferred uh Norm uh but as P keeps increasing and it tends to Infinity then something interesting happens so it's as P becomes Infinity then the norm just boils down to the max right so if you look at L Infinity Norm then it's just taking the max of the values that you have right so in the vector you have the values x 1 x 2 up to X N and L Infinity Norm is just the max of this and you can quickly prove why that is the case so if if I had uh if I wanted to uh yeah so let's see so I'm talking about x 1 raised to P plus X2 raised to p all the way up to X n raised to P right and then the 1 by pH root of that okay and I want to make a case for y this would be Infinity when P why would this be equal to the max of the values x 1 x 2 x n if P tends to Infinity rate so if I take limit P tends to Infinity I'll just give you a rough fight outline of the proof I will not do the whole thing I'll just do the necessary part now what I can do is I can take this X1 by P outside right and let I'm assuming that X1 is the maximum value let's assume without loss of generality we can assume that X1 is the maximum value so this would become X2 by X1 raised to p and so on and you will have x n by X1 raised to p and then the 1 by P at the root of the whole thing right now as P tends to Infinity what will happen is as each of these quantities is uh each of these quantities is actually less than one right because X1 is the largest value so all the other quantities divided by X1 is going to be a quantity less than one so now if you're going to raise this to the power of P all of NP tends to Infinity then all of these terms will disappear right so then you'll just have X1 raised to p and then the pth root of that which will just be X1 so what you get back is just the maximum element that you had in the vector right so that's why LP Norm the L Infinity Norm is just like taking is equivalent to taking the maximum value and that's all we need to do right so that's that's easy to compute there's no numerical instability there right so that's uh that's an interesting result okay so now we understand about LP Norms in general and we understand about L Infinity which is just taking the maximum value right now what is the point that we're trying to make here right so recall the equation of VT this was the equation of VT that we had and this was as we had seen equivalent to doing right uh beta so what if I just expand this right then what we had seen was we are doing uh beta 2 uh raised to at time step T right so T minus 1 into Delta W 0 squared Plus beta 2 into T minus 2 Delta W 1 square Plus all the way up to 1 minus beta Delta WT square right so this is what we had seen so your history VT is actually if I just ignore all the beta right let me just ignore all the betas then this is just like the L2 now right you're just taking the keeping the history of gradients and then you are taking the L2 Norm of that is just exponentially weighted average L2 Norm that you are taking right now the point is instead of doing that can we replace this by L Infinity norm and does it have any parameters that's the thing that we want to check okay uh yeah so that's where we are headed so now if we place it by Max then what it boils down to is that we're just going to take the max of these quantities right and that in turn will just boil down to whatever we had at the previous time step because you're taking the max let's say you're already taking the max of these two guys uh when you are at time step two you have taken the max of these two guys and retained one of those then again whatever was the max that you took at time step 3 and retain one of those right so then it just becomes like taking a pairwise Max whatever you had at time step T minus 1 the history that multiplied by meter and the max with the next guy right so that's all we are doing notice that here we are not scaling uh the gradient by 1 minus beta right so that's a change that we have made right so then essentially now this becomes the L Infinity Norm of the history of the gradients the exponentially weighted L Infinity Norm that you are taking here and if you're doing that uh let's see what happens so that's a change we are making right we are saying that instead of using the L2 Norm for VT we can use the uh L Infinity Norm which is just like taking the max it's also computationally very simple and let's see if that leads to some benefits right and now we don't need to take the square root because in the L Infinity Norm the max value comes out by P raised to 1 by P right so I have already taken the p through it and then you have got the max value so there is no more square root here square root is only associated associated with the L2 Norm but the proof that we saw on the previous slide said that the lp Norm is just the max right this after that there is no squaring square root of any of those things right so we do not need the square root here so this is what the WT plus 1 would be and we did not uh do any bias correction for VT right we just took we are just going to take VT as it is because it turns out that the max Norm is not susceptible to the initial zero bias as opposed to the exponentially weighted average norm that we have right so let's see why that is the case so this is what happens right so now if you have the max Norm so again what will happen is that your m 0 is sorry M minus 1 at initialization is 0 and now in this case my at time step 0 I had the value 0.5 so my Max Norm is going to be the max of these two quantities right and then it will end up being 0.5 so I'll start from here right and so max is of course more uh zigzag it's not as smooth as you had in the L2 Norm it's understandable because you're just moving to the maximum values and say this is so so that's why it's not susceptible to the initial zero bias so we don't need bias correction when we are using the max knob right and we are taking the max between beta V time uh beta times the history uh that means beta times the max up till time step T minus 1 and then the current value right so this history is again decaying exponentially right because every time beta gets multiplied by the history but the current gradient you are not multiplying by anything okay okay so that's the justification for not having bias correction okay now let's see now suppose we initialize w0 so that's that so now we are trying to see so we have proposed that we'll use the max Norm of for VT instead of using the L2 Norm now we want to see is there any benefit of doing that right so suppose that we initialize W such that the gradient at w0 is higher we just did a random initialization and it turns out that your initial gradient was high right and now suppose further that the gradients for the subsequent iterations are all zero right for a few iterations they are zero so this is what is pectorially depicted here right so you have the gradient along this axis your initial gradient was high and then for the next few time steps your gradient is zero and this could be possible because X is part so it's a x is passed so you in the next time steps when you are seeing the input that x is 0 so the gradient is going to be zero okay so this is again a typical situation that you encounter in fact this is the situation that we have been arguing about ever since we started the uh discussion on adaptive learning race right so now ideally we don't want the learning rate to change right uh when the gradient is zero so now you have looked at uh an update and your gradient was Zero you are in this time step and your gradient is zero you don't want to do anything to the learning rate right why do you want to change the learning rate at this time step right because you have not doing anything you did not get any update but now in the case of uh in the case of the max value this is what will happen right it will not change right because you're taking uh sorry please since you're using the max your current gradient is 0 right so then your goal is just going to go by the max value so far and hence the the history is not going to change right because this is going to be uh zero so that is uh uh what happens in the case of Max now let's see what happens in the case if you are using the L2 Norm right as we were using earlier let's see so let's look at the example now I've just changed the example a bit suppose that 50 of the inputs that you see are zero right so after every uh input here every time step you are seeing a every alternate time step you are seeing a zero input and hence the gradient is also zero right and let's say the other time steps you are seeing a high gradient it's just an artificial example constructed to prove a point so let's just see what the point is so ideally we don't want the learning rate to change as I said when the gradient is 0 or when the input is zero so in the case of Max initially you had say V 0 at time step V 0 you had Max of 0 comma the current gradient so your V 0 was 1 and your ETA was one over one so it was one right so ETA was uh I assume that the initial learning rate ETA naught is also set to 1 so it will be ETA naught divided by VT and since V T is one it would also be one right this is this clear yeah it's just a minute I'll just write it down remember that ETA T is ETA naught divided by VT right and I assumed that ETA naught is equal to 1 just for the sake of Simplicity okay so now ETA T remains 1 in this case uh becomes one in this case now let's see what happens at the next time Step at the next time step my derivative was 0 right so my V1 is just uh 0.999 times my history right so that's almost one so my effective learning rate again becomes 1 by V T which is 1.001 so my effective learning rate has not changed much right and same for time Step 2 now the gradient is one right oops yeah now the derivative is one and my history was 0.999 sorry this should have been Max of 0.999 uh multiplied by 0.99 because it is beta times VT minus 1 but it doesn't matter the max would anyways have been one so my V2 is 1 and my learning rate again becomes uh 1 right so my learning rate is not changing and the same thing will keep repeating right the next input is 0 so again it will be 1.001 and so on right so this is what my learning rate is looking like so remember on the x axis I have uh so this plus one you need to do right so uh it was easier to draw it like this so this is a plus one so this is 1.001 1.002 and so on right so my learning rate only changes from 1 to 1.001 which means it almost remains constant as I am moving across and that's what I want right I don't want the learning rate to change too much for the zero inputs right that's and that this is ensuring that right now let's see what would have happened in the case of I'll just do the full stuff and then we start using the pay point yeah so now when I'm using this L2 knob okay at time step 0 I get point zero zero one and then I use the bias corrected value and then my initial learning rate at time sorry my learning rate at time step uh 0 is going to be 1 okay that is same as what I had earlier now at time step 0 when I got the 0 input now you see that my learning rate is still changing right which should not have happened because I did not do anything at this time step so why should I change my learning rate but my learning rate is changing because of this exponentially moving uh L2 Norm average that I'm taking right so now uh it increases the learning rate even though the gradient was zero and this is what the learning rate profile looks like it was one then at time step two for no reason my input was Zero still my learning rate became 1.4 right and then time step 3 it again decreased but again for a zero input it increased and so on right so it's more uh it it's still making changes when I have a zero input and I don't want that to happen right so any have many sparse features and if you want the learning rate to kind of not get affected by these parts features then Adam X is useful because it's not changing the learning rate too much for such sparse features right whereas atom is changing okay okay so now suppose this is what your gradient profile looks like so at time step 0 suppose the derivative is this at time step one it's this and so on and so there are 100 time steps here and I've just smoothed in the curve so there should have been 100 points and I have just smoothened those curve drawn a curve through those hundred points right so this is what it looks like and now if I um foreign so this is what happens for RMS property so sorry earlier uh yeah when we're looking at the case of uh VT using the average norm in even in RMS prop we use our average normal so I'm comparing RMS prop with a version of RMS prop which uses L Infinity Norm instead of the L2 Norm right so in the case of RMS prop the learning rate is continuously increasing in this case okay and now let's see what is happening in the case of uh the max version of RMS Pro right so the learning rate is increasing more smoothly in this case as opposed to uh in the case of RMS okay so that's that brings us to the update rule of this Max prop right which is the version of RMS prop with the using the max Norm for VT as opposed to the L2 norm and that's it right so this is as simple as it gets you just replace the uh L to num by the max norm and we can also extend this to Adam when calling Adam access atom X is actually an algorithm that is used in practice right so the key idea here was to use the maximum instead of the editor right okay so this is what the update rule for Adam Max would be so this is Adam you had the momentum term you had the bias correction that will remain the same then you had VT which was uh L to Norm and then you had the bias correction but here you will have me T which is going to be the max knob and since its Max Norm you don't need the bias correction and then you will have that update tool for WT right so everything Remains the Same it's just the equation of VT that changes and VT does not need bias correction in atom X right so that's all there is