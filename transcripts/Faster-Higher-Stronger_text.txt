[Music] now so where are we in the history right we did a bit of back and forth so we came from the 1950s where there was enthusiasm then the and the spring period then the winter period then the more stable period where people were still looking at things there were some success in the 1990s in terms of convolutional neural networks actually being used for real world problems and but still some disappointment in terms of very large networks not being able to train despite back propagation being known and universal approximation theorem being understood right and then 2006 around that time these things changed and then from 2006 to 2012 a lot of other advances happened which gave us a better understanding of deep learning and we were able to train these networks to work for the image net and similar challenges and started showing uh again it's not just gain but starting winning some of these uh prestigious competitions right so that's where we are and then after that from 2016 onwards there was further acceleration right so we wanted to come with better optimization methods so now we know that it's possible to train deep neural networks but now can we do it faster can we lead to convergence faster right what this animation here is trying to show that i want to come to this configuration where the two points on the left-hand side figure exactly fit or the sigmoid function exactly fits to those two points and i'm trying to solve some optimization problem for that and it's taking me a while to reach to the solution right so can i speed that up right of course faster convergence was always a goal all right which existed in machine learning when we had the machine learning approaches which were again solved using optimization problems we always wanted faster conversions and nestrov in 1983 had proposed a method which does better than the gradient descent approach which i had mentioned earlier right it leads to faster convergence now that idea got scaled further and a series of uh optimization algorithms all of which we are going to cover and discuss on most of this not maybe all of this adagrad rms prop adam and adam adam w random and so on right this continues and all of these the goals was to have models or have algorithms which lead to faster and better conversions right you lead in deep learning there are multiple minima possible so can you lead to the can you reach to a better minima and faster time right and there's been a series of algorithms proposed to that which has led to now deep learning models being able to train faster in parallel that also be in progress in what are known as learning rate schedules now we have much more complex learning rate schedules than what we use 10 years back and all of that has led to stable as well as faster training of deep neural networks of course still much more is desired but we have made a lot of progress and these advances are something that we'll cover in the course uh yeah this is still about the better optimization methods and then we also had better activation functions right so earlier uh in the 1980s to 90s there's only the logistic function right this is the first function the blue colored function that you see on the slide here right but since then there's been like an industry of activation functions that has been proposed and each of them with the motivation of stabilizing the training right or leading to better performance and faster conversions right and many of these activation functions we are going to cover in the course tannic is something that we'll see we'll see relu we'll see leaky relu parametric relu and some of these other functions right galu-lu and so on right so many of these functions have been uh proposed and they all have shown to be useful in different contexts right so we will look at many of these throughout the course and all of this is also led to better stability and performance right so so once we discovered that deep neural networks can really be trained there was a lot of interest in that and making them better and better and that's where better activation functions better optimization functions better regularizations i forgot to include a slide on that so regularization in the terms of a dropout or even batch normalization you could think of it as a regularization in some sense have been proposed to improve the training and stability of these networks right so that's what this uh period was about okay so so far we have discussed about uh the progress of deep learning in general and then zoomed into a bit of image processing where we uh or image applications where we talked about convolutional neural networks right the other important category of problems that you deal with in your life and which are popular in the deep learning context is the problems involving sequences so let's look at what these problems are so you encounter sequences everywhere right so okay so we encounter sequences everywhere right if you beat videos or if you have speech a speech is like a sequence of uh phonemes right and we need to do speech processing we need to do video processing and these are all inputs here are naturally sequenced right so we need sequence processing angles of course this need were discovered i mean reid was realized long back and he as is the case with convolutional neural networks very long back in 1982 we had something known as an off field network i'm not going to the details of it it's not something that we'll cover in the course also but it was something which was able to uh do some kind of a sequence processing right more relevant to this course is this what is known as recurrent neural networks where the idea is that you have a sequence of inputs and you want to have some interaction between these inputs right so for example when i'm talking it's not that every phoneme or every microsecond of speech can be ah a millisecond of speech can be analyzed independently it has some connection to what i just spoke previously right so i should have networks which are aware that such dependency between the inputs or interactions between the inputs are useful and should allow for those interactions right so the jordan network which is one time of recurrent neural network allowed the output at each time step to be fed as input for the next time step and that will allow interactions between different times so think of a sequence of images or sequence of frames which forms a video and you want to do some kind of video classification at the end so it's important that every sequence interacts with the other sequence so that it understands what is happening in the entire sequence as a whole so rick canadian neural network allows us to do that it was it entered natural language processing in around 2013 or 14 but this idea has been there much before that right and there were two different types of neural recurrent neural networks which were popular well the question of course is that if they were proposed in 1990 and if it made sense for sequences and people were interested in a lot of nlp and speech at that time both nlp and speech have sequence of words and sequence of phonemes and why were these networks not being used again the same challenge a lot of work around that time which showed that it's very hard to train these networks in particular there was this problem of exploding and vanishing gradients which will again cover in the course which did not allow these networks at that time to be used for uh real world problems right but as things changed after 2006 2006 is when we discovered hey we can suddenly strain deep neural networks and in the next seven eight years we made uh advances in optimization algorithms activation functions uh regularization and so on things started improving and uh again in 1997 uh sorry i should have before going to 2006 i should have finished this in 1997 uh long short term memory cells lstms was proposed which again alleviated this problem of uh exploding uh of vanishing gradients in particular and but still they were not being used for real-world applications due to the other reasons which i had said right i mean compute was a challenge large scale data sets were a challenge but now when we enter 2014 a lot of things fell into place right deep learning got better we had a better understanding of how to stabilize the training we had much more compute by this time gpus had already entered the scene we had much more data large-scale data sets which allowed training these networks right so and that's when in 2014 uh deep neural networks or recurrent neural network based models entered nlp and since then there's been almost no looking back right all these traditional models which were probabilistic and statistical in nature of course you also have some probabilistic modeling here but the statistical machine translation models which were popular at that time uh were almost completely replaced by these recurrent neural networks and lstms right and then in 2017 came by what are known as transformer networks and we'll have a separate section on them later on which then slowly started replacing these rn and lstm it was but it took a couple of years at least for that transition to happen so i think around 2014 to 2019-ish is when this rnns and lstms which were proposed way back in 1990s and 1997 dominated the at least the nlp and speech uh scene uh as far as uh uh deep learning was considered concerned and then they were slowly replaced by transformers right and today we are in the era of transformers and we'll have a separate section discovering uh discussing the history of transformers later all right so i think i'll end this section here where we looked at a new type of input sequences and again the same story nothing new these models the need for processing sequences was felt much earlier the relevant neural network models itself for proposing speech were proposed in 1986 1990 1997 but at that time due to various reasons it was not conducive to train them on large scale data sets with good compute trained for longer durations they had also this vanishing exploding gradient problem some of these things were fixed in the period of 2006 to 2014 and then they became popular in the sequence learning problems in nlp and speech it and even envision for example captioning a video or an image okay now the next thing that i would like to talk about is another class of problems where deep learning became very popular which was at game playing right and we started seeing various results where in extremely complex game environments uh rl reinforcement learning agents started beating humans right so let's start around 2015 you have these atari games and reinforcement learning was always used for uh playing games so there was many uh classical textbooks on that where you had certain gaming environments like the pac-man environment or the ping-pong environment and you could train a rl agent to kind of learn how to play those games so these are those console based games that were popular many years ago right and in 2015 came this deep reinforcement learning paradigm which was able to play all these atari games and win all of them hands down right and beat humans at them what i mean by that then uh the one popular breakthrough which was which came was this uh the reinforcement learning agent which could play this game of go right if you're not aware of go then in very layman terms i would tell you that it's something much more complex than chess you all know that chess has many rules and there's an exponential number of possibilities there depending on what i play and then there are many roots or many parts which open up right so go is something which is even more complex than it is again a strategy based game and unlike previous algorithms which used brute force kind of strategies this reinforcement learning agent based on deep reinforcement learning was able to beat the best go players at that time right in 2015 and this of course gained a lot of popularity and attention and deep rl became a very popular area and it still continues to be right then in 2016 again using the same uh deep reinforcement learning based players or agents people were able to beat professional poker players right i mean these are the people who participate in these world championships in pokers and they were able to beat many of them in the game of poker right similar advances in much more complex strategy games which have complex visual environments as well as complex strategies which for example the defense of the ancients some of you would have played it and openly i did this very interesting thing that it just made an agent play with itself right and it played like 10 000 years of gaming and just imagine that i mean we play i mean there's no way you can compare to this kind of scale right and so 10 000 years of training they did on of course a distributed set of compute and they were able to demonstrate that just by playing with itself the agent is able to get this expert level performance and beat the best dota players at that time right so again very complex problems very strategy based games with the inputs also being complex you're getting the inputs in form of images right it's not like some signals but an entire image is being fed and looking at the image you're trying to decide what the next step is going to be and there are various possibilities there various strategies to learn it's able to learn all of this and being able to beat humans right and then around the same time to kind of popularize this research in rl right where are these complex games which require this 10 000 hours of playing and so on maybe only the big deep tech companies can do that but can you have simpler environments which have a lot of this complexity but you can you use those environments to train some of these or advance the research and deep reinforcement learning so that's why this open ai gym was released which became a toolkit for developing and comparing reinforcement learning algorithms because this entire deep reinforcement learning based work is something that we will not cover in this course but it's important that you are aware of that that's one branch which you could explore as you continue on your journey on deep learning right similarly there was another gym retro right a newer version of that which had thousand games very different environments which people could experiment with right and again complex strategy games then uh similarly alpha star which uh has to balance to learn short-term and long-term goals these are all games which you are aware of these are complex strategy games which you play and you can imagine as humans also we take a while to understand what needs to be done and we are never perfect in any of these but now rl agents are being able to learn how to be play these games on their own right this was again a very interesting uh demo video which uh or of the work that they had done where they had these agents the the blue orange guys that you see in the screenshot there where you have this a flat floor right you can imagine it's a floor in which you could have some places where you could hide you could construct a wall so that you can hide from the person who is chasing you if you are a chaser you could destroy a wild wall to find to look for people inside or you could build a ramp you can see that yellow ramp in the figure so you could climb up the ramp and peep inside the structure and see whether someone is hiding right and all of this the rl agents were able to learn uh quite well right that is the ability to chase and hide build a defensive shelter break a shelter use a ramp to search inside and so on as a very interesting demo video that you should see right and what is emerging in all of this the underlying theme is that very complex decision making strategies were being able to learn using these different deep reinforcement learning agents and while all of these specialized in like one task then came this mu zero which can do multiple things right because you always aim for always one argument is that hey your ai agent can do only one thing whereas a typical human and you might be able to beat a human in that one thing but a human can do many other things right so if you have a good go player go rl agent maybe it beats the best go player but that player can also play chess that could also play badminton could also play table tennis and so many other things whether our agent is doing one thing itself right and so now there's this push to have what are known as master of all models so this mu zero was the first among the first step in that direction which could not only play go but also chess shogi and atari right these are again all uh decision making games and so it could do multiple of these games uh in a single rl agent right so going towards more uh general intelligence right and becoming a master of many uh traits right and similar research is progressing where you now have this player of games a general purpose algorithm which learns to play in different conditions it's a lot of progress has happened in this deep reinforcement learning over the years none of this will cover in the course but this is important for you to know as i said and you could explore this on your own once you understand the other basic concepts in the course so i'll end this module here