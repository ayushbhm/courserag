foreign [Music] with this idea that we want to estimate the parameters and we reintroduce the concept of error and what we understood is that in our random guess algorithm or the guesswork algorithm right we were actually navigating the error surface although we were not realizing we're just moving from here and there and at times we are making a mistake where we went up the error surface right or went to a higher error and we don't want that to happen right we want a journey which is smooth that you keep going down as far as the error is concerned and then settle to a point where the error is minimum right if not zero so we need a more principled approach and that's what we're going to do in this module right so this is the goal as I just stated that find a way of traversing this error surface so that we can reach the minimum quickly and without a lot of back tracing or like going back or not back racing actually but random jumps here in there which don't necessarily decrease the error at each iteration right and we don't want to do this brute for search either right so that is not possible right we cannot plug in all the values of the parameters and then see that value which gives us the minimum value that would be the Brute Forces we cannot do that and we have argued why that is infeasible there are infinite values there okay so let's look at the setup now right so we have our parameters in the toy Network where W comma B and I'm just going to put them in a array or a vector and call that Vector as Theta right so now Theta is my parameter of vectors and now I'm looking for changing this value right so just as is always the case I don't know what the correct Theta is so I'm going to start off with some random value for Theta and when I start off with this random value of theta I want to know how much to change it so that at this Step at least I'm looking at it one step at a time at this Step at least the error should decrease right so I'm looking for that change in Theta which is nothing but a change in W and a change in B so that I get a new value of theta and then my loss is hopefully decreased I'm not sure of that yet but that's what the goal is right so this is my Theta some Vector Delta Theta is also a vector it is also a vector in R2 this is a vector in R2 and similarly this is also a vector in R2 right now these are two vectors now what I'm going to do is my change is going to be just given as Theta Nu is equal to wherever I am currently I'll move by this change right so Theta Nu is equal to theta plus Delta Theta okay so what is happening here is that I was here and I mean sorry this this is where I was and I then added this vector and what has happened effectively that I moved in the direction of Delta Theta right I've come closer to Delta Theta and we know this from the perceptron Rolling algorithm also when we add it it comes closer to it right so now instead of uh moving fully so let me just delete this right so instead of moving fully uh in the direction of Delta Theta what I'm going to do is I'm going to be a bit conservative and whatever change you tell me to do I'm not going to do that full change I'm just going to multiply that change by a very small value which is ETA so essentially what that does is the following right so this is my Delta Theta Vector the black one I am going to multiply by a small scalar so essentially the magnitude of the vector will decrease the direction does not change so I'm still going to move in that direction but I'm just going to be a bit conservative and take a small step right one small step at a time right so that gives me my update rule which is if you tell me what is Delta Theta then I'm going to move by a small value in that direction right that's that's what my update rule is going to be now the question of course is what is the right Delta Theta to use right I was earlier what was my algorithm in the random guess I was just coming up with some Delta Theta just moving there oh and then realizing oh the error has decreased so let me just go back and then try to find some new Delta Theta change that and I mean change that much and then find a new value I don't want to do that someone has to give me the right Delta Data to use and it turns out that what is the right Delta Theta to use the answer from this comes from the Taylor series right so let's see first what is Taylor series I'm sure some of you have seen this in high school well let's quickly refresh this right so Taylor series as I'm just going to read out the definition here is a way of approximating any continuously differentiable function right and this is important for us because whatever loss function we are dealing with is a differentiable function because we have the sigmoid neuron which is differentiable and then on top of that a loss function currently is the squared error loss which is again differentiable right so this is a differentiable function that we are dealing with and this differentiable function may have any shape right it could have a shape like this it could have any shape it just smooth and differentiable right and what Taylor series does is it gives us a way of approximating this using polynomials of degree n right and the higher the degree the better the approximation now this part may not be clear right what do I mean by approximate using polynomials of degree N I should have circled n also that part is not clear and we'll fix that right let's try to understand what we mean by that okay so I'll delete some of of this okay so here's a function right so let's assume this is my w I have a single parameter for a use of explanation and this is my loss function right this is L of w suppose okay and what it's saying is that as W increases L seems to increase right so that's that's okay whatever this function is I don't care about it but this is how the function looks like okay now Suppose there is a point uh W naught okay at which I know the loss okay and where is this coming from how does this relate to our original R uh uh our goal right which is to find uh a new value of uh W says that the loss is less right so that will soon get related but for now I'll give you some intuition right so Suppose there was only one parameter and I timed step 0 I started with the value W naught and I could compute the loss I don't know the full loss surface but I could compute the loss right now I'm looking to change this value W naught right and I want to decide whether to go here or here so that my loss decreases that's what the goal is and we'll relate all of this discussion to that goal uh soon right in the figure it's clear that I should move to the left because there is where the loss is decreasing right but that's that's what we are trying to do so now if I know this W naught what the Taylor series does is that if I know a point w okay which is W naught plus sum Delta okay if that is the point right so then Delta is equal to W minus W naught right and you see that W minus W naught quantity everywhere here right so if I know the value of the loss at a certain point then I can approximate the value of the function in a neighborhood around that what do I mean by that this w if this Delta is very solid then W lies in the neighborhood of w naught right so in that neighborhood I can approximate it by using this formula right and the interesting thing about this formula is that it has this uh first order term it has a second order term third order term and so on and I can cut off the formula wherever I want if I cut off the formula here I am doing a second order approximation if I cut off the formula here I'm doing a first order or a linear approximation so let's see what happens when I do a linear approximation okay so again uh delete stuff on the slide so this is what the formula looks like I've just cut off the formula at uh the first order terms right so I just have this much okay and now I want to find the value of w and I'm saying that I already know this right I already know what LW naught is right that is given to me now I'm saying that LW is just going to be this oh sorry LW is just going to be this which is whatever was my current value plus some quantity okay and what is this quantity this is L Dash W naught L Dash W naught is nothing but the first order derivative of the loss function W minus W naught is actually the Delta between that right now what is happening here all this looks very confusing to me so let's just try to draw a line here so what am I doing here right I am the actual function has a curve right it's some it's definitely not a linear function right but I'm approximating that function by a linear function I am saying that in this small neighborhood here right say from this is the small neighborhood in this small neighborhood I'm going to assume that the function is linear right now if the function is linear that means I have assumed that the function is of the form Y is equal to MX plus C okay and I already know y naught which is MX naught plus C right I already know that right and what is uh m m is simply the slope and the slope is just the derivative right so this is L Dash W right so this is what uh the slope is right now I want to find the value of y1 right and I know that say uh y1 lets you see right so y1 would be M into x 1 plus C right now if I subtract these two If I subtract the SEC first equation from the second equation I get y1 minus y naught okay is equal to the C's will cancel and you will have M into X1 minus X naught okay and if I rewrite this I'll have y1 is equal to Y naught okay plus M into x 1 minus X naught right and that is exactly what this formula is saying right so y naught is nothing but l w naught m is the slope and W minus W naught is the same as x 1 minus X naught right so that's what it's saying that now you can find y1 by assuming that this is a linear function okay and then you know the slope because you can compute the derivative at that point and now I can get a new value for y I can get the value for y1 where y1 is the value of the function at the point X1 and there is a small Delta between X1 minus X naught that means x 1 lies in the neighborhood of X naught okay and it will soon become clear why this is important okay so everything that needs to be explained has not been explained on the slide yet I still have some visuals on the next two slides which will make things more clear right but let's just go ahead with what we have now right let's just try to understand what it means to have a linear approximation you're just drawing a line at that point right and you're saying that if I just move along this line right so instead of moving along this curve which is what you should have done you are moving along that line and whatever value you get on that line that is the value you are assuming for y1 right so that's an approximation and you can already see right that this value here let me just mark it with a different color right so this blue value here is what you would get right that's the point on the line it's slightly above the red curve right so it's an approximation you will get some error there because you have done a linear approximation right now let me just get rid of all this content and now let's see if I had done a quadratic approximation that means if I had cut the formula here right so what happens in that case this is what the formula looks like right and just as the first one was of the form Y is equal to MX plus C right and here you have a quadratic term so this is nothing but Y is equal to ax Square plus BX plus C right so now I'm approximating the function as a quadratic function right and these coefficients are coming from the derivatives as was the case in the first case the first first order derivative the second order derivative right so what does that mean it means that now I'm approximating the red curve by this uh c n or blue colored curve right and you can see that in the small neighborhood now right this neighborhood now the second order approximation is actually better than the first other approximation right it's more closer the blue curve in that small neighborhood is more closer to the red curve than the black curve is to the red curve right so this is you can just keep cutting the formula wherever you want and that's what the approximation gives how this relates to what we are trying to do all that is not clear right now right now we just want to understand what Taylor series is Taylor series just gives a way of approximating uh the function and you can use any order of approximation n equal to 1 n equal to n equal to three the higher the order the better the approximation okay so now let's uh kind of move ahead from here and see way to go yeah so now here's the same plot right and now I'll do some uh stuff here so this Epsilon actually defines the neighborhood oops right so the smaller the value of Epsilon you can see that smaller is the neighborhood that I am considering here okay and I'll tell you why that is important okay uh now uh this function is log of w okay and now I know the value of the function at this point here right this point here I am going to approximate its value in a small neighborhood around it right and the value would be whatever is on the Orange Line because that's the linear approximation I have made that's the line that I have drawn and I'm trying to approximate the blue curve by this line now when the Epsilon is small that means when I am looking at a very small neighborhood this Orange Line acts as a good approximation of the blue curve right in that small neighborhood that you see between these two points here that is the orange line is very close to the Blue Line but as the neighborhood increases if you come go very far from the point that you were and now you're going to try it so this is your X naught right this this point here is your X naught right and let me consider some x one here right so now what is sorry this should have been X1 so now what it is telling me is that if I try to compute y1 is equal to Y naught right so this point here on the line is y naught okay if I try to compute y1 is equal to Y naught plus the formula that I had at M into X1 minus X naught right then the answer that I'll get is this and you can see that it's way off right it's not not really a good approximation even if uh my X1 I was here right and then what I'll get if I substitute in this formula I'll get the y1 as this point and you can see that it's actually very far off from the true value of y1 right so hence this as the neighborhood goes this approximation becomes bad so that is a key thing that you can do this only in a small neighborhood that means the difference between X1 and X naught should be very small that is one okay now let's see something else also the other thing is that as I increase uh W naught right I mean I'm just changing the value of w naught and you can see that the orange curve is a very good approximation let me just increase the Delta but let me reset this yeah uh now I'm just going to increase uh the neighborhood right okay now even with this increase in neighborhood the orange line still seems to be a very good approximation of the blue curve and why is that happening because the slope in that region right is quite less so my Delta or my M right it's very less and that's helping right but now if I change the value of w a bit right if I go here now here my slope is very high as you can see right the gradient is very high and now in that case my approximation is becoming very bad right so this is what the same window I have not changed the window right but uh now if you can see this is the approximation and this is the true value and the Gap is very high right whereas with the same window if I'm in a region where the slope is small right then still the approximation is quite good you can see that the points on the Orange Line are very close to the points on the blue line because this is a region where the slope is very low right whereas this was the reason where the slope was very high right that's just some observation so now you can see what is actually happening in the Taylor series approximation wherever you are you are drawing a tangent and you are saying that this line given by the tangent is my approximation for the original function and in small windows that approximation works very well right even when the gradient is large right so if my window is small oops then even as my gradient changes right my approximation is still quite good right so in the small neighborhoods my approximation always remains quite good even for high values of the gradient but if my uh my window is large then the approximation becomes quite bad as you can see here right so that's important that Taylor series is only used in small uh Windows okay now that was one now let me just reset this okay and I change the function okay I want to make it the sine function okay and now this is what my approximation looks like now I'm going to increase the degree so this is a linear approximation right now right because I've just cut off the formula at the first order term now this is a quadratic approximation you can see that it looks better right the orange curve is closer to the blue curve right and now what I'm trying to prove actually is the uh utility of the Taylor series or uh how good the approximations get as you keep increasing the degree so this is a degree to approximation where I cut the formula at the second order terms if I cut the formula the third order terms this is what I get right and now you can see that in a very larger neighborhood also right the approximation is quite good right I mean the the true curve is very close uh to the orange curve right let's increase it further further further I think I'm at degree 7 or degree 8 now and it's almost perfectly approximating right so as I keep increasing the degree the approximation becomes better than that and if I go till the very large terms then I'm all I'm constructing the entire function in by as a sum of its parts right and that's what the Taylor series actually says that if I use the full formula then you get the function back right that's the beauty of the Taylor series approximation if I keep decreasing then my approximations become bad and in the linear case my approximations are bad but they're very good in a small neighborhood around the point right that's the main in the small neighborhood yeah my approximations are quite good right so that's the main thing you need to understand about the Taylor series okay so now let's go ahead uh and see what happens if you have a 2d function right so I had earlier now I was just dealing with L of uh w now what if I have L of w comma B right and in this case instead of w comma B I just have X comma y right for convenience uh those are the two axes that I have taken okay but I just the main point is if you want to move to two variables right so now yeah so this is what the curve looks like okay and let me just delete my annotations yeah so this is what the curve looks like now again if I do a linear approximation as you can see here I'm doing a linear approximation then what I'll get is a line in 2D it would just be a plane right so I've just approximated the function by a plane and you can see that in this small neighborhood my approximation is still good right let me just get the right angle so that you can see it properly yeah see now I can see that the my plane is very close uh to the curve but now if I increase this ETA then as was the case earlier my approximation becomes bad right so at this point my approximation of my original point w naught was here then if my new point is very far out of the neighborhood then the approximation would be bad but even in a small neighborhood the linear approximation Works quite well right and again I could just change the value of x and if I go to a region where the slope is changing then the approximation accordingly will become good or bad but that doesn't matter if you keep the ETA small then your approximations would be good right and now if I make a quadratic approximation right I'll just remove the linear one and just keep the quadratic approximation uh I think you're not able to see it yeah so here's the quadratic approximation right and now even if I increase the W the window a bit you can see that my approximation is still quite good right so that's the idea here in fact the original function itself is uh quadratic in various parts right so that's why my approximation is quite good right so that's uh that's that's the idea here so the original function is of course 0.5 sine X Plus 0.25 sine y but what I meant it in some parts it's Contra I mean it's looking like quadratic function right so if you do a quadratic approximation it still would be very uh good right so you can do this whether you have a one variable function or a two variable function of course three variable functions I cannot I'm just trying to get the angle right yeah this is what it is and you can see that the approximation is good so this is what the Taylor series does and you can do this approximation in multiple Dimensions also right so it need not be a function of a single variable it can also be a function of multiple variables okay um second take a pause here I'll let you reflect on this you could also go back to the lecture slides right the slides are also available on my web page and you can just go and try to play around with this digest this idea and then come back and watch the next video okay I'll just take a pause here thank you