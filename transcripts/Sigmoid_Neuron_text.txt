foreign [Music] neurons gradient descent feed forward neural networks and representation power of feed forward neural networks right so uh before I begin just a quick set of acknowledgments so for module 3.4 I have borrowed ideas from the videos of Ryan Harris on visualizing back propagation these are available on YouTube and linked on the slide here and for module 3.4 I have borrowed ideas from this excellent book which is set in the see in the footer which is again available online and I'm sure I've taken ideas and been influenced by material elsewhere so I apologize if I have failed to acknowledge them and if you find anything that I should acknowledge please let me know I'll add it to the slides okay so let's start with the first module uh which is on sigmoid neurons right so where are we right now and what is the story ahead they just get that picture clear and then we can start uh so frankly enough about Boolean functions right so the last lecture and entire thing was about Boolean functions where we were looking at functions Y is equal to f of x where both X was Boolean and Y was Boolean of course when we were looking at perceptrons we said that real inputs are allowed but for most of the discussion we were focusing only on Boolean infosets I want to move away from there and where do I want to move right so we want to look at arbitrary functions of the form Y is equal to f of x okay where X belongs to r n instead of 0 comma 1 or instead of like Boolean and Y also belongs to R right so what that means is that both X and Y contain real numbers right and so now let's let's uh look at an example of this right so what do I mean by I want to move towards these problems so I'll take my favorite example which is the example of trying to find whether I'll find oil at a particular location or what is the quantity of oil I can find at that location and that would drive my uh decision on whether I should set up a drilling station there that's a practical problem and I could imagine that such a decision would depend on several Factor right so I'm looking let's imagine I'm looking at a location inside the ocean right and I want to decide whether this is worthwhile investing in setting up a drilling station there which means I want to know the quantity of oil that I can mine from here right and I would May is base my decision on several factors so say salinity could be one right then I could look at the density of the water there right sorry this pen is not really working well but I'll just go ahead so salinity density you understand what I'm saying then maybe I can look at the pressure at that location I could look at the temperature at that location I could look at the Marine diversity at that location right so all of these are real values so it's all of these belong to R all of these are real values and I might base my decision on N such different factors right and this we have been discussing for a while that whenever I take decision I consider a lot of inputs and now I'm talking about n inputs each of them is a real number so this Vector that I am looking at here is a vector which belongs to RM right so that's the idea now so that's what I meant by this that X belongs to RN and now what is it that I'm trying to predict in this case I'm trying to predict a y which again belongs to R right so y again belongs to R because Y is again a real number I am trying to predict the quantity of the oil that I can get from that location right so this is a real world problem and you could think of various such real world problems right so another example could be if you want to decide what is the interest rate that you should set for a particular customer right as a bank and this often depends on the past history of the person whether he'll be able to pay at a certain rate pay back the capital and so on right so you might have factors like what is the salary what is the family size right what was the last loan that that person had taken how many years that those loan was repaid in what was the interest rate for that loan was there any defaults and all of these you can see are real numbers and then the output is also a real number which you want to decide an interest rate maybe something between say five to twenty percent right that's what you want to decide so this is another problem right so these are the kind of problems that we are interested in dealing with where your inputs are real numbers and your outputs are also real numbers right and we'll take this idea further where we'll start later on in the course talking about images right and even if you look at images what is an image if you look at a 100 cross 100 image these are pixel values which tell you some colors right so these are all again real numbers and based on these real numbers which is a very high dimensional Vector because you're looking at a hundred cross hundred Vector now and I can just flatten it out and I'll have 10 000 values which correspond to the pixel values right so let's assume it's only a gray and white images right so you will have certain pixel values there and this is uh uh 100 dimensional vector and based on this you might want to predict a certain y right so this is again of all of these are cases where your inputs are real and your outputs are also here and you have a high dimensional input where I'm just calling it as RN which is an N dimensional Vector right so this is the situation that we want to uh deal with now given this situation or given these kinds of problems what is our Quest right our Quest would be just as it is on the case of perceptrons when we are dealing with or in the last lecture when we are dealing with Boolean functions we wanted to know if we can have a network which can exactly represent such function right and my definition of exactly represent was very clear in the case of Boolean functions where I was saying that I know what the truth table of this function is and then when I take an input say 0 comma 0 right and I pass it through the network I should get the same output as is decided by this truth table right that was my definition of exactly representing the function and then we also moved on to slightly more real worldish where we said that I would have been given data where I would have different inputs right and the output for that input right and if this data was linearly separable then I should have a network such that it takes any input from this training data and it gives me the same output as is given to me in the training date right in the training data I'm given these X comma y pairs right and then I have a network I pass this x through the network and it predicts a y I'll just call it y hat so I wanted the Y hat and the Y to be same and that's what it means by this network can exactly represent the function right now I'm just losing that definition a bit I am saying that can I have a network which can approximately represent the function right which means what that I want y hat to be approximately equal to y y this approximately y naught exact we'll come to that later all right for now if you want to assume I want something which is exactly equal that's also fine right but I'm just going to be a bit more relaxed in this uh lecture right I would just say approximately is also fine right so that is what I want can I construct a network such that when I give this real valued inputs right and I get a prediction from the network that should be as close to what is given to me in the training data right that's what my quest is going to be okay and just to make it more concrete so in the in the oil mining example there are already many uh locations where drilling stations have been set up and oil is being mined for many years right so I know what was the quantity of oil mine from those stations and I know what their X characteristics were right so what did the salinity of salinity what was the pressure what was the density what was the temperature what was the marine diversity all I know right so what I want is a network such that when I feed any of these X's to the network the Y predicted by the network should be very close to the Y that I actually have right so that is what our Quest is going to be I'm just defining the problem I'm not telling you a solution at all right I'm just just about what the story ahead is going to be okay so let's move ahead from here uh I better delete a few of these things so before answering the above question right we'll first have to graduate from perceptrons to sigmoid neurons right so that's the first thing that we're going to do so uh recall that when we are dealing with perceptrons a perceptrons fires when the weighted sum of its input is greater than the threshold right unless we had this thing that Theta is equal to minus W naught right because we move it to the other side of the equation okay so this is what uh perceptron like a single input perceptron looks like I have an input I have a certain weight associated with the input I have the output and then I have the weight uh I have the bias or the threshold which is say uh uh the minus of the threshold is 0.5 so minus of w naught is 5.5 right and now this uh neuron this perceptron will fire if the weighted sum of my input which is just W and X1 in this case is greater than 0.5 now I'm going to say that this logic used by a perceptron is actually very harsh right now what do I mean by harsh harsh is not like a mathematical term so what do I mean by that right so let us return to our example of deciding whether we would like to like or dislike a movie or whether uh yeah whether you like or dislike a movie right now let's say we have basing our decisions only on one input which is the critics rating and the critics rating is something between zero to one right on a scale of zero to one a Critic has given some rating and we look at that and decide whether uh we want to watch the movie or not right now if the threshold is 0.5 and let's assume that the weight is one because there's only one input so it doesn't matter what the weight is so it will just give it one and uh what now in this case when the threshold is 0.5 and the weight is one so what about the case when the critics rating was 0.49 the output of the network would be dislike right because 0.49 is less than the threshold so I will not like the movie but now if I had a movie well it's a Critic rating was 0.51 then my output would be like right and this is why I say that this is very harsh right this is not how you would take decisions in real life that 0.49 is not like but 0.51 is like there is a very little difference between in these two right when you wouldn't base your decisions like that what you're looking for is something more smoother than this right which calibrates that okay 0.49 is uh good 0.5 minus slightly better right it should have that calibration right so that is missing it's just very harsh that and I'll show you a figure on the next slide which will make this very clear right and that's the point I want to make that is not a characteristic of the problem that you have chosen it's not that about movie ratings or it's not about the way I have chosen this threshold or the examples that I have given it's the nature of the function itself right and I'll tell you what I mean by that so this is what the perceptron function looks like so uh what I have here this is my z-axis okay so on this axis I have Z where Z is the weighted sum of the inputs okay so this quantity I'm calling Z which you have seen is the weighted sum of the inputs okay and this is my threshold and I know that when Z crosses the threshold my output is going to be 1 and when Z is less than the threshold my output is going to be 0. so there is going to be this sharp increase at 0.5 and that's what was happening in our example still 0.49 I was not so upbeat about the movie the moment it cost 0.5 I said okay like this movie right so this is the nature of the function itself where if you take this weighted sum and it crosses a threshold you say one if not you say zeros this if else nature is what is bringing this discreetness right we're just saying that okay it will suddenly become one right so it's the nature of the function itself this is a step function right and there'll always be the sudden change whenever you have a set a threshold function right and as I said like for most real world applications we don't want this we want the decision to be much smoother where the difference between 0.49 and 0.51 is not yes and no right it should be something more smoother than that right so let's see how do we get there now this looks like a smoother version of the function right so what is happening here again this is my z-axis okay and this is what Z is right and now I have not explained what this function is I have not given you the formula for the perceptron I have given you a formula that Y is equal to 1 if something Y is equal to zero something I have not given your formula but for now we'll just appreciate that this looks more realistic right that it's not suddenly changing its decision it's gradually moving from zero to one right it's not like query uh sharp as was in the case of the earlier function right so to get this kind of an effect we'll now introduce what is known as the sigmoid neuron and sigma 8 function the function Sigma is like a family of functions and here's one member from that family which is called the logistic function later on in the course we'll see the tan H function which is also a member of Islam right and let's see what is happening here so this is this is a quantity which is familiar to us right this is the weighted sum of the inputs and the threshold okay this quantity is familiar to us and we have been writing it as summation I equal to 0 to n W naught X naught right it is the same quantity that we have been writing and this quantity we have actually been writing as W transpose X if you remember right where X is the vector from X 0 to X N and W is a vector from W 0 to WN right so this is just the dot product right so the quantity that I have underlined is just this quantity that we have been dealing with so far okay now let's look at this function a bit carefully so what happens when w transpose X tends to Infinity okay so this is as I go along this axis this is where W transpose X will start trending to Infinity rate because Z is what I've defined as double aspects X right so what what would happen as it goes towards Infinity if I substitute the value Infinity here right so it is 1 over 1 plus e raised to minus infinity right that would be 1 over 1 plus 1 raised to e raised to 1 1 by E raised to Infinity right should become 1 over 1 plus 1 over e raised to Infinity right that's what this would be and this term would become 0 so you'll just have 1 over 1 so that would be one and that's exactly what is happening here as the value of this W transpose X is increasing the function is approaching 1 right and uh what would happen if W transpose X tends to minus infinity this would become 1 over 1 plus infinity right so in that case the denominator would become Infinity so 1 over infinity would be 0 and so as I go in this direction so as I go in uh this direction which is minus infinity my function starts tending to zero right so that's why this behavior is there and now again let's just do one more thing if W transpose X equal to 0 if I substitute that value then I get 1 over 1 plus e raised to 0. right and that would be half right so that's what you are seeing here okay yeah so that's that's what is happening here uh so for this discussion just let's assume that W 0 was 0 right so that this uh so that all of this is in sync right so that this is this disappears and all of this Falls in sync if you have a w naught you can have a similar explanation it's just that the graph will move a bit right so I'll leave it for you to figure that out but the what is the important thing is how do you get this s shape at large values it saturates at small values it saturates and it goes smoothly from the small values to the large values right so that's what is happening here and we no longer see this sharp transition which we are seeing in the uh step function earlier or the perceptron function right let's just delete this also the output Y is no longer binary right but it's a real valued output it's not zero or one many values between 0 and 1 are possible now the other interesting thing about this function is that since the values are between 0 and 1 right so what is the other quantity of Interest which lies between 0 to 1. probability right so probabilities lie between 0 to 1. so now this output of the sigmoid function we could interpret as a probability so now when it gives the value 0.49 I can say that there's a 49 chance that I like the movie earlier I was saying that there is zero chance I not like the movie when it outputs a value of 0.51 I can say there's a 51 so one person chance that I like the movie and now this makes sense right the difference between 49 to 51 percent makes sense right earlier it was 0 and 1 when it was 49 and 15 so that harshness has disappeared now and more importantly I can now interpret these values as probability and this is something very important because in most modeling problems that is what we would want to do that you have to predict a certain binary value and you would like to predict it as a probability that this not harsh zero and one but this is a 49 chance of finding oil there there's a 49 chance that this person will not return the rule and then as a domain expert you can decide whether you want to invest in that or not right okay okay so that's that's what it is and this is how the two functions look like very similar right the inputs are actually similar as you can see the weights are also available you have an output you know that these are all real values you know that the output is real value right in fact in the perceptron also you could have had real values it's just that we were only looking at Boolean but the main difference is in this function here right you have the sigmoid function here and you have the step function there and that step function is obvious because of the formula that we were using which is this Y is equal to one if something Y is equal to 0 if something zero then go to one right whereas now you have this smoother formula which we just saw on the previous uh slide uh which is just one over one plus exponent of something right and that gives you this smooth function right and even if you look at the plots then you see that the uh perceptron function is not smooth and hence not differentiable at this sharp change at the value where it changes whereas the uh sigmoid function is smooth it is continuous and its differential so now why do we care about continuity why do we care about differentiability so for a large part of this course calculus is going to be the hero right so whatever algorithms that we study today and going forward in the next few lectures everywhere calculus would play an important role in particular derivatives would play an important role and hence it's important that the functions that we deal with are differentiable right so that's where I leave it for now I'll not delve further into it and you'll get to know why differentiability is important because I'll soon start taking derivatives of something right and if I'm taking derivatives of something in everything in the calculation is not differentiable of all the functions in world are not differentiable then you can't take the derivatives right so that's why it's important to have differentiable functions and that's why we have it's good to have the sigmoid Neutron okay