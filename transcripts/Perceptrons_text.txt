[Music] so let's begin the next module which is on uh perceptrons right and uh just again talk about the story uh so far and the story ahead right so so the one question which i asked is like what about non-boolean inputs right so we have so far been focusing on the case where the inputs are only boolean so we'll have to see what happens if the boolean inputs are not boolean then the second is in this toy example is the boolean functions that we are looking at so far we were just hand coding the threshold i gave you the example of the and function then asked you what should the threshold be and you are able to quickly calculate uh in your head of course you're doing some calculations in that case it's not hand coded but it's just like you worked it out by hand and you arrive at a solution but do we always have to do that is it like won't it become cumbersome in complex cases then the other more i wouldn't say more important but the other slightly different point here is that when we are looking at the uh mccaulick pits neuron right all the inputs were equal right and that is often not the case right when we make decisions if you are making a decision based on 10 different inputs some of them would be more important than the others so there's always this one like a few important things based on which you might which might sway your decision one way or the other while there might be many factors on which you are basing your decision and there is no notion of importance all the things like is it training am i in the mood is the movie is good all of these inputs were some of given equal weights right and if you go back to the biological neuron again there was this connection there and that connection is actually the weight or the strength of the connection between two neurons so there also it was of all the neurons giving an input to one neuron maybe some had more importance than the others right so that notion of weight or importance is not being captured and the last is i ended the previous video saying that could there be not non-linearly separable boolean functions and we'll try to address that what what are these functions and are what do you do if they're not linearly separable right so not maybe in this particular module or video but these are the questions that we'll try to answer over the course of this lecture so so you had this mp neuron or the mecolic bits neuron and then came this perceptron right which was proposed by frank rosenblatt again we had seen this in the history around 1958 and the main differences were the falling and they are some of them are already obvious in the diagram itself that earlier the neurons the inputs are only boolean but now the inputs could also be real that was one difference the other was that now you have these weights right so these weights could tell you which inputs are more important than the other and there's also now that you have these weights there's also a mechanism or a learning algorithm to learn these weights right so you might vaguely have an idea of which inputs are important which are not but typically you would want to learn this from data that in the past maybe while making decisions i have given more importance to who is the actor in the movie as opposed to who is the director in the movie right so maybe these things you want to learn from data so these weights are now learnable right so you have when we're going to look at a learning algorithm for learning these obvious so like three main differences now you have weights the inputs are no longer boolean you could have real input so that was one action item that we had and we have taken care of that in this model and the third is a learning algorithm so far we were talking about hard coding or like like mentally calculating what the threshold is but now we look at a good learning algorithm for learning the weights right so what we're going to learn in this course is not the original perceptron or the classical perceptron model but more the perceptron model which was later refined and analyzed by minsky and pappered again we had referred to this in the history in 1969 so that's the model that we're going to uh be talking about is just to make it clear that what we are talking about okay so let's go ahead right so what does the uh perceptron model look like so it's it's very similar right in form at least so you have y is equal to one if the weighted sum of the inputs is going to be greater than equal to threshold right so earlier oops in the uh mcculloch pits neuron or the mp neuron there was no notion of weights so it was just that if the sum of the inputs is greater than equal to threshold but now i have if the weighted sum of the inputs is greater than equal to threshold and that's where i get to assign weights to input so if the input value is small but the weight to the input is large then still the contribution would be higher right and notice that this index is over 1 to n because there are n inputs so from the first input to the last input right so 1 to n that's where the indices are coming from and it's going to be 0 if this weighted sum is less than threshold right so again a very similar form to the mp neuron now on the rest of the slide i'm just going to do some rewriting to arrive at a more neater form right i don't want to keep writing this let's see where i reach there so if i rewrite the above i'm just going to take theta on one side so this minus theta greater than equal to zero nothing wrong with this and minus theta less than zero right so i have just taken theta on one side now uh something interesting will happen so now the theta has disappeared looks like the theta has disappeared but actually it hasn't because i have added this i equal to 0 right so what i am saying now is that my sum earlier was w 1 x 1 plus w 2 x 2 plus blah blah blah up to minus theta greater than equal to 0 right that's what this equation was saying now i am starting from w naught right because it's i equal to 0 so i have w naught x naught plus all of this and then the theta is disappeared there is no theta in this equation so what have i done actually all i am saying is that i am going to have an imaginary input x naught which is equal to 1 and then another weight w naught which is actually equal to minus theta so if i substitute these values then these two inequalities become the same right if i make w naught as minus theta and x naught as 1 and i just get minus theta as the first term of this sum of terms and then uh that these two inequalities that i have here right so as i said this was same and then greater than equal to zero so these two inequalities become zero so the simple change which i have made quite obvious and what it means is that i have this extra input which is x 0 equal to 1 and w naught is actually minus theta and so now the inequality falls in place so now i my summation is from 0 to n with the understanding that x naught is 1 and w naught is minus theta which is the same as saying summation 1 to n minus theta that's what my original uh inequality is where i started from so this just makes it neater in some sense i have no theta now now theta has become w naught or minus w okay so we'll now try to answer the following questions right why are we trying to implement boolean functions right so why am i talking about boolean functions so much i said that we can implement real functions but still for the last part of this lecture i'm going to focus on boolean functions i'll tell you why i'm focusing on boolean functions the second is why do we need weights and why is this w naught called the bias right so these are some very i would say heterogeneous set of questions that i'm going to try to answer right so first why why are we talking about boolean functions so let's take the simple thing of task of predicting whether we would like to uh go for a movie or not or whether we like a movie or not right and this decision might depend on several factors like who is the actor of the movie who is the director of the movie what's the genre of the movie and all of this you could convert to bullion inputs right is actor matt damon his actor christian bale is john a thriller it is johnner comedy is director nolan and so on right so you could just convert it into these series of boolean inputs right so many real world problems you could just try to convert them to problems where the inputs are boolean and the output is of course bullying right whether the whether you are going to watch the movie or whether you want to watch the movie whether you're going to like the movie and so on right and now based on the past viewing experience now that this model has weights we could learn the waves from some data how we are going to learn that is not clear yet right and it's going to take a series of lectures to understand that in a much broader context we will do it in the context of perceptron soon but the broader context would get clear over a series of lectures right so this is something that we would like to learn and now this model has these placeholders for weights which could potentially be learned right and these weights could be such that what do we want right we want the weighted sum to be greater than some threshold right or the weighted sum including the threshold to be greater than equal to 0 that's what we want right so now you could imagine that there's an input signal which is weak but the weight for that input signal is so high that collectively it crosses the border right and so it's possible that you only have one input which is on right which is the director is nolan and all the other inputs are off but the weight of this input might be very high based on your data because you have always liked movies directed by christopher nolan so you would have a very high weight for this and because the weight is high even though there's only one input which is on the weighted sum might still cross the threshold right and that's that's what the weights help you and that's the notion of importance encoded in an inequality right so that's where the weights are helping right now coming to the bias why is w naught called the uh why is w naught called the bias so w naught actually refers to the prior or the prejudice so what's the role that it is playing right it is one of the terms in the summation and it's like the blockage right so if w naught or or if theta is a very high value right so then your sum has started off with a very low value it's a high negative value that you have started off and now to cross this so that your total weighted sum is greater than zero all of these inputs will have to contribute a lot right that means many inputs may have to be on and many high weighted inputs have to be on right so this theta is telling you that what is your inertia for crossing the border right so if you are a very niche moviegoer who only watches movies which have mad demon which are thriller and which are directed by christopher nolan right then your theta let's assume all the weights are one then your theta would be -3 and only if these 3 inputs are on then you will have minus theta which is minus 3 plus the actor is diamond plus the genre is thriller plus the director is nolan and then this would be greater than equal to zero and only then you will go and watch the movies if any of these inputs is not on right if either the actor is not what you're looking for then you'll not go to watch so this is the bias right this is what it tells you about that specific user and hence it's called the prior or the bias right but if you are like a movie buff who does not care about who the director director director is and in that case your threshold could be zero and even if none of these criteria are matched the actor is not matt damon the director is not nolan the honor is not thriller still you will cross the threshold and still you will go and watch the movie because you are just a movie buff so this threshold is trying to encode the initial bias that you have right and it could be adjusted to reflect whether this person is a movie buff or not right in other situations it could be reflected to uh it could be adjusted to reflect the prior as we say right so what is the without knowing the data without knowing the inputs what is the prior that is about you taking this decision as a positive versus negative right zero versus one so that's why it's called the prejudice and the same thing i have explained on the slides you can go back and read it later right okay so now coming to the question so this is this was to motivate a few things right that why are we interested in boolean functions because many real world problems you can map it to boolean functions then why do we have weights because in many real function world problems you want weighted inputs you want certain inputs to be more important than others and what is theta what is the prior so that has also been explained right now let's move on to a different set of questions so what kind of functions can be implemented using the perceptron right we know the answer for the mecolic bits neuron can it can only be used to implement boolean functions which are linearly separable that means you can draw a line and all inputs which have a positive output will lie on one side and all inputs which have a negative output will lie on the other side so is the perceptron any different or what kind of functions can it be used so if you look at the mathematical form of these two there is only the red part is different right so earlier you had summation x i greater than equal to zero now you have summation w i x i greater than equal to 0. so in essence both of these are actually finding linear boundaries right just that now you have certain coefficients for your inputs right so that's what is happening here it should be clear that even for a mecca for a perceptron you're just learning a line and then you will have some points which lie in the positive half space of the line and for those points the output would be one and some points which lie in the negative half space of the line and for those points the output would be zero so just in case this is not very clear we'll look at an example and it will become clear but then the question which already comes up is that if it's not different if it's also learning a decision boundary then what's linear decision boundary then what's the difference the difference is that now we have weights and we also have a learning algorithm for learning those weights right so we are slowly moving in the territory of learning with the perceptron and we'll go deeper in this territory as we go to sigmoid neurons and then eventually to a deep neural networks right so that's where we're slowly making a transition to right so let us first revisit some boolean functions to convince ourselves that this is actually drawing a linear decision boundary right so you have the or function i know what the or function looks like and now in terms of the perceptron what does it mean this is what my perceptron function is right so i'm going to take a weighted sum of the two inputs plus w naught and i want that to be less than zero in this case right i want that to be greater than equal to zero in this case greater than equal to zero in this case greater than equal to zero in this case because that's how my or function is right now let me just expand this right so what is this first guy is telling w naught plus w 1 into 0 plus w 2 into 0 should be less than 0 which implies that w naught should be less than 0. similarly the second equation is saying w naught plus w 1 into 0 plus w 2 into 1 should be greater than equal to 0 which is just saying that w 2 should be greater than equal to minus w naught right similarly the third equation is this is what it implies and similarly this is what the fourth equation implies right so i'll just take a pause for 10 seconds here so that you could just look at the equations and convince yourself that this is indeed the case okay now this is not the equations inequalities so these are the four inequalities given to me right and now i could just look try different values of w one and w two and w naught and try to figure out one combination of w one w two w naught which satisfies these inequalities of course in practice i would not like to do this randomly but have a learning algorithm which does it and we will go there we will go to the perceptron learning algorithm but for now it's ok if i just think a bit about it and i have already thought about it and this is a set of values which i am going to give you if w not equal to 1 also not 1 sorry this should have been minus 1 right so this should have been minus 1 so if w not equal to minus 1 if w 1 equal to 1.1 and w t equal to one point one then all these equations are satisfied you can just plug in these values and see that all the four inequalities are satisfied right and now i'll look at the geometric interpretation of that so what is the geometric interpretation i am saying that minus 1 okay plus 1.1 into x1 plus 1.1 into x2 greater than equal to 0 this is the positive half space less than 0 this is the negative half space so if i eq look at the line equal to 0 that's the separating line which is separating my inputs into this positive and negative half space right so let's see what that line is looking like so here i have this line so this is the line minus one plus one point one x one plus one point one x two equal to zero as i said earlier uh this is minus one right so this i already mentioned so that is why this is minus one here ok oh it is already deleted okay so now uh what is happening here right so the green shaded region is actually the positive half space of the line all the points in this half space will satisfy the equation minus 1 plus one point one x one plus one point one x two greater than equal to zero they'll satisfy that inequality hence there is a positive half space and all the points which are in the red zone and in my boolean world there is only one such point which is zero comma zero so that is going to lie in the negative half space and that is exactly what we wanted right so this is a correct solution of course this is not the only correct solution so i will go ahead and yeah so i'll go ahead and change this a bit so i've changed this this is also a valid solution right because again my green region has the positive points this is not a valid solution and you can see here the error is 2 why is the error 2 because this is the one point which lies in the positive half space and it rightly lies in the positive half space there are three points which lie in the negative half space of which actually only one should have lied the red point should have lied in the negative half space but there are these two green points which are also lying in negative half space so i'm making errors on two inputs so in this configuration my perceptron is not actually representing the or function it's not implementing the or function because it is making errors right so this is not an implementation of the or function but there could be other values this i already showed i could even adjust the threshold right so this w naught actually in the terms of a line tells me what my y intercept is going to be right so i'm going to adjust it and it will intersect or intercept the y axis at different points right and now so far the error is zero so all these solutions are valid but now if i come here then i have a problem you can see error is equal to one because one point which red point which should have been in the negative half space is now lying in the positive half space right so now my learning algorithm should be such that you should draw the line in a way that my negative points are on one side and the positive points or the points which give a negative output are on one side and positive output on the other side right so that's what we would want from the perceptron algorithm and this uh same thing right where i have put in these uh try to substitute the values and then come up with inequalities and then just hand solve for the inequalities the same thing you could have done for the mecolic pits neuron there you would have had only w naught right there was no w1 and w2 so you would have come up with four sets of inequalities and only one variable which is theta or w naught and then you could have tried to find the value of that data so you can go home and try this try to write the four inequalities for the mcculloch pits neuron remember there will be no w one w two and then just see if you can come up with the right value of theta so that is also doable right so this the idea of learning is or finding the right value for the parameters is not something that we have come to suddenly i'm just saying that we had already done it in the case of the mp neuron also where we had tried to find this value of theta it's just that there it was very obvious what the value of theta should be as here it could be a bit difficult right and we'll have a learning algorithm to do that so i'll end this module here and when we come back i am going to talk about errors and error surface