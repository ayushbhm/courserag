[
  {
    "text": "foreign",
    "start": 0.299,
    "duration": 3.0
  },
  {
    "text": "[Music]",
    "start": 6.62,
    "duration": 15.22
  },
  {
    "text": "technique that we are going to look at",
    "start": 20.06,
    "duration": 3.7
  },
  {
    "text": "is adding noise to the outputs so we saw",
    "start": 21.84,
    "duration": 3.599
  },
  {
    "text": "adding noise to the inputs but you could",
    "start": 23.76,
    "duration": 3.42
  },
  {
    "text": "also add noise to the output so let's",
    "start": 25.439,
    "duration": 5.041
  },
  {
    "text": "see how what that means so this is what",
    "start": 27.18,
    "duration": 5.34
  },
  {
    "text": "your output is right so you are given a",
    "start": 30.48,
    "duration": 3.599
  },
  {
    "text": "certain image so let's look at the",
    "start": 32.52,
    "duration": 3.66
  },
  {
    "text": "classification problem and this is what",
    "start": 34.079,
    "duration": 5.64
  },
  {
    "text": "your true Y is going to look like all",
    "start": 36.18,
    "duration": 4.92
  },
  {
    "text": "the probability masses on the correct",
    "start": 39.719,
    "duration": 3.961
  },
  {
    "text": "level which is 2 in this case so 0 1 2",
    "start": 41.1,
    "duration": 4.979
  },
  {
    "text": "all the way up to 9 and everything else",
    "start": 43.68,
    "duration": 5.039
  },
  {
    "text": "is 0 right and now this is what your",
    "start": 46.079,
    "duration": 4.921
  },
  {
    "text": "true Y is and you are trying to predict",
    "start": 48.719,
    "duration": 4.561
  },
  {
    "text": "uh",
    "start": 51.0,
    "duration": 5.76
  },
  {
    "text": "y hat or f x ah F hat of X which is very",
    "start": 53.28,
    "duration": 7.56
  },
  {
    "text": "close to this 2 y right and that could",
    "start": 56.76,
    "duration": 5.76
  },
  {
    "text": "lead to overfitting because if you have",
    "start": 60.84,
    "duration": 3.719
  },
  {
    "text": "you if you have a lot of parameters you",
    "start": 62.52,
    "duration": 3.72
  },
  {
    "text": "know that this error you can drive down",
    "start": 64.559,
    "duration": 3.961
  },
  {
    "text": "to zero but now what if I do the",
    "start": 66.24,
    "duration": 4.14
  },
  {
    "text": "following right so I instead of using",
    "start": 68.52,
    "duration": 3.84
  },
  {
    "text": "this output the true output that was",
    "start": 70.38,
    "duration": 4.68
  },
  {
    "text": "given to me uh and this is what my loss",
    "start": 72.36,
    "duration": 4.079
  },
  {
    "text": "this is what I'm going to minimize right",
    "start": 75.06,
    "duration": 2.879
  },
  {
    "text": "this is my cross entropy loss that I'm",
    "start": 76.439,
    "duration": 3.72
  },
  {
    "text": "going to minimize so P here is the true",
    "start": 77.939,
    "duration": 5.281
  },
  {
    "text": "label and Q is the predicted label so",
    "start": 80.159,
    "duration": 6.301
  },
  {
    "text": "instead of using the true distribution",
    "start": 83.22,
    "duration": 5.28
  },
  {
    "text": "what if I say that I don't trust the",
    "start": 86.46,
    "duration": 5.28
  },
  {
    "text": "true labels they may be noisy or I don't",
    "start": 88.5,
    "duration": 5.1
  },
  {
    "text": "want to trust the true labels because",
    "start": 91.74,
    "duration": 4.5
  },
  {
    "text": "then I am trying to map my input exactly",
    "start": 93.6,
    "duration": 4.199
  },
  {
    "text": "to the output and that is what",
    "start": 96.24,
    "duration": 3.72
  },
  {
    "text": "overfitting is right so instead I am",
    "start": 97.799,
    "duration": 3.901
  },
  {
    "text": "going to add some noise to the outputs",
    "start": 99.96,
    "duration": 3.479
  },
  {
    "text": "what I'm going to do is that in my",
    "start": 101.7,
    "duration": 5.099
  },
  {
    "text": "original distribution all my probability",
    "start": 103.439,
    "duration": 6.421
  },
  {
    "text": "Mass was on the correct label right so",
    "start": 106.799,
    "duration": 4.261
  },
  {
    "text": "what I'm going to do is I'm going to",
    "start": 109.86,
    "duration": 3.0
  },
  {
    "text": "take away a small probability Mass from",
    "start": 111.06,
    "duration": 3.78
  },
  {
    "text": "there and distribute it to the other",
    "start": 112.86,
    "duration": 4.439
  },
  {
    "text": "labels right so now instead of all the",
    "start": 114.84,
    "duration": 4.02
  },
  {
    "text": "mass being on one and everything else",
    "start": 117.299,
    "duration": 3.96
  },
  {
    "text": "being 0 I'm instead of using those hard",
    "start": 118.86,
    "duration": 4.2
  },
  {
    "text": "targets now I'm using the soft targets",
    "start": 121.259,
    "duration": 3.661
  },
  {
    "text": "where there's a small non-zero",
    "start": 123.06,
    "duration": 4.5
  },
  {
    "text": "probability for all the other outputs",
    "start": 124.92,
    "duration": 4.319
  },
  {
    "text": "also right so this I have corrupted the",
    "start": 127.56,
    "duration": 3.6
  },
  {
    "text": "output in some sense but not corrupted",
    "start": 129.239,
    "duration": 4.021
  },
  {
    "text": "it by a lot right because still if",
    "start": 131.16,
    "duration": 3.84
  },
  {
    "text": "Epsilon is small and Epsilon will be",
    "start": 133.26,
    "duration": 4.08
  },
  {
    "text": "small my majority of the probability",
    "start": 135.0,
    "duration": 4.44
  },
  {
    "text": "mass is still on the correct level but",
    "start": 137.34,
    "duration": 4.32
  },
  {
    "text": "now when I apply my formula the cross",
    "start": 139.44,
    "duration": 4.82
  },
  {
    "text": "entropy formula",
    "start": 141.66,
    "duration": 5.52
  },
  {
    "text": "earlier in my Pi log UI if you remember",
    "start": 144.26,
    "duration": 4.72
  },
  {
    "text": "only one term remained right the one",
    "start": 147.18,
    "duration": 4.559
  },
  {
    "text": "which correspond to p i equal to 1 so",
    "start": 148.98,
    "duration": 4.92
  },
  {
    "text": "only this the label the second term",
    "start": 151.739,
    "duration": 4.561
  },
  {
    "text": "would remain but now because all the",
    "start": 153.9,
    "duration": 4.26
  },
  {
    "text": "other values were 0 but now all the",
    "start": 156.3,
    "duration": 3.299
  },
  {
    "text": "other values are not zero right so my",
    "start": 158.16,
    "duration": 3.659
  },
  {
    "text": "computation is changing so the loss that",
    "start": 159.599,
    "duration": 5.041
  },
  {
    "text": "I am actually trying to minimize is uh",
    "start": 161.819,
    "duration": 4.741
  },
  {
    "text": "changing and that will act as a",
    "start": 164.64,
    "duration": 3.179
  },
  {
    "text": "regularizer right because now you are",
    "start": 166.56,
    "duration": 3.48
  },
  {
    "text": "not trying to minimize the true loss but",
    "start": 167.819,
    "duration": 4.861
  },
  {
    "text": "a slightly corrupted version of the loss",
    "start": 170.04,
    "duration": 4.919
  },
  {
    "text": "because you have made some change to the",
    "start": 172.68,
    "duration": 4.919
  },
  {
    "text": "outputs right so that's what uh that's",
    "start": 174.959,
    "duration": 5.761
  },
  {
    "text": "what you do in uh adding noise to the",
    "start": 177.599,
    "duration": 5.64
  },
  {
    "text": "outputs and now since you are not what",
    "start": 180.72,
    "duration": 3.9
  },
  {
    "text": "what you're doing done is again you have",
    "start": 183.239,
    "duration": 2.881
  },
  {
    "text": "added some kind of a corruption to the",
    "start": 184.62,
    "duration": 3.72
  },
  {
    "text": "loss function right so earlier you are",
    "start": 186.12,
    "duration": 5.52
  },
  {
    "text": "looking at L theta plus Omega Theta",
    "start": 188.34,
    "duration": 5.94
  },
  {
    "text": "right and now again if you open this up",
    "start": 191.64,
    "duration": 4.56
  },
  {
    "text": "right and now again what has happened is",
    "start": 194.28,
    "duration": 5.7
  },
  {
    "text": "earlier you had only uh so what you have",
    "start": 196.2,
    "duration": 5.64
  },
  {
    "text": "now is Epsilon",
    "start": 199.98,
    "duration": 7.52
  },
  {
    "text": "I Epsilon into log of P1",
    "start": 201.84,
    "duration": 5.66
  },
  {
    "text": "plus Epsilon into log of",
    "start": 209.04,
    "duration": 6.839
  },
  {
    "text": "uh sorry P0 P1",
    "start": 211.819,
    "duration": 6.661
  },
  {
    "text": "Plus",
    "start": 215.879,
    "duration": 2.601
  },
  {
    "text": "Epsilon into log of P3",
    "start": 218.76,
    "duration": 4.52
  },
  {
    "text": "log of P",
    "start": 227.819,
    "duration": 5.7
  },
  {
    "text": "9 and then you also have this 1 minus",
    "start": 230.519,
    "duration": 7.521
  },
  {
    "text": "Epsilon into log of",
    "start": 233.519,
    "duration": 7.201
  },
  {
    "text": "P2 which was the correct label right for",
    "start": 238.04,
    "duration": 4.54
  },
  {
    "text": "the correct label this is the weight so",
    "start": 240.72,
    "duration": 3.48
  },
  {
    "text": "now you can again think of this this was",
    "start": 242.58,
    "duration": 3.6
  },
  {
    "text": "actually your true loss earlier without",
    "start": 244.2,
    "duration": 4.56
  },
  {
    "text": "regularization right",
    "start": 246.18,
    "duration": 4.86
  },
  {
    "text": "so you can think of this as L Theta so",
    "start": 248.76,
    "duration": 4.559
  },
  {
    "text": "now you have some weighted L Theta so",
    "start": 251.04,
    "duration": 4.979
  },
  {
    "text": "let me just call it as alpha 1 L Theta",
    "start": 253.319,
    "duration": 5.1
  },
  {
    "text": "where alpha 1 is 1 minus Epsilon and",
    "start": 256.019,
    "duration": 4.981
  },
  {
    "text": "then you have Plus or let me just call",
    "start": 258.419,
    "duration": 6.261
  },
  {
    "text": "it 1 minus Epsilon only right",
    "start": 261.0,
    "duration": 3.68
  },
  {
    "text": "so you have 1 minus Epsilon times your",
    "start": 265.139,
    "duration": 5.161
  },
  {
    "text": "earlier loss plus Epsilon times",
    "start": 267.3,
    "duration": 5.64
  },
  {
    "text": "some other loss which I can call as",
    "start": 270.3,
    "duration": 4.679
  },
  {
    "text": "Omega Theta now so again you can see",
    "start": 272.94,
    "duration": 3.3
  },
  {
    "text": "that you are doing some kind of a",
    "start": 274.979,
    "duration": 3.841
  },
  {
    "text": "regularization here and that will help",
    "start": 276.24,
    "duration": 4.8
  },
  {
    "text": "avoid overfitting right so that's the",
    "start": 278.82,
    "duration": 4.86
  },
  {
    "text": "idea behind adding noise to the output",
    "start": 281.04,
    "duration": 6.68
  },
  {
    "text": "levels so we'll end this here",
    "start": 283.68,
    "duration": 4.04
  }
]