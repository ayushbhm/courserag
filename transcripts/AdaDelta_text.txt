foreign [Music] Delta which avoids setting an initial learning rate completely right so let's see what it does so I have some n epochs or n iterations so I have Computing the gradient then I keep exponentially moving history VT okay then I do compute the update right so this is what my update is going to be right and that update is a ratio of two variables one is UT I have not defined what UT is yet but it's ratio of u t and VT of course the square root is there but there is no ETA naught here right so we still need to see what is UT and how does this ratio take care of the challenges that I was talking about yeah so let's see so UT then I update WT because I've computed the Delta which needs to be there right so that's uh this is simply uh yeah so this part is simply equivalent to I mean the equivalent of minus ETA naught divided by square root of beta into Del WT which I had in the previous algorithm and then my update tool was WT minus this right so this is my Delta and then my update tool is just going to be WT plus Delta so that negative sign has been taken care of inside and I don't have the initial learning rate instead I have this in the numerator right so all of this is making sense and this is what my UT is right so my UT is an accumulation of the updates that I have made so remember this is nabla WT Square I am representing the gradient as nabla WT right so derivative is knabla WT and the update is Delta WT right so this is what this is keeping track of the Deltas and my current derivatives are lab loss right so I have like two histories being tracked and then I'm taking a ratio of these histories and then adjusting the learning rate how does it solve my problem is still not clear but right now we are just trying to understand what the equations are right so instead of one history which was VT I have two histories now it's also clear that one history is kind of ahead of the other history right so VT was tracking everything that is happening up to this time step but UT is also tracking what has happened at this time step because it was also taking into account the current update which has been made okay uh so these two changes are these two ratios are being uh these two histories are being tracked and then I'm taking a ratio of these two histories and that is my effective learning rate so how does this help is the question okay foreign that's what I meant by that one history is running ahead the other St VT is already used in the current iteration but the UT that I have computed now will be used in the next iteration as UT minus 1 right so the numerator is one step behind the denominator okay so now the numerator uh is a function of the history as opposed to being a constant in RMS prop and add a grid so we have made it a function of the history we have got rid of the initial learning rate but does it really solve our problem is the question right and the other thing to note which will be important in understanding that it indeed solves our problem is that whatever uh derivative we have computed here whatever derivative we had computed we are already taking only a fraction of that in VT right and even the update that we have right we are only taking a fraction of that when we are looking at UT okay this is something important to note why and how this makes a difference is something that we'll see but the main thing to note is that we are only taking a fraction of the update that we have made and this update that we had made was uh a function of this Delta WT that we had right the gradient that we had okay so uh don't worry I will just clear these things in the next couple of slides yeah so now suppose you are in a high curvature region right so you are in a steep region and what do you want in the Steep region you want the effective learning rate to be small okay so at time step 0 your V 0 so your you can refer to the equations here the equations for VT uh UT and all of that I'll just not bother too much about referring to them again and again but these computations are according to these equations right so at any point you're confused you can just pause and verify that we have used the same equations right so V 0 is going to be point one into Delta W 0 squares then you will have the sorry nabla W zero square and nabla W 0 is the derivative then you have the update which is the Delta W naught the change that you're going to make in W naught and you're currently your U minus 1 is 0 right so then you only have Epsilon in the numerator U minus 1 is going to be 0. so we make that update then you do W 1 is equal to W naught plus Delta W naught then you compute U naught right and this is what I was emphasizing on right that this U naught is just taking a fraction of the derivative that you had right and now uh so yeah you U naught was actually uh beta times the old history which is 0 in this case and 1 minus beta time the current update that we have made but the current update which is Delta is in turn a function of this nabla so I've just substituted the value of delta uh W naught and in terms of uh sorry uh yeah Delta W naught in terms of navbla w naught right so this is nabla just in case you're confused and this is Delta okay so that's done okay now let's see what happens next I'll just keep continuing for a while so fraction of the history is being stored for the next iteration now at t equal to 1 again you will just apply this B1 is equal to 0.9 into the previous history plus point one into the current gradient and if I substitute the value of the prevent history then I get the entire equation in terms of the nav loss so nabla w 0 square and abla W1 square right now I again uh do the update and now I'm ignoring the Epsilon because I have reached a state where I have non-zero U naught and Epsilon is very small so U naught plus Epsilon is going to be very close to U naught anyways right so this is what the update is I make this update and then I compute U1 right and again U1 is a function of Deltas remember but the Deltas in terms are functions of the nablas so I just put those values appropriately and I have computed this right and now if you look at this right so or maybe let me just go to one more time step t equal to 2 again I am doing the same thing as I'm just mechanically substituting values into the formula and just making sure that I express everything in terms of Delta W I's right that's the only idea so that all the formulas become comparable right all the values become comparative so this is where I am now I'm just going to focus on the U's and the v's okay so now if you look at this at this time step right yeah so when I was here at t equal to 2 time step I will use U1 divided by V 2 right so this is my U1 and this is my V2 and you can compare these two equations and you will know that U1 is actually smaller than V2 right so if U1 is smaller than V2 then the numerator is smaller than the denominator and hence the effective learning rate would be small and hence my problem has been taken care of right in the Steep region I am having a smaller denominator a smaller effective learning rate right that's exactly what I wanted and the entire picture will become clear also when I talk about what happens in the flat regions right so that's also something which is coming so let's wait for that to happen so both VT and UT are increasing right because both are accumulating gradients but because of this one time step delay between them right one is U T minus one the other is VT so in the Steep regions while both are growing UT is growing slightly slower than VT and hence the effective learning rate is going to be smaller right now yeah that's always same here whereas in the RMS prop the effective learning rate would just have been ETA by VT and hence there is no adoption Happening Here Right okay so this is all that I've already said you could just read what there's on the slide now let's see what happens in the flat regions okay so now I'm in the flat region so this is some Delta w i right at the ith time step I am trying to take the gradient now what will happen here right so now if I look at uh U T minus 1 and VT okay now U T minus 1 remember is one time step behind a VT so then one way of looking at it is that in ut minus 1 these all are participating right and in VT minus V T these all are participating because by this point the beta raised to K is so small that that guy is effectively not participating right because there's this one time step difference so this is what the situation is now what is happening if you look at this curve right so since all the gradients were high high and then they started decreasing my UT since it's looking at more High guys than the low guys right so because of this one time so so v v t has this current guy also which is again very small right and UT does not have this guy it has some other guy which was actually high right because it's one time step behind so now UT is going to be larger than VT does that make sense because VT is one step higher so once you enter the Steep regions after a while because this new guy is getting more weightage in VT that sum would stop start becoming smaller as compared to UT because UT still has to come to that step it's still in the older history and the older history was all high gradients so it's still taking more benefit from the older history whereas VT is now relying more on the current guy and this current guy will affect U T minus 1 only in the next time step right so now the history the U T minus 1 is going to be larger than VT and hence the ratio is going to be larger and hence the effective learning rate is increasing as you are entering the uh flat regions right and then you can continue arguing that once again if you enter the Steep region again because of this one time step delay what will happen is VT will start looking at a much bigger gradient than what UT is looking at right so UT might be looking at these guys whereas V T minus v u t minus 1 would be looking at sorry yeah U T minus 1 might be looking at these guys whereas VT will start looking at the current gradient also and this is a very large value so it has more benefit from the current large value and hence it would be larger than UT minus 1 and hence the effective learning rate will again decrease right so that's what is happening here okay so that's that's the explanation of why having this two histories one delayed by one time step is helping because it allows you to catch up with what is happening in the history and then update the gradients according right so these values are not important but as long as you have understood the pictorial explanation you will be able to relate it to what is happening with these values right so therefore Delta allows the numerator to increase or decrease based on the current and past gradients and that exactly what we wanted now let's see what happens if we run Ada Delta so as usual RMS prop is oscillating Ada grad is taking more time to reach the Minima because it aggressively kills the learning rate whereas adaptive Delta let's just look at it again right so yeah so in the Steep regions as it became steeper it's decreased and then it again it did whatever was required to reach the Minima quickly right well the other two algorithms are struggling one is oscillating while the other one is slow right yeah okay yeah now let's look at uh we we are also plotting now the VT and the UT right so uh because they are delayed by one time step their curvature would be the same right they would have the same shape so both of them have the same shape but their magnitudes are different right because of this delay one is relying on more on the past history like one more time step from the past history whereas the other one is relying more on the current history so what is happening is that when you are entering this uh uh let's see so what is happening is in this curve so what happened was initially you were in a high loss region right or a high loss region but also relatively flatter region right here the difference between the Constable contour lines is higher than what happens as you keep going down towards the valley right so that means initially your gradients were small right so the guy who is going to be behind which is U T minus 1 is going to look at smaller gradients whereas the guy who is ahead is started looking at faster gradients higher gradients right so that's why what is happening is that the magnitude of VT is large here and the magnitude of u t is small here because UT is still looking at the flat history whereas BT is one step ahead so it has looked at more uh steeper history than What U T minus 1 is looking at right therefore V T is larger than UT and since we were going in a steep Direction all the way around this is a favorable situation to have right we want VT to be larger than UT so that effectively the ratio is small and the learning rate is accordingly smaller right so that's exactly what we wanted and that is what is happening now I can show you uh this more clearly on the next slide where uh if you look at the learning rates for Ada Delta and RMS prop then RMS prop it constantly decreases right because it does not have any adaptive nature as it uh kind of uh it has to keep uh decreasing because it's one over the accumulated uh history in this case and since the history was always steep whereas in other Delta you see some kind of adoption happening adaptation happening and overall Adar Delta is able to converge faster on the plot that we had see wow yeah so let's look at some of these values also so here we had set the initial learning rate to 0.013 so this curve is 0.013 divided by the square root of whatever and this both started at Point 20 right that's the uh what the initial value was but this only decreased up to 0.12 and hence it was able to kind of converge faster whereas this was 0.05 and then it became uh uh so it decayed more as compared to what Ada Delta DK right so this is what has happened here and this is especially important because in the uh flat regions we did see a slight bump in the learning rate right so it's not like monotonically decreasing but in the events it reached a bit closer to the uh I know it's still decreasing only right yeah sorry that's not correct yeah so that's that's how the two decays happened okay so this is what the effective learning rates are looking like and now let's see how the algorithm proceeds so as you can say that ADA Delta has already converged because it was not aggressively decaying the learning rate it had a reasonably High Learning rate which allowed it to converge RMS prop is slowly getting there because it has aggressively decreased the learning rate right and now the maybe the usual problem of oscillations may be there or maybe not I think the figure has ended so I don't know the animation has ended so I don't know but the main thing is the difference between these two plots which is explained by these two products these are the same two plots drawn on the same scale right so now I've drawn both the plots on the same scale here and in the case of R Delta it was not decaying aggressively whereas in this case it was leaking aggressively right okay yeah so that's what we have already gone through now let's put all the algorithms together on one plot and see how they behave so the same loss function same starting point uh and we have these different algorithms right from gradient descent all the way up to Ida Delta and other Delta is the only one which has this adaptive learning rate in the sense of not having an initial learning rate right so let's see and you see the expected behaviors right uh okay let's see let it stabilize and we'll comment on it so you see all of it all the behaviors that we expected the black curve is gradient descent which is slowly moving we know that that's does not have any momentum does not have any adaptation the uh the the momentum and the nestrov have the usual effect of oscillations and next of you can again see slightly better at dealing with the oscillations but all three of them have a problem in moving in the W Direction because W was relatively sparse so they first try to move in the B Direction and then come back whereas other grid RMS prop in Ada Delta they all start moving in W and B Direction and of these other Delta converts the faster because it was able to adjust the learning rate RMS prop as usual had the oscillation problem right so all of these effects you may again wish to watch this a few times uh just play the video when you go to the slides and just make sure you note notice all the behaviors that you are familiar with okay yeah so we'll end this here