[
  {
    "text": "foreign",
    "start": 0.299,
    "duration": 3.0
  },
  {
    "text": "[Music]",
    "start": 6.62,
    "duration": 3.739
  },
  {
    "text": "ing",
    "start": 19.08,
    "duration": 3.0
  },
  {
    "text": "so the main question is right what has",
    "start": 23.76,
    "duration": 4.439
  },
  {
    "text": "changed now right what has happened in",
    "start": 26.1,
    "duration": 4.74
  },
  {
    "text": "2006 which has allowed us to train these",
    "start": 28.199,
    "duration": 4.08
  },
  {
    "text": "deep neural networks and why has big",
    "start": 30.84,
    "duration": 3.899
  },
  {
    "text": "deep learning become so popular that's",
    "start": 32.279,
    "duration": 6.78
  },
  {
    "text": "the main question so far right uh",
    "start": 34.739,
    "duration": 6.541
  },
  {
    "text": "so if we look at this seminal work in",
    "start": 39.059,
    "duration": 5.52
  },
  {
    "text": "2006 which made uh training deep neural",
    "start": 41.28,
    "duration": 7.74
  },
  {
    "text": "networks are more uh possible Right so",
    "start": 44.579,
    "duration": 6.841
  },
  {
    "text": "the original paper I was actually talks",
    "start": 49.02,
    "duration": 4.8
  },
  {
    "text": "about something known as rbms we have",
    "start": 51.42,
    "duration": 4.979
  },
  {
    "text": "not done ibms yet and we'll not do it in",
    "start": 53.82,
    "duration": 5.579
  },
  {
    "text": "this part of the course uh I will try to",
    "start": 56.399,
    "duration": 4.201
  },
  {
    "text": "explain it in the context of Auto",
    "start": 59.399,
    "duration": 2.701
  },
  {
    "text": "encoders again we have not done Auto",
    "start": 60.6,
    "duration": 3.86
  },
  {
    "text": "encoders now I've also",
    "start": 62.1,
    "duration": 4.98
  },
  {
    "text": "Auto encoders will also not be a part of",
    "start": 64.46,
    "duration": 4.0
  },
  {
    "text": "this course because we have replaced it",
    "start": 67.08,
    "duration": 3.6
  },
  {
    "text": "by other more advanced optimization",
    "start": 68.46,
    "duration": 4.5
  },
  {
    "text": "algorithms as well as discussion on",
    "start": 70.68,
    "duration": 4.02
  },
  {
    "text": "Transformers so we did not have place to",
    "start": 72.96,
    "duration": 4.8
  },
  {
    "text": "put in autoencoders given the number of",
    "start": 74.7,
    "duration": 4.919
  },
  {
    "text": "lectures that we had so but you could",
    "start": 77.76,
    "duration": 3.359
  },
  {
    "text": "refer to my older lectures and auto",
    "start": 79.619,
    "duration": 3.241
  },
  {
    "text": "encoders if you want and whatever we",
    "start": 81.119,
    "duration": 4.201
  },
  {
    "text": "need I'll anyways cover in the next few",
    "start": 82.86,
    "duration": 4.259
  },
  {
    "text": "slides right so I'll not have you have",
    "start": 85.32,
    "duration": 3.06
  },
  {
    "text": "not had a detailed discussion or to",
    "start": 87.119,
    "duration": 2.82
  },
  {
    "text": "encoders but I'll just quickly brush up",
    "start": 88.38,
    "duration": 3.12
  },
  {
    "text": "on whatever I need in the next few",
    "start": 89.939,
    "duration": 3.601
  },
  {
    "text": "slides right so two points here one is",
    "start": 91.5,
    "duration": 3.479
  },
  {
    "text": "the original paper talks in the context",
    "start": 93.54,
    "duration": 3.66
  },
  {
    "text": "of rbms I am going to talk about this",
    "start": 94.979,
    "duration": 4.5
  },
  {
    "text": "idea in the context of Auto encoders we",
    "start": 97.2,
    "duration": 4.08
  },
  {
    "text": "have not done Auto encoders in this",
    "start": 99.479,
    "duration": 3.541
  },
  {
    "text": "offering of the course but I'll just",
    "start": 101.28,
    "duration": 3.6
  },
  {
    "text": "quickly cover the concepts that I",
    "start": 103.02,
    "duration": 4.08
  },
  {
    "text": "require right so now consider this deep",
    "start": 104.88,
    "duration": 3.599
  },
  {
    "text": "neural network where there is like a 4",
    "start": 107.1,
    "duration": 3.6
  },
  {
    "text": "layer Network plus the input and the",
    "start": 108.479,
    "duration": 4.081
  },
  {
    "text": "output layer and as I was saying before",
    "start": 110.7,
    "duration": 5.22
  },
  {
    "text": "2006 it was hard to train networks which",
    "start": 112.56,
    "duration": 5.339
  },
  {
    "text": "are four five six layers and so on it",
    "start": 115.92,
    "duration": 5.28
  },
  {
    "text": "they did not converge well right now let",
    "start": 117.899,
    "duration": 6.061
  },
  {
    "text": "us focus on the first two layers right",
    "start": 121.2,
    "duration": 3.959
  },
  {
    "text": "now what I'm trying to do is I'm going",
    "start": 123.96,
    "duration": 3.6
  },
  {
    "text": "to try to explain the idea of",
    "start": 125.159,
    "duration": 5.1
  },
  {
    "text": "unsupervised pre-training right so now I",
    "start": 127.56,
    "duration": 4.08
  },
  {
    "text": "forget about training the entire network",
    "start": 130.259,
    "duration": 3.84
  },
  {
    "text": "so what is my input that might be some",
    "start": 131.64,
    "duration": 5.54
  },
  {
    "text": "x's that I have",
    "start": 134.099,
    "duration": 3.081
  },
  {
    "text": "right and this x belongs to some r n so",
    "start": 137.22,
    "duration": 4.44
  },
  {
    "text": "it's an N dimensional input and I have",
    "start": 140.28,
    "duration": 3.3
  },
  {
    "text": "taken the simple case when I'm just",
    "start": 141.66,
    "duration": 3.78
  },
  {
    "text": "trying to predict one output right so my",
    "start": 143.58,
    "duration": 4.14
  },
  {
    "text": "y belongs to R right now I'm not",
    "start": 145.44,
    "duration": 4.08
  },
  {
    "text": "focusing on this entire and I decided",
    "start": 147.72,
    "duration": 4.04
  },
  {
    "text": "that I want to use like a very complex",
    "start": 149.52,
    "duration": 4.98
  },
  {
    "text": "deep neural network as my function",
    "start": 151.76,
    "duration": 4.96
  },
  {
    "text": "approximation that this is the relation",
    "start": 154.5,
    "duration": 6.599
  },
  {
    "text": "between Y and ah X and Y right and now I",
    "start": 156.72,
    "duration": 6.9
  },
  {
    "text": "am focusing on this first layer where I",
    "start": 161.099,
    "duration": 4.201
  },
  {
    "text": "have the input and then the first hidden",
    "start": 163.62,
    "duration": 4.619
  },
  {
    "text": "layer H1 right now what we do in",
    "start": 165.3,
    "duration": 4.92
  },
  {
    "text": "unsupervised pre-training or at least as",
    "start": 168.239,
    "duration": 4.681
  },
  {
    "text": "it was introduced in this paper now",
    "start": 170.22,
    "duration": 4.98
  },
  {
    "text": "today unsupervised pre-training still",
    "start": 172.92,
    "duration": 4.26
  },
  {
    "text": "still exist but in a much different form",
    "start": 175.2,
    "duration": 5.46
  },
  {
    "text": "which is conceptually similar but with a",
    "start": 177.18,
    "duration": 5.58
  },
  {
    "text": "lot of moving Parts having changed right",
    "start": 180.66,
    "duration": 3.24
  },
  {
    "text": "this is in the context of feed forward",
    "start": 182.76,
    "duration": 3.18
  },
  {
    "text": "neural networks today we typically talk",
    "start": 183.9,
    "duration": 4.199
  },
  {
    "text": "about it in the context of Transformers",
    "start": 185.94,
    "duration": 4.14
  },
  {
    "text": "and the loss functions that evolved none",
    "start": 188.099,
    "duration": 3.42
  },
  {
    "text": "of that matters at this point but I'm",
    "start": 190.08,
    "duration": 3.6
  },
  {
    "text": "just saying that when I'm talking about",
    "start": 191.519,
    "duration": 3.841
  },
  {
    "text": "unsupervised pre-tuning right now I'm",
    "start": 193.68,
    "duration": 3.419
  },
  {
    "text": "talking in the context of this work in",
    "start": 195.36,
    "duration": 5.76
  },
  {
    "text": "2006 right ah",
    "start": 197.099,
    "duration": 4.021
  },
  {
    "text": "ok so what we will do is",
    "start": 201.18,
    "duration": 6.059
  },
  {
    "text": "we will take X as the input",
    "start": 203.94,
    "duration": 6.42
  },
  {
    "text": "okay and we will focus on a very simple",
    "start": 207.239,
    "duration": 5.28
  },
  {
    "text": "problem first right so this x is the",
    "start": 210.36,
    "duration": 4.62
  },
  {
    "text": "input I have just taken the first hidden",
    "start": 212.519,
    "duration": 3.841
  },
  {
    "text": "layer so I have taken the hidden layer",
    "start": 214.98,
    "duration": 3.6
  },
  {
    "text": "as it is and now instead of feeding this",
    "start": 216.36,
    "duration": 4.26
  },
  {
    "text": "hidden layer to the next layer I just",
    "start": 218.58,
    "duration": 3.719
  },
  {
    "text": "disconnected the network and what I have",
    "start": 220.62,
    "duration": 4.56
  },
  {
    "text": "done is I am now trying to predict X",
    "start": 222.299,
    "duration": 5.821
  },
  {
    "text": "again right so think of this this way",
    "start": 225.18,
    "duration": 4.979
  },
  {
    "text": "that I had say some 1 0 2 4 dimensional",
    "start": 228.12,
    "duration": 4.259
  },
  {
    "text": "input then I have a hidden layer in",
    "start": 230.159,
    "duration": 4.981
  },
  {
    "text": "between which is say 256 dimensional and",
    "start": 232.379,
    "duration": 4.381
  },
  {
    "text": "then again an output layer which is one",
    "start": 235.14,
    "duration": 3.9
  },
  {
    "text": "zero two four and my goal is to do the",
    "start": 236.76,
    "duration": 4.32
  },
  {
    "text": "following right first compute this 256",
    "start": 239.04,
    "duration": 4.619
  },
  {
    "text": "dimensional hidden representation and",
    "start": 241.08,
    "duration": 5.219
  },
  {
    "text": "then again reconstruct the output from",
    "start": 243.659,
    "duration": 5.22
  },
  {
    "text": "it right and I my goal would be that if",
    "start": 246.299,
    "duration": 5.281
  },
  {
    "text": "I call this x hat and this says X then",
    "start": 248.879,
    "duration": 4.801
  },
  {
    "text": "since I am reconstructing my X hat",
    "start": 251.58,
    "duration": 4.499
  },
  {
    "text": "should be as close to X right and that",
    "start": 253.68,
    "duration": 3.72
  },
  {
    "text": "is what this loss function is",
    "start": 256.079,
    "duration": 4.56
  },
  {
    "text": "calculating so my X was n dimensional so",
    "start": 257.4,
    "duration": 5.7
  },
  {
    "text": "along each of these n Dimensions I want",
    "start": 260.639,
    "duration": 4.981
  },
  {
    "text": "to reduce the error between the",
    "start": 263.1,
    "duration": 4.8
  },
  {
    "text": "prediction and the construction and I",
    "start": 265.62,
    "duration": 4.139
  },
  {
    "text": "want to do this for all the M training",
    "start": 267.9,
    "duration": 3.84
  },
  {
    "text": "examples and I want to consider the",
    "start": 269.759,
    "duration": 3.72
  },
  {
    "text": "average right so that's the objective",
    "start": 271.74,
    "duration": 4.26
  },
  {
    "text": "function this is clear now what what am",
    "start": 273.479,
    "duration": 4.201
  },
  {
    "text": "I trying to do here right what is",
    "start": 276.0,
    "duration": 3.6
  },
  {
    "text": "happening here right so let us try to",
    "start": 277.68,
    "duration": 4.079
  },
  {
    "text": "understand this conceptually that",
    "start": 279.6,
    "duration": 5.039
  },
  {
    "text": "suppose I'm able to do this suppose I'm",
    "start": 281.759,
    "duration": 5.281
  },
  {
    "text": "able to train a network which is able to",
    "start": 284.639,
    "duration": 5.221
  },
  {
    "text": "reconstruct X hat with zero error that",
    "start": 287.04,
    "duration": 7.68
  },
  {
    "text": "means I am taking an X I am doing a w x",
    "start": 289.86,
    "duration": 7.5
  },
  {
    "text": "y that is a transformation I do then I",
    "start": 294.72,
    "duration": 4.32
  },
  {
    "text": "pass it through a sigmoid function I am",
    "start": 297.36,
    "duration": 4.38
  },
  {
    "text": "ignoring the biases okay so I get some",
    "start": 299.04,
    "duration": 6.599
  },
  {
    "text": "hidden layer H1 right and then from the",
    "start": 301.74,
    "duration": 6.78
  },
  {
    "text": "hidden layer I am reconstructing X hat",
    "start": 305.639,
    "duration": 4.381
  },
  {
    "text": "as say",
    "start": 308.52,
    "duration": 4.86
  },
  {
    "text": "W Times H1 okay this is what I am doing",
    "start": 310.02,
    "duration": 6.06
  },
  {
    "text": "in this network right my input layer I",
    "start": 313.38,
    "duration": 4.2
  },
  {
    "text": "am doing first a linear transformation",
    "start": 316.08,
    "duration": 3.899
  },
  {
    "text": "followed by a non-linearity and then",
    "start": 317.58,
    "duration": 4.619
  },
  {
    "text": "whatever H1 I get I just reconstructed",
    "start": 319.979,
    "duration": 3.841
  },
  {
    "text": "the X hat from there if you want I can",
    "start": 322.199,
    "duration": 4.141
  },
  {
    "text": "just add the bias also here and I can",
    "start": 323.82,
    "duration": 5.7
  },
  {
    "text": "add the bias here also right so you if",
    "start": 326.34,
    "duration": 4.98
  },
  {
    "text": "you want that's that makes you more",
    "start": 329.52,
    "duration": 3.179
  },
  {
    "text": "comfortable then that is what it is",
    "start": 331.32,
    "duration": 2.52
  },
  {
    "text": "right",
    "start": 332.699,
    "duration": 5.581
  },
  {
    "text": "now suppose I am able to do this in a",
    "start": 333.84,
    "duration": 7.56
  },
  {
    "text": "way that I am able to reconstruct X hat",
    "start": 338.28,
    "duration": 6.0
  },
  {
    "text": "from X perfectly that means my error is",
    "start": 341.4,
    "duration": 8.06
  },
  {
    "text": "zero what does that tell you about H1",
    "start": 344.28,
    "duration": 5.18
  },
  {
    "text": "so so it tells you that",
    "start": 353.58,
    "duration": 3.839
  },
  {
    "text": "H1",
    "start": 355.919,
    "duration": 4.321
  },
  {
    "text": "actually character captures all the",
    "start": 357.419,
    "duration": 5.101
  },
  {
    "text": "information that was there in X right",
    "start": 360.24,
    "duration": 4.14
  },
  {
    "text": "because I have done a compression I have",
    "start": 362.52,
    "duration": 3.42
  },
  {
    "text": "gone from one zero to four dimensions to",
    "start": 364.38,
    "duration": 3.599
  },
  {
    "text": "256. now we know that in many",
    "start": 365.94,
    "duration": 3.9
  },
  {
    "text": "applications our input has a lot of",
    "start": 367.979,
    "duration": 3.06
  },
  {
    "text": "redundancy right so I would have",
    "start": 369.84,
    "duration": 3.96
  },
  {
    "text": "captured weight height and BMI all three",
    "start": 371.039,
    "duration": 3.961
  },
  {
    "text": "are not needed from the weight and",
    "start": 373.8,
    "duration": 2.58
  },
  {
    "text": "height I can know the BMI right",
    "start": 375.0,
    "duration": 3.24
  },
  {
    "text": "similarly I would have captured several",
    "start": 376.38,
    "duration": 3.48
  },
  {
    "text": "correlated things I would have captured",
    "start": 378.24,
    "duration": 3.54
  },
  {
    "text": "weight and centimeter oh sorry weight in",
    "start": 379.86,
    "duration": 3.6
  },
  {
    "text": "kilograms maybe I would have captured",
    "start": 381.78,
    "duration": 3.419
  },
  {
    "text": "weight in pounds also or weight in grams",
    "start": 383.46,
    "duration": 3.299
  },
  {
    "text": "also similarly height in centimeters",
    "start": 385.199,
    "duration": 3.541
  },
  {
    "text": "process data is flowing in from multiple",
    "start": 386.759,
    "duration": 3.961
  },
  {
    "text": "channels so I do not know even for",
    "start": 388.74,
    "duration": 3.54
  },
  {
    "text": "example I would have calculated the",
    "start": 390.72,
    "duration": 3.24
  },
  {
    "text": "salary I would have salary as one of the",
    "start": 392.28,
    "duration": 4.44
  },
  {
    "text": "inputs and I might also have the income",
    "start": 393.96,
    "duration": 4.26
  },
  {
    "text": "tax as one of the inputs right now these",
    "start": 396.72,
    "duration": 4.14
  },
  {
    "text": "two are completely correlated so in that",
    "start": 398.22,
    "duration": 4.44
  },
  {
    "text": "case I did not have both these inputs",
    "start": 400.86,
    "duration": 3.839
  },
  {
    "text": "right so my data has lot of redundancy",
    "start": 402.66,
    "duration": 3.9
  },
  {
    "text": "so I am doing some sort of a compression",
    "start": 404.699,
    "duration": 5.161
  },
  {
    "text": "where I am reducing it to 256 values and",
    "start": 406.56,
    "duration": 4.979
  },
  {
    "text": "then I'm trying to reconstruct the input",
    "start": 409.86,
    "duration": 4.08
  },
  {
    "text": "from these 256 values so if I am able to",
    "start": 411.539,
    "duration": 4.981
  },
  {
    "text": "do this perfectly that means this hidden",
    "start": 413.94,
    "duration": 5.699
  },
  {
    "text": "layer that I had is capturing all the",
    "start": 416.52,
    "duration": 4.739
  },
  {
    "text": "important characteristics of my input",
    "start": 419.639,
    "duration": 3.9
  },
  {
    "text": "right and that is enough for me to",
    "start": 421.259,
    "duration": 3.961
  },
  {
    "text": "reconstruct the input right that's what",
    "start": 423.539,
    "duration": 3.541
  },
  {
    "text": "typically happens in compression so",
    "start": 425.22,
    "duration": 3.599
  },
  {
    "text": "that's exactly what I am trying to do",
    "start": 427.08,
    "duration": 3.66
  },
  {
    "text": "here and this is the idea in an auto",
    "start": 428.819,
    "duration": 3.841
  },
  {
    "text": "encoder right I want to learn a more",
    "start": 430.74,
    "duration": 4.5
  },
  {
    "text": "compact representation of the input and",
    "start": 432.66,
    "duration": 4.379
  },
  {
    "text": "the way I do it is that I use this",
    "start": 435.24,
    "duration": 4.5
  },
  {
    "text": "bottleneck layer right and then I try to",
    "start": 437.039,
    "duration": 6.06
  },
  {
    "text": "reconstruct the loss and if I'm able to",
    "start": 439.74,
    "duration": 4.56
  },
  {
    "text": "do this",
    "start": 443.099,
    "duration": 3.54
  },
  {
    "text": "then I have captured all the important",
    "start": 444.3,
    "duration": 4.38
  },
  {
    "text": "information in this 1024 dimensional",
    "start": 446.639,
    "duration": 4.801
  },
  {
    "text": "data it was by just using 256 Dimensions",
    "start": 448.68,
    "duration": 5.16
  },
  {
    "text": "right and we can show that under certain",
    "start": 451.44,
    "duration": 4.56
  },
  {
    "text": "conditions this is actually equivalent",
    "start": 453.84,
    "duration": 4.62
  },
  {
    "text": "to principle component analysis which is",
    "start": 456.0,
    "duration": 4.22
  },
  {
    "text": "again a data",
    "start": 458.46,
    "duration": 4.139
  },
  {
    "text": "dimensional ID reduction technique right",
    "start": 460.22,
    "duration": 3.46
  },
  {
    "text": "and the same thing you are doing here",
    "start": 462.599,
    "duration": 2.82
  },
  {
    "text": "you're using the dimensions of the input",
    "start": 463.68,
    "duration": 4.26
  },
  {
    "text": "data right so that's all that we want to",
    "start": 465.419,
    "duration": 4.261
  },
  {
    "text": "know about Auto encoders this is what an",
    "start": 467.94,
    "duration": 3.539
  },
  {
    "text": "auto encoder tries to do and this is",
    "start": 469.68,
    "duration": 3.6
  },
  {
    "text": "what I am trying to do here right so",
    "start": 471.479,
    "duration": 3.84
  },
  {
    "text": "while doing so I am actually Computing",
    "start": 473.28,
    "duration": 4.02
  },
  {
    "text": "ending up Computing a abstract",
    "start": 475.319,
    "duration": 5.22
  },
  {
    "text": "representation of the input in this",
    "start": 477.3,
    "duration": 5.22
  },
  {
    "text": "layer right so my this layer is now",
    "start": 480.539,
    "duration": 4.261
  },
  {
    "text": "Computing a good representation of the",
    "start": 482.52,
    "duration": 3.66
  },
  {
    "text": "input it's capturing all the vital",
    "start": 484.8,
    "duration": 3.54
  },
  {
    "text": "characteristics of the input which are",
    "start": 486.18,
    "duration": 3.959
  },
  {
    "text": "enough to reconstruct the inputs right",
    "start": 488.34,
    "duration": 3.96
  },
  {
    "text": "so that is what is happening here and",
    "start": 490.139,
    "duration": 4.5
  },
  {
    "text": "this is the objective that I am going to",
    "start": 492.3,
    "duration": 5.1
  },
  {
    "text": "work with now notice that this is a",
    "start": 494.639,
    "duration": 4.56
  },
  {
    "text": "single layer network network right so I",
    "start": 497.4,
    "duration": 3.359
  },
  {
    "text": "had a problem I had difficulties in",
    "start": 499.199,
    "duration": 3.361
  },
  {
    "text": "training a deep neural network so I've",
    "start": 500.759,
    "duration": 3.66
  },
  {
    "text": "ignored that problem I've said okay I'll",
    "start": 502.56,
    "duration": 3.84
  },
  {
    "text": "worry about that later right now let me",
    "start": 504.419,
    "duration": 3.84
  },
  {
    "text": "just focus on this right so if I am able",
    "start": 506.4,
    "duration": 3.9
  },
  {
    "text": "to train this entire network what does",
    "start": 508.259,
    "duration": 4.02
  },
  {
    "text": "that mean that this layer is producing",
    "start": 510.3,
    "duration": 3.72
  },
  {
    "text": "good outputs this layer is producing",
    "start": 512.279,
    "duration": 3.361
  },
  {
    "text": "good outputs this layer is producing",
    "start": 514.02,
    "duration": 3.6
  },
  {
    "text": "good outputs and this layer is good in",
    "start": 515.64,
    "duration": 3.779
  },
  {
    "text": "producing good outputs and finally the",
    "start": 517.62,
    "duration": 4.5
  },
  {
    "text": "final output is good right so this",
    "start": 519.419,
    "duration": 4.201
  },
  {
    "text": "entire problem was becoming difficult",
    "start": 522.12,
    "duration": 2.94
  },
  {
    "text": "for me I was not able to train the",
    "start": 523.62,
    "duration": 3.06
  },
  {
    "text": "entire network so I'm saying let me just",
    "start": 525.06,
    "duration": 3.6
  },
  {
    "text": "focus on one layer now and for this",
    "start": 526.68,
    "duration": 3.42
  },
  {
    "text": "layer what is the definition of good",
    "start": 528.66,
    "duration": 3.179
  },
  {
    "text": "that it should capture all the",
    "start": 530.1,
    "duration": 3.54
  },
  {
    "text": "characteristics of the input that is how",
    "start": 531.839,
    "duration": 3.721
  },
  {
    "text": "I have defined goodness and I have",
    "start": 533.64,
    "duration": 3.06
  },
  {
    "text": "designed this objective function",
    "start": 535.56,
    "duration": 2.82
  },
  {
    "text": "accordingly and I've reduced the problem",
    "start": 536.7,
    "duration": 3.84
  },
  {
    "text": "to training a network containing just",
    "start": 538.38,
    "duration": 3.959
  },
  {
    "text": "one hidden layer so input hidden and",
    "start": 540.54,
    "duration": 4.08
  },
  {
    "text": "output and this is a shallow Network and",
    "start": 542.339,
    "duration": 3.961
  },
  {
    "text": "shallow Network training was not a",
    "start": 544.62,
    "duration": 3.0
  },
  {
    "text": "challenge right so I could train a",
    "start": 546.3,
    "duration": 2.94
  },
  {
    "text": "shallow neural network so I've reduced",
    "start": 547.62,
    "duration": 3.839
  },
  {
    "text": "my problem to training a shallow neural",
    "start": 549.24,
    "duration": 4.14
  },
  {
    "text": "network now how do I go from here again",
    "start": 551.459,
    "duration": 3.601
  },
  {
    "text": "because eventually I want to train the",
    "start": 553.38,
    "duration": 3.66
  },
  {
    "text": "Deep inner Network that is not clear now",
    "start": 555.06,
    "duration": 4.02
  },
  {
    "text": "but the first step is to just learn",
    "start": 557.04,
    "duration": 5.1
  },
  {
    "text": "layer 1 effectively is that clear",
    "start": 559.08,
    "duration": 6.18
  },
  {
    "text": "no no this could be W1 and W2",
    "start": 562.14,
    "duration": 4.98
  },
  {
    "text": "yeah so you when you compute the loss",
    "start": 565.26,
    "duration": 3.48
  },
  {
    "text": "you are using that right so excited you",
    "start": 567.12,
    "duration": 2.64
  },
  {
    "text": "are Computing the difference between X",
    "start": 568.74,
    "duration": 3.24
  },
  {
    "text": "hat and X so there you are using that",
    "start": 569.76,
    "duration": 3.48
  },
  {
    "text": "information right what was my original",
    "start": 571.98,
    "duration": 3.6
  },
  {
    "text": "input so loss is defined in terms of",
    "start": 573.24,
    "duration": 4.08
  },
  {
    "text": "what your original input was right so",
    "start": 575.58,
    "duration": 4.759
  },
  {
    "text": "that is getting used there",
    "start": 577.32,
    "duration": 3.019
  },
  {
    "text": "so it will be the same",
    "start": 581.279,
    "duration": 3.021
  },
  {
    "text": "depends upon the input layers right no",
    "start": 584.519,
    "duration": 4.561
  },
  {
    "text": "so this is n dimensional",
    "start": 586.86,
    "duration": 3.78
  },
  {
    "text": "and then you are reconstructing the",
    "start": 589.08,
    "duration": 3.12
  },
  {
    "text": "n-dimensional input",
    "start": 590.64,
    "duration": 5.819
  },
  {
    "text": "yeah right yeah yeah okay so ah so this",
    "start": 592.2,
    "duration": 5.46
  },
  {
    "text": "is what we have done",
    "start": 596.459,
    "duration": 3.241
  },
  {
    "text": "now once we have understood this we're",
    "start": 597.66,
    "duration": 3.9
  },
  {
    "text": "just going to repeat this process again",
    "start": 599.7,
    "duration": 3.24
  },
  {
    "text": "right and let's see what I mean by",
    "start": 601.56,
    "duration": 4.38
  },
  {
    "text": "repeat this process again",
    "start": 602.94,
    "duration": 4.38
  },
  {
    "text": "yeah this is all written in words",
    "start": 605.94,
    "duration": 4.019
  },
  {
    "text": "whatever I had said and yeah and one",
    "start": 607.32,
    "duration": 3.84
  },
  {
    "text": "important thing is where is the word",
    "start": 609.959,
    "duration": 3.601
  },
  {
    "text": "unsupervised coming in here right so",
    "start": 611.16,
    "duration": 4.38
  },
  {
    "text": "remember that in this problem that I",
    "start": 613.56,
    "duration": 3.719
  },
  {
    "text": "have I'm currently interested in solving",
    "start": 615.54,
    "duration": 4.02
  },
  {
    "text": "there is no y There Is No Label here",
    "start": 617.279,
    "duration": 4.321
  },
  {
    "text": "hence it is in unsupervised I am just",
    "start": 619.56,
    "duration": 4.02
  },
  {
    "text": "having the inputs and my loss function",
    "start": 621.6,
    "duration": 3.54
  },
  {
    "text": "completely depends on the input so I",
    "start": 623.58,
    "duration": 2.759
  },
  {
    "text": "don't care right even if you had not",
    "start": 625.14,
    "duration": 3.6
  },
  {
    "text": "given me y I don't care so there is no",
    "start": 626.339,
    "duration": 4.68
  },
  {
    "text": "label or supervision being used here and",
    "start": 628.74,
    "duration": 3.96
  },
  {
    "text": "this is whatever I am doing right now is",
    "start": 631.019,
    "duration": 3.661
  },
  {
    "text": "unsupervised later on of course I will",
    "start": 632.7,
    "duration": 3.48
  },
  {
    "text": "move to a supervised objective but right",
    "start": 634.68,
    "duration": 5.3
  },
  {
    "text": "now whatever I am doing is unsupervised",
    "start": 636.18,
    "duration": 3.8
  },
  {
    "text": "ok now once I have done this I am going",
    "start": 642.959,
    "duration": 4.38
  },
  {
    "text": "to repeat this process so what I'm going",
    "start": 645.6,
    "duration": 4.62
  },
  {
    "text": "to do now is that I was happy that I had",
    "start": 647.339,
    "duration": 5.18
  },
  {
    "text": "learned H1 well right from the previous",
    "start": 650.22,
    "duration": 4.739
  },
  {
    "text": "training so I did that round of training",
    "start": 652.519,
    "duration": 4.241
  },
  {
    "text": "and I kept training that my training",
    "start": 654.959,
    "duration": 3.721
  },
  {
    "text": "loss reduce and my H1 was a good",
    "start": 656.76,
    "duration": 4.199
  },
  {
    "text": "representation now I am happy with H1",
    "start": 658.68,
    "duration": 4.74
  },
  {
    "text": "right so now H1 is some say d",
    "start": 660.959,
    "duration": 4.921
  },
  {
    "text": "dimensional input now I'm going to plug",
    "start": 663.42,
    "duration": 3.9
  },
  {
    "text": "in H2",
    "start": 665.88,
    "duration": 4.139
  },
  {
    "text": "okay and then again try to reconstruct",
    "start": 667.32,
    "duration": 5.459
  },
  {
    "text": "H1 hat right and I'm not going to touch",
    "start": 670.019,
    "duration": 4.38
  },
  {
    "text": "these parameters so you can think of the",
    "start": 672.779,
    "duration": 3.06
  },
  {
    "text": "entire training data that was given me",
    "start": 674.399,
    "duration": 3.541
  },
  {
    "text": "the M training samples that were given",
    "start": 675.839,
    "duration": 3.901
  },
  {
    "text": "to me for each of those training samples",
    "start": 677.94,
    "duration": 4.32
  },
  {
    "text": "I have computed h okay I have done that",
    "start": 679.74,
    "duration": 5.219
  },
  {
    "text": "sorry H one I have done that right now",
    "start": 682.26,
    "duration": 5.819
  },
  {
    "text": "my training data becomes H1 I pass it",
    "start": 684.959,
    "duration": 5.401
  },
  {
    "text": "through a network a layer called H2",
    "start": 688.079,
    "duration": 4.2
  },
  {
    "text": "right so this was maybe D dimensional",
    "start": 690.36,
    "duration": 4.14
  },
  {
    "text": "this could be D dimensional or some D1",
    "start": 692.279,
    "duration": 4.321
  },
  {
    "text": "dimensional and then I again try to",
    "start": 694.5,
    "duration": 4.5
  },
  {
    "text": "reconstruct H1 from there so it's the",
    "start": 696.6,
    "duration": 4.14
  },
  {
    "text": "same training objective right so now",
    "start": 699.0,
    "duration": 4.32
  },
  {
    "text": "remember that the H is while we think of",
    "start": 700.74,
    "duration": 4.98
  },
  {
    "text": "them as hidden layers or outputs they",
    "start": 703.32,
    "duration": 4.139
  },
  {
    "text": "are also the inputs to the net say to",
    "start": 705.72,
    "duration": 4.02
  },
  {
    "text": "the next layer right so just as X is H 0",
    "start": 707.459,
    "duration": 4.681
  },
  {
    "text": "and my first problem that I had solved",
    "start": 709.74,
    "duration": 4.8
  },
  {
    "text": "was for its 0 now I am solving the same",
    "start": 712.14,
    "duration": 4.98
  },
  {
    "text": "problem for H1 given an H1 I want to",
    "start": 714.54,
    "duration": 5.28
  },
  {
    "text": "reconstruct it through a single layered",
    "start": 717.12,
    "duration": 4.8
  },
  {
    "text": "Network right so again once again I am",
    "start": 719.82,
    "duration": 4.38
  },
  {
    "text": "training a shallow Network one input",
    "start": 721.92,
    "duration": 4.26
  },
  {
    "text": "layer which is H1 which is fixed I have",
    "start": 724.2,
    "duration": 4.079
  },
  {
    "text": "computed all the H1s using the x that",
    "start": 726.18,
    "duration": 4.32
  },
  {
    "text": "was given to me and the training that I",
    "start": 728.279,
    "duration": 4.62
  },
  {
    "text": "had done in the first iteration I had",
    "start": 730.5,
    "duration": 4.38
  },
  {
    "text": "computed all the W's I was happy with",
    "start": 732.899,
    "duration": 4.261
  },
  {
    "text": "whatever WS I have computed and using",
    "start": 734.88,
    "duration": 4.92
  },
  {
    "text": "those W's I have computed H1 now as W",
    "start": 737.16,
    "duration": 3.96
  },
  {
    "text": "times x",
    "start": 739.8,
    "duration": 4.38
  },
  {
    "text": "plus C or B whatever we had used into",
    "start": 741.12,
    "duration": 4.68
  },
  {
    "text": "that right so this computation I have",
    "start": 744.18,
    "duration": 3.96
  },
  {
    "text": "done now I'm thinking of H I's as H1s as",
    "start": 745.8,
    "duration": 4.26
  },
  {
    "text": "my input passing it through a hidden",
    "start": 748.14,
    "duration": 4.199
  },
  {
    "text": "layer and then reconstructing H1 hat and",
    "start": 750.06,
    "duration": 4.2
  },
  {
    "text": "again using the same objective function",
    "start": 752.339,
    "duration": 4.321
  },
  {
    "text": "with X replaced by H1 right so nothing",
    "start": 754.26,
    "duration": 4.259
  },
  {
    "text": "new that I am doing so again I'm just",
    "start": 756.66,
    "duration": 4.14
  },
  {
    "text": "training one layer at a time right this",
    "start": 758.519,
    "duration": 3.421
  },
  {
    "text": "is the only layer that is getting",
    "start": 760.8,
    "duration": 3.42
  },
  {
    "text": "trained which means this is the only",
    "start": 761.94,
    "duration": 4.62
  },
  {
    "text": "layer that I am trying to reconstruct",
    "start": 764.22,
    "duration": 4.44
  },
  {
    "text": "right and now what am I doing I had said",
    "start": 766.56,
    "duration": 4.5
  },
  {
    "text": "that H1 captures all the characteristics",
    "start": 768.66,
    "duration": 5.76
  },
  {
    "text": "of X similarly now H2 is capturing all",
    "start": 771.06,
    "duration": 5.64
  },
  {
    "text": "the important information of H1 so it's",
    "start": 774.42,
    "duration": 4.26
  },
  {
    "text": "yet another level of abstract",
    "start": 776.7,
    "duration": 3.6
  },
  {
    "text": "representation of the original input",
    "start": 778.68,
    "duration": 3.36
  },
  {
    "text": "right so this was already an abstract",
    "start": 780.3,
    "duration": 3.3
  },
  {
    "text": "representation of the input which had",
    "start": 782.04,
    "duration": 3.18
  },
  {
    "text": "discarded all the unimportant",
    "start": 783.6,
    "duration": 3.239
  },
  {
    "text": "information and totally everything that",
    "start": 785.22,
    "duration": 3.72
  },
  {
    "text": "you need to know about X and I can fully",
    "start": 786.839,
    "duration": 4.5
  },
  {
    "text": "reconstruct X from here now H2 is an",
    "start": 788.94,
    "duration": 4.56
  },
  {
    "text": "abstract representation of H1 which kind",
    "start": 791.339,
    "duration": 4.081
  },
  {
    "text": "of captures every information in H1 and",
    "start": 793.5,
    "duration": 4.139
  },
  {
    "text": "it's enough to cap reconstruct H1 from",
    "start": 795.42,
    "duration": 4.5
  },
  {
    "text": "there so H1 is an abstract",
    "start": 797.639,
    "duration": 4.681
  },
  {
    "text": "representation of x and it's to in turn",
    "start": 799.92,
    "duration": 5.039
  },
  {
    "text": "is an abstract representation of H 1 so",
    "start": 802.32,
    "duration": 4.199
  },
  {
    "text": "H2 is like a deeper abstract",
    "start": 804.959,
    "duration": 3.241
  },
  {
    "text": "representation of X right so that is",
    "start": 806.519,
    "duration": 3.12
  },
  {
    "text": "what you are doing and that's what the",
    "start": 808.2,
    "duration": 2.699
  },
  {
    "text": "idea and a feed forward neural network",
    "start": 809.639,
    "duration": 4.26
  },
  {
    "text": "is right every layer captures a more and",
    "start": 810.899,
    "duration": 4.62
  },
  {
    "text": "more abstract representation of the",
    "start": 813.899,
    "duration": 3.601
  },
  {
    "text": "input right so now again you have",
    "start": 815.519,
    "duration": 4.5
  },
  {
    "text": "focused on layer H2 and made sure that",
    "start": 817.5,
    "duration": 4.56
  },
  {
    "text": "this computes good representations that",
    "start": 820.019,
    "duration": 4.141
  },
  {
    "text": "means you have cap and ensure that these",
    "start": 822.06,
    "duration": 4.14
  },
  {
    "text": "weights that are sitting here are now",
    "start": 824.16,
    "duration": 4.38
  },
  {
    "text": "some good or well initialized ways right",
    "start": 826.2,
    "duration": 4.139
  },
  {
    "text": "so that's what you have done and now you",
    "start": 828.54,
    "duration": 4.02
  },
  {
    "text": "will continue this process",
    "start": 830.339,
    "duration": 4.201
  },
  {
    "text": "in the next iteration",
    "start": 832.56,
    "duration": 5.279
  },
  {
    "text": "now you will freeze H1 and H2 now H2",
    "start": 834.54,
    "duration": 5.64
  },
  {
    "text": "becomes your input and now we will throw",
    "start": 837.839,
    "duration": 3.541
  },
  {
    "text": "in H3",
    "start": 840.18,
    "duration": 3.24
  },
  {
    "text": "right and then you will try to learn",
    "start": 841.38,
    "duration": 4.62
  },
  {
    "text": "these weights such that from H3 you are",
    "start": 843.42,
    "duration": 4.8
  },
  {
    "text": "able to reconstruct H2 perfectly right",
    "start": 846.0,
    "duration": 4.74
  },
  {
    "text": "and you keep doing this till all the L",
    "start": 848.22,
    "duration": 4.679
  },
  {
    "text": "layers that you have so you are learning",
    "start": 850.74,
    "duration": 4.92
  },
  {
    "text": "one layer at a time and each layer is",
    "start": 852.899,
    "duration": 4.74
  },
  {
    "text": "trying to learn some weights which are",
    "start": 855.66,
    "duration": 3.72
  },
  {
    "text": "an abstract representation which give",
    "start": 857.639,
    "duration": 3.121
  },
  {
    "text": "you allow you to compute an abstract",
    "start": 859.38,
    "duration": 4.199
  },
  {
    "text": "representation of the input right so now",
    "start": 860.76,
    "duration": 4.92
  },
  {
    "text": "this is what you are going to do",
    "start": 863.579,
    "duration": 5.841
  },
  {
    "text": "so at the end right",
    "start": 865.68,
    "duration": 3.74
  },
  {
    "text": "what have we achieved that all our",
    "start": 869.519,
    "duration": 5.281
  },
  {
    "text": "layers have been trained to compute a",
    "start": 871.62,
    "duration": 5.219
  },
  {
    "text": "better representation of the input right",
    "start": 874.8,
    "duration": 4.14
  },
  {
    "text": "and now I return back to my original",
    "start": 876.839,
    "duration": 4.981
  },
  {
    "text": "problem my original problem was that I",
    "start": 878.94,
    "duration": 6.06
  },
  {
    "text": "was given some X",
    "start": 881.82,
    "duration": 6.78
  },
  {
    "text": "and some y right and I wanted to learn",
    "start": 885.0,
    "duration": 5.1
  },
  {
    "text": "the relation between them and this was",
    "start": 888.6,
    "duration": 4.62
  },
  {
    "text": "what my f x is now individual components",
    "start": 890.1,
    "duration": 5.039
  },
  {
    "text": "and f x was itself a composite function",
    "start": 893.22,
    "duration": 3.96
  },
  {
    "text": "right I was first Computing this then",
    "start": 895.139,
    "duration": 3.961
  },
  {
    "text": "passing it then Computing this and so on",
    "start": 897.18,
    "duration": 4.26
  },
  {
    "text": "so all these individual components of f",
    "start": 899.1,
    "duration": 5.34
  },
  {
    "text": "of x now I have learned well right so",
    "start": 901.44,
    "duration": 6.12
  },
  {
    "text": "I'll whatever weights I learned in those",
    "start": 904.44,
    "duration": 5.88
  },
  {
    "text": "individual pre-training steps I'll just",
    "start": 907.56,
    "duration": 4.68
  },
  {
    "text": "retain those I'll start from there I'll",
    "start": 910.32,
    "duration": 3.42
  },
  {
    "text": "not start from random initialization",
    "start": 912.24,
    "duration": 3.839
  },
  {
    "text": "I'll start from those right and I will",
    "start": 913.74,
    "duration": 3.899
  },
  {
    "text": "plug in these weights randomly because",
    "start": 916.079,
    "duration": 3.721
  },
  {
    "text": "these weights I did not learn so I'll",
    "start": 917.639,
    "duration": 4.56
  },
  {
    "text": "plug in these weights randomly and now",
    "start": 919.8,
    "duration": 3.96
  },
  {
    "text": "I'll go back to my original training",
    "start": 922.199,
    "duration": 3.961
  },
  {
    "text": "objective which dependent on y hat and f",
    "start": 923.76,
    "duration": 6.6
  },
  {
    "text": "of x right or yeah sorry not y hat",
    "start": 926.16,
    "duration": 8.66
  },
  {
    "text": "Y and f of x right or",
    "start": 930.36,
    "duration": 4.46
  },
  {
    "text": "this is what my objective function was",
    "start": 935.16,
    "duration": 3.84
  },
  {
    "text": "so now I'll train for this objective",
    "start": 937.26,
    "duration": 4.5
  },
  {
    "text": "function and when I'm doing this I will",
    "start": 939.0,
    "duration": 4.32
  },
  {
    "text": "of course update these weights I'll do",
    "start": 941.76,
    "duration": 3.36
  },
  {
    "text": "the full back propagation I'll update",
    "start": 943.32,
    "duration": 4.139
  },
  {
    "text": "these weights I'll update all of these",
    "start": 945.12,
    "duration": 5.219
  },
  {
    "text": "weights right but when I updating these",
    "start": 947.459,
    "duration": 4.141
  },
  {
    "text": "weights remember that I have not",
    "start": 950.339,
    "duration": 3.36
  },
  {
    "text": "randomly initialized them I have started",
    "start": 951.6,
    "duration": 4.02
  },
  {
    "text": "from whatever my best configuration was",
    "start": 953.699,
    "duration": 4.021
  },
  {
    "text": "while doing those individual",
    "start": 955.62,
    "duration": 4.32
  },
  {
    "text": "unsupervised pre-training right so this",
    "start": 957.72,
    "duration": 4.14
  },
  {
    "text": "is what I have done now why does this",
    "start": 959.94,
    "duration": 3.48
  },
  {
    "text": "work what is the implication all of that",
    "start": 961.86,
    "duration": 3.18
  },
  {
    "text": "we will discuss right but we have",
    "start": 963.42,
    "duration": 3.18
  },
  {
    "text": "understood the procedure that we are",
    "start": 965.04,
    "duration": 3.659
  },
  {
    "text": "training one layer at a time it's",
    "start": 966.6,
    "duration": 3.78
  },
  {
    "text": "unsupervised because we are not using",
    "start": 968.699,
    "duration": 3.961
  },
  {
    "text": "the final label once you have done the",
    "start": 970.38,
    "duration": 4.139
  },
  {
    "text": "full unsupervised pre-training we plug",
    "start": 972.66,
    "duration": 4.08
  },
  {
    "text": "in the supervised objective and we train",
    "start": 974.519,
    "duration": 4.32
  },
  {
    "text": "for that supervised objective so I can",
    "start": 976.74,
    "duration": 4.38
  },
  {
    "text": "think of this as L of Omega right this",
    "start": 978.839,
    "duration": 4.981
  },
  {
    "text": "is what my L of Omega is right",
    "start": 981.12,
    "duration": 5.04
  },
  {
    "text": "this is what was the unsupervised",
    "start": 983.82,
    "duration": 3.959
  },
  {
    "text": "training objective and when they did",
    "start": 986.16,
    "duration": 3.66
  },
  {
    "text": "this they showed that it's possible to",
    "start": 987.779,
    "duration": 4.321
  },
  {
    "text": "Now train the entire Duke neural network",
    "start": 989.82,
    "duration": 4.56
  },
  {
    "text": "effectively a thing which was difficult",
    "start": 992.1,
    "duration": 4.32
  },
  {
    "text": "earlier and they trained quite a few I",
    "start": 994.38,
    "duration": 3.84
  },
  {
    "text": "mean quite a bit deep neural network I",
    "start": 996.42,
    "duration": 3.779
  },
  {
    "text": "have shown still in modern terms this is",
    "start": 998.22,
    "duration": 3.6
  },
  {
    "text": "still a shallow network but they train",
    "start": 1000.199,
    "duration": 3.301
  },
  {
    "text": "quite a bit of a deep neural network",
    "start": 1001.82,
    "duration": 4.1
  },
  {
    "text": "right",
    "start": 1003.5,
    "duration": 2.42
  },
  {
    "text": "so what have we done right so in effect",
    "start": 1006.68,
    "duration": 3.839
  },
  {
    "text": "what we have done is we have initialized",
    "start": 1008.54,
    "duration": 4.08
  },
  {
    "text": "the weights of the network using that",
    "start": 1010.519,
    "duration": 4.201
  },
  {
    "text": "greedy unsupervised object right where",
    "start": 1012.62,
    "duration": 4.019
  },
  {
    "text": "at F why am I calling it greedy because",
    "start": 1014.72,
    "duration": 3.72
  },
  {
    "text": "at every layer I did not care about what",
    "start": 1016.639,
    "duration": 3.601
  },
  {
    "text": "is happening in the other layers I just",
    "start": 1018.44,
    "duration": 3.72
  },
  {
    "text": "wanted to make sure that that layer is",
    "start": 1020.24,
    "duration": 3.3
  },
  {
    "text": "trained properly that's why it was",
    "start": 1022.16,
    "duration": 3.72
  },
  {
    "text": "greedy and it was unsupervised and",
    "start": 1023.54,
    "duration": 4.019
  },
  {
    "text": "whatever was the result of that greedy",
    "start": 1025.88,
    "duration": 3.299
  },
  {
    "text": "and supervised training that is the",
    "start": 1027.559,
    "duration": 3.541
  },
  {
    "text": "initialization that exists in my network",
    "start": 1029.179,
    "duration": 4.681
  },
  {
    "text": "right so now why does this work better",
    "start": 1031.1,
    "duration": 4.8
  },
  {
    "text": "is the question and I'll give you two",
    "start": 1033.86,
    "duration": 5.18
  },
  {
    "text": "options is it due to better optimization",
    "start": 1035.9,
    "duration": 6.24
  },
  {
    "text": "or is it due to better regularization",
    "start": 1039.04,
    "duration": 5.44
  },
  {
    "text": "right now the first thing that I want to",
    "start": 1042.14,
    "duration": 3.96
  },
  {
    "text": "ask is whether you understand the",
    "start": 1044.48,
    "duration": 3.359
  },
  {
    "text": "difference between these two questions",
    "start": 1046.1,
    "duration": 3.92
  },
  {
    "text": "so what do we mean by optimization",
    "start": 1047.839,
    "duration": 6.96
  },
  {
    "text": "optimization deals with Dash data",
    "start": 1050.02,
    "duration": 7.84
  },
  {
    "text": "training data or test data",
    "start": 1054.799,
    "duration": 4.441
  },
  {
    "text": "training data so when you are doing",
    "start": 1057.86,
    "duration": 3.66
  },
  {
    "text": "optimization when you are minimizing the",
    "start": 1059.24,
    "duration": 4.799
  },
  {
    "text": "loss function",
    "start": 1061.52,
    "duration": 5.76
  },
  {
    "text": "right remember that you are taking the",
    "start": 1064.039,
    "duration": 7.941
  },
  {
    "text": "average whatever is your loss function",
    "start": 1067.28,
    "duration": 4.7
  },
  {
    "text": "over",
    "start": 1075.08,
    "duration": 2.66
  },
  {
    "text": "your Computing with respect to some",
    "start": 1080.5,
    "duration": 6.16
  },
  {
    "text": "um",
    "start": 1084.679,
    "duration": 6.541
  },
  {
    "text": "y i and F hat of X I right",
    "start": 1086.66,
    "duration": 6.899
  },
  {
    "text": "and this loss function depends on the",
    "start": 1091.22,
    "duration": 4.199
  },
  {
    "text": "training data these X I's and Y i's are",
    "start": 1093.559,
    "duration": 4.441
  },
  {
    "text": "your training data so optimization you",
    "start": 1095.419,
    "duration": 4.081
  },
  {
    "text": "only have access to training data you",
    "start": 1098.0,
    "duration": 3.72
  },
  {
    "text": "don't have access to test it",
    "start": 1099.5,
    "duration": 4.02
  },
  {
    "text": "but during regularization what do you",
    "start": 1101.72,
    "duration": 3.9
  },
  {
    "text": "mean by regularization regularization is",
    "start": 1103.52,
    "duration": 4.38
  },
  {
    "text": "used for better generalization So when",
    "start": 1105.62,
    "duration": 3.96
  },
  {
    "text": "you say better generalization it means",
    "start": 1107.9,
    "duration": 3.36
  },
  {
    "text": "that of course I know you will do well",
    "start": 1109.58,
    "duration": 3.42
  },
  {
    "text": "on the training data you will be able to",
    "start": 1111.26,
    "duration": 3.9
  },
  {
    "text": "drive the error to zero but what I care",
    "start": 1113.0,
    "duration": 4.559
  },
  {
    "text": "about is generalization which means on",
    "start": 1115.16,
    "duration": 4.139
  },
  {
    "text": "test data your performance should be",
    "start": 1117.559,
    "duration": 3.781
  },
  {
    "text": "better right so what is happening here",
    "start": 1119.299,
    "duration": 5.701
  },
  {
    "text": "is it that IFD did not do unsupervised",
    "start": 1121.34,
    "duration": 5.4
  },
  {
    "text": "pre-training",
    "start": 1125.0,
    "duration": 3.419
  },
  {
    "text": "you are not able to do better",
    "start": 1126.74,
    "duration": 3.78
  },
  {
    "text": "optimization itself that means even for",
    "start": 1128.419,
    "duration": 4.741
  },
  {
    "text": "the training uh data for which I have",
    "start": 1130.52,
    "duration": 4.62
  },
  {
    "text": "been assuming in all our discussions hey",
    "start": 1133.16,
    "duration": 3.54
  },
  {
    "text": "training data we can easily drive to",
    "start": 1135.14,
    "duration": 3.36
  },
  {
    "text": "zero we just improve the complexity of",
    "start": 1136.7,
    "duration": 3.24
  },
  {
    "text": "the model and we can drive it to zero",
    "start": 1138.5,
    "duration": 4.679
  },
  {
    "text": "but before 2006 is it that even on the",
    "start": 1139.94,
    "duration": 4.92
  },
  {
    "text": "training data I was not able to drive",
    "start": 1143.179,
    "duration": 5.101
  },
  {
    "text": "the error to zero or was it that because",
    "start": 1144.86,
    "duration": 6.24
  },
  {
    "text": "of unsupervised pre-training I was able",
    "start": 1148.28,
    "duration": 4.44
  },
  {
    "text": "to get better performance on the test",
    "start": 1151.1,
    "duration": 4.14
  },
  {
    "text": "data right so which of these is the",
    "start": 1152.72,
    "duration": 4.38
  },
  {
    "text": "reason for it to work better was it",
    "start": 1155.24,
    "duration": 4.26
  },
  {
    "text": "acting as a regularizer or was it acting",
    "start": 1157.1,
    "duration": 5.04
  },
  {
    "text": "as a better Optimizer right so now I",
    "start": 1159.5,
    "duration": 4.14
  },
  {
    "text": "comment that I want to make here right",
    "start": 1162.14,
    "duration": 3.06
  },
  {
    "text": "so this is I'm talking about the work",
    "start": 1163.64,
    "duration": 4.68
  },
  {
    "text": "which happened in the uh period of 2006",
    "start": 1165.2,
    "duration": 5.82
  },
  {
    "text": "to 2009 right so this work 2006 happened",
    "start": 1168.32,
    "duration": 4.5
  },
  {
    "text": "this unsuper SP training idea came out",
    "start": 1171.02,
    "duration": 3.96
  },
  {
    "text": "everybody was excited that now we can",
    "start": 1172.82,
    "duration": 4.08
  },
  {
    "text": "train deep neural networks and then",
    "start": 1174.98,
    "duration": 4.68
  },
  {
    "text": "people started inspecting right why is",
    "start": 1176.9,
    "duration": 3.84
  },
  {
    "text": "this working is it optimization",
    "start": 1179.66,
    "duration": 3.54
  },
  {
    "text": "regularization and so on and not many",
    "start": 1180.74,
    "duration": 5.819
  },
  {
    "text": "clear answers emerged right but what got",
    "start": 1183.2,
    "duration": 6.06
  },
  {
    "text": "reinforced that hey indeed something is",
    "start": 1186.559,
    "duration": 4.921
  },
  {
    "text": "happening and now it is possible to",
    "start": 1189.26,
    "duration": 4.2
  },
  {
    "text": "train the Deep neural networks and it",
    "start": 1191.48,
    "duration": 3.54
  },
  {
    "text": "could be because of optimization it",
    "start": 1193.46,
    "duration": 4.079
  },
  {
    "text": "could be because of regularization so",
    "start": 1195.02,
    "duration": 4.68
  },
  {
    "text": "why don't we design better methods of",
    "start": 1197.539,
    "duration": 4.02
  },
  {
    "text": "optimization why don't we design method",
    "start": 1199.7,
    "duration": 4.2
  },
  {
    "text": "methods of regularization and so on and",
    "start": 1201.559,
    "duration": 4.441
  },
  {
    "text": "that's the effect that we saw from 2009",
    "start": 1203.9,
    "duration": 5.22
  },
  {
    "text": "onwards our 2011-12 onwards we saw a",
    "start": 1206.0,
    "duration": 4.86
  },
  {
    "text": "series of better optimization rhythms",
    "start": 1209.12,
    "duration": 3.84
  },
  {
    "text": "right we saw all of that converging into",
    "start": 1210.86,
    "duration": 4.199
  },
  {
    "text": "Adam and then Adam and then again a few",
    "start": 1212.96,
    "duration": 4.5
  },
  {
    "text": "variants of that right so this sparked",
    "start": 1215.059,
    "duration": 3.901
  },
  {
    "text": "interest so whatever happened in this",
    "start": 1217.46,
    "duration": 3.24
  },
  {
    "text": "period maybe people did not have correct",
    "start": 1218.96,
    "duration": 3.12
  },
  {
    "text": "answers because these were again early",
    "start": 1220.7,
    "duration": 4.56
  },
  {
    "text": "days but these investigations suggested",
    "start": 1222.08,
    "duration": 5.94
  },
  {
    "text": "that hey maybe there is uh there is",
    "start": 1225.26,
    "duration": 4.56
  },
  {
    "text": "Merit in going after better optimization",
    "start": 1228.02,
    "duration": 3.72
  },
  {
    "text": "algorithms hey maybe there is better",
    "start": 1229.82,
    "duration": 3.359
  },
  {
    "text": "Merit in going after better",
    "start": 1231.74,
    "duration": 3.059
  },
  {
    "text": "regularization methods and we saw that",
    "start": 1233.179,
    "duration": 2.941
  },
  {
    "text": "one of the regularization methods that",
    "start": 1234.799,
    "duration": 4.441
  },
  {
    "text": "came out drop out around 2012 to 2014",
    "start": 1236.12,
    "duration": 5.76
  },
  {
    "text": "that is still very popular right so",
    "start": 1239.24,
    "duration": 4.14
  },
  {
    "text": "because of these investigations people",
    "start": 1241.88,
    "duration": 3.84
  },
  {
    "text": "got excited and said okay it could be",
    "start": 1243.38,
    "duration": 3.72
  },
  {
    "text": "because of optimization it could be",
    "start": 1245.72,
    "duration": 3.42
  },
  {
    "text": "because of regularization if that is the",
    "start": 1247.1,
    "duration": 3.6
  },
  {
    "text": "case then maybe unsupervised",
    "start": 1249.14,
    "duration": 3.3
  },
  {
    "text": "pre-training is not the only method",
    "start": 1250.7,
    "duration": 3.12
  },
  {
    "text": "which leads to better optimization I",
    "start": 1252.44,
    "duration": 3.359
  },
  {
    "text": "could come up with better optimizers or",
    "start": 1253.82,
    "duration": 3.479
  },
  {
    "text": "maybe unsupervised pre-training is not",
    "start": 1255.799,
    "duration": 2.76
  },
  {
    "text": "the only method which leads to better",
    "start": 1257.299,
    "duration": 2.461
  },
  {
    "text": "regularization I could come up with",
    "start": 1258.559,
    "duration": 3.301
  },
  {
    "text": "better regularizers then early stopping",
    "start": 1259.76,
    "duration": 3.96
  },
  {
    "text": "came drop Dropout came and all of these",
    "start": 1261.86,
    "duration": 3.6
  },
  {
    "text": "right so then all of these got used in",
    "start": 1263.72,
    "duration": 3.839
  },
  {
    "text": "the context of deep neural networks and",
    "start": 1265.46,
    "duration": 3.42
  },
  {
    "text": "then people also thought maybe there are",
    "start": 1267.559,
    "duration": 2.521
  },
  {
    "text": "other things maybe you could initialize",
    "start": 1268.88,
    "duration": 2.94
  },
  {
    "text": "the weights better because looks like",
    "start": 1270.08,
    "duration": 4.08
  },
  {
    "text": "unsupervised pre-training is doing some",
    "start": 1271.82,
    "duration": 4.08
  },
  {
    "text": "kind of initialization of the baits so",
    "start": 1274.16,
    "duration": 2.82
  },
  {
    "text": "why don't I come up with better",
    "start": 1275.9,
    "duration": 3.48
  },
  {
    "text": "initialization methods then people said",
    "start": 1276.98,
    "duration": 3.72
  },
  {
    "text": "okay maybe there are something to do",
    "start": 1279.38,
    "duration": 2.82
  },
  {
    "text": "with activation functions because we're",
    "start": 1280.7,
    "duration": 3.359
  },
  {
    "text": "talking about gradients and maybe",
    "start": 1282.2,
    "duration": 3.479
  },
  {
    "text": "something happens and if you change the",
    "start": 1284.059,
    "duration": 3.181
  },
  {
    "text": "activation functions we might get better",
    "start": 1285.679,
    "duration": 3.421
  },
  {
    "text": "gradients and maybe things would improve",
    "start": 1287.24,
    "duration": 3.72
  },
  {
    "text": "right so all of that whatever work",
    "start": 1289.1,
    "duration": 4.26
  },
  {
    "text": "happened in this 2006 to 2009 period",
    "start": 1290.96,
    "duration": 3.959
  },
  {
    "text": "which I'm going to talk about for the",
    "start": 1293.36,
    "duration": 3.84
  },
  {
    "text": "next 15-20 minutes",
    "start": 1294.919,
    "duration": 4.021
  },
  {
    "text": "none of this I will be able to give you",
    "start": 1297.2,
    "duration": 3.719
  },
  {
    "text": "very conclusive answers hey it was",
    "start": 1298.94,
    "duration": 3.599
  },
  {
    "text": "definitely due to optimization or",
    "start": 1300.919,
    "duration": 3.12
  },
  {
    "text": "definitely due to regulation but that",
    "start": 1302.539,
    "duration": 3.301
  },
  {
    "text": "does not matter what matters is it",
    "start": 1304.039,
    "duration": 3.241
  },
  {
    "text": "opened up these possibilities and",
    "start": 1305.84,
    "duration": 3.36
  },
  {
    "text": "following this work we saw a lot of",
    "start": 1307.28,
    "duration": 3.899
  },
  {
    "text": "other advances half of which we have",
    "start": 1309.2,
    "duration": 3.359
  },
  {
    "text": "already seen we have seen regularization",
    "start": 1311.179,
    "duration": 2.701
  },
  {
    "text": "methods we have seen optimization",
    "start": 1312.559,
    "duration": 3.061
  },
  {
    "text": "methods and today we are going to see",
    "start": 1313.88,
    "duration": 4.02
  },
  {
    "text": "the remaining half of this once I go",
    "start": 1315.62,
    "duration": 5.039
  },
  {
    "text": "over this period of 2006 to 2009 okay so",
    "start": 1317.9,
    "duration": 4.44
  },
  {
    "text": "these are the two possibilities we don't",
    "start": 1320.659,
    "duration": 3.361
  },
  {
    "text": "know what it was and I'll show you some",
    "start": 1322.34,
    "duration": 3.9
  },
  {
    "text": "Works which hinted at both of these and",
    "start": 1324.02,
    "duration": 4.08
  },
  {
    "text": "then from there on we'll continue the",
    "start": 1326.24,
    "duration": 4.34
  },
  {
    "text": "discussion",
    "start": 1328.1,
    "duration": 2.48
  }
]