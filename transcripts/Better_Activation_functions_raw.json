[
  {
    "text": "foreign",
    "start": 0.299,
    "duration": 3.0
  },
  {
    "text": "[Music]",
    "start": 6.62,
    "duration": 3.739
  },
  {
    "text": "so this is where we were when we left",
    "start": 19.199,
    "duration": 4.441
  },
  {
    "text": "off yesterday we were talking about how",
    "start": 21.119,
    "duration": 4.5
  },
  {
    "text": "deep learning has evolved and there are",
    "start": 23.64,
    "duration": 4.08
  },
  {
    "text": "these four axes that we were considering",
    "start": 25.619,
    "duration": 4.441
  },
  {
    "text": "and we have already talked in detail",
    "start": 27.72,
    "duration": 4.98
  },
  {
    "text": "about the first two and today we'll",
    "start": 30.06,
    "duration": 4.499
  },
  {
    "text": "start a discussion on better activation",
    "start": 32.7,
    "duration": 3.9
  },
  {
    "text": "functions right so that's where we are",
    "start": 34.559,
    "duration": 5.401
  },
  {
    "text": "so with that let me just go to the next",
    "start": 36.6,
    "duration": 5.7
  },
  {
    "text": "module",
    "start": 39.96,
    "duration": 5.18
  },
  {
    "text": "uh",
    "start": 42.3,
    "duration": 2.84
  },
  {
    "text": "yeah",
    "start": 45.66,
    "duration": 3.36
  },
  {
    "text": "so so before we look at these activation",
    "start": 46.62,
    "duration": 5.34
  },
  {
    "text": "functions right so let's try to uh first",
    "start": 49.02,
    "duration": 4.68
  },
  {
    "text": "motivate why do we need to talk about",
    "start": 51.96,
    "duration": 3.72
  },
  {
    "text": "activation functions and the motivation",
    "start": 53.7,
    "duration": 3.839
  },
  {
    "text": "comes if we try to introspect this",
    "start": 55.68,
    "duration": 4.26
  },
  {
    "text": "question what makes deep neural Nets",
    "start": 57.539,
    "duration": 4.381
  },
  {
    "text": "powerful right and we already saw this",
    "start": 59.94,
    "duration": 3.959
  },
  {
    "text": "uh briefly when we were talking about",
    "start": 61.92,
    "duration": 4.08
  },
  {
    "text": "the universal not briefly actually quite",
    "start": 63.899,
    "duration": 3.921
  },
  {
    "text": "in detail when we are talking about the",
    "start": 66.0,
    "duration": 4.14
  },
  {
    "text": "universal approximation theorems I just",
    "start": 67.82,
    "duration": 5.979
  },
  {
    "text": "want to revisit that and emphasize on uh",
    "start": 70.14,
    "duration": 6.78
  },
  {
    "text": "some a component of the deep neural",
    "start": 73.799,
    "duration": 6.481
  },
  {
    "text": "network which is important in granting",
    "start": 76.92,
    "duration": 5.1
  },
  {
    "text": "it its power right so let's see what",
    "start": 80.28,
    "duration": 4.26
  },
  {
    "text": "that is so now suppose I have this deep",
    "start": 82.02,
    "duration": 4.02
  },
  {
    "text": "neural network right it's a thin network",
    "start": 84.54,
    "duration": 2.88
  },
  {
    "text": "but doesn't matter it's still a deep",
    "start": 86.04,
    "duration": 4.14
  },
  {
    "text": "neural network and suppose all my",
    "start": 87.42,
    "duration": 4.8
  },
  {
    "text": "sigmars which are the sigmoid or the",
    "start": 90.18,
    "duration": 4.5
  },
  {
    "text": "logistic function I just replace them by",
    "start": 92.22,
    "duration": 4.259
  },
  {
    "text": "a simple linear transformation it's",
    "start": 94.68,
    "duration": 4.68
  },
  {
    "text": "earlier I had a",
    "start": 96.479,
    "duration": 6.78
  },
  {
    "text": "any of the A's is equal to W into h plus",
    "start": 99.36,
    "duration": 8.46
  },
  {
    "text": "b and then H was sigmoid of a so I am",
    "start": 103.259,
    "duration": 5.761
  },
  {
    "text": "just saying instead of H equal to",
    "start": 107.82,
    "duration": 3.24
  },
  {
    "text": "sigmoid of a if I just make it h equal",
    "start": 109.02,
    "duration": 4.26
  },
  {
    "text": "to a right so there's only a linear",
    "start": 111.06,
    "duration": 4.26
  },
  {
    "text": "layer and there is no non-linear layer",
    "start": 113.28,
    "duration": 3.96
  },
  {
    "text": "so that's the situation that I am",
    "start": 115.32,
    "duration": 4.74
  },
  {
    "text": "considering uh so this is what it would",
    "start": 117.24,
    "duration": 5.4
  },
  {
    "text": "look like right so Y is a function of X",
    "start": 120.06,
    "duration": 6.239
  },
  {
    "text": "so X first gets transformed linearly by",
    "start": 122.64,
    "duration": 5.94
  },
  {
    "text": "W1",
    "start": 126.299,
    "duration": 5.101
  },
  {
    "text": "so that is what a one is a one is W one",
    "start": 128.58,
    "duration": 6.0
  },
  {
    "text": "into X but then H1 is just a one so it",
    "start": 131.4,
    "duration": 5.339
  },
  {
    "text": "is again W one into X then this x is the",
    "start": 134.58,
    "duration": 4.08
  },
  {
    "text": "input to the next layer and then you get",
    "start": 136.739,
    "duration": 4.981
  },
  {
    "text": "W 2 is into W1 into X right so w 2 into",
    "start": 138.66,
    "duration": 5.88
  },
  {
    "text": "H1 which is W 2 into W 1 into X which is",
    "start": 141.72,
    "duration": 5.12
  },
  {
    "text": "the same as",
    "start": 144.54,
    "duration": 4.74
  },
  {
    "text": "H2 right because H2 is equal to a two",
    "start": 146.84,
    "duration": 4.42
  },
  {
    "text": "and so on you continue and your output",
    "start": 149.28,
    "duration": 4.44
  },
  {
    "text": "is just a linear uh it's just a",
    "start": 151.26,
    "duration": 5.52
  },
  {
    "text": "multiplication of Weights uh by the",
    "start": 153.72,
    "duration": 7.44
  },
  {
    "text": "input X right and now here this is as",
    "start": 156.78,
    "duration": 7.5
  },
  {
    "text": "good as just having",
    "start": 161.16,
    "duration": 4.98
  },
  {
    "text": "a single linear transformation right so",
    "start": 164.28,
    "duration": 4.739
  },
  {
    "text": "this is just as good as saying that Y is",
    "start": 166.14,
    "duration": 6.84
  },
  {
    "text": "equal to sum w x where W is a product of",
    "start": 169.019,
    "duration": 6.541
  },
  {
    "text": "these four W's right so this is just a",
    "start": 172.98,
    "duration": 4.619
  },
  {
    "text": "linear transformation this is not even",
    "start": 175.56,
    "duration": 4.319
  },
  {
    "text": "though it's a deep Network it does not",
    "start": 177.599,
    "duration": 4.981
  },
  {
    "text": "add any value to me beyond what a linear",
    "start": 179.879,
    "duration": 4.981
  },
  {
    "text": "transformation would act right so the",
    "start": 182.58,
    "duration": 5.04
  },
  {
    "text": "depth of the network is not what ah",
    "start": 184.86,
    "duration": 4.56
  },
  {
    "text": "solely gives the Deep neural network a",
    "start": 187.62,
    "duration": 3.119
  },
  {
    "text": "power of course it plays a role because",
    "start": 189.42,
    "duration": 3.42
  },
  {
    "text": "then you have these composite",
    "start": 190.739,
    "duration": 4.201
  },
  {
    "text": "non-linearities but if you remove the",
    "start": 192.84,
    "duration": 4.14
  },
  {
    "text": "non-linearities then you just have a",
    "start": 194.94,
    "duration": 5.04
  },
  {
    "text": "linear transformation of X and then you",
    "start": 196.98,
    "duration": 5.22
  },
  {
    "text": "are just going to learn linear decision",
    "start": 199.98,
    "duration": 3.479
  },
  {
    "text": "boundaries right because you have just",
    "start": 202.2,
    "duration": 4.02
  },
  {
    "text": "said that Y is equal to w x so your y",
    "start": 203.459,
    "duration": 4.081
  },
  {
    "text": "you are just assuming that the",
    "start": 206.22,
    "duration": 3.659
  },
  {
    "text": "relationship between Y and X is given by",
    "start": 207.54,
    "duration": 3.839
  },
  {
    "text": "a line or a hyperplane in higher",
    "start": 209.879,
    "duration": 3.601
  },
  {
    "text": "Dimensions right so then in particular",
    "start": 211.379,
    "duration": 4.801
  },
  {
    "text": "if you have these complex decision",
    "start": 213.48,
    "duration": 5.1
  },
  {
    "text": "boundaries like that we had here where",
    "start": 216.18,
    "duration": 4.02
  },
  {
    "text": "you really need something long linear",
    "start": 218.58,
    "duration": 4.079
  },
  {
    "text": "right so here you are not learning a",
    "start": 220.2,
    "duration": 4.02
  },
  {
    "text": "line you need something like a circular",
    "start": 222.659,
    "duration": 4.021
  },
  {
    "text": "either a decision boundary which",
    "start": 224.22,
    "duration": 4.32
  },
  {
    "text": "separates the positive points which are",
    "start": 226.68,
    "duration": 3.779
  },
  {
    "text": "inside the brown points from the",
    "start": 228.54,
    "duration": 3.059
  },
  {
    "text": "negative points which are the blue",
    "start": 230.459,
    "duration": 3.181
  },
  {
    "text": "points right I cannot draw any line",
    "start": 231.599,
    "duration": 6.78
  },
  {
    "text": "which suitably separates my uh the brown",
    "start": 233.64,
    "duration": 6.54
  },
  {
    "text": "points from the blue points right",
    "start": 238.379,
    "duration": 3.661
  },
  {
    "text": "without giving me a high error so that",
    "start": 240.18,
    "duration": 4.38
  },
  {
    "text": "is not possible right so uh the reason",
    "start": 242.04,
    "duration": 4.619
  },
  {
    "text": "uh deep neural network",
    "start": 244.56,
    "duration": 5.7
  },
  {
    "text": "can learn such arbitrary boundaries and",
    "start": 246.659,
    "duration": 6.66
  },
  {
    "text": "hence act as a universal approximator",
    "start": 250.26,
    "duration": 5.1
  },
  {
    "text": "right because now we have a very complex",
    "start": 253.319,
    "duration": 4.98
  },
  {
    "text": "function between Y and X which is",
    "start": 255.36,
    "duration": 4.74
  },
  {
    "text": "looking like this right this is the",
    "start": 258.299,
    "duration": 3.961
  },
  {
    "text": "function that we are interested in and",
    "start": 260.1,
    "duration": 3.539
  },
  {
    "text": "as we had seen while looking at the",
    "start": 262.26,
    "duration": 3.06
  },
  {
    "text": "universal approximation theorem and the",
    "start": 263.639,
    "duration": 3.181
  },
  {
    "text": "illustrative proof of it where we are",
    "start": 265.32,
    "duration": 3.54
  },
  {
    "text": "drawn these towers we could do that",
    "start": 266.82,
    "duration": 4.98
  },
  {
    "text": "because we had a lean a non-linear layer",
    "start": 268.86,
    "duration": 4.86
  },
  {
    "text": "it what the theorem said is that if you",
    "start": 271.8,
    "duration": 6.06
  },
  {
    "text": "have even a single sigmoid layer then",
    "start": 273.72,
    "duration": 6.12
  },
  {
    "text": "you can then the then the network can",
    "start": 277.86,
    "duration": 4.08
  },
  {
    "text": "act as a universe Universal approximator",
    "start": 279.84,
    "duration": 4.5
  },
  {
    "text": "so sigmoid was important there right so",
    "start": 281.94,
    "duration": 5.1
  },
  {
    "text": "these non-linearities are what give deep",
    "start": 284.34,
    "duration": 4.26
  },
  {
    "text": "neural networks their power right so",
    "start": 287.04,
    "duration": 3.24
  },
  {
    "text": "there's nothing new that I have said we",
    "start": 288.6,
    "duration": 3.659
  },
  {
    "text": "have already discussed this but I just",
    "start": 290.28,
    "duration": 3.6
  },
  {
    "text": "wanted to quickly refresh it because",
    "start": 292.259,
    "duration": 4.681
  },
  {
    "text": "it's relevant for the discussion so now",
    "start": 293.88,
    "duration": 5.4
  },
  {
    "text": "if the power of deep neural networks",
    "start": 296.94,
    "duration": 5.58
  },
  {
    "text": "comes from such non-linearities where do",
    "start": 299.28,
    "duration": 4.68
  },
  {
    "text": "the nonlinearities come from the",
    "start": 302.52,
    "duration": 3.54
  },
  {
    "text": "non-linearities essentially come from",
    "start": 303.96,
    "duration": 4.26
  },
  {
    "text": "the activation function and hence we",
    "start": 306.06,
    "duration": 3.78
  },
  {
    "text": "need to discuss about these activation",
    "start": 308.22,
    "duration": 4.02
  },
  {
    "text": "functions and what do these bring and",
    "start": 309.84,
    "duration": 3.96
  },
  {
    "text": "are there better activation functions",
    "start": 312.24,
    "duration": 4.08
  },
  {
    "text": "possible is is any non-linear function",
    "start": 313.8,
    "duration": 4.98
  },
  {
    "text": "okay or we want certain better and",
    "start": 316.32,
    "duration": 4.56
  },
  {
    "text": "better properties should be satisfied by",
    "start": 318.78,
    "duration": 3.66
  },
  {
    "text": "these nonlinear functions so that's what",
    "start": 320.88,
    "duration": 3.659
  },
  {
    "text": "we are going to focus on today right so",
    "start": 322.44,
    "duration": 4.14
  },
  {
    "text": "that's the reason why our discussion on",
    "start": 324.539,
    "duration": 4.021
  },
  {
    "text": "activation functions is important right",
    "start": 326.58,
    "duration": 3.839
  },
  {
    "text": "now we look at some of the popular",
    "start": 328.56,
    "duration": 4.44
  },
  {
    "text": "activation functions the quite a bit of",
    "start": 330.419,
    "duration": 4.021
  },
  {
    "text": "this material at least the initial part",
    "start": 333.0,
    "duration": 5.34
  },
  {
    "text": "is taken from the lecture like a few",
    "start": 334.44,
    "duration": 8.22
  },
  {
    "text": "years back uh lecture slides from Andre",
    "start": 338.34,
    "duration": 6.5
  },
  {
    "text": "carpathy's course",
    "start": 342.66,
    "duration": 5.4
  },
  {
    "text": "I'm not sure if that those videos are",
    "start": 344.84,
    "duration": 4.66
  },
  {
    "text": "still available but they were available",
    "start": 348.06,
    "duration": 2.76
  },
  {
    "text": "a few years back and I had taken",
    "start": 349.5,
    "duration": 3.9
  },
  {
    "text": "material from there so",
    "start": 350.82,
    "duration": 4.92
  },
  {
    "text": "Let's uh let's start with the sigmoid",
    "start": 353.4,
    "duration": 5.28
  },
  {
    "text": "function so sigmoid is given by this",
    "start": 355.74,
    "duration": 4.92
  },
  {
    "text": "Sigma of X is equal to 1 over 1 plus e",
    "start": 358.68,
    "duration": 4.44
  },
  {
    "text": "raised to minus X and this is what it",
    "start": 360.66,
    "duration": 5.58
  },
  {
    "text": "looks like it is between 0 and 1. so",
    "start": 363.12,
    "duration": 5.16
  },
  {
    "text": "whatever input you pass it in our case",
    "start": 366.24,
    "duration": 5.459
  },
  {
    "text": "uh the input that we passed to uh the",
    "start": 368.28,
    "duration": 6.0
  },
  {
    "text": "sigmoid function is sum a which in turn",
    "start": 371.699,
    "duration": 5.581
  },
  {
    "text": "is some linear transformation of the",
    "start": 374.28,
    "duration": 5.759
  },
  {
    "text": "previous layer right we pass some Ai and",
    "start": 377.28,
    "duration": 4.62
  },
  {
    "text": "it is some linear transformation of the",
    "start": 380.039,
    "duration": 5.301
  },
  {
    "text": "previous layer right that's what our",
    "start": 381.9,
    "duration": 6.239
  },
  {
    "text": "input is going to be and whatever is the",
    "start": 385.34,
    "duration": 5.38
  },
  {
    "text": "input that applies element wise on this",
    "start": 388.139,
    "duration": 5.221
  },
  {
    "text": "Vector a and it just squishes it or",
    "start": 390.72,
    "duration": 4.8
  },
  {
    "text": "compresses it between 0 to 1 right so we",
    "start": 393.36,
    "duration": 6.14
  },
  {
    "text": "already have seen this in the past",
    "start": 395.52,
    "duration": 3.98
  },
  {
    "text": "uh so now since we are always interested",
    "start": 400.74,
    "duration": 3.66
  },
  {
    "text": "in gradients right because as I said",
    "start": 403.139,
    "duration": 3.68
  },
  {
    "text": "that training deep neural networks is",
    "start": 404.4,
    "duration": 5.1
  },
  {
    "text": "largely about Computing gradients and",
    "start": 406.819,
    "duration": 4.72
  },
  {
    "text": "then based on the gradients either the",
    "start": 409.5,
    "duration": 3.9
  },
  {
    "text": "training would go fast low and so on",
    "start": 411.539,
    "duration": 3.061
  },
  {
    "text": "right so let's see what is the gradient",
    "start": 413.4,
    "duration": 3.0
  },
  {
    "text": "of the sigmoid function right and this",
    "start": 414.6,
    "duration": 3.42
  },
  {
    "text": "again we have computed if you have not",
    "start": 416.4,
    "duration": 3.359
  },
  {
    "text": "you can just take this as an exercise",
    "start": 418.02,
    "duration": 4.019
  },
  {
    "text": "and try to find the gradient of in fact",
    "start": 419.759,
    "duration": 4.5
  },
  {
    "text": "we have but you can just revisit it and",
    "start": 422.039,
    "duration": 3.541
  },
  {
    "text": "try to compute the gradient of this",
    "start": 424.259,
    "duration": 3.241
  },
  {
    "text": "function and you will end up with this",
    "start": 425.58,
    "duration": 3.72
  },
  {
    "text": "formula right which is Sigma X into 1",
    "start": 427.5,
    "duration": 4.62
  },
  {
    "text": "minus Sigma X right and now let's focus",
    "start": 429.3,
    "duration": 6.54
  },
  {
    "text": "on this formula so what what could",
    "start": 432.12,
    "duration": 5.94
  },
  {
    "text": "happen if we use sigmoid in a deep",
    "start": 435.84,
    "duration": 4.38
  },
  {
    "text": "neural network given that the gradient",
    "start": 438.06,
    "duration": 5.1
  },
  {
    "text": "of the sigmoid is given by this formula",
    "start": 440.22,
    "duration": 3.8
  },
  {
    "text": "right",
    "start": 443.16,
    "duration": 4.5
  },
  {
    "text": "in particular note that at the extremes",
    "start": 444.02,
    "duration": 6.22
  },
  {
    "text": "ah when the sigmoid neuron saturates",
    "start": 447.66,
    "duration": 4.68
  },
  {
    "text": "what do I mean by saturate it takes its",
    "start": 450.24,
    "duration": 4.799
  },
  {
    "text": "maximum value which is 1 or if it takes",
    "start": 452.34,
    "duration": 6.9
  },
  {
    "text": "its minimum value which is 0 right or",
    "start": 455.039,
    "duration": 6.66
  },
  {
    "text": "close to 0 in those cases the gradient",
    "start": 459.24,
    "duration": 4.38
  },
  {
    "text": "vanishes right so if I put Sigma x equal",
    "start": 461.699,
    "duration": 4.62
  },
  {
    "text": "to 1 then I'll get 1 into 1 minus 1 so",
    "start": 463.62,
    "duration": 4.62
  },
  {
    "text": "the gradient will vanish that is in this",
    "start": 466.319,
    "duration": 4.5
  },
  {
    "text": "region or if I put Sigma x equal to 0",
    "start": 468.24,
    "duration": 4.44
  },
  {
    "text": "which is corresponding to this region",
    "start": 470.819,
    "duration": 4.261
  },
  {
    "text": "then again 0 into anything is 0 right so",
    "start": 472.68,
    "duration": 4.98
  },
  {
    "text": "the gradient actually vanishes and we",
    "start": 475.08,
    "duration": 4.26
  },
  {
    "text": "know that if the gradient vanishes or if",
    "start": 477.66,
    "duration": 3.42
  },
  {
    "text": "it's very smaller so in these regions",
    "start": 479.34,
    "duration": 3.6
  },
  {
    "text": "starting from here the gradient is very",
    "start": 481.08,
    "duration": 4.559
  },
  {
    "text": "small and that is a problem in gradient",
    "start": 482.94,
    "duration": 4.92
  },
  {
    "text": "based learning methods because the",
    "start": 485.639,
    "duration": 4.261
  },
  {
    "text": "updates will not be high right so that's",
    "start": 487.86,
    "duration": 4.92
  },
  {
    "text": "where this discussion is headed ah so",
    "start": 489.9,
    "duration": 6.06
  },
  {
    "text": "let's let's try to go there now okay so",
    "start": 492.78,
    "duration": 5.699
  },
  {
    "text": "this is a deep neural network ah and if",
    "start": 495.96,
    "duration": 6.299
  },
  {
    "text": "I have used a sigmoid non-linearity then",
    "start": 498.479,
    "duration": 5.641
  },
  {
    "text": "in some point in this chain rule right",
    "start": 502.259,
    "duration": 3.301
  },
  {
    "text": "suppose I am trying to find the",
    "start": 504.12,
    "duration": 4.199
  },
  {
    "text": "derivative of this weight with respect",
    "start": 505.56,
    "duration": 5.22
  },
  {
    "text": "to the loss function at some point I am",
    "start": 508.319,
    "duration": 3.84
  },
  {
    "text": "going to encounter this in the chain",
    "start": 510.78,
    "duration": 3.66
  },
  {
    "text": "rule I will encounter derivative of H3",
    "start": 512.159,
    "duration": 4.5
  },
  {
    "text": "with respect to A3 and you can convince",
    "start": 514.44,
    "duration": 5.459
  },
  {
    "text": "yourself right I mean any uh any even if",
    "start": 516.659,
    "duration": 7.021
  },
  {
    "text": "W1 or W2 or W3 if I'm trying to find the",
    "start": 519.899,
    "duration": 5.101
  },
  {
    "text": "derivative of this with respect to the",
    "start": 523.68,
    "duration": 3.54
  },
  {
    "text": "loss function then this guy is sitting",
    "start": 525.0,
    "duration": 3.959
  },
  {
    "text": "in the path so you will encounter that",
    "start": 527.22,
    "duration": 3.72
  },
  {
    "text": "and that is nothing but the sigmoid of",
    "start": 528.959,
    "duration": 3.901
  },
  {
    "text": "the derivative of the sigmoid function",
    "start": 530.94,
    "duration": 5.399
  },
  {
    "text": "with respect to A3 and that is given by",
    "start": 532.86,
    "duration": 5.4
  },
  {
    "text": "this formula right so that's something",
    "start": 536.339,
    "duration": 4.141
  },
  {
    "text": "that we'll always encounter whenever we",
    "start": 538.26,
    "duration": 4.32
  },
  {
    "text": "are doing a back propagation with and",
    "start": 540.48,
    "duration": 4.62
  },
  {
    "text": "using any gradient based method so now",
    "start": 542.58,
    "duration": 5.28
  },
  {
    "text": "what is the consequence of this right uh",
    "start": 545.1,
    "duration": 4.739
  },
  {
    "text": "so as I said there is this concept of",
    "start": 547.86,
    "duration": 3.96
  },
  {
    "text": "saturated neuron so a neuron is said to",
    "start": 549.839,
    "duration": 4.56
  },
  {
    "text": "be saturated if it is at its peak or",
    "start": 551.82,
    "duration": 4.38
  },
  {
    "text": "lowest value rate minimum or maximum",
    "start": 554.399,
    "duration": 3.241
  },
  {
    "text": "value in the case of sigmoid neurons",
    "start": 556.2,
    "duration": 4.5
  },
  {
    "text": "it's 0 or 1. so whenever it is saturated",
    "start": 557.64,
    "duration": 6.12
  },
  {
    "text": "the gradient is going to be 0 right and",
    "start": 560.7,
    "duration": 5.54
  },
  {
    "text": "if it is 0 then your training will not",
    "start": 563.76,
    "duration": 4.579
  },
  {
    "text": "progress because",
    "start": 566.24,
    "duration": 5.2
  },
  {
    "text": "once the neuron is saturated your",
    "start": 568.339,
    "duration": 5.68
  },
  {
    "text": "gradients are zero now the weights are",
    "start": 571.44,
    "duration": 4.74
  },
  {
    "text": "not getting updated and then it's likely",
    "start": 574.019,
    "duration": 3.841
  },
  {
    "text": "that because of these weights right",
    "start": 576.18,
    "duration": 3.24
  },
  {
    "text": "especially if the weights are high so",
    "start": 577.86,
    "duration": 3.0
  },
  {
    "text": "let's see why would the neurons get",
    "start": 579.42,
    "duration": 2.88
  },
  {
    "text": "saturated right so let's try to",
    "start": 580.86,
    "duration": 2.599
  },
  {
    "text": "understand that",
    "start": 582.3,
    "duration": 3.479
  },
  {
    "text": "why is it that it would get saturated",
    "start": 583.459,
    "duration": 4.481
  },
  {
    "text": "and it's not given right because the",
    "start": 585.779,
    "duration": 4.441
  },
  {
    "text": "function can take values between 0 to 1.",
    "start": 587.94,
    "duration": 3.6
  },
  {
    "text": "so why is it that it would suddenly",
    "start": 590.22,
    "duration": 3.239
  },
  {
    "text": "become one or suddenly become zero right",
    "start": 591.54,
    "duration": 3.96
  },
  {
    "text": "so let's let's try to answer that so",
    "start": 593.459,
    "duration": 4.021
  },
  {
    "text": "this if I were to consider any such",
    "start": 595.5,
    "duration": 3.779
  },
  {
    "text": "neuron in the network right so suppose",
    "start": 597.48,
    "duration": 3.12
  },
  {
    "text": "this is one of the neurons this is not",
    "start": 599.279,
    "duration": 3.06
  },
  {
    "text": "the output neuron but say some neuron in",
    "start": 600.6,
    "duration": 5.64
  },
  {
    "text": "the network and this is what the output",
    "start": 602.339,
    "duration": 5.401
  },
  {
    "text": "of this neuron would be right it would",
    "start": 606.24,
    "duration": 4.32
  },
  {
    "text": "be the sigmoid of the linear combination",
    "start": 607.74,
    "duration": 4.68
  },
  {
    "text": "of the inputs connected to it right so",
    "start": 610.56,
    "duration": 6.14
  },
  {
    "text": "this is what the output would be now",
    "start": 612.42,
    "duration": 4.28
  },
  {
    "text": "suppose I have initialized the weights",
    "start": 620.42,
    "duration": 3.94
  },
  {
    "text": "to a very high value",
    "start": 622.8,
    "duration": 3.719
  },
  {
    "text": "right suppose my weights are say suppose",
    "start": 624.36,
    "duration": 6.419
  },
  {
    "text": "100 200 and so on and now my inputs are",
    "start": 626.519,
    "duration": 5.341
  },
  {
    "text": "all standardized it'll be anyway",
    "start": 630.779,
    "duration": 3.0
  },
  {
    "text": "standardize the inputs that means we",
    "start": 631.86,
    "duration": 4.38
  },
  {
    "text": "divide by the subtract the mean and then",
    "start": 633.779,
    "duration": 4.261
  },
  {
    "text": "divide by the variance so all our inputs",
    "start": 636.24,
    "duration": 4.02
  },
  {
    "text": "are between 0 to 1. so the inputs are",
    "start": 638.04,
    "duration": 3.96
  },
  {
    "text": "typically standardized that's anyways",
    "start": 640.26,
    "duration": 4.199
  },
  {
    "text": "recommended for using gradient based",
    "start": 642.0,
    "duration": 5.339
  },
  {
    "text": "methods so the inputs will not cause a",
    "start": 644.459,
    "duration": 4.081
  },
  {
    "text": "problem there will be anyway small",
    "start": 647.339,
    "duration": 2.821
  },
  {
    "text": "between zero to one but if the weights",
    "start": 648.54,
    "duration": 3.78
  },
  {
    "text": "are initialized to high value then this",
    "start": 650.16,
    "duration": 4.739
  },
  {
    "text": "sub w i Sigma I is w i x is going to be",
    "start": 652.32,
    "duration": 5.16
  },
  {
    "text": "high and remember see in the plot even",
    "start": 654.899,
    "duration": 5.461
  },
  {
    "text": "for values like 2.5 the sigmoid neuron",
    "start": 657.48,
    "duration": 4.44
  },
  {
    "text": "has already saturated right on the x",
    "start": 660.36,
    "duration": 3.659
  },
  {
    "text": "axis you have 2.5 and the y axis you",
    "start": 661.92,
    "duration": 4.26
  },
  {
    "text": "have almost one right so if your weights",
    "start": 664.019,
    "duration": 4.44
  },
  {
    "text": "are initialized even reasonably High",
    "start": 666.18,
    "duration": 6.0
  },
  {
    "text": "then your neurons your uh the sigma this",
    "start": 668.459,
    "duration": 6.12
  },
  {
    "text": "the quantity pass through the sigmoid",
    "start": 672.18,
    "duration": 4.5
  },
  {
    "text": "neuron is going to be high quantity and",
    "start": 674.579,
    "duration": 4.021
  },
  {
    "text": "then for that the sigma is going to be",
    "start": 676.68,
    "duration": 4.98
  },
  {
    "text": "close to 1 right uh similarly if the",
    "start": 678.6,
    "duration": 4.56
  },
  {
    "text": "weights are initiates to high negative",
    "start": 681.66,
    "duration": 3.239
  },
  {
    "text": "value the same would happen you have a",
    "start": 683.16,
    "duration": 3.419
  },
  {
    "text": "high negative value and the neuron would",
    "start": 684.899,
    "duration": 3.781
  },
  {
    "text": "saturate to zero now of course you could",
    "start": 686.579,
    "duration": 3.781
  },
  {
    "text": "argue that why would the wage be",
    "start": 688.68,
    "duration": 3.719
  },
  {
    "text": "highered I would maybe initialize them",
    "start": 690.36,
    "duration": 4.44
  },
  {
    "text": "to low values but now if there are like",
    "start": 692.399,
    "duration": 5.041
  },
  {
    "text": "a thousand neurons in one layer right",
    "start": 694.8,
    "duration": 5.4
  },
  {
    "text": "then this is a sum of thousand terms and",
    "start": 697.44,
    "duration": 4.86
  },
  {
    "text": "even with very small weights a sum of",
    "start": 700.2,
    "duration": 4.74
  },
  {
    "text": "thousand terms reaching a value of 2 or",
    "start": 702.3,
    "duration": 5.4
  },
  {
    "text": "3 is not really surprising right so you",
    "start": 704.94,
    "duration": 4.26
  },
  {
    "text": "have to very carefully initialize the",
    "start": 707.7,
    "duration": 3.12
  },
  {
    "text": "weights and that's why this part would",
    "start": 709.2,
    "duration": 3.6
  },
  {
    "text": "be connected to the second part of this",
    "start": 710.82,
    "duration": 3.24
  },
  {
    "text": "lecture where we will talk about weight",
    "start": 712.8,
    "duration": 3.36
  },
  {
    "text": "initialization methods and we will try",
    "start": 714.06,
    "duration": 4.26
  },
  {
    "text": "to see how to initialize the weights",
    "start": 716.16,
    "duration": 4.26
  },
  {
    "text": "correctly so that we don't end up with",
    "start": 718.32,
    "duration": 4.079
  },
  {
    "text": "saturated neurons right so the main",
    "start": 720.42,
    "duration": 3.84
  },
  {
    "text": "point here is that neurons can saturate",
    "start": 722.399,
    "duration": 4.081
  },
  {
    "text": "and if they saturate then the training",
    "start": 724.26,
    "duration": 4.86
  },
  {
    "text": "kind of becomes problematic because the",
    "start": 726.48,
    "duration": 4.56
  },
  {
    "text": "gradients are not flowing through right",
    "start": 729.12,
    "duration": 4.68
  },
  {
    "text": "and if if the neuron is saturating with",
    "start": 731.04,
    "duration": 5.4
  },
  {
    "text": "a value one that means actually these",
    "start": 733.8,
    "duration": 5.88
  },
  {
    "text": "weights were like uh strongly",
    "start": 736.44,
    "duration": 6.18
  },
  {
    "text": "participating in the output right and",
    "start": 739.68,
    "duration": 4.68
  },
  {
    "text": "hence they would have had some impact on",
    "start": 742.62,
    "duration": 3.719
  },
  {
    "text": "the loss function but that signal will",
    "start": 744.36,
    "duration": 3.9
  },
  {
    "text": "not flow back to these weights because",
    "start": 746.339,
    "duration": 3.961
  },
  {
    "text": "the derivatives have died right so",
    "start": 748.26,
    "duration": 4.019
  },
  {
    "text": "that's a serious problem so this could",
    "start": 750.3,
    "duration": 6.479
  },
  {
    "text": "lead to challenges during training",
    "start": 752.279,
    "duration": 8.541
  },
  {
    "text": "since this is not a good",
    "start": 756.779,
    "duration": 4.041
  },
  {
    "text": "the next problem is that sigmoids are",
    "start": 761.16,
    "duration": 4.02
  },
  {
    "text": "not a zero Center that's again obvious",
    "start": 763.019,
    "duration": 5.041
  },
  {
    "text": "from the plot because the plot is from",
    "start": 765.18,
    "duration": 5.459
  },
  {
    "text": "zero to one zero centered would be",
    "start": 768.06,
    "duration": 4.26
  },
  {
    "text": "something like say from example minus",
    "start": 770.639,
    "duration": 3.121
  },
  {
    "text": "one to one so there is a zero in between",
    "start": 772.32,
    "duration": 3.78
  },
  {
    "text": "so it's equally distributed around zero",
    "start": 773.76,
    "duration": 4.259
  },
  {
    "text": "whereas this itself starts from zero and",
    "start": 776.1,
    "duration": 3.72
  },
  {
    "text": "goes to one so the center is around 0.5",
    "start": 778.019,
    "duration": 4.081
  },
  {
    "text": "right so what if it's not zero center",
    "start": 779.82,
    "duration": 4.319
  },
  {
    "text": "right what could the possible what could",
    "start": 782.1,
    "duration": 4.2
  },
  {
    "text": "the problem be right so now let us try",
    "start": 784.139,
    "duration": 4.021
  },
  {
    "text": "to again see that through an example",
    "start": 786.3,
    "duration": 4.62
  },
  {
    "text": "suppose I have this output layer and",
    "start": 788.16,
    "duration": 5.0
  },
  {
    "text": "before that I have this one sigmoid",
    "start": 790.92,
    "duration": 6.3
  },
  {
    "text": "neuron right and it is connected to two",
    "start": 793.16,
    "duration": 7.84
  },
  {
    "text": "inputs h21 and H22 so I'll have the A3",
    "start": 797.22,
    "duration": 6.419
  },
  {
    "text": "would be this and then I'll passing it",
    "start": 801.0,
    "duration": 4.5
  },
  {
    "text": "through the sigmoid neuron right now",
    "start": 803.639,
    "duration": 3.841
  },
  {
    "text": "again if I were to update the weights W1",
    "start": 805.5,
    "duration": 4.8
  },
  {
    "text": "or W2 let's see what chain rule I will",
    "start": 807.48,
    "duration": 5.299
  },
  {
    "text": "use there",
    "start": 810.3,
    "duration": 2.479
  },
  {
    "text": "so this is what my derivative would be",
    "start": 813.12,
    "duration": 4.5
  },
  {
    "text": "right so it would be derivative of the",
    "start": 815.459,
    "duration": 3.961
  },
  {
    "text": "loss function which is sitting here with",
    "start": 817.62,
    "duration": 4.62
  },
  {
    "text": "respect to Y right then the derivative",
    "start": 819.42,
    "duration": 5.159
  },
  {
    "text": "of y with respect to H3 so the output is",
    "start": 822.24,
    "duration": 4.62
  },
  {
    "text": "H3 then the derivative of H3 with",
    "start": 824.579,
    "duration": 6.241
  },
  {
    "text": "respect to A3 right and then the uh oh",
    "start": 826.86,
    "duration": 5.58
  },
  {
    "text": "sorry this",
    "start": 830.82,
    "duration": 4.44
  },
  {
    "text": "and then the derivative of uh",
    "start": 832.44,
    "duration": 6.899
  },
  {
    "text": "A3 with respect to W1 which I have",
    "start": 835.26,
    "duration": 6.12
  },
  {
    "text": "already written as h21 so this is what",
    "start": 839.339,
    "duration": 4.321
  },
  {
    "text": "A3 is if I take the derivative of this",
    "start": 841.38,
    "duration": 4.56
  },
  {
    "text": "with respect to W 1 this quantity",
    "start": 843.66,
    "duration": 3.84
  },
  {
    "text": "disappears because it's a constant and",
    "start": 845.94,
    "duration": 3.839
  },
  {
    "text": "the derivative of W1 h21 is just h21",
    "start": 847.5,
    "duration": 5.579
  },
  {
    "text": "similarly the derivative of W2 or rather",
    "start": 849.779,
    "duration": 6.601
  },
  {
    "text": "eighth of the loss function with respect",
    "start": 853.079,
    "duration": 6.301
  },
  {
    "text": "to W 2 would have an H22 here right",
    "start": 856.38,
    "duration": 4.86
  },
  {
    "text": "again because derivative of A3 with",
    "start": 859.38,
    "duration": 3.899
  },
  {
    "text": "respect to W2 this quantity would",
    "start": 861.24,
    "duration": 3.839
  },
  {
    "text": "disappear and you will get an H22 right",
    "start": 863.279,
    "duration": 4.201
  },
  {
    "text": "so this is fine this formula you",
    "start": 865.079,
    "duration": 4.141
  },
  {
    "text": "understand now what's with the color",
    "start": 867.48,
    "duration": 3.24
  },
  {
    "text": "coding right what's the red part and",
    "start": 869.22,
    "duration": 3.9
  },
  {
    "text": "what's the blue part so the red part is",
    "start": 870.72,
    "duration": 5.58
  },
  {
    "text": "common for both the derivatives right so",
    "start": 873.12,
    "duration": 6.839
  },
  {
    "text": "this part is actually the same in both",
    "start": 876.3,
    "duration": 6.3
  },
  {
    "text": "the derivatives okay and this part is",
    "start": 879.959,
    "duration": 6.481
  },
  {
    "text": "changing okay this is h21 and H22 now",
    "start": 882.6,
    "duration": 7.32
  },
  {
    "text": "what is h21 h21 is the output of the",
    "start": 886.44,
    "duration": 6.48
  },
  {
    "text": "previous sigmoid neuron right so I know",
    "start": 889.92,
    "duration": 5.64
  },
  {
    "text": "that this is going to be positive",
    "start": 892.92,
    "duration": 4.979
  },
  {
    "text": "right both the blue quantities I know",
    "start": 895.56,
    "duration": 5.219
  },
  {
    "text": "are going to be positive right now if",
    "start": 897.899,
    "duration": 4.38
  },
  {
    "text": "both the blue quantities are going to be",
    "start": 900.779,
    "duration": 3.721
  },
  {
    "text": "positive then the sign of these two",
    "start": 902.279,
    "duration": 5.161
  },
  {
    "text": "gradients is decided by the red quantity",
    "start": 904.5,
    "duration": 5.04
  },
  {
    "text": "right and the red quantity is the same",
    "start": 907.44,
    "duration": 4.56
  },
  {
    "text": "for both the gradients so now both the",
    "start": 909.54,
    "duration": 5.64
  },
  {
    "text": "partial derivatives so now ah either the",
    "start": 912.0,
    "duration": 5.22
  },
  {
    "text": "red quantity is negative in which case",
    "start": 915.18,
    "duration": 4.5
  },
  {
    "text": "both these derivatives are negative or",
    "start": 917.22,
    "duration": 4.38
  },
  {
    "text": "the red quantity is positive in which",
    "start": 919.68,
    "duration": 3.42
  },
  {
    "text": "case again both these derivatives are",
    "start": 921.6,
    "duration": 3.239
  },
  {
    "text": "positive right so I cannot have a",
    "start": 923.1,
    "duration": 3.239
  },
  {
    "text": "situation where I have my gradient",
    "start": 924.839,
    "duration": 4.201
  },
  {
    "text": "Vector which contains all these ah",
    "start": 926.339,
    "duration": 4.8
  },
  {
    "text": "derivatives and either it will be all",
    "start": 929.04,
    "duration": 4.62
  },
  {
    "text": "positives or it will be all negatives",
    "start": 931.139,
    "duration": 4.621
  },
  {
    "text": "right and this would be true even if I",
    "start": 933.66,
    "duration": 3.78
  },
  {
    "text": "had like n neurons here the same",
    "start": 935.76,
    "duration": 3.48
  },
  {
    "text": "argument I could have extended to it so",
    "start": 937.44,
    "duration": 3.6
  },
  {
    "text": "if I had computed those n partial",
    "start": 939.24,
    "duration": 3.839
  },
  {
    "text": "derivatives I would have had this red",
    "start": 941.04,
    "duration": 4.68
  },
  {
    "text": "term common and then this blue term the",
    "start": 943.079,
    "duration": 4.5
  },
  {
    "text": "blue term is always positive because it",
    "start": 945.72,
    "duration": 3.84
  },
  {
    "text": "is the output of a sigmoid neuron so it",
    "start": 947.579,
    "duration": 4.681
  },
  {
    "text": "has no say on the sign and then the sign",
    "start": 949.56,
    "duration": 6.06
  },
  {
    "text": "is then determined by or other I mean it",
    "start": 952.26,
    "duration": 4.86
  },
  {
    "text": "contributes equally I mean whatever it",
    "start": 955.62,
    "duration": 3.779
  },
  {
    "text": "is all of it is positive the sign will",
    "start": 957.12,
    "duration": 4.26
  },
  {
    "text": "just depend on the red part and the red",
    "start": 959.399,
    "duration": 3.961
  },
  {
    "text": "part is same for all these guys so if",
    "start": 961.38,
    "duration": 3.42
  },
  {
    "text": "the red part is negative all of them",
    "start": 963.36,
    "duration": 3.0
  },
  {
    "text": "would be negative it's positive all of",
    "start": 964.8,
    "duration": 3.12
  },
  {
    "text": "them would be positive right so a",
    "start": 966.36,
    "duration": 3.539
  },
  {
    "text": "gradient Vector is either consisting all",
    "start": 967.92,
    "duration": 4.32
  },
  {
    "text": "positive values or all negative values",
    "start": 969.899,
    "duration": 4.201
  },
  {
    "text": "right so that's what is happening and",
    "start": 972.24,
    "duration": 3.959
  },
  {
    "text": "the why am I discussing this in the case",
    "start": 974.1,
    "duration": 4.02
  },
  {
    "text": "of sigmoid Duron because this happened",
    "start": 976.199,
    "duration": 4.14
  },
  {
    "text": "because you had used sigmoid neurons",
    "start": 978.12,
    "duration": 3.839
  },
  {
    "text": "hence the blue parts were always",
    "start": 980.339,
    "duration": 3.781
  },
  {
    "text": "positive right if you had some use some",
    "start": 981.959,
    "duration": 3.961
  },
  {
    "text": "other neuron which had a range between",
    "start": 984.12,
    "duration": 3.899
  },
  {
    "text": "minus one to one then it would have been",
    "start": 985.92,
    "duration": 3.539
  },
  {
    "text": "possible some of these Blues were",
    "start": 988.019,
    "duration": 2.701
  },
  {
    "text": "positive some of these Blues were",
    "start": 989.459,
    "duration": 3.721
  },
  {
    "text": "negative and then accordingly the the",
    "start": 990.72,
    "duration": 4.859
  },
  {
    "text": "vectors the elements of the vector some",
    "start": 993.18,
    "duration": 3.659
  },
  {
    "text": "of them would be positive some of them",
    "start": 995.579,
    "duration": 2.88
  },
  {
    "text": "would be negative but because you are",
    "start": 996.839,
    "duration": 3.541
  },
  {
    "text": "using a sigmoid neuron all W's are",
    "start": 998.459,
    "duration": 3.721
  },
  {
    "text": "positive and hence all the elements of",
    "start": 1000.38,
    "duration": 3.3
  },
  {
    "text": "the gradient Vector would have the same",
    "start": 1002.18,
    "duration": 4.019
  },
  {
    "text": "sign now what is the problem with that",
    "start": 1003.68,
    "duration": 5.159
  },
  {
    "text": "that is I've just told you what the what",
    "start": 1006.199,
    "duration": 5.281
  },
  {
    "text": "is happening but we don't know what the",
    "start": 1008.839,
    "duration": 4.56
  },
  {
    "text": "problem with that is right so this is",
    "start": 1011.48,
    "duration": 3.419
  },
  {
    "text": "just in words whatever I explained so",
    "start": 1013.399,
    "duration": 5.281
  },
  {
    "text": "let's see what the problem is right",
    "start": 1014.899,
    "duration": 8.041
  },
  {
    "text": "so this is the uh uh say derivative of",
    "start": 1018.68,
    "duration": 5.46
  },
  {
    "text": "the partial derivative with respect to",
    "start": 1022.94,
    "duration": 3.3
  },
  {
    "text": "W1 partial derivative with respect to W2",
    "start": 1024.14,
    "duration": 4.679
  },
  {
    "text": "so now if I consider the plane right",
    "start": 1026.24,
    "duration": 5.76
  },
  {
    "text": "which has uh all possible values of",
    "start": 1028.819,
    "duration": 5.64
  },
  {
    "text": "partial derivative of W1 and W2 so now",
    "start": 1032.0,
    "duration": 4.559
  },
  {
    "text": "which are the quadrants which are",
    "start": 1034.459,
    "duration": 4.62
  },
  {
    "text": "possible either the quadrant where both",
    "start": 1036.559,
    "duration": 5.4
  },
  {
    "text": "the values are positive or the quadrant",
    "start": 1039.079,
    "duration": 4.921
  },
  {
    "text": "where both the values are negative right",
    "start": 1041.959,
    "duration": 4.321
  },
  {
    "text": "so you can never have a vector in this",
    "start": 1044.0,
    "duration": 4.5
  },
  {
    "text": "direction your gradient Vector cannot be",
    "start": 1046.28,
    "duration": 4.56
  },
  {
    "text": "in this direction nor can be it in any",
    "start": 1048.5,
    "duration": 4.2
  },
  {
    "text": "of these directions it can only be in",
    "start": 1050.84,
    "duration": 4.5
  },
  {
    "text": "these directions or these directions",
    "start": 1052.7,
    "duration": 4.859
  },
  {
    "text": "right that is what this previous",
    "start": 1055.34,
    "duration": 4.02
  },
  {
    "text": "explanation is telling you so you can",
    "start": 1057.559,
    "duration": 4.381
  },
  {
    "text": "already see that when you're doing",
    "start": 1059.36,
    "duration": 4.08
  },
  {
    "text": "gradient descent you're moving in",
    "start": 1061.94,
    "duration": 4.14
  },
  {
    "text": "directions now half the directions have",
    "start": 1063.44,
    "duration": 5.22
  },
  {
    "text": "been thrown away because of using the",
    "start": 1066.08,
    "duration": 4.5
  },
  {
    "text": "sigmoid neuron these directions are not",
    "start": 1068.66,
    "duration": 4.08
  },
  {
    "text": "possible at all right so you can only",
    "start": 1070.58,
    "duration": 3.78
  },
  {
    "text": "move in certain directions and that",
    "start": 1072.74,
    "duration": 3.66
  },
  {
    "text": "already looks like a problem and here's",
    "start": 1074.36,
    "duration": 4.26
  },
  {
    "text": "a toy example to show that it is indeed",
    "start": 1076.4,
    "duration": 5.36
  },
  {
    "text": "a problem right so suppose you have",
    "start": 1078.62,
    "duration": 4.76
  },
  {
    "text": "initialized",
    "start": 1081.76,
    "duration": 4.36
  },
  {
    "text": "suppose this this Arrow here this is the",
    "start": 1083.38,
    "duration": 4.72
  },
  {
    "text": "optimal value of w right this is where",
    "start": 1086.12,
    "duration": 4.919
  },
  {
    "text": "you want to go and suppose this is where",
    "start": 1088.1,
    "duration": 6.54
  },
  {
    "text": "you had initialized your uh weight right",
    "start": 1091.039,
    "duration": 6.901
  },
  {
    "text": "so now I would want to have a possible",
    "start": 1094.64,
    "duration": 6.18
  },
  {
    "text": "movement like this right",
    "start": 1097.94,
    "duration": 6.119
  },
  {
    "text": "and reach the optimum but this movement",
    "start": 1100.82,
    "duration": 5.4
  },
  {
    "text": "is not possible because it has one value",
    "start": 1104.059,
    "duration": 3.961
  },
  {
    "text": "positive one value negative and we know",
    "start": 1106.22,
    "duration": 3.42
  },
  {
    "text": "that that is not possible we already saw",
    "start": 1108.02,
    "duration": 3.659
  },
  {
    "text": "that all these kind of movements are not",
    "start": 1109.64,
    "duration": 3.48
  },
  {
    "text": "possible right so this is not going to",
    "start": 1111.679,
    "duration": 3.661
  },
  {
    "text": "be possible so now how will I have to",
    "start": 1113.12,
    "duration": 4.74
  },
  {
    "text": "reach the optimum I'll have to just take",
    "start": 1115.34,
    "duration": 5.1
  },
  {
    "text": "the movements which are allowed so I",
    "start": 1117.86,
    "duration": 4.439
  },
  {
    "text": "will have to perhaps take this movement",
    "start": 1120.44,
    "duration": 5.76
  },
  {
    "text": "this is uh okay this is the node",
    "start": 1122.299,
    "duration": 5.821
  },
  {
    "text": "yeah this is allowed right so this is",
    "start": 1126.2,
    "duration": 4.68
  },
  {
    "text": "where like 1 is 0 and the other is uh",
    "start": 1128.12,
    "duration": 4.74
  },
  {
    "text": "negative so that is allowed this this",
    "start": 1130.88,
    "duration": 3.659
  },
  {
    "text": "and so on right so I'll have to move",
    "start": 1132.86,
    "duration": 4.559
  },
  {
    "text": "like this in a zigzag pattern to kind of",
    "start": 1134.539,
    "duration": 4.5
  },
  {
    "text": "reach the optimum right so I'll take",
    "start": 1137.419,
    "duration": 3.961
  },
  {
    "text": "more steps to converge and",
    "start": 1139.039,
    "duration": 4.681
  },
  {
    "text": "and keeping this toy example aside right",
    "start": 1141.38,
    "duration": 3.84
  },
  {
    "text": "the simple thing is this right so I'm",
    "start": 1143.72,
    "duration": 3.54
  },
  {
    "text": "asking you to go from place a to place B",
    "start": 1145.22,
    "duration": 4.199
  },
  {
    "text": "but I am saying that hey certain types",
    "start": 1147.26,
    "duration": 4.14
  },
  {
    "text": "of turns are not allowed if you're not",
    "start": 1149.419,
    "duration": 3.601
  },
  {
    "text": "allowed to take certain times of turns",
    "start": 1151.4,
    "duration": 3.24
  },
  {
    "text": "and you are restricted only to some",
    "start": 1153.02,
    "duration": 3.18
  },
  {
    "text": "types of turns then you are going to",
    "start": 1154.64,
    "duration": 3.419
  },
  {
    "text": "take more time right so that's the main",
    "start": 1156.2,
    "duration": 4.14
  },
  {
    "text": "idea and that is happening because",
    "start": 1158.059,
    "duration": 4.5
  },
  {
    "text": "sigmoids are not zero centered hence we",
    "start": 1160.34,
    "duration": 4.38
  },
  {
    "text": "get all values as positive hence this",
    "start": 1162.559,
    "duration": 3.841
  },
  {
    "text": "negative positive combination is not",
    "start": 1164.72,
    "duration": 3.839
  },
  {
    "text": "possible in the uh directions that you",
    "start": 1166.4,
    "duration": 4.26
  },
  {
    "text": "choose to move in right and lastly of",
    "start": 1168.559,
    "duration": 3.301
  },
  {
    "text": "course sigmoids are computationally",
    "start": 1170.66,
    "duration": 3.84
  },
  {
    "text": "expensive because you have the exponent",
    "start": 1171.86,
    "duration": 4.86
  },
  {
    "text": "to compute because there are faster ways",
    "start": 1174.5,
    "duration": 4.02
  },
  {
    "text": "of computing it but still it's a",
    "start": 1176.72,
    "duration": 4.319
  },
  {
    "text": "computation that you need to do right so",
    "start": 1178.52,
    "duration": 4.08
  },
  {
    "text": "that's the other reason",
    "start": 1181.039,
    "duration": 4.321
  },
  {
    "text": "so that is all about the sigmoid neurons",
    "start": 1182.6,
    "duration": 6.48
  },
  {
    "text": "and why they are not the most convenient",
    "start": 1185.36,
    "duration": 7.62
  },
  {
    "text": "activation function so then ah",
    "start": 1189.08,
    "duration": 6.3
  },
  {
    "text": "because of these limitations right so at",
    "start": 1192.98,
    "duration": 3.96
  },
  {
    "text": "some point uh",
    "start": 1195.38,
    "duration": 4.2
  },
  {
    "text": "so now because of these limitations the",
    "start": 1196.94,
    "duration": 5.82
  },
  {
    "text": "tan H function is popular because one of",
    "start": 1199.58,
    "duration": 4.86
  },
  {
    "text": "the things that you immediately see is",
    "start": 1202.76,
    "duration": 3.36
  },
  {
    "text": "that it's zero centered right and then",
    "start": 1204.44,
    "duration": 3.42
  },
  {
    "text": "whatever explanation I gave at the end",
    "start": 1206.12,
    "duration": 3.9
  },
  {
    "text": "that is kind of taken care of of course",
    "start": 1207.86,
    "duration": 4.08
  },
  {
    "text": "tanh is not a new activation function it",
    "start": 1210.02,
    "duration": 5.58
  },
  {
    "text": "has been used since 1990s but we are",
    "start": 1211.94,
    "duration": 4.68
  },
  {
    "text": "just going over all the activation",
    "start": 1215.6,
    "duration": 4.02
  },
  {
    "text": "functions so sigmoid is the default that",
    "start": 1216.62,
    "duration": 5.28
  },
  {
    "text": "we start explanations with but now I",
    "start": 1219.62,
    "duration": 3.84
  },
  {
    "text": "have shown you certain problems with it",
    "start": 1221.9,
    "duration": 3.48
  },
  {
    "text": "and then tanh overcomes some of those",
    "start": 1223.46,
    "duration": 4.74
  },
  {
    "text": "problems it is zero centered uh but",
    "start": 1225.38,
    "duration": 4.26
  },
  {
    "text": "let's see what is the derivative of this",
    "start": 1228.2,
    "duration": 3.859
  },
  {
    "text": "so it's a derivative of this is 1 minus",
    "start": 1229.64,
    "duration": 7.32
  },
  {
    "text": "tan H Square X and again at saturation",
    "start": 1232.059,
    "duration": 6.701
  },
  {
    "text": "the gradients will vanish right and that",
    "start": 1236.96,
    "duration": 3.24
  },
  {
    "text": "you don't need to look at the formula",
    "start": 1238.76,
    "duration": 3.48
  },
  {
    "text": "the same problem as the graph itself",
    "start": 1240.2,
    "duration": 4.56
  },
  {
    "text": "shows you that if the function value",
    "start": 1242.24,
    "duration": 4.319
  },
  {
    "text": "lies here then the derivative is very",
    "start": 1244.76,
    "duration": 4.919
  },
  {
    "text": "small or zero same thing here right and",
    "start": 1246.559,
    "duration": 5.461
  },
  {
    "text": "if you look at the formula so when Tan x",
    "start": 1249.679,
    "duration": 4.681
  },
  {
    "text": "square is 1 your derivative would be uh",
    "start": 1252.02,
    "duration": 4.86
  },
  {
    "text": "zero right or when it would be minus 1",
    "start": 1254.36,
    "duration": 4.74
  },
  {
    "text": "whenever it is plus or minus 1 your",
    "start": 1256.88,
    "duration": 3.96
  },
  {
    "text": "derivative is going to be 0 right so",
    "start": 1259.1,
    "duration": 4.38
  },
  {
    "text": "that's again clear so the problem of the",
    "start": 1260.84,
    "duration": 4.8
  },
  {
    "text": "saturating neuron and hence the gradient",
    "start": 1263.48,
    "duration": 5.22
  },
  {
    "text": "Vanishing still exist the problem of",
    "start": 1265.64,
    "duration": 5.159
  },
  {
    "text": "limited directions to move in has been",
    "start": 1268.7,
    "duration": 5.94
  },
  {
    "text": "overcome and also the problem of",
    "start": 1270.799,
    "duration": 6.061
  },
  {
    "text": "computational expensive Still Remains it",
    "start": 1274.64,
    "duration": 5.64
  },
  {
    "text": "but still it has the E component in it",
    "start": 1276.86,
    "duration": 5.699
  },
  {
    "text": "right so tanets the formula is contains",
    "start": 1280.28,
    "duration": 7.58
  },
  {
    "text": "the uh Eep part of it right so yeah",
    "start": 1282.559,
    "duration": 5.301
  },
  {
    "text": "okay so the next that will look next for",
    "start": 1288.5,
    "duration": 4.14
  },
  {
    "text": "Activation function that we look at is",
    "start": 1291.26,
    "duration": 5.76
  },
  {
    "text": "the rectified linear unit and this is a",
    "start": 1292.64,
    "duration": 6.96
  },
  {
    "text": "very one of the most popular activation",
    "start": 1297.02,
    "duration": 5.279
  },
  {
    "text": "functions as we will see at the end of",
    "start": 1299.6,
    "duration": 4.5
  },
  {
    "text": "the discussion we'll show you what are",
    "start": 1302.299,
    "duration": 3.421
  },
  {
    "text": "the popular activation functions and",
    "start": 1304.1,
    "duration": 4.559
  },
  {
    "text": "this is proposed almost a decade back in",
    "start": 1305.72,
    "duration": 4.38
  },
  {
    "text": "the context of convolutional neural",
    "start": 1308.659,
    "duration": 3.961
  },
  {
    "text": "networks and it's still one of the de",
    "start": 1310.1,
    "duration": 4.439
  },
  {
    "text": "facto activation functions to be used",
    "start": 1312.62,
    "duration": 4.14
  },
  {
    "text": "across a wide variety of networks right",
    "start": 1314.539,
    "duration": 3.841
  },
  {
    "text": "the first question that I have this is",
    "start": 1316.76,
    "duration": 3.36
  },
  {
    "text": "what the value relu function looks like",
    "start": 1318.38,
    "duration": 3.32
  },
  {
    "text": "right so it is",
    "start": 1320.12,
    "duration": 5.34
  },
  {
    "text": "simply Max of 0 comma X right so what",
    "start": 1321.7,
    "duration": 6.52
  },
  {
    "text": "does that mean that again in the context",
    "start": 1325.46,
    "duration": 5.52
  },
  {
    "text": "of a deep neural network so say this is",
    "start": 1328.22,
    "duration": 5.28
  },
  {
    "text": "one of the new translate so this is",
    "start": 1330.98,
    "duration": 5.699
  },
  {
    "text": "suppose my H1 this is say x now I have",
    "start": 1333.5,
    "duration": 6.96
  },
  {
    "text": "computed the A's right so this guy here",
    "start": 1336.679,
    "duration": 7.681
  },
  {
    "text": "lets let me call it say a 1 2 right that",
    "start": 1340.46,
    "duration": 6.839
  },
  {
    "text": "is going to be again summation uh w i x",
    "start": 1344.36,
    "duration": 6.12
  },
  {
    "text": "i that we know right now that is this a",
    "start": 1347.299,
    "duration": 5.88
  },
  {
    "text": "1 2 is going to be the input to the relu",
    "start": 1350.48,
    "duration": 4.02
  },
  {
    "text": "function that is the non-linearity",
    "start": 1353.179,
    "duration": 4.021
  },
  {
    "text": "setting here and it will simply look at",
    "start": 1354.5,
    "duration": 6.299
  },
  {
    "text": "whether a 1 2 is greater than 0 if so it",
    "start": 1357.2,
    "duration": 5.58
  },
  {
    "text": "will just return a 1 2 otherwise it will",
    "start": 1360.799,
    "duration": 4.321
  },
  {
    "text": "return 0 right so if you have any",
    "start": 1362.78,
    "duration": 4.259
  },
  {
    "text": "negative value being passed to it it",
    "start": 1365.12,
    "duration": 4.14
  },
  {
    "text": "clamps it to 0 and if it's a positive",
    "start": 1367.039,
    "duration": 4.921
  },
  {
    "text": "value it just lets it let it be as it is",
    "start": 1369.26,
    "duration": 4.2
  },
  {
    "text": "right so let's see what are the",
    "start": 1371.96,
    "duration": 4.02
  },
  {
    "text": "implications of that right so the first",
    "start": 1373.46,
    "duration": 4.32
  },
  {
    "text": "question is that is this really a",
    "start": 1375.98,
    "duration": 3.96
  },
  {
    "text": "non-linear function so indeed it is a",
    "start": 1377.78,
    "duration": 4.259
  },
  {
    "text": "longer function the max function the way",
    "start": 1379.94,
    "duration": 3.359
  },
  {
    "text": "it is shown here is a non-linear",
    "start": 1382.039,
    "duration": 3.481
  },
  {
    "text": "function a linear function would just uh",
    "start": 1383.299,
    "duration": 4.74
  },
  {
    "text": "behave uh linearly on both sides but",
    "start": 1385.52,
    "duration": 5.399
  },
  {
    "text": "this is on the negative side you have uh",
    "start": 1388.039,
    "duration": 5.581
  },
  {
    "text": "clamped it and in fact you can show that",
    "start": 1390.919,
    "duration": 5.461
  },
  {
    "text": "if we combine two relu units right so",
    "start": 1393.62,
    "duration": 6.48
  },
  {
    "text": "these are two relu units that have",
    "start": 1396.38,
    "duration": 5.52
  },
  {
    "text": "combined",
    "start": 1400.1,
    "duration": 4.14
  },
  {
    "text": "right so this is a relu function and",
    "start": 1401.9,
    "duration": 4.32
  },
  {
    "text": "this is another relu function and I have",
    "start": 1404.24,
    "duration": 3.419
  },
  {
    "text": "just taken the difference between these",
    "start": 1406.22,
    "duration": 2.939
  },
  {
    "text": "two right so you could think of it in",
    "start": 1407.659,
    "duration": 3.961
  },
  {
    "text": "terms of neurons I had two neurons both",
    "start": 1409.159,
    "duration": 5.281
  },
  {
    "text": "of these were relu neurons right and",
    "start": 1411.62,
    "duration": 4.86
  },
  {
    "text": "then I'm subtracting one from the other",
    "start": 1414.44,
    "duration": 4.5
  },
  {
    "text": "and I'm getting some output and that",
    "start": 1416.48,
    "duration": 5.28
  },
  {
    "text": "output looks almost like the sigmoid",
    "start": 1418.94,
    "duration": 3.96
  },
  {
    "text": "function it looks like a linear",
    "start": 1421.76,
    "duration": 3.24
  },
  {
    "text": "approximation of the sigmoid function so",
    "start": 1422.9,
    "duration": 4.38
  },
  {
    "text": "it is indeed a non-linear function right",
    "start": 1425.0,
    "duration": 4.559
  },
  {
    "text": "it's as close to almost like the step",
    "start": 1427.28,
    "duration": 4.259
  },
  {
    "text": "function that you have and if you adjust",
    "start": 1429.559,
    "duration": 4.201
  },
  {
    "text": "these parameters a bit more you will get",
    "start": 1431.539,
    "duration": 4.62
  },
  {
    "text": "even better kind of a step function and",
    "start": 1433.76,
    "duration": 4.38
  },
  {
    "text": "so on right so indeed a relu is a",
    "start": 1436.159,
    "duration": 4.26
  },
  {
    "text": "non-linear function some of you have not",
    "start": 1438.14,
    "duration": 4.2
  },
  {
    "text": "seen it before but might just look at",
    "start": 1440.419,
    "duration": 3.181
  },
  {
    "text": "the line and think that it's a linear",
    "start": 1442.34,
    "duration": 4.079
  },
  {
    "text": "function but it is not",
    "start": 1443.6,
    "duration": 6.48
  },
  {
    "text": "okay uh this specific function that I've",
    "start": 1446.419,
    "duration": 6.5
  },
  {
    "text": "drawn is called relu 6 because 6",
    "start": 1450.08,
    "duration": 5.459
  },
  {
    "text": "denotes the maximum value of the relu",
    "start": 1452.919,
    "duration": 4.961
  },
  {
    "text": "that you have here okay",
    "start": 1455.539,
    "duration": 5.341
  },
  {
    "text": "okay so what are the advantages of relu",
    "start": 1457.88,
    "duration": 5.039
  },
  {
    "text": "so it does not saturate in the positive",
    "start": 1460.88,
    "duration": 4.02
  },
  {
    "text": "region right what do I mean by that so",
    "start": 1462.919,
    "duration": 6.901
  },
  {
    "text": "as long as your A3 right was or a was",
    "start": 1464.9,
    "duration": 6.3
  },
  {
    "text": "positive",
    "start": 1469.82,
    "duration": 3.38
  },
  {
    "text": "the output would remain",
    "start": 1471.2,
    "duration": 4.62
  },
  {
    "text": "positive and it will not saturated there",
    "start": 1473.2,
    "duration": 4.479
  },
  {
    "text": "is no saturation here there is no",
    "start": 1475.82,
    "duration": 4.8
  },
  {
    "text": "clamping on this side as opposed to what",
    "start": 1477.679,
    "duration": 4.86
  },
  {
    "text": "you had in the sigmoid or any of the",
    "start": 1480.62,
    "duration": 3.96
  },
  {
    "text": "sigmoid shaped functions right so that",
    "start": 1482.539,
    "duration": 4.5
  },
  {
    "text": "is what I mean here but in the negative",
    "start": 1484.58,
    "duration": 4.68
  },
  {
    "text": "region it can still saturate right so in",
    "start": 1487.039,
    "duration": 3.38
  },
  {
    "text": "the if your",
    "start": 1489.26,
    "duration": 3.84
  },
  {
    "text": "contribution is negative right if the a",
    "start": 1490.419,
    "duration": 5.081
  },
  {
    "text": "is negative then it will be clamped to",
    "start": 1493.1,
    "duration": 3.84
  },
  {
    "text": "zero and of course the derivative would",
    "start": 1495.5,
    "duration": 3.12
  },
  {
    "text": "again then be zero right so it does",
    "start": 1496.94,
    "duration": 3.42
  },
  {
    "text": "saturate in the negative region so",
    "start": 1498.62,
    "duration": 5.1
  },
  {
    "text": "roughly half the problem solved but",
    "start": 1500.36,
    "duration": 4.74
  },
  {
    "text": "computational is of course very",
    "start": 1503.72,
    "duration": 2.819
  },
  {
    "text": "efficient right there is no exponent",
    "start": 1505.1,
    "duration": 3.42
  },
  {
    "text": "here you just look at the value and the",
    "start": 1506.539,
    "duration": 3.841
  },
  {
    "text": "simple implementation is if greater than",
    "start": 1508.52,
    "duration": 3.86
  },
  {
    "text": "zero pass it right so it's a very simple",
    "start": 1510.38,
    "duration": 4.279
  },
  {
    "text": "uh uh",
    "start": 1512.38,
    "duration": 5.14
  },
  {
    "text": "function to compute and in practice at",
    "start": 1514.659,
    "duration": 4.361
  },
  {
    "text": "least that's what this paper and as I",
    "start": 1517.52,
    "duration": 3.36
  },
  {
    "text": "said it's a decade old paper had shown",
    "start": 1519.02,
    "duration": 3.56
  },
  {
    "text": "that it converges much faster than",
    "start": 1520.88,
    "duration": 3.84
  },
  {
    "text": "sigmoid and tannage and this was in the",
    "start": 1522.58,
    "duration": 3.94
  },
  {
    "text": "context of deep convolutional neural",
    "start": 1524.72,
    "duration": 3.959
  },
  {
    "text": "networks then but thereafter it has been",
    "start": 1526.52,
    "duration": 4.62
  },
  {
    "text": "used in all sorts of networks as I said",
    "start": 1528.679,
    "duration": 6.301
  },
  {
    "text": "great feed power networks recurrent",
    "start": 1531.14,
    "duration": 6.3
  },
  {
    "text": "networks Transformers also right",
    "start": 1534.98,
    "duration": 4.439
  },
  {
    "text": "all the now in Transformers there are",
    "start": 1537.44,
    "duration": 3.42
  },
  {
    "text": "there's the Gedo activation function",
    "start": 1539.419,
    "duration": 3.721
  },
  {
    "text": "which is perhaps the default but yeah",
    "start": 1540.86,
    "duration": 4.14
  },
  {
    "text": "this has been used in multiple contexts",
    "start": 1543.14,
    "duration": 2.58
  },
  {
    "text": "right",
    "start": 1545.0,
    "duration": 2.76
  },
  {
    "text": "so now uh although this all sounds good",
    "start": 1545.72,
    "duration": 3.6
  },
  {
    "text": "right so what all sounds good one is it",
    "start": 1547.76,
    "duration": 3.24
  },
  {
    "text": "does not saturate at least half the time",
    "start": 1549.32,
    "duration": 3.26
  },
  {
    "text": "so it does not saturate as opposed to",
    "start": 1551.0,
    "duration": 3.72
  },
  {
    "text": "sigmoid and tanets which have two",
    "start": 1552.58,
    "duration": 5.38
  },
  {
    "text": "saturation regions uh it's of",
    "start": 1554.72,
    "duration": 4.74
  },
  {
    "text": "computationally efficient so these two",
    "start": 1557.96,
    "duration": 5.459
  },
  {
    "text": "things are taken care of but still there",
    "start": 1559.46,
    "duration": 5.819
  },
  {
    "text": "is some challenges here right and we'll",
    "start": 1563.419,
    "duration": 4.86
  },
  {
    "text": "see what those are",
    "start": 1565.279,
    "duration": 5.28
  },
  {
    "text": "so if you look at the derivative of relu",
    "start": 1568.279,
    "duration": 5.461
  },
  {
    "text": "then it is going to be 0 if x is less",
    "start": 1570.559,
    "duration": 5.461
  },
  {
    "text": "than 0 and it is going to be 1 if x is",
    "start": 1573.74,
    "duration": 4.02
  },
  {
    "text": "greater than 0 right that simply follows",
    "start": 1576.02,
    "duration": 4.8
  },
  {
    "text": "from the fact that relu is equal to 0 or",
    "start": 1577.76,
    "duration": 5.82
  },
  {
    "text": "X so when it's X the derivative of this",
    "start": 1580.82,
    "duration": 4.62
  },
  {
    "text": "with respect to X is just going to be 1",
    "start": 1583.58,
    "duration": 4.62
  },
  {
    "text": "right so this is what the derivative",
    "start": 1585.44,
    "duration": 5.219
  },
  {
    "text": "looks like and now if this is what the",
    "start": 1588.2,
    "duration": 5.7
  },
  {
    "text": "derivative looks like uh let's see",
    "start": 1590.659,
    "duration": 5.221
  },
  {
    "text": "what's the implication that we have",
    "start": 1593.9,
    "duration": 4.32
  },
  {
    "text": "right so I've given you some toy example",
    "start": 1595.88,
    "duration": 6.539
  },
  {
    "text": "here a toy Network now at some point",
    "start": 1598.22,
    "duration": 6.54
  },
  {
    "text": "Suppose there is a large gradient which",
    "start": 1602.419,
    "duration": 4.201
  },
  {
    "text": "flows through the network right there is",
    "start": 1604.76,
    "duration": 4.26
  },
  {
    "text": "a large gradient which flows and it",
    "start": 1606.62,
    "duration": 7.08
  },
  {
    "text": "makes B's value uh a very negative right",
    "start": 1609.02,
    "duration": 6.72
  },
  {
    "text": "suppose B takes on a large negative",
    "start": 1613.7,
    "duration": 5.219
  },
  {
    "text": "value okay that is what happens and this",
    "start": 1615.74,
    "duration": 4.86
  },
  {
    "text": "is not like cooked up this is",
    "start": 1618.919,
    "duration": 3.781
  },
  {
    "text": "conceivable that this could happen you",
    "start": 1620.6,
    "duration": 4.38
  },
  {
    "text": "have some training example which causes",
    "start": 1622.7,
    "duration": 5.459
  },
  {
    "text": "B to update and then B becomes a very",
    "start": 1624.98,
    "duration": 5.939
  },
  {
    "text": "large negative value right now if that",
    "start": 1628.159,
    "duration": 5.4
  },
  {
    "text": "happens what would the consequence of it",
    "start": 1630.919,
    "duration": 4.921
  },
  {
    "text": "be",
    "start": 1633.559,
    "duration": 6.781
  },
  {
    "text": "so now your uh A1 right which is the",
    "start": 1635.84,
    "duration": 6.48
  },
  {
    "text": "input to the value function right the",
    "start": 1640.34,
    "duration": 4.38
  },
  {
    "text": "value function is sitting here would is",
    "start": 1642.32,
    "duration": 5.52
  },
  {
    "text": "just W and X1 plus W 2 x 2 plus b now if",
    "start": 1644.72,
    "duration": 5.579
  },
  {
    "text": "B is very negative irrespective of what",
    "start": 1647.84,
    "duration": 7.199
  },
  {
    "text": "W1 X1 and W 2 x 2 are right this b a",
    "start": 1650.299,
    "duration": 6.061
  },
  {
    "text": "very large negative quantity would get",
    "start": 1655.039,
    "duration": 3.841
  },
  {
    "text": "added so this is going to be 0. so this",
    "start": 1656.36,
    "duration": 4.14
  },
  {
    "text": "is going to be 0",
    "start": 1658.88,
    "duration": 3.899
  },
  {
    "text": "then the derivative is going to be 0. so",
    "start": 1660.5,
    "duration": 3.9
  },
  {
    "text": "if the derivative is 0 again you have",
    "start": 1662.779,
    "duration": 2.76
  },
  {
    "text": "this problem that when you are trying to",
    "start": 1664.4,
    "duration": 4.019
  },
  {
    "text": "update W1 at some point in the chain you",
    "start": 1665.539,
    "duration": 4.801
  },
  {
    "text": "have this derivative of H1 with respect",
    "start": 1668.419,
    "duration": 4.561
  },
  {
    "text": "to A1 and that derivative is going to be",
    "start": 1670.34,
    "duration": 5.04
  },
  {
    "text": "0 right which means the derivative of",
    "start": 1672.98,
    "duration": 4.62
  },
  {
    "text": "the loss function with respect to W 1 W",
    "start": 1675.38,
    "duration": 6.06
  },
  {
    "text": "2 is going to be 0 right now if and also",
    "start": 1677.6,
    "duration": 6.3
  },
  {
    "text": "with respect to B is going to be 0. so",
    "start": 1681.44,
    "duration": 5.46
  },
  {
    "text": "now if that happens the neuron of course",
    "start": 1683.9,
    "duration": 4.86
  },
  {
    "text": "output at zeros or the neurons output",
    "start": 1686.9,
    "duration": 3.24
  },
  {
    "text": "was Zero which means it was a dead",
    "start": 1688.76,
    "duration": 3.48
  },
  {
    "text": "neuron",
    "start": 1690.14,
    "duration": 3.84
  },
  {
    "text": "which is fine if it's dead for this",
    "start": 1692.24,
    "duration": 3.12
  },
  {
    "text": "training example I can live with it",
    "start": 1693.98,
    "duration": 3.0
  },
  {
    "text": "right but now it's not got updated",
    "start": 1695.36,
    "duration": 5.4
  },
  {
    "text": "neither has B got an update right and",
    "start": 1696.98,
    "duration": 5.88
  },
  {
    "text": "now from here on what will happen is",
    "start": 1700.76,
    "duration": 3.18
  },
  {
    "text": "that",
    "start": 1702.86,
    "duration": 3.54
  },
  {
    "text": "the neuron will remain dead for the rest",
    "start": 1703.94,
    "duration": 5.119
  },
  {
    "text": "of the training",
    "start": 1706.4,
    "duration": 2.659
  },
  {
    "text": "it will never become alive again right",
    "start": 1709.159,
    "duration": 5.961
  },
  {
    "text": "so why is that the case",
    "start": 1711.74,
    "duration": 3.38
  },
  {
    "text": "so this is all just the explanation that",
    "start": 1721.7,
    "duration": 3.959
  },
  {
    "text": "I gave in uh just I was speaking through",
    "start": 1723.62,
    "duration": 4.2
  },
  {
    "text": "it this is all on the slides also",
    "start": 1725.659,
    "duration": 4.14
  },
  {
    "text": "and this is the main point that I'm",
    "start": 1727.82,
    "duration": 3.42
  },
  {
    "text": "talking about now right that the neuron",
    "start": 1729.799,
    "duration": 4.021
  },
  {
    "text": "will stay Dead Forever right so why",
    "start": 1731.24,
    "duration": 5.48
  },
  {
    "text": "would that happen",
    "start": 1733.82,
    "duration": 2.9
  },
  {
    "text": "yeah",
    "start": 1737.539,
    "duration": 2.52
  },
  {
    "text": "so weights are never getting updated",
    "start": 1738.679,
    "duration": 4.441
  },
  {
    "text": "right so once it became dead W1 W2 and B",
    "start": 1740.059,
    "duration": 4.86
  },
  {
    "text": "all three will not get updated right so",
    "start": 1743.12,
    "duration": 3.419
  },
  {
    "text": "B Still Remains this large negative",
    "start": 1744.919,
    "duration": 4.38
  },
  {
    "text": "value now the next input comes in again",
    "start": 1746.539,
    "duration": 4.441
  },
  {
    "text": "the same will happen the B is large",
    "start": 1749.299,
    "duration": 4.681
  },
  {
    "text": "negative value so this sum W 1 x 1 plus",
    "start": 1750.98,
    "duration": 5.1
  },
  {
    "text": "W 2 x 2 plus b would again be less than",
    "start": 1753.98,
    "duration": 5.04
  },
  {
    "text": "zero again the neuron is dead and then",
    "start": 1756.08,
    "duration": 4.38
  },
  {
    "text": "again the gradient does not flow through",
    "start": 1759.02,
    "duration": 4.379
  },
  {
    "text": "again W and W2 B don't get updated again",
    "start": 1760.46,
    "duration": 4.56
  },
  {
    "text": "the next input comes in it's the same",
    "start": 1763.399,
    "duration": 4.441
  },
  {
    "text": "story right so once it becomes dead it",
    "start": 1765.02,
    "duration": 4.56
  },
  {
    "text": "stays Dead Forever right and in practice",
    "start": 1767.84,
    "duration": 4.74
  },
  {
    "text": "people have observed that uh as high as",
    "start": 1769.58,
    "duration": 4.86
  },
  {
    "text": "greater than 50 percent of the relu",
    "start": 1772.58,
    "duration": 4.26
  },
  {
    "text": "units can die if the learning rate is",
    "start": 1774.44,
    "duration": 4.32
  },
  {
    "text": "set to high right why is this related to",
    "start": 1776.84,
    "duration": 3.18
  },
  {
    "text": "the running rate if the learning rate is",
    "start": 1778.76,
    "duration": 3.659
  },
  {
    "text": "very high then this problem of a large",
    "start": 1780.02,
    "duration": 4.5
  },
  {
    "text": "gradient flowing to B and then getting",
    "start": 1782.419,
    "duration": 3.901
  },
  {
    "text": "multiplied by a reasonable learning rate",
    "start": 1784.52,
    "duration": 4.259
  },
  {
    "text": "can cause B to be very negative right so",
    "start": 1786.32,
    "duration": 3.78
  },
  {
    "text": "that's where the learning rate has to be",
    "start": 1788.779,
    "duration": 3.861
  },
  {
    "text": "carefully uh satisf",
    "start": 1790.1,
    "duration": 5.52
  },
  {
    "text": "carefully set and also when you are",
    "start": 1792.64,
    "duration": 4.96
  },
  {
    "text": "using uh relu it is advised to",
    "start": 1795.62,
    "duration": 4.38
  },
  {
    "text": "initialize bias to a positive value",
    "start": 1797.6,
    "duration": 5.52
  },
  {
    "text": "right and in context of deep learning",
    "start": 1800.0,
    "duration": 4.74
  },
  {
    "text": "this is a fairly large positive value",
    "start": 1803.12,
    "duration": 4.32
  },
  {
    "text": "0.01 because you have many elements",
    "start": 1804.74,
    "duration": 4.62
  },
  {
    "text": "contributing to the sum so one of them",
    "start": 1807.44,
    "duration": 4.68
  },
  {
    "text": "is point zero one right so you should",
    "start": 1809.36,
    "duration": 4.919
  },
  {
    "text": "set it properly otherwise many of your",
    "start": 1812.12,
    "duration": 5.58
  },
  {
    "text": "neurons would die or you could use some",
    "start": 1814.279,
    "duration": 5.221
  },
  {
    "text": "of the other variants of relu that we",
    "start": 1817.7,
    "duration": 3.78
  },
  {
    "text": "will see soon",
    "start": 1819.5,
    "duration": 4.08
  },
  {
    "text": "so this is one variant which is the",
    "start": 1821.48,
    "duration": 4.86
  },
  {
    "text": "Leaky relu right and as the name",
    "start": 1823.58,
    "duration": 4.86
  },
  {
    "text": "suggests it's allowed some leakage on",
    "start": 1826.34,
    "duration": 4.88
  },
  {
    "text": "the negative side so earlier",
    "start": 1828.44,
    "duration": 5.58
  },
  {
    "text": "whenever you had a negative value you",
    "start": 1831.22,
    "duration": 5.98
  },
  {
    "text": "were kind of making it zero now you make",
    "start": 1834.02,
    "duration": 5.34
  },
  {
    "text": "it small you multiply by a very small",
    "start": 1837.2,
    "duration": 4.979
  },
  {
    "text": "constant so it becomes small right so",
    "start": 1839.36,
    "duration": 4.62
  },
  {
    "text": "it's close to zero but it does not die",
    "start": 1842.179,
    "duration": 4.801
  },
  {
    "text": "completely so it allows some gradients",
    "start": 1843.98,
    "duration": 5.64
  },
  {
    "text": "to flow even when your X is negative",
    "start": 1846.98,
    "duration": 5.24
  },
  {
    "text": "right so it takes care of neurons not",
    "start": 1849.62,
    "duration": 7.279
  },
  {
    "text": "saturating and ah",
    "start": 1852.22,
    "duration": 4.679
  },
  {
    "text": "yeah and it will not die because it will",
    "start": 1858.98,
    "duration": 4.98
  },
  {
    "text": "ensure that some gradient always flows",
    "start": 1862.22,
    "duration": 3.059
  },
  {
    "text": "through and it remains computationally",
    "start": 1863.96,
    "duration": 3.42
  },
  {
    "text": "efficient right just making it 0.1 x",
    "start": 1865.279,
    "duration": 4.681
  },
  {
    "text": "does not make it any more expensive than",
    "start": 1867.38,
    "duration": 4.32
  },
  {
    "text": "the value function right maybe slightly",
    "start": 1869.96,
    "duration": 3.18
  },
  {
    "text": "more",
    "start": 1871.7,
    "duration": 3.18
  },
  {
    "text": "and also it gives you these close to",
    "start": 1873.14,
    "duration": 3.6
  },
  {
    "text": "zero centered outputs because now you",
    "start": 1874.88,
    "duration": 3.84
  },
  {
    "text": "have outputs which are negative also and",
    "start": 1876.74,
    "duration": 4.02
  },
  {
    "text": "your outputs which are positive also so",
    "start": 1878.72,
    "duration": 4.559
  },
  {
    "text": "this problem of gradients only being in",
    "start": 1880.76,
    "duration": 4.32
  },
  {
    "text": "One Direction is also taken care of it",
    "start": 1883.279,
    "duration": 4.5
  },
  {
    "text": "so multiple advantages of leaky relu and",
    "start": 1885.08,
    "duration": 4.74
  },
  {
    "text": "then there are a few such other variants",
    "start": 1887.779,
    "duration": 4.26
  },
  {
    "text": "right so one of them is parametric relu",
    "start": 1889.82,
    "duration": 4.7
  },
  {
    "text": "so then the question is right that",
    "start": 1892.039,
    "duration": 5.461
  },
  {
    "text": "y set this to point One X right so let",
    "start": 1894.52,
    "duration": 5.56
  },
  {
    "text": "it be some Alpha and this Alpha could",
    "start": 1897.5,
    "duration": 4.919
  },
  {
    "text": "also be learned and it will also get",
    "start": 1900.08,
    "duration": 4.079
  },
  {
    "text": "updated during back preparation right so",
    "start": 1902.419,
    "duration": 3.48
  },
  {
    "text": "one simple way of thinking is that you",
    "start": 1904.159,
    "duration": 3.721
  },
  {
    "text": "set Alpha equal to 0.1 to begin with",
    "start": 1905.899,
    "duration": 4.26
  },
  {
    "text": "right and then just as all the weights",
    "start": 1907.88,
    "duration": 3.899
  },
  {
    "text": "are getting updated compute this",
    "start": 1910.159,
    "duration": 3.061
  },
  {
    "text": "derivative of the loss function with",
    "start": 1911.779,
    "duration": 3.841
  },
  {
    "text": "respect to Alpha also and then update",
    "start": 1913.22,
    "duration": 5.52
  },
  {
    "text": "Alpha also as Alpha minus ETA times this",
    "start": 1915.62,
    "duration": 4.919
  },
  {
    "text": "gradient or whatever gradient doesn't",
    "start": 1918.74,
    "duration": 4.74
  },
  {
    "text": "update rule you are using right ah so",
    "start": 1920.539,
    "duration": 5.161
  },
  {
    "text": "this is it kind of again makes it uh",
    "start": 1923.48,
    "duration": 4.74
  },
  {
    "text": "more careful that why have you selected",
    "start": 1925.7,
    "duration": 4.32
  },
  {
    "text": "a certain slope here maybe that slope",
    "start": 1928.22,
    "duration": 4.14
  },
  {
    "text": "needs to be adjusted as the training is",
    "start": 1930.02,
    "duration": 3.6
  },
  {
    "text": "going right so it gives me more",
    "start": 1932.36,
    "duration": 3.539
  },
  {
    "text": "flexibility and again takes care of all",
    "start": 1933.62,
    "duration": 4.08
  },
  {
    "text": "the problems that you had with relu of",
    "start": 1935.899,
    "duration": 4.321
  },
  {
    "text": "neurons dying and it not being zero",
    "start": 1937.7,
    "duration": 5.28
  },
  {
    "text": "center right while these variants have",
    "start": 1940.22,
    "duration": 4.38
  },
  {
    "text": "been proposed they are not like so",
    "start": 1942.98,
    "duration": 4.02
  },
  {
    "text": "popular I think relu the default version",
    "start": 1944.6,
    "duration": 5.459
  },
  {
    "text": "is still the most popular among these",
    "start": 1947.0,
    "duration": 5.779
  },
  {
    "text": "variants",
    "start": 1950.059,
    "duration": 2.72
  },
  {
    "text": "and one more variant of relu is the",
    "start": 1953.059,
    "duration": 7.081
  },
  {
    "text": "exponential linear unit right so yeah so",
    "start": 1956.24,
    "duration": 6.5
  },
  {
    "text": "far everything was linear",
    "start": 1960.14,
    "duration": 6.539
  },
  {
    "text": "on both sides it was like this but now",
    "start": 1962.74,
    "duration": 6.039
  },
  {
    "text": "on the whenever the input is less than",
    "start": 1966.679,
    "duration": 4.38
  },
  {
    "text": "zero we are making it exponentially",
    "start": 1968.779,
    "duration": 5.52
  },
  {
    "text": "Decay so there's still some gradient",
    "start": 1971.059,
    "duration": 5.041
  },
  {
    "text": "flowing on the negative side also it has",
    "start": 1974.299,
    "duration": 3.961
  },
  {
    "text": "all the benefits of relu because largely",
    "start": 1976.1,
    "duration": 5.52
  },
  {
    "text": "it has a linear uh",
    "start": 1978.26,
    "duration": 6.659
  },
  {
    "text": "on the positive side and it's also now",
    "start": 1981.62,
    "duration": 6.48
  },
  {
    "text": "again close to zero centered",
    "start": 1984.919,
    "duration": 4.38
  },
  {
    "text": "right",
    "start": 1988.1,
    "duration": 3.12
  },
  {
    "text": "and some gradient always flows through",
    "start": 1989.299,
    "duration": 5.581
  },
  {
    "text": "but it again becomes exponentially uh",
    "start": 1991.22,
    "duration": 5.22
  },
  {
    "text": "it becomes expensive right because you",
    "start": 1994.88,
    "duration": 3.48
  },
  {
    "text": "have to compute this exponent right so",
    "start": 1996.44,
    "duration": 3.9
  },
  {
    "text": "now all of these they were proposed in",
    "start": 1998.36,
    "duration": 4.02
  },
  {
    "text": "certain contexts and in those contexts",
    "start": 2000.34,
    "duration": 4.26
  },
  {
    "text": "they were like found to be better than",
    "start": 2002.38,
    "duration": 4.679
  },
  {
    "text": "relu also some of them theoretically",
    "start": 2004.6,
    "duration": 3.84
  },
  {
    "text": "showed that why they are better than",
    "start": 2007.059,
    "duration": 4.5
  },
  {
    "text": "relu but at least in practice many of",
    "start": 2008.44,
    "duration": 5.04
  },
  {
    "text": "these are not very popular right you",
    "start": 2011.559,
    "duration": 4.86
  },
  {
    "text": "just use the default relu and largely",
    "start": 2013.48,
    "duration": 4.799
  },
  {
    "text": "that works or you use some of the more",
    "start": 2016.419,
    "duration": 3.961
  },
  {
    "text": "recent ones which is angelu and swish",
    "start": 2018.279,
    "duration": 4.62
  },
  {
    "text": "right so they are more popular now right",
    "start": 2020.38,
    "duration": 6.24
  },
  {
    "text": "so that ends the discussion on three of",
    "start": 2022.899,
    "duration": 6.481
  },
  {
    "text": "the most popular or so of the initial",
    "start": 2026.62,
    "duration": 4.62
  },
  {
    "text": "activation functions which were there",
    "start": 2029.38,
    "duration": 5.7
  },
  {
    "text": "around 2012 2014 which is the logistic",
    "start": 2031.24,
    "duration": 7.319
  },
  {
    "text": "activation function then the tanh and",
    "start": 2035.08,
    "duration": 5.219
  },
  {
    "text": "then the relu right so I'll end this",
    "start": 2038.559,
    "duration": 4.321
  },
  {
    "text": "video here and when you come back we'll",
    "start": 2040.299,
    "duration": 6.12
  },
  {
    "text": "discuss a few more uh later activation",
    "start": 2042.88,
    "duration": 6.799
  },
  {
    "text": "functions that came out",
    "start": 2046.419,
    "duration": 3.26
  }
]