foreign [Music] schedulers and so this is the chart for learning rate schemes right so you have what we saw earlier was based on epochs where we had step Decay so after one Epoch if something goes wrong then just decade or for every Epoch just exponentially keep decaying the learning rate uh then we have based on validation where we had line search right so we were doing trying multiple learning rates and then based on which one gives the smallest loss we were trying to use that as the learning rate it could also be a log search similar to a line search where you would search for learning rates on a log scale and then based on gradients right these were the Adaptive learning rates that we had so we had adagrad RMS Prof ATA Delta Adam atomax we did an Adam also uh we did not do AMS grad and we did not do Adam W uh so these are all the things that we have seen so far but there are a few more learning rate schedules that we'll see now which are based on Epoch so one is cyclical epochs or the number of iterations right the other is cosine annealing bomb restart so these three are what we're going to see now right uh so we'll first talk about this cyclical learning rate so uh suppose the loss surface looks like this and for some of you have exposure to uh optimization uh you would know that this loss function is lost surface is what is known as a saddle so what happens in a saddle uh shape loss surface that if I look it from here right then this looks like a Minima right this point here looks like a Minima because it's at the end of the valley right but that's clearly not where I want the algorithm to stop because if I look at it this way then I would could have gone down from here I could have gone further down from here right my Minima would have been somewhere down right but it's because of this saddle shape where in One Direction you are seeing that okay I've reached the Minima but there is another Direction across which you could have found a better Minima right and many times because the Deep learning uh the loss surfaces that you typically encounter in deep learning are not convex loss functions it has been observed and this has been reported in multiple papers that often when your training is getting stuck or it's low or something it's because you're stuck in some saddle point right where there is a Minima down the valley but since you are already in this Valley here you're not able to go further down right so suppose we had we initialize the weights to this yellow point right this is where you had initialized now you're coming down nicely and you'll get stuck in that Valley you'll get stuck somewhere at the right point that I have marked where my mouse is currently and you will not be able to go down right because your learning rate is decree amazing exponentially over iterations now uh now this since the learning rate has decreased so you're you're going down and you are going on a fast loop and in most algorithms the learning rate will start decaying and by the time you reach the valley the learning rate would have become very small and now you won't be able to come out of that you'll be stuck there right so now what if we allow the learning rate to increase after some iterations right at least there's a chance that it might escape the saddle point right and that's what the idea is that you try to have this adaptive learning rate which will change but that so that is often expensive right because you're doing these computations WT the history all of that you are maintaining then you're doing ETA T ETA are divided by this history so you're creating a lot of keeping a lot of variables and making things a bit complex right and uh this often happens right that the minimizing the loss arises because of the saddle points so therefore it is beneficial to have these learning rate schemes which could increase the learning rate near the saddle point right and adaptive learning rate schemes that we have seen are actually trying to do that right they are trying to do that but as I said it comes with an additional computational cost instead what if we'd have a simple uh learning rate scheme where we alternatively vary the learning right so we start with small go to high again make it small again go high again make it small just alternately slightly keep doing it so now we are not looking at history or anything right we are gradually increasing it then again decreasing so if it's in a high region it just has to wait out that cycle and then again the learning rate will start decreasing right if it's in a low region it just has to wait out the cycle and the learning rate will increase and then push it out again right this is a very simple schedule that you could use and this is one such cyclic learning rate which is the Triangular schedule that you could use and this here mu which is the uh period right so you could mu is 20 here so that that is the number of time steps it takes from going to the minimum value to the maximum value and once it reaches the maximum value it again goes down right so on the x axis you have the learning rate and this is the maximum value which is in this case is 0.5 the minimum value is 0.01 and the value at any time step can be given by this uh complex equation but it just looks complex if you break it down it's very simple so let's look at it right so if T is equal to 20 I'll just substitute the value t equal to 20 in this equation let me just get rid of the annotations yeah so let me just substitute the value t equal to 20. now uh I'm going to substitute it here first right so when I'm Computing this quantity it's turning out to be T right so everything inside the bracket is T oops and now I'm looking at this quantity which I had already computed everything inside the bracket which came out to be two I'm in fact looking at this quantity right so then this plus 1 from here and this T by mu which is 20 by 20. so that quantity comes out to 0 in this case then I'm going to look at Max of 0 so now I'm looking at this quantity which is Max of 0 comma what I had computed earlier so this is what I had computed earlier so that is going to be 1 right so then I have ETA min Plus ETA Max minus ETA min and this entire thing has evaluated to 1 so this is going to be 1 right so then it's going to be 0.01 plus 0.5 minus 0.01 which is just going to be 0.5 right and that's exactly what I get at time step t equal to 20. similarly you could substitute the value 30 here and you will get uh this as the value right somewhere here as the value right so that's that's how you compute this cyclically running rate the increase keep increasing the again decrease keep increasing then again decrease right so this is uh it was almost like having like a very uh simplistic idea right you are just saying that at some points I want the learning rate to increase at some points I want the learning rate to decrease and to make this happen you we had all these complex algorithms which were adapting the learning Aid according to when it should increase when it decreased now this is like a very uh Baseline idea right you wanted to increase you wanted to decrease just make it cyclically and if you're in a bad part you just have to wait out that cycle and then again you might if you want a smaller learning rate it is going to come in sometime steps if you want a larger learning rate it's going to come into some time steps you just have to wait out that cycle right so that's that's the simple idea that you have here okay uh so now let's see whether this actually solves the problem that we started off right so we have this saddle point and we started with that initialization and looks like we will get stuck in the valley right so I have this exponentially decaying learning rate which is the black guy and that comes into the valley and gets stuck there but this blue guy has a chance because the exponentially decayed learning rate has become very small but now the blue guy because it can increase the learning rate at some point so even though the gradients there are small when the learning it just waits till the learning rate increases and then it gets per push and then it comes down into the valley right so with this cyclic learning rate you are able to get out of this saddle point and saddle points are a big pain as reported in multiple papers when you're dealing with deep neural networks right so this kind of takes care of such bad uh learning rate studies right um yeah so now again we are going to use another cyclic learning rate which is the cosine learning rate and uh here instead of having a fixed learning rate which is one we are going to use this cyclic one and you can see that this algorithm has converged much faster right because it still has some oscillations here but you could just take care of that by using some kind of an early stopping or something right so now let's see what this uh cosine early annealing look likes and I just see that so this is what cosine annealing look like it has a different formula again this is just based on a cosine function so if you substitute the time step T the value of T here right and your capital T would just be the period of the uh uh uh of the wave that you have uh and if you substitute these values you will get the effective learning rate right and it's better to just show this as the graph and that you you'll understand what is happening here so ETA Max is the maximum value for the learning rate ETA mean is the minimum value T is the current Epoch and capital t is the restart interval which is until what time you increase and then again restart right so that's what capital t is and so let's see what happens so this is what it looks like right so it you have is you start with a high value and then it keeps decreasing and when it reaches this interval this interval here is 50. so when it reaches the 50 and you can substitute these values that you can substitute 50 here and you substitute 50 and substitute here so you if you compute these values right so T mod 50 plus 1 would be T mod sorry 50 mod 51 which is going to be uh 50 and then divided by 50 so this quantity will just become 1 so we'll have COS of Pi which is going to be a minus 1 or sorry cos of Pi is going to be uh minus 1 and then you have 1 minus 1 so this will become 0 so we'll just have the minimum value here right and that's exactly what is happening here right okay and this is called a warm restart because from uh once you come to the minimum point you quickly jump to the maximum Point again as opposed to the Triangular learning rate where you are gradually increasing gradually decreasing here from the start from a high point come down and then quickly go to the maximum value and then again come down so this is called a warm restart because when you're restarting you're not like gradually restarting but you are starting from a warm point which is a high learning rate and then coming down from there right so this uh warm uh start warm restart is a popular term that you'll hear in many of the modern papers right so this is in Max and Min t uh and the update change after T which is why it is called the bomb resta right and there's also warm start which is quite popular in the uh uh Transformer literature and most of the current literature is on Transformers I mean they're popularly used in many applications so what happens is that in warm start you initially let the learning rate increase right in all the other algorithms you start whatever adaptive algorithms we had seen in you start with some learning rate and then it keeps decaying right and even the earlier learning rate schedules that we had said step DK exponential decay you start with some value and there's always a DK there right incident warm start what you do is you let the learning rate gradually increase to the maximum point and from that point onwards you start the Decay right so initially when you don't know much right the parameters are all randomly initialized you're not sure of your gradients then slowly slowly have the learning rate very small so that you are not making very large updates and then as you gain confidence you keep increasing the learning rate to a high value and then once you reach a high value from there on you start decaying at explanation right so this is the default learning rate schedule used in uh a Transformer based architectures and this is what uh it looks like right so as you keep and if you use different so the warm-up step is the number of steps for which you will let the learning rate to climb so in one case I have the warm-up step as uh 2000 and in the other case I have the warm-up step is four thousand so the 2000 curve of course Rises rapidly and then Falls and the 4000 step Rises more uh smoothly and then it uh Falls right so starts ticking exponentially from there okay so that's that's all we had we covered the different learning rate schedules that we had uh and this you could use in conjunction with the optimization algorithms that we have used also right so both can be uh done so that's all for this lecture and we'll see you next week again thank you