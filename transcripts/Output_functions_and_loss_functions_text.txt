[Music] okay so now let's talk about output functions and loss functions right so this is where we were we wanted to understand how to choose a loss function and that in turn would depend on what your output function is so let's look at that right so the choice of loss function depends on the problem at hand and what i'm going to do is that i'm going to talk about two very popular problems side one is regression and this other is classification and in most cases like 90 of the problems that you deal with would fall in one of these two categories right so if you do these then we are largely covered from an application or perspective right even from the understanding perspective right so let's look at what these two examples are so the first example is that i have an input about movies and i want to predict the ratings for the movies right and these ratings could be the imdb ratings or the critics rating or the rotten tomatoes rating right now deliberately i have moved to a setup where you have an input which is n-dimensional and now you have an output which is k dimensional which in this case is three dimensional right so just to make sure that you understand that it may not be always the case that we're just trying to predict one value using the same information about the input so just imagine that i have taken a lot of inputs a lot of movies from the past right and for each of these movies i have their input representation which is who was a director actor blah blah and then i also have the output and this output is just three dimensional right i have the imdb rating i have the critics rating and i have the rotten tomatoes it's all three are available to me and now using this input data right so this becomes my x comma y pairs i want to train a model which takes in as x as input right and gives me these three ratings as output which is the same as saying that it gives me a three dimensional output right so that's that's what it is now here i belongs to r3 as i said so now the loss function that we want should be able to deviate how much does the predicted value should be able to capture how much does the predicted value deviate from the true value right now here you have a vector as the true vector of true values and vector of predicted values and you want to find the difference between them so the obvious loss function would be these mean squared error loss function right so you have three values that you're predicting for each of these values you calculate the square of the difference between the true value and the predicted value you sum it over all the n training examples that you have and then take the mean or the average so this is the mean squared error that you would compute so this we have already seen it when you're trying to predict real values it's best to take the squared error loss because it computes the deviation between the true value and the captured value right now a related question here should be what should the output function be right so you have the output function sitting here right so this dark green guy is the output right and i'm taking a3 and i'm going to pass it through some output function and that is going to give me uh the output y hat which is shown as the dark or the shaded green part here right so what should my output function be right so now let me ask you a question it can it be the logistic function right so we had seen the logistic function which takes any input right so a3 would be the input and i am asking whether o can be the logistic function right and what would that mean it would just take the input and it would compute the logistic function for that right now can that uh is is that okay is the question right so it just should be it will compute it element wise eight so a three one a three two a three three is what it will compute and that would be y hat one y hat two y hat three right because the three values that i'm trying to predict so is that a good choice right and as all of you realize it wouldn't be a good choice because the logistic function i know is going to clamp all my outputs between zero to one right so the logistic function as we had seen is something like this right so it the output goes just from zero to one right so no matter what the input is my outputs are going to lie between 0 to 1 right now why is that not good here because here i know that the ratings are on a scale of 0 to 10 in fact they could also be on a scale of 0 to 100 so then if i choose the output as sigmoid function then my network is always going to predict values between zero to one whereas my true values are between zero to ten or zero to hundred and then that doesn't make sense right so i need to predict be able to predict uh unbounded values or values having a much higher range than zero to one right so that's what the constraint that i have and in such cases when your output is going to be a real value right and not just bounded between zero to one then the best thing to use is just use a linear activation function what does that mean so i have done all the computations up to this point right that means up to this white portion right of the green neuron right and that what i get there is a l a l is of course a l 1 a l 2 a l 3 right in this case it's a collection of 3 elements now i'm going to just multiply it by a matrix w o okay and what is the dimension of that w going to be it's again going to be 3 cross 3 right because at the output again i want h right nh is again 3 cross 3 right i want 3 values there sorry 3 cross 1 right and which is just the same as y hat 1 y hat 2 y hat 3 right so i want 3 values i have input which is a 3 cross 1 vector so i should multiply it by a 3 cross 3 matrix right so this is the transformation that i'm going to do i have some values computed here now i am going to just multiply it by weight vector w and now plus add the bias right so that's always there i'm going to add the bias also and i'll get a three dimensional output and those are my values and these are not going to be bounded between 0 to 1 because my w is not bounded my a is not bounded my b is not bounded right so that's that's what i'm going to use but now the question that you could ask is that if a is not bounded b is not bounded w is not bounded this could take out any values then how am i sure that my uh algorithm will not start or my y hats which i am predicting it which is the output here why would it be like thousand ten thousand 100 and so on whereas actually i just want values between 0 to 10 right so i want that the output should be 0 to 10 right that's why i did not choose the logistic function because i told you that logistic function will give you between 0 to 1 right then perhaps i should have chosen something which gives me values between 0 to 10 i did not do that i chose it choose a linear function which is not bounded it could give me any values it could give me 10 thousands 1 lakhs and so on so then how how how would you how would the network deal with the situation right so the answer to this as most of you would understand is that it's through the loss function right so now if my network starts predicting the value of y hat one right which was say the imdb rating as thousand whereas actually the value is somewhere between 0 to 10 right let's say it was 9.5 then my difference between 9.5 minus 1000 the whole square right this is my loss function this is going to be very high right if this is going to be very high then my loss is very high that means what is the network uh what is the signal that the network is getting that the current parameter configuration which is giving me these high values of the output is not a good configuration and it will start pushing it away from those configurations right using the gradient descent algorithm so if i get a large loss that's an indication that i should start moving away from those configurations and the network will start doing that so hence that is the reason why it will not learn to predict these high values the only way it can get minimize the loss is by staying within that range the moment it goes very far from that range it will not uh uh the the loss values would become very high and it will be pushed away from those configurations and so the loss will make sure that my values stay in that range but if i chosen the logistic function even if the loss is giving me that signal i can't do much because no matter what happens the logistic function is only going to give me values between 0 to 1. it doesn't matter how i adjust w1 w2 w3 b1 b2 b3 at the end my values are always going to be 0 right hence that wouldn't have worked out okay okay so that's what you would do in the case of regression now the next thing i'm going to do is talk about classification and to understand the loss function in the classification context you need to know about cross entropy so i have a separate video which is from a previous course which will point to so that would be kind of the preparation video for this lecture which talks about information content cross entropy and so on so you could see that video first before you look at the next part and we'll just put a link to that video here right okay so assuming that you have seen that video let's go to this discussion so now let us consider another example which is a classification problem so you are given an image and you want to classify it into say one of four categories right and now again what would uh what would the output you want to predict is you want to predict a probability distribution right so in the previous video the helper video that i just pointed you to uh we see that in the case when the class is known right you could still think of this as y being a random variable which can take on four values one two three four one for apple two for banana three for mango four for orange and so on right so it can take on these four values right and what your network will do is it will predict what is the probability of each of these values right whereas from the true label you know what the actual probability distribution is if this is an apple image then you know with 100 certain because that's what the label has already told you so you know with probability one that it's an apple image and the probability of mango orange and banana is zero whereas your network is going to predict some values right point two point three point four point one right and somehow you want to be able to capture the difference between these two values right so there are two problems that we need to solve here one is given that i know that i'm trying to predict a probability distribution because my true output is a probability distribution right the sum of the values is one right uh how do i make sure that my output is also a probability distribution right that means all the values are between zero to one and they sum up to one right so how do i choose an output function such that my output is also a probability distribution if i'm able to do that how do i come up with a loss function right such that it tells me the difference between these two now one loss function that you can always choose is the squared error loss function right you can again treat these as just two vectors and try to find the difference between them but given that you know that these are probabilities what is the special thing that you can do and the video that i pointed out says that you can use the cross entropy loss function so i'm going to rely on that and come back to the discussion on cross country right so this is the intuition that i want you to build that in the case of classification problems you have a true probability distribution which looks like one and all zeros that one will be on the class which is the probability mass of one would be on the correct class and zero on all other classes whereas your network will also predict some probability distribution because you will make an appropriate choice of the output function and now you're looking for a loss function which finds the difference between these two probability distributions so that's what you want to do in the case of classification so now let's fill in the glaps gaps suppose you want to classify an image into one of k classes here again we could use the squared error loss to capture the deviation but can you think of a better function right so that's the idea that's the motivation that i have set up now notice that y is a probability distribution where the two y is a probability distribution as i said all the mass is on the correct class and zero on the other class right that is something that you understand therefore we should ensure that whatever y hat we are going to predict should also be a probability distribution again i've already explained all this just re doing it so now what is the choice of the output function that you should choose right and this is what we have so far right again up to this point we have done all the computations and i'm trying to understand what should be the output function so that i get this shaded green part right so i've done up to a l and what should o be right so the choice of o that we are going to make is what is known as the soft mac function and this is how you write the softmax function and let me now explain what we are trying to do here right what does this function trying to say right so let's just look at an explanation of that so i can do it i need some space to do that let me just make some space okay so i can use this so now suppose our a l right this is a l okay now suppose it had again we had say in this case we were looking at a four class classification problem right so let's see it at a l the all the computations that have happened so far this is what my al vector looks like right suppose it's minus 10 10 20 30 right so this is what my a l looks like and from that i am not happy with this because this is clearly not a probability distribution it has values greater than one it has also negative values right so this is not what i want from here i want to go to a y hat which is looking like a probability distribution right and what i have chosen is the soft max function so what i'm going to do how am i going to compute y hat from here so this is what my y hat is going to be so i'm just going to do e raised to minus 10 okay divided by uh and i'm just going to write it here e raised to minus 10 plus e raised to 10 plus e raised to 20 plus e raised to 30 right so this is what my first output is going to be that is what y hat 1 is going to be what is y hat 2 going to be it's going to be e raised to 10 divided by this same sum then e raised to 20 divided by the same sum and then e raised to 30 divided by the same sum right and now you can understand that the sum of these values is going to be one right because it's the sum of all the values is the denominator so the sum of the four values that i'm going to compute here is going to be 1 and each of these values is going to be between 0 and 1. why is that the case let's try to understand that why is that the case so my original vector my original vector was say minus 10 10 20 30 right and now each value that i've computed is e raised to the corresponding value divided by the sum of the e raised to all the values right now even if i have a negative value in my original vector e raised to that is still going to be positive right so that's one thing for sure that once i compute the y i's there are going to be no negative values there right the other thing for sure is that the sum of the values is going to be one right so two criteria that we have for probability distributions are satisfied since the sum of the values is 1 and every value is going to be greater than 0 that means that each of these values is going to be between 0 and 1 right it cannot be greater than 1 because then the sum cannot be 1 because every value is positive and it cannot be negative because it's all e raised to something and even if the original value is negative so e raised to minus 10 is going to be 1 over e raised to 10 and this is still going to be a positive value right so i've taken the original computations al that i had have been able to convert that into a probability distribution using the softmax function i've also explained what the softmax function looks like and just want to talk about uh one more thing here right so you could have asked me that why did why did you do this e raised to x right i mean why did you raise every value to the exponent and then did that why couldn't i have just taken every value here right for example for this guy take 10 and then divide it by the sum of the remaining elements would that have not been enough right and obviously the answer is no because then for this guy which would have been minus 10 divided by minus 10 plus 10 plus 20 plus 30 the answer would have been some negative value right and you cannot have the probabilities as negative values right so that's why you use e because once you raise it to e then all the negative values even the the answer for those will also be positive right so that's why you're using the exponent here and that's why the form of the softmax function right so for classification functions once you have done this computation you will then pass it through the softmax function to get a probability distribution right so now the first part of the problem is solved that my actual outputs were a probability distribution so now i have chosen an output function which also gives me a a probability distribution right so this part has been done so now given that both of these are probability distributions can we choose a good loss function right obviously as i said you could use the squared error loss function right but if you know this is a probability distribution we should use something better from probability theory and that is what i explained in the video that i pointed to and what you can do is you can use the cross entropy loss function this is what the cross entropy loss function looks like you have k classes right so you look at the true probability for each of those classes and then the predicted probability for each of these classes right so the true probability for each of these classes multiplied by the log of the predicted property for probability for each of these classes right so where does this formula come from this is something that is explained in that video you should again look at it i'm pointing you to it again and again because that's important right now notice something about this formula right so that this y c right now y c can be y 1 y 2 y 3 all the way up to y k but it will take on the value 1 only for the correct class right only when c is equal to l that means only for the true cross label in this case only y one is going to be one and all the other y's are going to be zero right that means when you're taking this summation and you have y one multiplied by something y two multiplied by something y three multiplied by something and so on then only one of these terms is going to survive right the term which had a value 1 and all the terms which had value 0 are going to disappear in the summation so although i've written this as a summation it will only have one term which would be y l multiplied by log of y hat l and that y hat yl is again going to be just one right so because l is the true class and for the true class y l is equal to 1 right so then what remains and of course there's a negative sign here is just log of y hat n right so this is what remain it's and let's understand what that quantity is right so it is negative log of the predicted probability of the true class right negative log predicted probability of the true class and hence this is also called the negative log likelihood right because likelihood meaning it's telling you the probability of the true class the predicted probability of the true class so cross entropy minimizing the cross entropy is the same as uh maximizing or the log likelihood of the correct class right so that's what it means okay because you are going to minimize minus of log of y hat l i will do this on the next slide anyways so we are minimizing the minus of log of y hat l right that is the same as maximizing the negative of that which is the same as maximizing the log likelihood right okay so you are minimizing the negative log likelihood which is the same as maximizing the log likelihood and obvious statement so this is what the loss function and the output function looks like for the classification class right but now one question here this y hat l is it a function of all these parameters because if it's not a function of these parameters then how what am i doing here right is it a function of all these parameters of course it's a function of all these parameters so remember that y hat l is this computed like this right so you have the input x it passes through these series of transformations at the end you have the output function and then you're looking you get this vector of y's right and then you're looking at the l element of that vector right so all of this is of course connected all of this is dependent on the parameters hence the loss function depends on the parameters and now once you have this all you need to do is know how to compute the derivative of the loss function with respect to the parameters and that's something that we'll do later yeah so this is what y hat l encodes it encodes the probability that x belongs to the ellith class so we want to bring it as close to 1 right because that's the probability of the predicted probability of the true class so you want that predicted probability to be as close to 1 because we know that for the true class the probability is 1. okay so this is what we have done so far we have looked at two types of problems one problem where the output values are real values these are the regression problems and the other values where the outputs are probabilities these are the classification problems are two popular family of problems we have looked at for the regression problems we said that the output function should just be linear because we don't want the output to be bounded between anything and whatever is the natural range of the output the loss function will make sure that the network predicts something in that range itself because otherwise the loss will become very large right for the classification problems the output function was the soft max function and then the loss function for the regression problems for squared error and for the classification problems was cross entry right there could be other loss functions but these are the most popular loss functions that you will encounter in a very large number of problems are greater than 90 of the problems that you will encounter these two loss functions uh should suffice right or some variant of these two loss functions for the rest of this lecture we are going to focus on the case where the output activation is soft max and the loss function is cross entropy everything that we learn for that case is also applicable for this case by just one small tweak right so that's i'm just going to focus on the uh softmax and cross entropy part okay so i'll end this module here and then i'll come back and talk about the intuition behind back propagation and then we'll do the back propagation algorithm in g2