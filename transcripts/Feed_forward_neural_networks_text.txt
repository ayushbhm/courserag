[Music] hi everyone uh welcome back uh today we'll uh start with feed forward neural networks and talk about the back propagation algorithm right so where are we so far in the course right so we started with mp neurons then we moved on to perceptrons we dived quite a bit deep into perceptrons where we spoke about the representation power of perceptrons that it's just a linear separator and it can only work for data which are linearly separable then we looked at the perceptron learning algorithm we also looked at why there's a guarantee that it will converge for linearly separable data right and then we moved on that okay this linear boundary or this threshold kind of a thing which we have in a perceptron that's that's not very convenient and in real world we take decisions which are much more smoother so we move to the sigmoid neuron right and then for a single sigmoid neuron which are just two weights uh w and b we looked at the gradient descent algorithm for learning these parameters right and then we also spoke about the representation power of a kind of a network of sigmoid neurons right and so we saw that if you have quite many layers in fact the universal approximation theorem said that if you have one non-linear layer then you could approximate any arbitrary function to the desired degree of precision right and we went through that statement we also saw an illustrative proof of what that statement does what that statement uh means and then of course in the proof we had more than one layer but that was just an illustrative proof the formal proof uh which is beyond the scope of this lecture you could prove it in with one layer right but uh it doesn't matter whether we had one or two or three i mean just a small number of layers we were able to show that you could up our approximate arbitrary complex functions right and that's the main takeaway that we had and that was the power of the deep uh a network of sigmoid neurons so now we are going to formalize this concept of uh deep layer of sigmoid neurons by uh introducing some notation and a network and then see when you have this deep network how do you learn the weights in that deep network okay so before i begin some references and acknowledgements i learned this from uh from the lectures that hugo larochel has on back propagation way back maybe six seven years back you could still they're still available on the net and you could take a look at them right so with that i start the first module which is feed forward neural networks or a multi-layered network of neurons and in our case it would be it could be sigmoid neurons or any other non-linear function that we might consider right so let's start defining every uh component of this feed forward neural network right so at the input we have an n dimensional vector so these are some n inputs that we might have and we have uh i think in the past refer to this oil uh drilling example have you referred to that the or whether i can find oil from a given location i think you have done that where you have a bunch of parameters right so you might consider the uh salinity the pressure the temperature and so on right so these are some n variables that are given to you and based on that you want to make a certain decision and this is an n dimensional input and i'll soon be just referring to it i've already referred to as rn right so this is an input which belongs to r n okay then we have a network which contains l minus one hidden layers okay and in the example that i'm going to show there will be just two hidden layers so the network has a total of l layers of which l minus one layers are hidden and i'll tell you what the ellipt layer is going to be so in this case i have shown two hidden layers and each of them have n neurons right and for a large part of this discussion i'll just assume n equal to three right so i'll explain with the help of n equal to three but in general they're going to be n neurons right and also uh it it need not be like all of these are n that's just for convenience i've taken it this could be n1 this could be n2 and this could be n3 and so on that could be different the value of n could be different across layers is what i mean but for introducing the concepts i'm just going to take n it does not have any bearing on the concepts that we learn it's without loss of generality it could be any n right or my explanation holds for any value of n so it's just for convenience that i'm keeping it as n throughout okay finally there is an output layer containing k neurons right so you have these l minus 1 so that is 2 and then the third layer is going to be the output layer in this example so i have a total of l layers in this case l is equal to three and n minus one hidden layers and one output layer right now each neuron in the hidden layer and the output layer i'm going to split it into two parts right so what are these two parts uh i'll let the annotations be for now so there's going to be a part called the pre-activation so the bottom part which is the white part that you see i'm going to refer it to as pre-activation and the shaded part is the activation right so this is how you refer to it in neural network terminology so you have the input x then you have a1 the bunch of outputs that you are producing the first hidden layer a2 the outputs that you are producing in the second hidden layer and a3 these are all the pre-activations that you have in these layers right now maybe i'll get rid of the activations and then you have the activation which is the shaded part h1 h2 and then for the last layer it could be h3 or hl i could also call it as y hat because that's the output that i'm interested in so for the last guy the dark green or the shaded green guys there are multiple names right i could call it h l or the y hat because it's the output or i can also call it as f hat of x right because this is my approximation okay so they have the pre-activation and the activation now how to compute these uh pre-activations and activations is something that we'll see today but remember that each of these is a vector right so now here the entire input i could call it as x and i already said that x belongs to rn and since i have assumed that all these layers the hidden layers have n neurons so remember that a1 is also rn right and just to clarify this is a11 this is a12 all the way up to a 1 n similarly this is a 2 1 a 2 2 all the way up to a to n right and the same analogy for h every a has a corresponding edge so just as you have a 2 1 a 2 up to a 2 n you'll have h 2 1 h 2 2 up to h 2 n right so these are vectors in this case they are vectors belonging to rn okay the input layer can be called the zeroth layer and the output layer can be called the l3 right so you have layer 0 hidden layer 1 hidden layer 2 and then the output layer i'm going a bit slow about this i'm being very deliberate about every statement that i'm making because this is something that will stay with us for the rest of the course so these small things that okay the input layer is actually layer 0 the output layer is actually layer uh l is something that you would not need to remember right so i'm going to then soon start calling this layer h zero right so h0 is the same as x in my notation okay okay now let's get rid of the annotations again yeah so now every neuron in the previous layer so now this is the previous for this layer this is going to be the previous layer right this is the previous layer so every neuron in the previous layer is connected to every neuron in the next layer by a weight right so there are n neurons here each of them is connected to n neurons in this layer right so how many weights would you have you would have n cross n weights right so all those weights i'm going to put together in a matrix and call it w 1 for here this is w 1 as already mentioned there so w1 belongs to n cross n and they'll also be a bias right so every neuron in this layer is going to have a bias connected to it so i'll have n such biases so i'll have b1 equal to rn right and since i have all the hidden layers have the same number of neurons and i've also assumed the input also has the same number of neurons for sake of convenience all of these are going to be n cross n matrices right so i have w1 which is n cross n and b1 which belongs to rn similarly i'll have w2 which is also going to be n cross n because there are n neurons here connected to each of the n neurons here so you'll have n square weights so that's the n cross n matrix and they'll again be n biases okay then the output layer of course here there are k neurons and here there are n neurons so each of these n neurons is connected to each of the k neurons in the output layer so you'll have a total of n cross k or k cross n weights right and you will have only k bias term so it's one for each of the output you know right so so that's the overall uh structure of the network so i'll just quickly summarize input layer some hidden layers output layers within all the layers including the output layer you have the pre activation and then the activation we'll see how to compute the pre-activation and the activation all of these are vectors the a's and the edges the output layer is spatial you could call it h l or y hat or f hat of x right and then you have weights every neuron in every layer is connected to every neuron in the next layer i'm going to refer to the input also as a spatial layer as h0 right so that's and you also have biases connected to every neuron right so that kind of summarizes the slide and you should so we'll do more of this but you should get used to these dimensions okay so now with that let's go to the next side now how do you compute the ai's okay so let's see that now remember a i is a vector so let me just focus on a 1 right so that means i am focusing on this white part here okay so there are let's assume n equal to 3 for this example there are only three neurons that have drawn so i'm actually a 1 1 a 1 2 a 1 3 okay this is actually equal to the vector a right and how am i computing that i have b 1 1 b 1 2 b 1 3 plus so this is also a vector okay because i have three biases one bias for every neuron in neuron in this layer then i have w what was the dimension of w it was a three cross three uh matrix i am going to squeeze it in here so i have w1 this is the weight layer 1 matrix then the first element of it w 1 1 2 w 1 1 3 right and then w 2 1 w 1 2 1 w 1 2 2 w 1 2 3 w 1 3 1 okay let me just write this one all the way up to w 1 3 3 right so this is a 3 cross 3 matrix and that's going to get multiplied by x what is x x again has three components the input x one x two x three right so this is how a is going to be computed it's a simple matrix vector multiplication so this is a matrix vector multiplication and followed by a addition with a vector and all the dimensions here make sense right so a is equal to r a belongs to r n b also belongs to r n we just did that on the previous slide w belongs to r n cross n and i can multiply it with h i minus 1 h i minus 1 is h 0 in this case which is just the input and that also i had assumed belongs to r right so all this makes sense so this is just a simple matrix vector multiplication followed by an addition right so there's absolutely nothing uh fancy happening here right so just keep that in mind and that's how you compute a1 that's all that's there's nothing more to it right so this is the pre-activation now what is the activation the activation takes the pre-activation right so this is and passes it through a function so what does that mean i have already computed a 1 1 a 1 2 a 1 3 okay and now i am going to pass this vector to a function right a 1 1 a 1 2 a 1 3 okay and now there are various functions which can operate on vectors but in this case this is going to be an element wise function what does that mean that the output is just going to be g of a 1 1 g of a 1 2 g of a 1 3 right so i've just put the function inside that means it's just an element wise function these are known as element wise functions which operate on vectors that means they operate on every element of the vector right now if i choose sigmoid as the function then g of a 1 1 is simply going to be 1 over 1 plus e raised to minus a 1 1 right so that's all so once i have computed each of these a elements the computation of h is very simple and i'm going to compute these each of the edges so i'll get these three dimensional uh h1 right so this is what my h1 looks like it's a three dimensional vector so this is what it looks like and every element there is very easy to compute because you already have the a's and you're just applying a function onto every element of a right so that's that's all there is right so this is called the activation function there are many activation functions uh in the deep learning literature we'll be covering a few in this course we already saw the logistic function from the sigmoid family there's also tannage function that could be linear later on we'll be seeing functions like relu uh leaky relu and so on and we'll be seeing a whole bunch of functions for g right and all of them are going to operate element whether this one thing you need to just get into your heads is that i'm talking about non-linear functions and all that but at the end it's just simple right it's just operating element wise you give me a vector i'll take every element of the vector and pass it through a function right and just tan h is a function that you know so i'm just taking every element of the vector and passing it to the tanning function right so nothing very complex is happening there i mean in that one operation of course all of this gets combined and gives you a very complex composite function but in that particular computation there is nothing great right so the activation at the output layer is given by uh so you have alx right so this is your free activation at the output layer and i'm going to use a sum spatial function right so i have called g as the functions for the hidden layers the activation functions for the hidden layers but for the output i'll need some spatial function right so i'll tell you give you some intuition why so example i cannot always use the sigmoid function at the output because the sigmoid function will only give me values between 0 to 1 whereas in some cases my output could be greater than that so what if i am trying to predict say the rating of a movie on a scale of 0 to 100 or the score of a test on a scale of 0 to 100. if i bound it to sigmoid function then it will only give me values between 0 to 1. so depending on the problem that i'm handling i would want some output functions which allow me to cover those range right so we'll do that in more detail in this lecture but for now just remember that all the hidden activation functions i'm calling as g and the output activation function i'm calling as o okay and one example of the output activation function is softmax the other is a simple linear function both of these we are going to see soon right now to simplify notation what i'm going to do is i'm going to get rid of this right so it's understood that all of these are actually functions of the input right i don't need to write it again and again that i have an input and i'm computing a function of that input right it's understood that all of these are functions so i'm just going to remove that instead of a i of x that converts something i'm just going to write it as a i h i and so on right so that off x is something that i have deleted but it's understood that you have this input everything that is getting computed here all the white guys red guys and the green guys they are all the functions of the input right so that's very obvious i'm not going to write it explicitly it just makes the notation more cumbersome right so i have introduced a feed forward neural network now in the previous lecture we saw this supervised machine learning setup right where we said that in machine learning you always have an input x you know there's some true function which x is between x and y but you don't know that so you come up with an approximation of that function and that is your y hat right so i see a y hat here and this should have been f hat of x you can ask the t is to correct that with the f f x right so i have the x here and i have the y hat but what is the function right i see what i'm seeing here is some kind of a network but what is the function f hat of x i mean can you write the output as a function of x all these arrows and all make sense and i can see that somehow these computations are happening but if i ask you to write that function so that function in the previous lecture was just the sigmoid function right so i could write it it's 1 over 1 plus e raised to minus w transpose x plus b i could write that y hat explicitly now can i write this y hat explicitly or all that explanation that i had in the last class where f hat is your approximation and now instead of telling you what f hat is i've just given you a diagram where is the f hat in this diagram right so that's something that we should try to figure out right what does the f hat look like and that should give you confidence that this is just yet another choice of function families within the machine learning paradigm where you could choose different functions you could choose the linear function quadratic function and this is yet another function it just happens to be a very complex composite function but you can still write down what f hat is right so we will try to do that so i'll go back to that setting of a typical machine learning supervised machine learning setup where you have data which comprises of x's and y's so that is there i have the x i have the y's and i want to now come up with a model okay which is my approximation of the result of the relation between x and y and here i'm not come up with a model i'm not given you a function actually i've come up with a model but i've not given you what that function is i've just given you a diagram right so now can you write down that function right can you write down f hat so you could pause the video here and try to write it on your own and when you come back i'll just talk about it okay so let me talk about it so this is what the function is right this is exactly what the function looks like so let me explain you had the input x what did you do to the input you multiplied it by the w1 weight and added the b1 vector what did you get here you got a one right you got this output okay then what did you do you passed that through a non-linear function g and that function was simple in the sense that it was just operating on every element of a1 and then from here you got sorry oops i okay so this was a one this gave you h1 once you had h1 that became the input for the next layer what did you do with that you again multiplied it by w2 right and then you added b2 and that gave you a2 then you passed it through this non-linearity and you got not g2 sorry h2 right then h2 became the input for the next layer so you multiplied it by w3 and then you added b3 and that gave you a3 and then you passed it through a spatial output function to get h 3 which is the same as y hat which is the same as f hat of x right so you have actually written the output as a function of the input it just happens to be a very complex composite function right and let's just do a bit more about this so just to make sure that we understand all the dimensions right so remember that this was our n okay this was r n cross n so i'm multiplying an n cross n matrix with an n-dimensional vector so the output is going to be n-dimensional and then i'm adding an n-dimensional vector to it so that output is also going to be n-dimensional then i am passing it to an element-wise function so that is also going to be n-dimensional now you can go along this chain and convince yourself that all the di all the dimensions are compatible right so now that's that's the main thing i wanted to say uh yeah now one last thing i'll say suppose i have just assumed everything is n right for the sake of convince convenience now let me just assume uh make it different let's let this be p i'm sorry m and let this b be p now what would the weights w1 v w1 would belong to what so you have n inputs each of them connected to these m neurons so you have m cross n weights so this would be m cross n okay and what would b1 be you have one bias for every neuron in this layer so you will have m such right and now again you can see that these computations go well so this is an m cross n matrix multiplied by an n dimensional input okay and then add it with an m dimensional vector right so m cross n multiplied by n dimensional vector will give you an m dimensional output and then you can add it to a m dimensional vector and then when you pass it to this element wise non-linearity you will again get an m dimensional vector right and now if the next layer has p neurons then this w 2 should belong to p cross m and again this multiplication goes through this is p cross m multiplied by m which is a valid operation and now you can again justify the whole series of computations that you are doing okay so i want you to be comfortable with this computation so this should give you some com confidence that although this network diagram looks a bit complex there are so many connections going from one layer to the other layer at the end it's just a series of matrix vector operations followed by some element-wise non-linearities which are very simple at every element you just pass it through a function and you get the output right so it's not as complex as it looks you can actually write it down and even if they give you a hundred layered network technically you could still write it down it just would be a very laborious equation to write down that's all right now what are the parameters all the w's all the b's are the parameters and those are the parameters that you had introduced everything else was just x but these are the parameters that you have introduced and now you want to learn these parameters so you'll use gradient descent with back propagation so we'll see what back propagation that is one of the main topics of this lecture and then what's the loss function you could use uh the squared error loss function right so you are predicting k quantities you know what the true k quantities are right so you just take the squared error difference between those k quantities sum it over all the n training examples that you have and then take the average so that's what your loss function is and you want to minimize this loss function that means you want to learn the parameters theta which is all of these such that this equation or this expression gets minimized or this quantity l theta gets minimized right so i'm going to call this as l of theta this quantity gets minimized so you want to find theta which is a collection of all these parameters such that this quantity gets minimized so this deep learning still fits in the paradigm of machine learning that we had these five components which we had data model parameters algorithm objective function and we'll have to come up with objective functions which will help in guiding the training uh criteria i mean guiding the training right so so we'll end this module here and then we'll come back and talk a bit more about output functions and loss functions