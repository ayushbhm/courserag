foreign [Music] so first let's see people who try to hint that this is perhaps due to better optimization right so what is optimization problem that we're trying to solve this is the problem optimization problem we are trying to reduce the mean squared error so what is the optimization problem that we're trying to solve this is the problem optimization problem we are trying to reduce the mean square error cross into velocity whatever on the training data right uh is it the case that in the absence of unsupervised pre-training this problem itself is hard you cannot really reduce L Theta to zero but now once you do unsupervised pre-training L Theta is able to reduce to zero hence earlier you had poor optimization but now you have better optimization right so let us see this right so the error surface as I said for a deep neural network is highly non-connection it has these many plateaus many valleys some very steep points where you go in and then it's very hard to come out and so on right and somewhere there would be multiple Minima then possibly one Global Minima and so on it or multiple equal minimize so it's like a hardly a highly con complex non-convex surface as opposed to a nice convex surface we just have one Minima right so uh given the large capacity of deep neural networks it is still easy to land in one of these zero error regions right so there might be these multiple zero error regions of course in this diagram there is only one visible but there are a few behind which are all at a zero error level right so given this large capacity it's still possible to land in these zero error regions and why am I saying that which theorem am I using while saying this the universal approximation theorem right it said that you could sufficient you could consider construct a sufficiently large neural network to get an arbitrary degree of precision that means you could get an arbitrary degree of low error and in particular you could drive the error to Zero by just having a large number of neurons in your deep neural network so I know that despite this non-convex surface highly complex surface I can still drive the training error to zero right so maybe optimization was not a problem as long as the Deep neural network has a large capacity right for the universal approximation theorem it talks about large capacity in the term of really large right I mean you are talking about exponential number of neurons but maybe even not going to the exponential level with sufficiently large capacity which is reasonable I could still drive the training error to zero so then someone did experiments to prove that whether this is indeed the case right so what they did is they took a deep neural network and they increased the capacity of the last layer right so just before the prediction they increase the capacity what does that mean you add large number of Weights in the last year how do you do that you add a large number of neurons in the layer before it and now from this large number of neurons you're trying to predict one output so you have many weights here right so for example I could have say a network which has say some let's say 100 input neurons then I have a few layers which have 20 20 neurons right then I have the last layer maybe it has 100 neurons or even more than that right so now I have a lot of Weights here right I have 20 into 100 weights here right so the capacity in the last layer has increased and from this last layer I'm going to again predict say 10 outputs right so then I again have 100 into 10 weights there now if I increase this 100 to 1000 then my capacity increases further right so I can just increase the number of neurons in the last layer and that would result in an increase in the number of Weights associated with that layer right and hence your neural network would become large capacitor that's exactly what they did and they showed that if you do this then you're able to drive the error to zero even without training right so what are they saying that that if you have large capacity then you don't need pre-training right but large capacity of course comes at a cost right your network becomes large and training it takes more time and so on so maybe when you are not able to construct large capacity neural networks in that case pre-training is useful right so that's what they showed that if you don't have large capacity in the neural network then pre-training is still becoming uh uh useful right so what is it that they're seeing in effect that we always knew that in deep neural networks you can drive the error to zero because the universal approximation theorem says that but the hidden catch there was that you're talking about really large neural networks who can do that so what these guys showed that is indeed the case if you have a large neural network even without pre-training you can write the error to zero but when you have smaller capacity networks they are not able to drive the error to zero that means they find it hard to solve the optimization problem itself and then if you add pre-training then they are able to drive the error to zero that means pre-training is leading to better optimization right so that's the argument that they made of course this is empirical ah and these experiments maybe were much smaller scale as compared to what we see in deep learning today but that's not the point right I mean a lot of these were initial days and people were still figuring out what to do but it led to this important Insight that perhaps it is helping in optimization and if you can improve the optimization then you can train large neural networks so hey let's focus on designing better and better methods of optimization on supervised pre-training is one such method but what what are the other possibilities there now let us look at the other view here right sorry this should have been generalization we already saw it should have been regularization right so does it is it because of better regularization some people try to argue that too and let's see what that argument was right so what does regularization actually do it constrains the weights to lies within certain regions of the parameter space right we saw this when we were doing L2 regularization that it constrains the weights to dry in some circle right because you are not allowing the magnitude to grow beyond that that means only those weight configurations which lie on this circle are possible or within that are possible you cannot go outside that boundary similarly Urban regularization restricts you within this diamond that you see there right and we even saw with early stopping that is the effect that you don't allow your weights to grow too much right so you are constraining the weights to lie in certain regions of the parameter space right that's one way of saying it right instead of saying lying in a small region or something it constrains it to rise in certain regions right now is the same happening in the case of unsupervised pre-training right indeed unsupervised pre-training is also causing the weights to lie in certain regions of the parameter space what are these weights so these are the weights or these are the regions where the weights are better able to capture capture the characteristics of the input data why am I saying that because where did you start with you started with this unsupervised objective you said that forget about my main loss function which depends on y first for every layer I want to solve this unsupervised objective the moment you do that for every layer you are putting the weights in certain region of the parameter space where this objective is getting minimized right so they are this is the entire say weight space now you have gone to some region there where this weights this objective is getting minimized once you have minimized this Omega Theta then you are throwing in L Theta right but you are starting from the weight initialization here itself now this entire space is not available to you at all right and now once you start optimizing L Theta from there you will be moving from this region right so again you will be constrained to region around that of course you may move out of that region right but you have done the initialization in a certain region and now from there on if you train you are going to move wherever you move is going to be governed by where you restricted your initial weights to B right so that in that sense is acting like a regularizer and in fact even if you look at the regularization ah the way we studied is that there is an L Theta and an Omega Theta the same thing is happening here right you first had an Omega Theta you said I want to minimize that and then I will throw in L Theta and then I'll try to minimize L Theta so in the case of regularization we were seeing that we wanted to minimize the loss function as L theta plus Omega Theta right this is what our regularized loss function was I am doing something similar here it's just that I am doing it sequentially I am first minimizing Omega Theta and then I'm minimizing L Theta so in this view it is acting as a regularizer right and it also matches this other view that you're kind of constraining the weights in a certain region and then that will govern how your optimization is going right so in L2 you add a certain way of doing that constraint ah constraining the weights to a certain region in early stopping you had a certain way of doing that here also you have a certain way that you first train that I want these individual layers to learn well wherever you end up now from there start optimizing for the main problem which is L Theta right so that has the same effect so if you look at this view then it's actually acting as a regularizer right so both things could be possible right and some other ah experiments have also shown that retaining is more robust to random initializations what does that mean so this is what it means right so they trained deep neural networks let's focus on this plot first of no maybe on this plot first with different number of layers right so I have a deep neural network which has some W Capital parameters like some large number of parameters and I initialize these parameters randomly and train the network once right and then I got certain laws I note what that loss is L1 or the error which is L1 then I again initialize the weights again train the network again noted the loss right so I did this some 100 times and then I am Computing the variance actually I've plotted the box plot so which tells me the variance which among other things tells me the variance and they're saying that for shallow networks this variance is not very large right but when you have a deep neural network every time you do a different regularization a different initialization and then you compute the loss then my loss varies a lot across this different initializations what does that mean that these networks are not robust to initialization it is very specific to what initialization I have done but now if I throw in unsupervised pre-training and then I do the same thing now again I have first unsupervised pre-trained the weight so I'll just call them Wu right uh and now I am doing this experiment a hundred times so what am I doing that that I have started with some uh W random initialization okay so this is one experiment I end up with some w u after uh pre-training right and then I compute my final loss okay then I took W2 which is a different initialization again got a different W maybe right and then I computed L2 and this way I did this L 100 times so again I am doing different weight initializations but I am passing through an intermediate layer of unsupervised pre-training now if I look at the variance in these quantities then even for deep neural networks is actually quite low right so it's kind of making it more robust to random initializations right so now this led to the idea that maybe the whole deep neural networks are sensitive to initialization so now can we come up with better methods of initialization so that sparked interest in that the conclusions of these experiments are not as important as the research directions that they led to right so this now when I look at this I feel hey maybe initialization is an important thing and I should try to come up with better initialization methods and people did that and came up with better initialization methods and now that started making training deep neural networks even better right so what happened roughly is this right so if I were to summarize this period between ah 2006 to 2009 right uh people thought that okay first was that unsupervised pre-training works right people did really completely understand why does it work but people started investigating it through different lenses one set of people analyze it to the lens of optimization is it leading to better optimization it led to some conclusive non-conclusive answers but it did seem like maybe it's leading to better optimization similarly people started looking at it through the lens of regularization and thought oh looks like regularization is what it is doing people looked at it from the lens of initialization and thought hey maybe it leads to better initialization right and then these lens became important that oh all of this seem to be important so why let me focus on better initialization methods better regulation better optimization methods and so on and that is what has happened right deep learning has evolved since 2009 people came up with better optimization algorithms and we saw a series of those people came up with better regularization methods and we saw a series of those some was already existing L2 L1 all of that was existing ah uh early stopping also to an extent but it got popularized in deep learning and then Dropout was something which just specifically came in the context of deep learning right and now today we are going to talk about activation functions maybe activations if I change then maybe perhaps something could happen and better weight initialization strategies because some of these studies also saw that maybe initialization is what it is doing right so that's the context of this lecture or the context of the past two lectures also right where we looked at a series of optimization methods regularization methods so now you know that why we were studying that right because of this spark that happened in 2006 and that led to some investigations and pointed out in this direction that hey let's focus on these four areas and then maybe we'll be able to better train the Deep neural networks right so today we are going to focus on these two parts activation functions and weight initialization strategies so I'll end this video here and we'll then start the discussion on activation functions