foreign [Music] ERS so we'll do an introduction to Transformers and compare them largely with the recurrent neural networks which was the previous dominant set of models in various applications in NLP as well as a few Vision applications like image captioning and so on right and now a lot of them have been replaced by Transformers so that's what we're going to talk about today so to uh motivated what what is the content today going to look like right so we have seen three types of architectures in this course one was the feed forward neural networks then the convolutional neural networks and then recurrent neural networks right and in each of this we saw the building block and then once we knew understood the building block properly we could Envision deep wide networks right so in the case of feed forward neural networks the building block was essentially this sigmoid neuron right or any non-linear neuron which used to take um a bunch of inputs do a weighted aggregation and then pass it through a non-linearity right so that was the basic building block and then we had many of these connected in a layer and across layers to get a deep and wide neural network okay similarly in the case of convolutional neural networks the basic building block was the convolution operation and maybe even the max pooling operation then the case of recurrent neural networks the basic building block was this recurrent equation where you could compute HT which was a state at time t as a function of HT minus 1. and the input at that time step right so this is what the basic building block was and then we saw the attention-based recurrent neural network where we had the attention function which was like a basic building block which I'm calling these basic building blocks because if you understand these then it's not very difficult to understand the full Network right so similarly today we'll focus on the basic building blocks of Transformers which is lastly the attention uh the self attention and the cross attention uh layers that it uses and then try to relate it to what we have already seen in attention-based models in the context of recurrent neural networks right so that's the idea so with that let's zoom into uh the RNN based models and just see some limitations of it right so one challenge in RNN based models was that if I'm looking at the use case of translation right then my input is I am going home and I want to produce its translation in the target language right now the input is given to me at one go right the input is not like coming to me one word at a time where I just tell the machine hey this is the sentence that I want to translate right but the computation is not happening at one go right so let's see what I mean by that right so what happens here is that you get the uh you have the initialization Vector right and then you have this word I and then you pass it through the nuclear neural network so what you have here is say the embedding of the word I right so let's call this as X1 so this is the first words you have the embedding of that and then what the output is H1 right or what you're Computing here is H1 which is a function of h0 and x 1 right and now once I have done that this H1 then becomes an input to the next computation because H2 will be computed as H1 comma x 2 right so let's see this in the figure now so I have which one I compute H1 then I get the next input which is enjoyed right it has some another sentence so I think it's I enjoyed the movie Transformers so I got H1 and now I use that and H1 right so essentially I'm using this function which is H1 comma X2 to compute this hidden representation or this yellow representation that you see here right so what is happening here is that my computations are happening sequentially although my input was given to me at one go I had the entire sentence at one go and now let's just get rid of some of these annotations and just see the whole thing right so I had the entire sentence I am still Computing it one step at a time and I'll compute it at Z H1 then H2 then I am Computing H3 then H4 and so on right so I'm just doing all of this one step at a time although this entire sentence was available to me right at the beginning right so now the question is what's the benefit of what is happening here right so why am I Computing H2 only after I have looked at H1 right so the reason for that is that I know that for every word I already have the word embedding right so that's what my input is and this word embedding could be computed from your favorite algorithm like what to make or fast X or glove embedding or it could just be randomly initialized it could just be a learnable parameter in the network right but that is the word that is the embedding of the word computed from a corpus but I want to know the embedding of this word in the context of the center so that's what these Yellow Boxes are allowing me to compute because they are taking input from the rest of the sentence right now you might ask hey I'm only taking input from one side right I am not really considering Transformers when I am Computing the yellow box for uh movie right but there are also known as something known as bi-directional rnns or bidirectional lstms where you start from right to left right so this is how the computation proceeds for all practical purposes you could think of I have one sentence I enjoyed the movie Transformers so I computed these Yellow Boxes the way RNN does it then I reverse the sentence so I feed in Transformers movie is the Android I and I again do the computation and then again compute some other say green boxes right and now I have two representations for every word same movie I have one representation computed from the forward Direction and another representation computed from the backward Direction and I could just simply concatenate the those two representations to get the final representation of the word movie right and the reason I'm doing this or reason I am Computing this one step at a time is because I am interested in the context of the sentence right I want a contextual representation for the word movie so that's why I was doing this one step at a time to get the contextual representations for every word in the input right so which is also aware about what is happening in the words around it so this is important it's a contextual representation is important what is not good is in the interest of doing this contextual computation I'm not being able to do a parallel processing I have to wait every time one word at a time which is significantly which reduces my computational efficiency right because I am just doing things sequentially right so now my wish list would be to be able to do this to get the contextual representation that means when I'm Computing the yellow box for the word movie I want to know what is happening around me I want to take inputs from the other words and right now these inputs are flowing through these hidden represent additions at 0 H1 h2h3 so I want that to continue right and of course I'm going bi-directional so it's flowing from both sides so I want that to happen I want to take the inputs from all the surrounding words but I don't want to do this in a sequential manner I want a model which is not recurrent in nature because the idea behind a recurrent equation is that you do something at time step T minus 1 and then feed it as input to time step T so I don't want that to happen right so that's the basic problem I have with the sequencer sequence a models which I would like to overcome right and problem as well as a good thing a good thing is that I want contextual representations the bad thing is I don't want to do parallel sequential process right and once this is done then you have the entire Encore decoder block right which then takes in the final representation so ignore this for some reason this s2s1 is showing up on the slide it's not supposed to but it continues to show up oh no yeah so this is the final State and then it's passed to the decoder and then the decoder again does this sequential processing right and then of course I don't have much of a choice because I produce the first output which say in this case is none and then that has to be fed to the next state anyways right so the decoder uh this processing will still happen sequentially because I need to know what was produced and then feed it as the next input so unlike here where the entire input was available at one go here the input itself right is being generated one step at a time so I'll have to do a sequential processing right but then the encoder can I do something to speed up the computation is what my uh question is right so we'll go towards answering that question okay so this is for the decoder where I'm doing one word at a time but in the traditional encoder decoder model the problem is that I just do this computation once I have computed this H1 to H5 and then I take H5 as the like my final representation for the entire sentence and then this is the only thing which is fed to the uh decoder and then the decoder just produces the entire output based on this one representation that was given to it there's no notion of alignment right so there's no notion that when I'm producing none I should actually focus more on I when I am producing a Transformer I should actually pay more attention to Transformer and so on right so that notion is not there and you know where that notion comes from or what kind of models have that notion and that is the attention based model so it's in the attention-based models I have that where I have I compute the RNN encodings right and now I'm just going to look at it a bit differently right so once I have computed this I once I've computed H1 to H5 now I don't need the RNN block I just need these yellow representation that I have computed which are the H1 to H5 which I can just take them out and those can be my those are now with me right so I don't need to do any further computation on this right and once these five blocks are available to me right so I've just made a copy of those representations and kept it sorry the network seems to be yeah so once all these vectors are available I can just throw away the encoder and just have the output of the encoder which is these five vectors in this case in general it would be capital T vectors where T is the length of my input sequence right so that's what I'll have now once I have these the I'll feed these as input to the attention mechanism right and this is what the attention mechanism does at every point it now feeds a contextual representation to the decoder so it will just compute uh what is the most important word at this point right so you start okay go start Computing the output or start building the output so it will just take a weighted representation of all the inputs to compute the contextual representation which is just going to be like a attention weighted uh some of the inputs right so that's what it's going to be so I have these Alphas coming in here so for some reason the animations are showing up very slowly yeah so I have these Alphas showing up here and then I take a weighted sum and I compute this uh with a contextual representation uh C Wagner so this is called the context Vector it's also called The Thought Vector uh and for the rest of the discussion I'll typically call it the context Vector if I'm calling it something else I'll let you know uh at that point right so just think of this C1 as a context Vector which is a weighted sum or the attention weighted sum of the inputs right so you have just taken the outputs of the encoder which were these yellow representations which are context aware representations and now again to the decoder you are feeding a contextual representation which is now here the context is basically where am I on the output I'm producing the first word so what is the most important uh set of weights or what is the most important words that I need to focus right and then you keep doing this at every time step so you could think of is that you had this H1 to H5 weights and then you're multiplying them uh sorry the H1 to H5 vector us right so you can think of putting them in a matrix and then you have this Vector of Weights so now you're taking the doing this Matrix Vector multiplication which is essentially taking a linear combination of all these columns right so alpha 1 1 into this alpha 1 2 into this Alpha One three into this and so on right so that's the operation that is happening here and then you get the uh you feed it to the decoder RNN which then produces a output at the end right and you keep repeating this at every time step you do it at C2 then you compute C3 C4 C5 and so on right so you keep doing that now what I'm showing here is what is known as a heat map so this is what you typically look at when you are using an attention-based model so this has uh this is a say a T1 cross T2 Matrix where T1 is the length of the input and T2 is the length of the output so or the other way around whichever way you can look at it and now in this wherever you see a light spot that is the place where the attention weight is maximum right so when I was generating I my attention on none was maximum when I was generating enjoyed my attention on uh Racine was maximum generating fill this was the maximum weighted word and similarly here right it makes sense because these are almost like one-to-one correspondences in the translation output right so that's what the heat map shows you yeah so this is how the heat map relates to what is happening in the sentence as I said that when I'm using none uh the maximum attention is on I and the color coded you can understand the colors here so this blue color I'm focusing here and this is the corresponding weight and so on right and you have some attention function and then we had seen this function so used to compute the attention weight right as some function of the previous state of the decoder and the any input vectors that you had so this was the attention to be paid to input I at time step T which depended on the state of the decoder at time step T minus 1. and input I right and then this had this soft Max function to make sure that this uh align this Alphas form the distribution rate so they summed up to one right so that's what we have seen and you can use any alignment function here uh we had seen one specific function in when we were discussing recurrent neural networks so just showing the alignment of function again right and here's a question all right so and this is what will lead us to our eventual discussion on Transformers right so can Alpha TI be computed in parallel for all I at time step T so what is the question that I'm asking so I'm at a particular time step say I'm at time step 4 right so I'm asking can all these Alphas Alpha TI and T is equal to 4 so I'm asking whether Alpha 4 1 Alpha four two four three four four four five because I can take values from one to five can they be computed in parallel right and the answer is yes right because this only depends on St minus 1 which is already available and on hi which is already available right so it depends only on these two values so you can compute this this in parallel right and of course for normalization you need all the values but you can compute the scores in parallel and then once you have this course you can again compute the normalization in parallel right so for a given T you can compute the alpha tis in parallel for All Eyes no matter how many eyes you have right so no matter how long your sequence is you don't need to wait on the previous computation to happen or to have something happen at I minus 1 to be able to compute Alpha TI right you can just compute all of that in well right now the other question is so the main takeaway here is that the attention can be paralyzed but now the other question that I have is that can you compute this in parallel for all T's right so I said that at a particular T you can compute it in parallel right now I'm asking that suppose there's this time step 5 also here so can I compute Alpha 4 eyes right all the alpha force and all the alpha phi's and all the alpha threes right so all these Alphas across different T's right so my T is changing here can I compute them in parallel a given set of values all the Alpha Four Stars right Alpha Four one up to Alpha 40 I can compute in parallel that we have already seen but can I compute all of these in parallel at one go and the answer is clearly no right the reason is that it depends on S T minus y right so unless I have computed St minus 1 I cannot compute any of the alpha T's right and St minus 1 actually depends on Alpha T minus 1 because that's the input right so St minus 1 here this one would depend on uh sorry this would depend on c 3 and C3 in turn would be a function of alpha 3 all the alpha threes right so unless I have computed Alpha 3 I cannot compute C3 unless I have computed C3 I cannot compute S3 unless I have computed S3 I cannot compute the alpha Force right so all the alphas cannot be computed in parallel but for a given uh T the alphas can be computed in parallel right so now the two things to notice here right so one is that the attention can be parallel at least in a given T it can be parallelized right it cannot be paralyzed across T's but for a given T it can be parallelized and this is something that we would like to exploit and see that if we can get rid of this recurrent connection right because this recurring connection is still a problem for us right because because of the recurrent connection we have to do things sequentially but if we could get rid of the recurrent connections and then rely on the fact that the alphas can still be computed in parallel then can we get to an architecture which allows us to compute these Alphas in parallel right so it's still a bit hard to visualize where we are headed but just keep these questions in mind along the way and once we read there all these questions and the answers will make sense so just to summarize the discussion so far right so I mean everything about the RNA model is good right so what do I mean by that uh we saw that across papers right I mean we saw the architecture they are used for machine translation summarization video captioning image captioning right so they gave very good performance and a wide variety of tasks right but the only uh issue that we have is that given a training example we cannot paralyze the sequence of computations because each of these guys needs to be computed one at a time that I cannot compute all of them in parallel of course on top of that if I have attention attention at a given time step can be computed in power right so now a wish list would be can we come up with a new architecture right that incorporates the attention mechanism and also allows us to do things in parallel so we don't want to get rid of the attention mechanism because the attention mechanism helps us to compute the contextual representation but we want parallelism we also don't have a problem with the basic idea of recurrence right that the recurrence actually allows us to compute things which are contextual right so we don't have a problem with this here so we don't have a problem with this here this is fine because this is allowing us to compute these recurrent uh or the contextual representations but we have a problem with the computational curves which comes with recurrence that's the problem that we want to solve okay so that's the context so that's a quick recap of recurrent neural networks and what we see as problems in the recurrent neural network and now we'll try to go towards a solution for that right and that might probably lead us to a new architecture so I'll end this video here and we'll come back and continue from this point