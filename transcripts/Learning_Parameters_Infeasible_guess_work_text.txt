[Music] so welcome back uh so in the previous module we looked at this typical machine learning supervised machine learning setup and the key there was to now come up with a algorithm for learning the parameters right so what we'll do is now start with an infeasible algorithm right which will not really work in practice but it will give us intuitions about how what should we do to make it i mean go towards the more practical approach right so let's start so this is the supervised machine learning setup that i have i have n inputs okay and i have some weights and then i have the output now this is the model right this is my approximation of the relation between y and f y and x right so this is f hat actually this is the model and now i want to know an algorithm okay which can be used to learn the parameters of this model given the data that is would be given to me and using some objective function which makes sense right so that's what my quest is okay now then this is what my f hat of x is i generally use therefore fake side i mean i've explained the difference between f of x and f dot x but i'll just call it f of x and from the context it should be clear that i'm talking about my approximation because the true function of course is not known right so i cannot even define that right and i'll use sigma here which is written here is actually a short form or the notation for the sigmoid function okay so now we'll consider a very simplified version of this model right for all that explanation that there's only one input which is connected by the weight w and then there's this constantly on input which was connected by minus w naught but now to be consistent with the literature going forward we are going to call it as b right b stands for bias so instead of w naught we are going to use b and our function would then become minus of w x plus b earlier it was minus of w x plus w naught right so you're just changing that notation and i'll stick to this notation for the rest of the course now right yeah and lastly right so so far what we have been doing is we have been trying to talk about the problem that you are given one input and you are trying to predict the value of the uh zero or one right whether this movie are going to like it or not but now i'm going to make it uh make it a problem where you're trying to predict a real value instead of a boolean value and you're trying to predict the imdb rating on a scale of zero to one right so it could take values like zero point zero five point one point two whatever right it could take any of these values so it's a real input a real output right so that's it's easier for me to explain a few things in this setup but it does not change anything that i'm going to talk about even if you had boolean inputs we'll see later on how to adjust the same explanation to boolean inputs but for now i'll assume we have a real output right sorry not input output so we have a real valued output okay so that's the setup that i'm going to work with right and now the first thing is that i need some training data right so i've been given some training data so as i said that there are capital n pairs of now this is a good notation capital n for the number of training pairs and small n for the number of inputs that i have the small n is equal to 1 in this case right so this is the training data that i have now my training objective as i said is going to be that when i make predictions and these are the true values and these are the prediction values i want the difference between them to be as small as possible so this is my difference function and this i'm going to calculate over all the training points take the average and i'm going to define that as my loss function and my objective is to find w and b right such that my loss function is minimized right so that's an english explanation of this entire expression written here minimize with respect to w and b which means find the values of w and b which minimizes this quantity and this is how this quantity is defined it's the average difference of the predictions from the two values for all the training points right so that's a clear notation right now suppose i was given only two training points okay these are the two training points given to me these are the x's and these are the y's so this is one x one comma y one pair and this is the other x2 comma y2 pair i'm just given two training examples okay and these are the values also right i mean these are actual points on the graph so i can see that one point is 0.5 comma 0.2 right so this is the point 0.5 comma 0.2 as you can see and the other point is 2.5 comma 0.9 right so these are the points which are given to me okay this is my input training data and using this data i want to learn w and b and that's the setup that i have okay now um what is happening yeah so at the end of training we've we expect to find the optimal values of w and p right what are the optimal values though these are those values which minim minimize the loss function that we had defined in the previous slide right so those are the values that we want to find so now suppose so now what do you expect right so suppose at the end of training i told you that i found w star b star what is it that you expect what would happen i'll ask a more specific question if i plug in the value 0.5 here if i pass 0.5 as the input and if you have learned w and b well what do you expect to happen the output should be the output should be close to 0.2 similarly if i plug in the value 2.5 i would want the output to be close to 0.9 right this is what i expect to be and this is what i've written here right so if i plug in f of 0.5 then it should be as close to 0.2 right tending to 0.2 and f of 2.5 should be as close to 0.9 right now let's try to relate this algebra to the geometry right so now geometrically i know that i am looking for a sigmoid function which is a s-shaped function right now algebraically what i am telling you is that when i plug in these values they should be as close to 0.2 and as close to 0.9 so now can you tell me what is the geometric interpretation of that i'm going to get some function because this actually defines a sigmoid function right and now i'm found on w and b so i'm going to plug in those values right and then i can draw that function so what should that function look like or what should be a characteristic of that function of course it will be a sigmoid function of course it will have s shaped but there is respect to the training data what should happen the two points that i have shown here should lie on the function right so what what that means is let me just draw it apply or close to that function right so i'm assuming it's perfectly done so this is what should happen right so whatever w comma b values i have when i plug them here i know that when i plug in the value 0.5 i want something which is close to 0.2 right i'm drawing the best case here that i should get a sigmoid function such that these two points lie on that and that's what it means right that when i plug in the value of x i get this y when i plug in the value x2 i get the other way right so this is what we are looking for it so this connects uh i would say the math to the geometry right so the algebra to the geometry that uh when you plug in these values you get some output and this is what it would mean geometrically right okay so now let us see this in more detail right so as usual i never know what the w's and b's are right i've given these two points i'm going to start with some random w's and b's so here i have started with uh if you can't see it let me just see if it's on the file ah so w equal to three and b is equal to minus one 1. that's what i have started with okay and now this is the sigmoid function that we get i know that intuitively i know that this is not correct why is this not correct both the points are not on the function right this point is also not on the function right so both the points are not on the function and just on the previous slide we saw that when the network is trained or when i have learned the values of wnb i would expect these points to lie on the sigmoid function but right now it's clearly not the case so this is bad there is no argument about that but how bad is it can you quantify that all of you are saying that it's bad right that's a qualitative answer can you quantify that yeah right so the answer given is that we could look at the loss function right so we now what we will do is that we look at the loss function which depends on w and b and we'll see how to calculate the loss function and then we'll be able to say exactly how bad this is right so let's do that right so my values of w and b i know are 3 and minus 1. so i know that this is equal to 1 over 1 plus e raised to minus of 3x plus b and b i have chosen s minus 1 so this is what my function looks like now in this function i'll plug in all the points that i have how many points do i have only two points so for one point the value would be the x is 0.5 and the other x is 2.5 so i'll plug in these values and i'll be able to compute this quantity and i already know what the true y should be right so i can calculate this so i'm just going to go ahead and calculate this right so this is what it looks like okay i just expanded the sum now i've going to calculate those values and this is what the values turn out to be now i'm just going to do the math and this is the value right so this is 0.099 that's the loss if it was perfect the loss would have been zero right but this is 0.099 so that's how bad this is now i am able to quantify my answer using the loss value if you give me a w comma b i can tell you how bad that is right that's right what i am able to do right now we want the loss to be as close to zero as possible ideally zero but as i said i would settle for approximately zero also right so that's what we want to do so let us try some other values of w comma b right and we'll change we'll use the slider to change it okay so first i'll try uh let me just see if i can do that so first i'm going to try w is equal to 0.5 okay okay this is 0.5 and b is equal to 0 right so let me just oh for some reason i'm not able to change it okay so suppose i do w is equal to 0.5 and b is equal to minus 1 right this is also bad and how bad is it it's saying it's 0.121 right so earlier my loss was 0.09 now my loss is 0.121 right so it's even worse right and you can see from the figure also that's even worse right so let me try something else i'll try minus 0.1 now okay like some of these values are not here but let's say i try this value right again i see that my loss is increasing so then i keep changing right so i keep trying different values and then i tried 0.94 and minus 0.94 and i saw that the loss is 0.0214 okay so this is good okay this seems to be good so let's let's see what my rational was it it started with 0.50 i got a loss of 0.07 then i randomly jumped to some other point the value increased okay so then i concluded that i made the w negative maybe that's a problem right this is what my thinking was this is how i have come up with these numbers so let me interest increase the w so i increased the w and i made b a bit negative and the loss decreased okay so then i assumed okay increasing the w further and decreasing would be further might help and that actually helped and i kept continuing that way and it had right i'm doing random guesswork but it's slightly not so random because i'm looking at the loss values and slowly trying to adjust the weights right and all these values that i tried these are all the different functions that i got right so you can see what had happened for w equal to 0.50 you can go back and look at all these values so i was getting different uh functions and then as i kept changing it i somehow found the right function right so this is what my guesswork was and you can all imagine that this guesswork could have possibly never ended these are very specific values and i chose them because i knew what the right value was and i was just trying to get that i mean i have reverse engineer these wasn't if i was completely randomly trying this this would not really converge that i would not end up anywhere right so now let us look at uh something better than this guesswork algorithm right so this is slightly better than the guesswork algorithm so what i have done here is again i had seen we had seen this earlier so i've just plotted so my this is my uh oops this is my w axis this is my b axis okay and the vertical axis is the error axis right so for all possible values of w and b i have just computed the loss i can do that i can do that programmatically and i have just plotted it right and this is what the plot looks like okay and you can see that there is this region here where the loss is low or minimum right and there are certain regions where the loss is very high right so i could just plot this and then i could just pick up some value from this good region here and that would what my answer be right and we had seen a similar argument when we're doing the perceptron algorithm and we need the know the fallacy of this argument also that this is okay for this toy example uh but it will become intractable as the number of data points increasing right so as the number of data points increases just computing this loss would be very difficult for all the infinite values of w and b right and here although i'm saying infinite i've actually plotted this only from minus 6 to plus 6. right those are the only values i'm not really plot i mean i cannot plot for minus infinity so what if there was a value here right going forward this side for which the loss was even lower right so what would what why i could have chosen that also right so this kind of just plotting the error surface and looking at it and figuring out the value that does not work in practice right even although this is a good way of seeing what is happening it does not you cannot do this in practice right you won't be able to get anywhere okay so now let us with that plot in mind and my guesswork in mind let us try to look at the geometric interpretation of what was happening in my guesswork algorithm right so we will try to do that okay so what had i done i had initially chosen 3 and one as my values right and then uh three and one or was it three n minus one i guess right so three n minus one is what i had chosen right so this is where i was on the lost surface this is my three comma minus one point and my loss was very high right and then from there i randomly jumped to this point uh 0.5 uh comma 0. and before i click enter just see that when the value of x w1 wnb is 3 comma minus 1 this is what the sigmoid function looks like and it's obvious that the error is high because it's nowhere close to fitting the two points that i have right so then i tried 0.5 comma 0 and let's see what happened now if you look at the sigmoid function it looks a bit more favorable right and you can see that the loss has also decreased right but since i was randomly guessing i did not have this error surface in front of me so i just continued randomly right and then i went to the point minus 0.1 comma 0 okay and i goofed up you can see that i actually went up on the error surfaces i was somewhere um yeah it was somewhere here and i climbed up on the error surface right and then i changed it further so what i did is oops yeah then i changed it to a 0.94 and minus 0.94 and then the loss decreased right and i can see from the sigmoid function also that i am going in the right direction and then i kept in continuing in this direction but you can see that my movements if you look at the wb plane they're very random that is jumping from one point to another i'm just getting lucky somewhere and i once i hit this point 0.94 minus 0.94 then i get a sense of what is required i need to increase the w's and decrease the b's and then i continued that way and then i went finally reach to the point 1.78 comma minus 2.27 where my error was close to zero and you can see that the sigmoid function is in the way it should be and you can see on the error surface also i am in that low error region in fact the error is actually zero right so that's that's what i was doing i was actually uh uh subconsciously or like without really knowing it actually navigating the error surface but i was navigating it randomly which is problematic because then i went down then suddenly i came up then again went down came up and i want to avoid that i want to have a more principled way of navigating this error surface right so that's what my quest is and that is what we will do in the next module okay