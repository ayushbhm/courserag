foreign [Music] methods are good right so what does that mean that if we average the output of several models then it always helps right but now in the context of deep neural networks this is of course a challenge right so if you have to train several large neural networks then it is going to be prohibitively expensive right because then you are going to combine these neural networks but then you have to train so many of those right and what uh there are two options here right one is you train several neural networks having different architectures so that's what I've shown in the first figure here where you have all of these are neural networks but they have different architectures or the other option is that you have the same neural network same architecture rather and you're going to train it using different subsets of the training data right now both these options are obviously expensive right because training a deep neural network is going to be expensive and now you're talking about training K of those right and even if I were able to train K neural networks at test time again I have a problem right because in test time typically I need results faster but again every test instance that I'm going to get I'm going to pass it through all the K uh neural networks and then combine their output rate so it's not just a training time I have to have a lot of compute but even at test time I'm going to have a challenge right so how do we then use this model averaging idea which we just saw in the last video is useful how do you use it in the context of deep learning right so the two challenges are that at training time your cost is high and even a test time your cost is high right irrespective of whether you use option one or option two right so Dropout actually addresses both these issues so what it does is it effectively allows us to train several neural networks without incurring the cost of actually training K neural networks right and we'll see how exactly it does that and it also at inference time we don't need to pass the test instance to several networks we just need to pass it through one neural network but just do a simple trick to get the averaging effect right so we'll see how that works so the idea in Dropout is that you have this original Network which I have shown on the left hand side you drop out some units from it that means you drop out some new neurons from it so this is I've dropped on some randomly some six neurons from here and then you get a different network right so you have got a different architecture now right so just as an option one uh you are training neural networks with different architectures and you are training key of those I have got the same effect here by dropping some neurons so it had the base architecture I dropped some neurons and I now my neural network looks a bit different right and just doing it temporarily just for that current training instance I'm going to drop a few neurons or that batch a mini batch I'm going to drop a few neurons right and then again do something different in the next device right now each node is retained with some fixed probability right so every uh so every time I get a mini batch for every node I take a decision whether I should drop it or retain it and this decision is taken with a fixed probability right that means with 80 chance I'll retain it and 20 chance I'll drop right so that's uh the decision that I take so that every time I receive a mini batch I visit every node and decide whether to drop it or retain it of course it's not done like sequentially the way I'm saying it uh you have more efficient ways of doing it but conceptually this is what you will do for every mini patch right now suppose the neural network has n nodes then using this Dropout idea how many neural networks can you create right so I have n nodes given to you and for every node I can either retain it or drop it right so it's a binary decision I'm going to take for every node and each such configuration gives me a new neural network right so I just think of it that I have these n nodes here right and I'm going to write a 0 if I'm going to drop that node and a 1 if I am going to retain that load right so now this is my n dimensional vector and this will decide which neurons here get dropped or uh retained right and there are such two raised to n vectors that I can construct right which means there are 2 raised to n networks that I can construct from a given neural network right so if I use the Dropout idea and if I have n neurons then I can actually construct 2 raised to n different neural networks right but then how does it help me I just said that ah even if I have like K different neural networks and K was of course much smaller than 2 raised to n then I have a challenge in training it so now I'm talking about two raised to n different neural networks which are possible so how am I going to train these uh many networks right so the trick that we use is that you share the weights across all the networks so you have two raised to n networks but the weight matrices are the same right so now suppose this weight exists or this node is retained suppose in such uh 2 raised to n by 2 of the networks right because half the networks if you are retaining and dropping with 50 probability then half the networks would have this node how the networks would not have this node right so that means this weight will be retained in half the networks right but this weight would remain the same in all the networks right I'll be using the same copy of the weight uh that's the first trick I'm going to use what's the implication of that we will see soon and the second trick that we are going to use is that we are going to sample a different network for each training instance so it's not that I have like these two raised to n networks created at beginning and then train all of these two raised to N I just have one network whenever I get a mini batch I'm going to sample one of these two raised to a networks that means I'm going to randomly drop some nodes in the network and then train using that uh Network right so let's see what that means right it's like a bit difficult to understand but we will break it down into steps and then it should become clear right so we initialize all the weights of the network so there are in this network this is my base Network right I'm not creating multiple copies of this this is my only Network so whatever weights are there so let's assume there are n parameters everywhere uh sorry n nodes everywhere so you have have a n cross n weight sitting here you have an N cross n weight Matrix sitting here I'm ignoring the biases for the while and suppose you have only one output neuron then you are n weights sitting here right so these all these uh 2N squared plus n weights I have initialized okay and I have started training now I received the first mini batch for training and I just drop some notes from the network right so I've got a thinned version of the network for the first mini batch right and now I'm just going to assume that this is what my neural network looks like and just do my forward propagation and backward propagation right so I'll compute the loss using forward propagation and then I'll back propagate now when I do back propagation which are the parameters that need to be updated I had n square plus n parameters now if I do backward propagation which are the parameters that I should update yet only the ones which had participated in the computation right so this parameter for example was not there this was because both these nodes were dropped so this parameter was not there this parameter was also not there this parameter was also not there so obviously they did not participate in forward propagation so they will not get updated in backward propagation right that's as simple as that so let's see what that means right so these were the ones which were active so these will get updated these will get updated and these will get updated right the other weights will not get updated and it's a bi-directional Arab that means in the forward propagation also these only participated and the backward propagation also only those will participate right so this is what I have done for the first mini batch that I received now when I received the second mini batch I again sample a new neural network from my original Network that means I just drop some other set of nodes right because this is a random process that every mini batch I'm going to decide for every node whether to retain it or drop it so my decisions would change from one batch to another and I'll get a different version of the original neutral Network and now again I'm going to update only those weights which actually participate in the computation right and now if you look at it right so let's look at some weight which was there this one right so if you look at this weight this was present in both the networks there were some other weights also which were present in both the networks maybe I should use a different color right so this weight participated in both the networks so let me just call that weight as W okay so uh at time step one I had updated the value of w using whatever update rule I wanted right let's assume I was just using gradient descent so my value of w has changed now at time Step 2 since my weights are shared I will start with the updated value of w I will not start from W naught I'll start with W 1 because that weight has already been upgraded and then update it again using this equation right so that's what sharing weights means these are two different networks conceptually but actually it's the same network from which certain weights have been dropped right and those weights which are participated in all in the first batch as well as the second batch for those weights I have done two updates and for those weights which only participated in the first batch or only participated in the second batch I would have done only one update right so the main thing to notice here is that even though I am kind of it looks like I'm using a different neural network at every time step it's not the case because the weights are the same they are the shared weights and I just start from the previous value of the weight which was at the say the kth iteration and then update that value I don't start with the value which was at the zero titration right okay yeah yeah so that's what is being said in this slide so each thinned Network will get trained rarely because there are two raised to end networks right so it's very unlikely that this network if n is very large right which is typically the case that the same network will get sampled many times because there are two ways to end different configurations possible so it was also possible that some of these networks will never get sampled at all right because if your number of steps which you run the data for is less that run the training for is less than 2 raised to n right suppose you have a million nodes the two raised to million is going to be very large and you're going to train for much fewer steps than that right so all these two raised to million networks will not even get sampled some of them would get sample right but still because of its weight sharing it's like every network is getting trained because even if this network was never sampled there were other networks which got sampled in which these weights would have been active and hence those weights were getting regular updates right and since every uh weight is going to be present with an 80 probability because you are going to retain 80 percent with probability 80 percent you're going to retain a node right and also okay so 80 into 80 because both these nodes need to be remained so with 80 probability this will be retained and with 80 probability this will be retained so with 64 probability both will be retained and hence this way it would be retained right so every weight with 64 probability it will be retained right and hence it will get updated many times it will get 64 percent of the times that you are doing training it will get updates if you run for a thousand steps every weight will get updated around 640 different uh times right at least 640 times okay so now what do you do at test time this is what you are doing at training time at training time every node was present only with probability P right now what do you do at test time at test time now again you cannot sample these two raised to n networks past the output through all of those and then take the final decision right so at test time you can use this neat trick which is saying that hey every node was only present with probability P that means it was only present P faction of the times right so that's let's say 80 percent of the times or sixty percent of times whatever is the probability that you have chosen right so then I should trust the output of this node with only that fraction this guy only participated in 80 of the discussions so whatever it says I only trust it with 80 value that's the same as saying that whatever output this guy gives just scale it by that fraction as simple as that right so you participated only in 80 of the discussions so I'm going to trust your output with only 80 confidence that's the same as saying that whatever output you give which is going to be passed to the next layer I'm just going to scale it down by B right so that's all you do at test time so again at test time you're just passing through a single Network and every weight in the network is going to get Scaled or the output of every neuron in that network is going to be scaled by this probability P okay that's all you are doing right uh so now what let's let's try to get some more intuition right into what Dropout is actually trying to do and why does it act as a regularizer right so it actually applies a masking noise to the hidden knit so what does that mean is that in every you could think that Suppose there are n vectors n nodes here at every mini batch I am creating an N dimensional uh Vector where some values are one say 80 of the values are one and the remaining 20 are zeros right and that's the same as whatever output you produced here I'm going to multiply this by this mask right so if you were a masked out then you are not going to participate in the computation that means your output would be zero because I'm just multiplying You by this Mass Vector so that is what is happening here some units are getting dropped and this is how you actually implement it it's not that every time you visit every node and decide whether to keep it or not you just create this random vector by saying that I want 20 of the units to be off and then at this layer whatever output you have computed you just multiply it by this mask so 20 of the outputs will be set to zero that's the same as this node not participating in the computation at all right now when you're masking it what is it that you are effectively doing right so what happens is it prevents the nodes from co-adapting right so what does co-adapting mean that if you have all the nodes active right then there could be this one lazy guy who says that okay I mean this other guy suppose I'm trying to detect faces and this other guy is going to detect noses a nose and someone is going to detect eyes and I don't need to do anything then right I can slack I can I did not actually work much the other guys can do the work right so that's what co-adapting means right or it could be that hey you detect knows and I will detect ice I will only fire if there is a clear eyes in the picture if it's taken from the side maybe the eyes are not visible and you fire only when there is no sweat of course this is not that the neurons are talking to each other but this kind of co-adapting could happen where every neuron just focuses on doing one thing right but now if you are going to drop the nodes randomly right so what will happen in that case suppose this was the guy which was focusing on detecting noses now you have received a training instance and that training instance say it has a nose right and you have dropped this guy house now there is no node in this layer which is detecting noses hence the other guys will have to wake up and say that hey I cannot rely on anyone else I cannot co-adapt because this guy is unreliable and sometimes he is active sometimes he is not active so better I also learn how to detect nose in addition to learning how to detect say ice right so every node will now have to act independently and take more responsibility right so this is what actually happens in a Dropout and I'll just now Flash the content on the slides yeah so either the multiple nodes need to learn how to detect noses or other nodes to need to learn how to detect noses how to detect the face even if the nose detecting neuron is not active that means some other node needs to now detect how to uh I mean detect a face using ice let's say what the problem here is simple and I'm showing you an image and I want to tell you whether there's a human face in it or not and you can use the features of a human face to detect right so some neurons could fire if their eyes is there some neurons could fire if nose is there but now if you drop out things then some of these guys will no longer be available so then the other guys have to become creative and earlier maybe the network was never learning to rely on the eyebrows to detect a face but now some neurons will start picking that up because the other guys are not working so I better become more creative right so that way the network is becoming more robust right so that's all I had about Dropout and what I'll do next is I'll quickly give you a summary of all the regularization techniques that we have studied need