foreign [Music] okay so now let's talk about stochastic and mini batch gradient descent let me first motivate this with some intuition and then we will look at how to go about it right so we'll dig us a bit uh actually after nag there are a few other algorithms that we should cover but before that since we are still in a slightly easier territory gradient descent momentum nag those are easier to understand so in this easier territory I'll talk about the stochastic version of these algorithms and then later on I'll talk about other algorithms like Adam adagrad at adult and so on okay yeah so if you look at the code for uh gradient descent right and the same code I mean with some variations was used for momentum as well as nag then what you're doing here is that for all the training points so this for Loop is looking at all the training data in this case I had only two training samples but the for Loop is going over all the training samples Computing the gradient keeps on accumulating it in the sum DW and DB and then comes out of the loop and makes one update right so that's one observation that I want you to make it's obvious from the code that it's looking over the entire data and then comes out of the loop and makes one update uh and then keeps going about it right so there are like two for Loops so the inner for Loop runs over the entire training data okay now the issue with that is that the question is like why are we doing this right so why are we going over the entire training Data before making one update to the weight parameters right so the reason for that is that because this is a true gradient right we had painfully computed that when we were doing that toy Network that to compute the loss function so the loss function is actually a sum over all the training examples and then whatever formula you use right squared error or cross entropy and then you take the average of that right so the true value of the loss is this right and it includes all the training examples now if I want to take the derivative of this with respect to the loss function then the derivative would also be a sum of all the values so it's also going to have a sum over all the N terms and that is exactly what this for Loop is doing here right so this is exactly the formula you just applied the formula and the formula says that you should sum over all the variables and this is the true gradient there's no approximation happening here right so what you're doing is correct but and because what you're doing is correct all the theoretical guarantees hold so what does that mean so when we are derived gradient descent using the Taylor series approximation we had said that at every step the loss will keep decrease right and that guarantee holds because you are not doing any approximation you have computed the true gradient or the true partial derivative and you are moving the direction opposite to this true partial derivative right and it will become K what I mean by true and not true right what true means here is that there is no approximation in the formula whatever formula you derived you're using exactly the same formula hence all theoretical guarantees hold so that's the good part but what's the flip side what's the bad part the bad part is that suppose you have a million points in the training data right then you'll run this for Loop for all the million points okay you will painfully compute this derivatives then come out and make one update right so now if you have to run the algorithm for 100 steps which is quite small right in terms of modern deep learning then you would be making like 100 million computations and making 100 steps and 100 steps your weights wouldn't have actually moved much right so you would be nowhere close to conversions and you have done so many computations right so that's the flip side of this to make one update you have to do so many calculations and obviously this is going to be very slow so the question is can we do something better and the answer is that we can do what is known as stochastic gradient descent so as opposed to uh the Computing the true gradients can we just estimate the gradients using fewer points instead of looking at all the endpoints so that's the idea I will talk more about it right so let's not look at the code for now so so now in this stochastic version of the algor I'm sorry we have to look at the code so now in the stochastic version of the algorithm what I've done is just a small change right so this this part of the code has been indented and bought into the loop right so what is happening is now for all points in the training data I'm Computing the derivatives and immediately updating the weights right so I'm doing a greedy update so what I'm saying is that my true derivative was actually the average 1 by n into the summation over the derivative for all the points right so that's what my true derivative was now instead of calculating this sum I'm saying I'll just look at one point and I'm just making an approximation that the average is as good as one point right whatever estimates I make from one point are actually as good as the average that I'll get from a million points so that's of course a bad approximation but you do that and while it seems bad we'll see under what what you could do to not make it so look so bad right foreign so now if you have a million data points we'll make a million updates in each Epoch what is one Epoch Epoch is the outer loop right when you are making updates so far uh every Epoch you are going over the entire data and for every data point you're making an update so if there are million data points then in one Epoch you're making one million updates right so one Epoch is one pass over the entire data and one step is equal to one update right now what's the flip side now we have gone into bad territory right you know what you're Computing is an approximate gradient right your true gradient was the average of the derivative computed over all the points now we are approximated that average by using just one point estimate right and that's definitely bad so you have an approximate gradient so hence uh the guarantees that you had may be off right because this is called stochastic because we are estimating the total gradient based on a single data point so this is like you asked me to estimate the probability of suppose I give you a biased coin and you ask me to estimate the probability of heads then you'll just toss it once and you get heads and you say Okay probability of heads is one as opposed to like really tossing it's a hundred or thousand or even more times and then trying to estimate the probability right so that's what you're doing you're almost like tossing the coin once and estimating the probability of heads because you're just taking one data point and estimating the derivative from that point instead of computing the average over a large number of data points right and as you can imagine right as what we do in the case of coin toss also we don't really do like we don't really need to do like a million coin tosses that's bad you won't be able to do that similarly just doing one coin toss is also bad because you cannot estimate the probability of Heads Best by doing one coin toss but somewhere in between maybe 100 coin tosses would have been okay right and you could get a fair estimate of what the probability of heads is so a thousand may have been okay right similarly what you could do is that waiting for all the million data points to get over before you make update is bad making an update after one data point is also bad but some there in between maybe making an update after looking at 100 data points right so instead of computing the average as 1 by n summation I equal to 1 by n and then the loss value right instead of that maybe instead of using n equal to 1 million or all the training points I could use n equal to 100 and that might give me a fair approximation right so that's the idea that we'll head towards so while one approximating from one point really looks bad we need not do that that's only for illustrating uh the concept right so I'll do that and once you do that whether you approximate from one point ten point or 100 points as long as you're not using all the data points you are in trouble right because you are not really doing a true estimate of the gradient but you have to make this trade off between what is true which is all the multimedial points taken together versus what is approximate and let's see what happens when you do this approximate right so uh now what I'm going to do is I'm going to run stochastic gradient descent and gradient descent on this loss surface and we'll make some observations about it yeah so the green guy is gradient descent and the black guy is stochastic gradient descent and as you can see in the black guy there are a few oscillations whereas the green guy is going finer it's smoothly going down whereas the black guy is often getting off course and then coming back right and the reason that is happening is because you are not making a true estimate of the gradient you're just making like a approximation or a stochastic update here well and at this point right where it was say oscillating right at the points where you see an oscillation let me just use the mouse right so this is one point where you see an oscillation and what is happening there is that you approximated the gradient by one point and that point was actually very far off as compared to what the true gradient would have been and hence it led to a bad update and now again you come back slowly and then you go back towards the minimal right so that's why you see these oscillations now uh this is when you use k equal to 1 right but uh and we understand why we see many oscillations because we are making greedy decisions and what is happening is here that each point is trying to push the parameter so I have a million data points right and I look at one of the training uh instances one of the points and I compute the gradient and this point says Hey to decrease the loss with respect to uh my values you need to move here right then the second Point comes I again Ask it hey what should the gradient would be and it again pushes me in Direction which is favorable to it so each point is acting independently and trying to move you in a direction which is most convenient for itself right and that's why you see these all oscillations they're not working together as opposed to in the gradient descent where all of them I'm asking all of them at one go taking the average of their consensus and then moving uh in that direction right or the opposite to that the direction of the gradient but here that is not what I am doing and I'm asking every point this point say hey go here go here go here go here and so on that's why I'm oscillating uh that's why you see the oscillations right uh and as I mean as the case and a parameter which is a favorable for one point may not be a favorable for the next points that's why this point moved you here and then the next Point said no no no come back here then again go back here go back here and so on and that's why you keep oscillating like that right now can we reduce the oscillations by improving our stochastic estimates right so currently our estimate is taken from one point right we have taken n equal to one now instead of one if I take 10 if I take 20 then would these oscillations reduce right what this uh I would would I please take closer to the green curve right I know in the limit if I take n equal to all the points then I would be exactly following the green curve right because and there's no surprise there right but if I look at a mini batch as opposed to the full data and as opposed to just one point if I look at a mini Point mini batch which is say 25 points 100 points then what my estimates be better right so that's the question uh so let's look at that uh so now we are using a mini batch version of gradient descent so in stochastic gradient descent you are updating after every point right whereas here you are keeping track of the number of points that you have seen and if the number of points is equal to some mini batch size let's say the mini batch size is 100 or 200 so if the number of points that you have seen so far if it's equal to some mini batch size then you make an update right so after every uh batch of say 100 points 200 points only then you make update till that point you keep accumulating the uh gradients right so instead of accumulating the gradients for all the points you're accumulating the gradients for say 100 points and then making an update right so that's the only difference in the uh code and now these stochastic estimates are better because now instead of uh asking instead of flipping the coin once and then estimating the probability of heads maybe you are flipping it 25 times or 50 times and then trying to estimate the probability offense right so let's see what happens when we have k equal to 25 and we'll compare k equal to 25 versus k equal to 1 right so let's see that okay so we are going to run gradient descent stochastic gradient descent and mini match gradient descent where the batch size was 25 and what I would like to see is that the mini batch gradient descent is somewhere in between that green curve and the black curve right in terms of movement and you'll understand what I mean by that so let's start this so you can see that the blue curve is very close to the green curve whereas the black curve is oscillating quite a bit more right of course the blue curve uh which is for mini batch also has oscillations is just that it's much more smoother than the black guy because the black guy relies on one point and makes an estimate whereas the blue guy is relying on 25 different points and making an estimate okay yeah so the oscillations have reduced to a good extent uh because we now have slightly better estimates and as I said right it's like estimating the property heads from uh 25 coin tosses as opposed to a single coin toss right and the higher the value of K the more accurate the estimates are uh there are still oscillations right there's no denying that it's just that it becomes more and more smooth as you increase the batch size and I again repeat in the limit when you make the batch size equal to the total training points that will just follow gradient descent but that is not what is desired we'll typically want much smaller batch sizes as compared to the size of the data that we have and also I should mention at this point that uh many of the modern uh algorithms right which use batch updates they are often quite sensitive to this batch size you know when experimenting with Transformers for example we've often seen that a batch size of thousand versus a batch size of 4000 1024 these bat sizes are often in multiples of two or powers of two a batch size of 1024 may give you very different results from a batch size of four zero nine six right and it's common wisdom is that larger batch sizes are better right so that's uh while not of course going to the full training data okay so that's where we are and some things to remember so one Epoch is one pass over the entire data one step is One update to the parameters n is the total number of data points that you have B is the mini batch size right so now in vanilla uh gradient descent which is also called batch gradient descent so number of steps in one Epoch is just one right because after uh in one Epoch you just keep accumulating all the gradients and then update once in stochastic gradient descent the number of steps in one Epoch is equal to n right so because you are making an update for every data point and mini batch gradient is said the number of steps in one Epoch is n by B which is the total number of data points divided by your batch size so if you have thousand data points and hundred batch size then after every 100 points you'll be making an update so you'll be making a total of 10 updates right so that's what you should remember okay uh similarly we can have the stochastic versions of momentum based gradient descent and nest of accelerated gradient the same idea applies that you for stochastic momentum based gradient descent you'll just indent the update uh the lines of code corresponding to update because after every data point you will make the update right and similarly for nestro which I'll not show you the code for that I think it's straightforward so now let's look at uh on this slide I think this is written as gradient descent and SGD but this is the momentum based versions of both the stochastic momentum based gradient descent and the vanilla momentum based gradient descent okay so let's see how it goes so you can see that in addition to the oscillations that you have right and this was quite a complex uh surface right because it had some steep slopes remember from the Contour discussion that when you see the lines are very close to each other the slope is a bit Steep and we know that there is a problem in steep slopes so the green curve was going here and there because uh not is the true momentum based gradient descendant so you are taking the derivative or you're taking Computing the gradient from all the points but the reason you still see oscillations is because you have this very steep slopes and you are moving fast so you're over overshooting and then trying to come back whereas the black curve has these oscillations uh because of the stochastic nature of the estimates right so you're not making perfect estimates so you are going here and there and then coming back right and also see that they diverse right initially they both Go in different directions and the reason for that is that maybe the first few points that you selected in the stochastic version they were not a true representation of the total gradient right so they said okay I need to go in this direction and you went in that direction and then you later had to course correct and come back whereas when you're looking at the full uh gradient you're always moving in the right direction that you need to go to but of course you still see oscillations that is there because of the momentum related oscillations you're moving too fast and hence you have to oscillate in the green curve right okay okay so now let's look at uh nag also okay so stochastic version of nag so now I have all three I have nestorov right and you can see the expected Behavior it's all the three algorithms are the stochastic versions of the algorithms therefore even the vanilla gradient descent you still see oscillations there those oscillations are because of the stochastic nature of the updates in momentum as well as nestrov you see some oscillations as we are going again ahead again because of the stochastic version of the algorithms but the relative strengths remain the same right so you can see that the moment of curve which is green it takes a longer U-turn as compared to the Maestra one which is blue it quickly comes back right and that is the expected behavior of nestra versus momentum right so the relative qualities remain the same in addition you see this stochastic related uh oscillations because your estimates are not perfect right so that's the only difference that you have and I'll make the same commentary on the next like yeah so while the stochastic versions uh retain their relative advantages of Nag over momentum uh there is still this oscillation Behavior which is there and both of them converge much faster than the stochastic gradient descent right so after 500 steps you can see that the black guy is still somewhere on the plateau it has not even entered the valley whereas of both of these algorithms have conversed despite the stochastic nature of the algorithm and despite their own problems with oscillations they're still much faster than grain gradient descent which is again nothing new this is what is expected uh where we have discussed this during uh while discussing the vanilla versions of these algorithms as opposed to the stochastic or not the vanilla the full batch version of these algorithms as opposed to the stochastic versions okay and of course you could also have the mini batch versions of momentum and nag so what I showed on the previous slide was uh stochastic but you can also have mini batch the same thing after a batch you compute the uh derivatives and then make the update uh and again you will see the same relative advantage that the oscillations due to the stochastic nature would reduce because now your estimates are getting better they are not coming from a single point but from a collection of points so that ends our discussion on the stochastic and the mini batch version of these algorithms and the same ideas will apply to all the optimization algorithms that you see and in practice we use the mini batch versions of all these algorithms because you will have a large number of training points so you'll have to use a batch and make updates instead of waiting for all the data to be seen and similarly you'll not use the stochastic version which is just like one data point so you'll use the mini batch version and this mini batch size would be a bit dependent on the application that you're using and as I said in NLP when you're using Transformers some popular batch sizes are force uh around 4096 but it again there's nothing to memorize here or tell you this is a good value as you read papers and look at existing implementations of uh these algorithms you will understand what the right batch size is to use is all I'm saying is that in practice you'll use the mini batch version of these algorithms and there would be slight sensitivity to the bats size okay yeah so now again before going to uh some of the other algorithms which I want to cover I'll talk a bit about adjusting the learning rate and the moment right uh so uh so let's look at what the problem is