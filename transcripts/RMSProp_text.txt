foreign [Music] decays a learning rate very aggressively as the denominator Grows Right and we are allowing the denominator to grow inhibitedly because it's just accumulating the squares right so now uh frequent parameters will start receiving small updates so can we avoid this and prevent the rapid growth of the development right can we try to scale down the growth of the denominator and we know how to do that right and it's a simple uh thing that we could do is that instead of uh yeah instead of the original equation which was allowing the gradients to get added we can just take a fraction of the gradients right so now the typical value of beta would be say 0.9 so at any point you're just taking uh let me just get rid of this yeah so now you are taking 1 minus beta which is 0.1 times the current gradient and also the history you are repeatedly exponentially decaying right the history continuously gets multiplied by 0.9 so whatever was my accumulated history that has also become 0.9 times the previous history and my current gradient also I am not taking it fully I'm just taking it a fraction of it so that's why I'm not allowing my history to grow as rapidly as it was going in the case of adagra right so that's the change I have made in RMS prop uh and then the update rule Remains the Same right so now everything else Remains the Same the only thing that I've changed is this denominator VT which was earlier growing aggressively now I've added like these multipliers beta and one minus beta which are both less than one which means I'm scaling down right I'm not allowing the history to grow at every point I am multiplying it by a exponentially decaying Factor right so that is what is happening here and that will become clear as I expand this equation here right so this is what was happening for radagride at VT it was Delta B 0 square plus Delta B1 square plus B2 square and so on but in the case of RMS prop uh you will have this equation right where beta is between 0 to 1. so beta if it's 0.9 then your V 0 is 0.1 times B 0 square B2 is 0.09 times Delta B 0 square plus this and so on and when you come to uh you can already compare these two quantities right so where's the pen yeah huh yeah um yeah so if I compare the V2 here with the V2 here it's clear that this is going to be much smaller because my history is now exponentially decaying right here I was taking the whole of b0 square whereas here I am taking a small fraction again the whole of B1 square but a small fraction here again the whole of B2 square but even again a small fraction of P2 square right so now I'm controlling the rate at which V2 is going I'm not allowing it to shoot quickly I'm doing this exponentially decaying average foreign formula uh no I think we have not seen this earlier but you can see where the formula is coming from uh have you seen we have seen this formula earlier when we have been discussing momentum right this kind of a formula so if I just substitute here I have actually substituted the value of beta as 0.9 and 1 minus beta is 0.1 but instead if I just kept beta and 1 minus beta and solve for that I would have got the formula as they said that at timestamp VT my uh wheat at time step T my v t is going to be given by this formula and what you see at V2 is just a manifestation of this formula applied at time Step 2 with the value of beta set to 0.9 right and the main takeaway here right this formula you can go back and ponder about but the main takeaway here is that you're using an exponentially decaying average and hence now your denominator is going to grow less aggressively and hence now your effective learning rate right and remember this is what I refer to was the effective learning rate my effective learning rate is going to Decay slowly okay so now let's see what the effect of this is yeah again I'm not going to ponder too much about the code here but you can see that now instead of only one line has changed right so VW plus DW Square I am doing beta into VW plus 1 minus beta into DW square right so that's the main change that I've done and now let me just play this okay yeah so you can see that RMS prop is now the yellow curve is RMS prop which is quickly reads the minimum right because its learning rate is not decaying and it's also following this hypotenuse part as I wanted right it's making updates in the direction of w and B uh proportionately even though the updates in the direction of w are smaller right but notice something and we'll come back to what you notice here oops okay just play it in the minimized version here so it's reached there uh quickly but now it's oscillating a bit around the Minima right so you can see the sigmoid curve it had reached the solution right all the points were lying on the sigmoid curve but then the yellow curve started oscillating there now why does that happen is not clear but for this Slide the takeaway is that RMS prop converged more quickly than attack rad because it is being less aggressive in decaying the learning rate right that's the main takeaway why are there oscillations right so that is something that we need to understand right so one of the implications could be that maybe the learning rate is becoming constant and that is causing of oscillations right why the two questions to be answered here right why is the learning rate becoming constant and if the learning rate becomes constant why should there be oscillations right so these are the two questions that we need to okay so now recall that in adagrad uh this VT was uh sorry this should have been Square so this Delta WT Square should have been there and I when I had shown you the plots I had shown that VT never decreases right it keeps increasing and then it saturates but does not decrease even though the derivative has become zero VT keeps increasing and then it saturates right so it does not ever decrease to it does not start falling down right uh now what happens in the case of RMS prompt right so let's see what is happening in the case of RMS prop so you have this uh let me just annotate so this is your derivative so derivative increased increase in the negative Direction and then it started decreasing and then it became zero and then you are seeing this oscillations right so that let's not go into the oscillations but the main point here is that if you look at your VT it is increasing and then it starts decreasing also right so e v t can decrease in the case of RMS prop and why does that happen can you think about an answer for that what is it uh no no but it's an accumulation of the history right so the accumulation should keep growing right so how is the accumulation decreasing in the case of undergrad we said that VT is an accumulator so it's keeping adding terms so it should only increase and in whatever it's adding if it becomes zero then it will become steady at that so here why is it increasing and then it can decrease also why is that happening same for uh now we are taking VT Square we are taking Square so direction does not matter see the reason this is happening is because of the exponentially decaying average right so now uh initially if you look at the sum so say at time step T the sum was some uh so if you look at the derivative at time step 0 right so let's look at the formula if I have it yeah so here remember that this is like a moving window right so at time step V1 the weightage of beta 2 was 0.09 at time step V2 the weightage of beta 0 was 0.08 so at each time step we see this Behavior where the blue curve which is the accumulator curve which is the curve for VT it starts decreasing also because now the current sum is less than the previous sum right uh because of the exponential weighting right and now because of that what is happening is that in the case of ADA grad because your history was always accumulating your denominator was always growing so hence there was always a decay in the learning rate but now since your history can grow and shrink and it can also become a constant right because now what is happening is as your current gradients are all zero then there all the sums are becoming close to zero right and so then after a point the history is becoming more or less constant and that the history is becoming constraint from one time step to another ETA divided by that history is again going to remain constant right so as you go ahead your learning rate is not going to change right so that is why the learning rate becomes constant in the case of RMS could become constant in the case of RMS prop after a certain number of iterations right it can increase it can decrease or it can remain constant because of the moving average of the gradients in the denominator right and after across 500 iterations I have plotted the uh gradients here and this is what is uh happening and you have this oscillation around the Minima so if the learning rate is constant so now we have made a case that the learning rate could become constant now if the learning rate becomes constant why should there be oscillations right why could there be oscillation so let's try to see an explanation for that right suppose let me take a simple example this is what my curve is okay and now suppose I was here okay at this point okay suppose I was at this point uh yeah and I computed the derivative so this slope will give me the derivative and I made the update and after the update so let me just see I'm on the W axis I was here and I added the I mean I moved in the direction oppose it to the gradient and I ended up here right now again I compute the derivative here again I'll move in the direction opposite to the gradient with the same learning rate my learning rate has not changed so again I'll end up here at the same point because I'm moving from one point on the curve to its symmetrically opposite point on the curve this is possible I'm not saying this will always happen but this could happen right you're moving from here to here and again from here to here and you keep oscillating between those two points because from every movement you have the learning rate is constant so if the derivative is also similar you'll just keep moving from here to here right even if the derivative is slightly different you will keep making these movements from here to here right instead of directly going to the uh minimum all right so that's what can happen if your learning rate becomes constant and that's why you're seeing those oscillations in the case of RMS prop where the learning rate is indeed becoming constant uh after a while because your denominator VT is not changing right so it's accumulated history has also become like close to a very small number and your current derivatives are also small so now when you are adding all these terms from one time step to another nothing much is changing there right that's what is happening and that was not the case in Ada grad because you were just adding the gradients so the previous terms always remain right they are not getting exponentially weighted out right so what is the solution for this say this learning rate becoming constant and then resulting in oscillations so one solution is to set the learning rate initial learning rate appropriately right so this ETA that we have so remember our effective learning rate is ETA divided by that square root of VT so this ETA that we have in the numerator that's the initial learning rate and we could if we set that appropriately then we could solve this oscillation problem right so now for the same example I will show you the case where we had set the learning rate initial learning rate to be 0.05 and now if I uh run this algorithm RMS prop you can see that there are no oscillations here right so how does that happen so if you have this ETA set to a right value then in the example that I was giving earlier what could happen is that I was showing that this okay maybe let's do it again so I was saying that it was perhaps oscillating from this point to this point and then back and forth between these two points right and if the ETA was such that uh even though the the denominator has become more or less constant if the effective learning rate because of the initial learning rated so it depends on the initial learning rate if that was such that instead of just moving from the two symmetric points right two points on either side of the curve if it was moving like this it came here and then from it here and then from here then it would still converge right so if the learning rate is appropriate says that it's not uh kind of oscillating from one side to the other but it's gradually becoming uh towards the Minima right so if the learning rate is set appropriately then this could happen now of course this is hard right I mean you don't really know in this case since it's a toy example we experimented with a few learning rates and we found out that for a particular learning rate it kind of uh stabilizes the point that I'm trying to make here is not that hey you should use RMS prop and then try to set the learning rate appropriately this may work in some examples but in some examples you may not be able to get the right learning rate right the point that I'm trying to make is in fact the opposite that RMS prop is indeed sensitive to the initial learning rate and if you don't set it properly then you could see oscillations despite the other properties of the algorithm which allows for a more smoother DK of the learning rate despite that good property it might still oscillate if the learning rate is not proper right so the main takeaway is that it is sensitive to the learning rate and setting this learning rate to 0.05 and showing you that it converges is not to give the message that hey you could do this by setting the learning rate appropriately it is just to show that different learning initial learning rates could have different effects right so that's the idea here and we'll have to move to a stage where we kind of get to algorithms which are not too sensitive to the initial learning rate okay which one ha so VT plus Epsilon so V T is initially 0 so initially it would be ETA divided by square root of 10 raised to minus 4 right okay now let's see again just continuing in that direction right so if we uh were to initialize ETA with different values right so I have suppose ETA equal to Eta not equal to 0.6 that's what I should say so our effective learning rate is ETA ETA naught divided by square root of VT plus Epsilon and of course this ETA naught remains constant throughout right that's what you have said and only the denominator is change right so now if I change if I use different values for ETA naught what happens I already showed you that if I choose if I had chosen some favorable value for uh ETA naught then my algorithm would have conversation so I'm just going to show you a few more examples along that direction okay so I'll consider two values ETA not equal to 0.6 and 0.1 and let's see what happens to the to uh example right so first let me run 0.6 and here you can see that there's a wide oscillation right so this is almost like a case where uh the following is happening right it's oscillating from a high loss region to another high loss region and is oscillating between these two points as opposed to a slightly better loss oscillation where it's perhaps oscillating between two low loss regions right so that's not happening here you can clearly see that it's oscillating between very high loss regions right so from one high loss point to another it is going and you can see that because this where it is stable I mean the point of oscillation this is what the sigmoid curve looks like which is nowhere close to fitting the points that I have right now let's look at the other example here the learning rate is slow so you can see that the curve is moving slowly right the algorithm is progressing slower to the Minima and now in this case also there are oscillations this is still not good in fact ETA not equal to 0.05 is what I had shown on the previous slide was good so even with ETA not equal to 0.1 the learning rate is still high right so the numerator is still high so this is now point one this is still high enough to cause the oscillations it's just that the oscillations are slightly better now they are between these two low loss regions right and when I change it to 0.05 as I had shown on the previous slide then it was better it was perhaps able to get here and then converge quickly from that point right so that's that's what the sensitivity to the initial learning rate is right so the point to note here is that everything else here is remaining the same right the Lost surface is the same the training points are the same the only thing that has changed is my initial learning rate and that is causing a difference between convergence and we saw three scenarios one where it converges one where it oscillates around a solution which is acceptable right so this is perhaps not as bad as this one right so it converges or it reaches to an acceptable solution or is like really far off from the solution right all three things could happen depending on how we have chosen the learning rate so this is in indeed not a favorable situation right so we may want to look at Alternatives and in most cases these initial learning rates for several problems have been kind of discovered through experiments I mean like for example for machine translation there are certain data sets training data sets which are used quite often right across different papers and so many experiments have been done with those data sets that people now have a good range of what is the right initial learning rate to use and in that case you might not set such CSS problems right but if you're dealing with an esoteric problem a new data set which only you are looking at for the first time or if not me enough people have looked at then you might not really know what the right ETA naught should be and then you could land up in any of these situations right so let's just talk about it a bit more so this is what the effective learning rate is this is what VT is and which value of ETA naught is a good one right so uh we don't know that and what we really want is that some ETA not might be better some eat or not may not be better right and it's also not often that one ETA naught is always better right so let's see what I mean by that right so in this again uh suppose ETA naught was set to 0.6 okay that's the animation that I'm going to show you now and I'll tell you what all I'm going to show you in this animation right so this is VT oops yeah so this is VT which is the accumulation right so remember your I don't know how many times I have written this formula now yeah so this is VT the blue curve is going to be VT which is not coming yet this green curve is your initial ETA naught in my example it has been set to 0.6 so that's why the green line does not change across iterations and this is going to be your effective learning rate or I have not taken the square root here but it's going to be proportional to the effective learning rate right so now uh so far so good [Music] let's see what is happening as the iterations go along we are seeing some behavior of the plots right in uh before I comment on that I just want you to look at what is happening here right so you started off with some Eternal which was 0.6 initially your gradients your history was accumulating right this VT was accumulating and you ended up with some effective learning rate right so remember this orange curve is the effective learning rate as what is happening in these regions these are the regions where your uh slope was high so your VT was accumulating right and even though this is uh exponentially weighted average you have beta into VT minus 1 plus 1 minus beta into VT as long as you are in regions uh where not really sorry current gradient as long as you are in regions where the current gradient is high this will keep accumulating right so as you are going down steep regions your history was also high and your current thing is also high so your VT will keep increasing right and now when you enter regions where you are in a flat region then over a period of time this VT will become flat right because now your history is contributing history's importance is gradually decreasing because of the beta factor and your new stuff is very small to make a difference till it reaches a point that you start seeing a Tipping Point right now your his this new factor is very small and your history is also diminished significantly because the initial higher gradients are now multiplied by a very large power of beta right so that initial history has diminished and your current gradients are very small when you are close to the Minima or to the flat surface so then as we had said right the is history can then start decreasing right so this is the region where you had ah High gradients right this is the region where your gradients started decreasing right I mean this is till this point they were increasing and from here onwards they started decreasing and if you now look at this and compare it to the behavior of the learning rate in this uh let me just uh change the color yeah so in this region where your gradients were increasing the learning rate is steadily decreasing right and this is exactly what you want when the gradients are high you want the learning rate to decrease and now in this region once your gradient started becoming small the learning rate started increasing because your denominator which was accumulating the history over a period of time it starts decreasing because now all for the past many time steps your history has been smaller gradients so all the high weightage things of R of small values and the initial High values of gradients are now have diminished because they are being multiplied by a very high power of beta right so as the VT starts decreasing the learning state starts increasing right so just to repeat till this point your gradients were high hence your history was increasing after this point the current gradients were very small and hence your history started decreasing because now all all you're in a steep region so all the things that are getting added from this point onwards they're all small all so your effective sum is decreasing and in these two halves you see the right behavior of the learning rate initially it decreases and then it increases when the curve becomes flat okay so this is what is expected right this is what we like about RMS prop but what we were talking about its sensitivity to uh learning rate right so the initial learning rate so this is what happened when I chose the initial learning rate to be 0.6 right now uh so this satisfies our wish list whatever I just spoke is written on this slide now if we had set it to 0.1 right then let's see what would have happened maybe I should maximize this right so again let me just Define the things here you would have already understood but uh yeah so this is my history the blue curve is the history that behavior does not change the orange curve is for the ETA node of 0.6 which we just discussed in detail and the black curve is for the ETA naught of point Y right and now what is happening is that because my initial learning rate is small this behavior of how quickly it changes with steep slopes and gentle slopes is also changing right now you can see that initially this was decreasing rapidly right but now this is not decreasing rapidly it's decreasing slowly but it's still always lower than the lowest point that I had reached here right so what does that mean that it is still going to be lower than the lowest point that I have reached and then again in the Steep regions when it starts increasing it does not increase that rapidly right because it's still the numerator is still small so it nowhere grows as much as I wanted here right hence in this case you are seeing very wild oscillations in this case you are seeing mylar oscillations and if I also draw the curve for 0.05 then that you would see would have even smaller oscillations such that it allows it to then settle it into the Minima right so this what these curves are trying to show is that how fast or slow the learning rate adopts actually depends to an extent on the initial learning rate right so that's what is being shown here so again multiple slides conveying the same point that RMS prop is indeed sensitive to the initial learning rate right and we might want to get to a situation or to an algorithm where this dependency can be avoided now you might argue right that I might find something which is good right so I might find ETA not equal to 0.1 and that is perhaps always good or ETA not equal to 0.6 which is always good right now one more thing which I'm going to make an argument for now is that that is also not guaranteed so suppose you are in a steep region right so you are in a steep region so your history has been accumulating and suppose your current value of VT is 1.25 then what would your effective learning rate be it would be 0.48 right if the learning rate was set to 0.6 and if the learning rate was set to 0.1 then it would have been 0.08 right so same situation I'm just trying to make a case that depending on the initial learning rate your effective learning rate a Time step would be different right and since I'm in a steep region this is my preferred effective learning rate I want a smaller effective learning rate because I'm in a steep region and that I would have obtained if my initial learning rate was 0.1 right but now suppose I am in a uh not in a steep region okay I am in a flat region and say now because I'm in a flat region my history is accumulating slowly so my VT is 0.1 and now if I compute the effective learning rate with 0.6 then it's high but with 0.1 it's going to be low right and now in this case I might prefer uh this as the learning rate because I'm in a flat region so I want the learning rate to be high which means I am preferring a different initial learning rate right so across along the same loss surface your preference for which would have been a good value of ETA naught that decision might change right so this is yet again uh kind of making a case that you don't want the sensitivity towards eat or not right because you really want it to adapt and not get stuck with an initial choice that you have made because at the start of the algorithm you make this choice that Eternal is going to be 0.1 or 0.6 or 0.05 and then you are stuck with that choice right instead can I get rid of this ETA naught and have like a more adaptive setup right so all of this is trying to make a case for that okay so that's what is being said on the slide also and therefore we wish the numerator also to change with respect to the gradient or the current slope right so if I'm in a steeper region I want the numerator and denominator to adjust in uh kind of concurrence so that I get a good effective learning rate and if I'm in a flat region again I want to want them to adjust in kind of correlation so that I have have a good effective learning rate right right now the only the denominator is adjusting the numerator is not adjusting so let's quickly summarize the drawbacks of adagrad and RMS prop so both are sensitive to the initial learning rate uh then if the initial learning rates are great if the initial gradients are large right if the initial gradients are large my VT would be large if my V 0 was large right so my VT would be uh so V 0 itself would be Delta V 0 square right so if and then after that it will keep adding so this is at a grad where there is no exponential decay so if my initial gradients are large my VT takes on a large value very early on my v0 V1 itself would be large and then my effective learning rate right from the beginning would become very small and then there's no way for me to recover from there because my VT has become very large and in other grad it never decreases again right it keeps growing because I don't take an exponential decay right so that's clearly a disadvantage of ADA grad and that we were able to kind of alleviate using RMS prop then if a gentle curvature is encountered in Ada grad but there is no way to increase the learning rate because your history has again accumulated right and now I want uh my history has been accumulated and now I really want this to be small VT to be small only then my effective learning rate will increase but that cannot happen because once VT starts getting big it never comes back down right so again the same problem in order grad that as you once you start accumulating a lot of history then your learning rate is bound to get killed right and that's what we saw in that initial diagram that when I had come close to my Minima my movement had become very slow in the B direction right because my effective learning rate was I cannot increase it now because I'm not taking an exponential average right so this is also kind of resolved in RMS Prof but this sensitivity to the initial learning rate still exists and we would like to get rid of that okay