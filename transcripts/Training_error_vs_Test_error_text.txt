foreign [Music] so now we are going to talk about train error versus test error so let's see ah so now consider a new point x comma Y which has not been seen during training and so what I mean by that that this is where I was kind of ending in the last video so you are given some training data and based on that you have estimated F hat X right and now consider some new points right and one such point x comma Y which was not there in the training data right and now ah you want to see what is the error that this model makes on this unseen Point why unseen because it was not seen during training right so that's what the setup is so if you use the model f at X to predict the value of y then the mean square error is given by this quantity right so this is the expected error and why do we have an expected expectation means like roughly average or mean so what is it that what is it that we are taking the average on so again the situation was I had certain training data and then I had certain test data and this test data had many X comma y pairs so the average error over all these X comma y Pairs and that's what this expectation stands for I want to take this mean error over all the X comma y Pairs and potentially there could be like a large large number of test points right so if I have built an image classifier this is an infinite set of I mean practically infinite a large number of images that I could feed it at test time at which it has not seen at training time right so every day I don't know how many images get uploaded on Instagram Facebook Etc so if you have trained a model using all the images that were there till today in another one week some million other images will get generated and now these are all of these images are not seen by the train by the model during training and now all of this if you feed it to this model you are interested in knowing what your expected error would be right which means the average error across all these unseen images that you have right and average you can represent as expectation right so this is the quantity that you are interested in and if you can show that this quantity is actually equal to bias square plus variance plus Sigma square and it's not I mean uh kind of I mean hard to see how you'll go from here to there because this is f of x right and we had seen that in the formula of bias you had e of f hat of x minus f of x and similarly in the formula of variance you had e of f hat of x minus E of f of x ah sorry the whole Square the expectation of that right so these terms are there in the bias formula these terms are there here also and there is a square here so if you open up the square by applying the formula for a minus B the whole square and then do some rearrangement of terms you will end up with this quantity right because this bias square is again going to be some square of this the variance again has the terms that you have on the left hand side so if you rearrange all of this you will get this and there is of course a formal derivation of this available online um which we have linked here so if you want to see the full derivation you can see this but what you can show is that the expected error on unseen data is actually dependent on the bias and the variance also so if you have a model which has a high bias like the simple model then your expected error on the test set is going to be high if you have a model which has a high variance like this High degree polynomial that you had then again your error on the test set is going to be high so what you need for this quantity to be small you need the bias also to be small and the variance also to be small and the intuition that we just developed is that these are contradictory right you cannot have both you will either have a high bias and low variance or you will have a low bias and high variance so somewhere in Middle you have to find the sweet spot where you have medium bias and medium variance that would give you the minimum error that you are seeking on the test data right so that's the connection between bias variance and the expected test error or the mean square error that you would have on the test data right or the Unseen data yeah so this is the situation that we have right the parameters of f hat X all the W's that we had they are estimated using the training data because that's all you have right but now why are we training the model because at test time you will get new images and you want to classify those images you want to say whether this image contains a bird or a parrot or does it contain a lion and so on right and that is what you want to do at test time you are going to work with unseen uh images right unseen meaning images which are not seen at training time right so now this gives us to the following two quantities right one is the training error right which is the error that you get on the training data so what does that mean uh I think we'll Define that quantity formally soon and the other is the test error which is the error that you get on the test data right and you would ideally want both to be zero you want the training error to also be zero that means once I have trained the model at least for the training data it should be able to predict everything perfectly right give me close to zero and then of course on the test data also I want it to be 0 right so typically what happens is these errors exhibit the following behavior that if you have a high model complexity right and in our case we saw this degree 25 polynomial if you had increased it further it would have almost perfectly fit the training data that means my training error would have gone 0 as my model complexity increased right but what would happen is the test error would behave like this right if my model is not complex like the like the linear model that I had had which is the one which is the one extreme then my test error is going to be high as the red error red curve is I there right but if my model is very complex then also my test error is going to be high for reasons that I mentioned earlier because now I have trained the model to completely overfit on one training data that I had seen and now if I have another point which are not similar to this or which are not like which were not seen during training then I don't know whether it will be able to perform well because its entire universe was restricted to these points and it did like everything that it could to fit those points properly and in that effort it might have now missed uh considering the other points which are unseen right and for those points it will make a high error right so this is typically the behavior that the training error will keep Inc decreasing as the model complexity increasing the test error will decrease up to a certain point but if you make the model extremely complex it is going to overfit on the training data and make severe errors on the test data right so what you are looking for is The Sweet Spot somewhere in the center right and on the left hand side you have high bias models like the simple models which give you very high error both on the training data as well as test data right because the simple line model even on the training data it was giving us a high error it was nowhere close to the two sinusoidal function and on the right hand side you have the high variance models which give a low error on the training data but still give you a high error on the test data right and you're looking for this Sweet Spot somewhere in the middle where you have a good trade-off and you have the ideal model complexity right so now let us formally ah Define the train error and the test error right so let there be n training points and M test points right so these are the N training points that were given to you then you can Define the training error as this right which is known to you so over all the end points I'm going to take the average mean square error right so average square error or the mean square error that's what I'm going to do and this is for the N training points and then for the m test points which is n plus 1 to n plus 1 M points I can similarly Define the test error the average test error right so this is already known to you intuitively I've just formally defined it yeah so as the model complexity increases the train error becomes overly optimistic right because you saw that the blue curve was showing you or the training error is zero and you would think okay perfectly I've come from I have found the perfect relationship between Y and X but that's only for this training data right and as you as soon as you test it on the test data you will find the error is high and so what you actually want is that the validation error should be low right so the true picture of whether your F hat is really good comes from the validation data or the test data and not from the training data right you want the test error to be small that means the error on unknown points to be small driving the error to zero on the known points is easy you can just keep increasing the model complexity and it will go to zero right but what you want is to keep track of that sweet spot and to keep track of that sweet spot you need the test error because the moment the test error increases you need to stop you see I don't want more complexity than this because my test error is increasing right now whatever intuition we have developed so far we will try to convert this into a mathematical formulation and then we will try to make a case for regularization okay so I'll end this video here