[
  {
    "text": "foreign",
    "start": 0.299,
    "duration": 3.0
  },
  {
    "text": "[Music]",
    "start": 6.6,
    "duration": 3.75
  },
  {
    "text": "so that brings us to the next body which",
    "start": 19.74,
    "duration": 4.5
  },
  {
    "text": "is on this some manual work on attention",
    "start": 22.02,
    "duration": 4.86
  },
  {
    "text": "is all you need or the Transformers",
    "start": 24.24,
    "duration": 4.68
  },
  {
    "text": "right so the Transformer architecture",
    "start": 26.88,
    "duration": 5.699
  },
  {
    "text": "which was introduced around 2017 maybe",
    "start": 28.92,
    "duration": 6.42
  },
  {
    "text": "yeah uh so that's that's what you're",
    "start": 32.579,
    "duration": 5.281
  },
  {
    "text": "going to focus on while keeping in mind",
    "start": 35.34,
    "duration": 4.559
  },
  {
    "text": "the limitations that we saw about in the",
    "start": 37.86,
    "duration": 3.96
  },
  {
    "text": "case of a recurrent neural networks",
    "start": 39.899,
    "duration": 4.32
  },
  {
    "text": "right so this is how we'll transition to",
    "start": 41.82,
    "duration": 3.96
  },
  {
    "text": "Transformers right so we have the basic",
    "start": 44.219,
    "duration": 3.781
  },
  {
    "text": "encoder decoder RNN based model where at",
    "start": 45.78,
    "duration": 3.84
  },
  {
    "text": "first you have recurrent connections and",
    "start": 48.0,
    "duration": 3.84
  },
  {
    "text": "that's causes a problem then you also",
    "start": 49.62,
    "duration": 4.439
  },
  {
    "text": "have the attention-based encoder decoder",
    "start": 51.84,
    "duration": 4.26
  },
  {
    "text": "model where we said we'll do the uh",
    "start": 54.059,
    "duration": 4.02
  },
  {
    "text": "encoder computations then get rid of",
    "start": 56.1,
    "duration": 4.56
  },
  {
    "text": "them and then we have the uh attention",
    "start": 58.079,
    "duration": 4.561
  },
  {
    "text": "we just take the outputs of the encoder",
    "start": 60.66,
    "duration": 3.539
  },
  {
    "text": "right",
    "start": 62.64,
    "duration": 2.339
  },
  {
    "text": "um",
    "start": 64.199,
    "duration": 2.761
  },
  {
    "text": "we just take the outputs of the encoder",
    "start": 64.979,
    "duration": 4.441
  },
  {
    "text": "and then we have the attention function",
    "start": 66.96,
    "duration": 4.62
  },
  {
    "text": "on top of that at every step we compute",
    "start": 69.42,
    "duration": 4.26
  },
  {
    "text": "a new contextual Vector right so that's",
    "start": 71.58,
    "duration": 4.98
  },
  {
    "text": "the attention-based model but here again",
    "start": 73.68,
    "duration": 7.079
  },
  {
    "text": "uh we were able to compute the attention",
    "start": 76.56,
    "duration": 6.599
  },
  {
    "text": "weights for one time Step In Parallel",
    "start": 80.759,
    "duration": 5.521
  },
  {
    "text": "but across time steps T1 T2 and so on we",
    "start": 83.159,
    "duration": 4.381
  },
  {
    "text": "had to wait for the previous",
    "start": 86.28,
    "duration": 3.12
  },
  {
    "text": "computations to finish right that's",
    "start": 87.54,
    "duration": 4.14
  },
  {
    "text": "where we were so from here we'll",
    "start": 89.4,
    "duration": 4.579
  },
  {
    "text": "transition to the",
    "start": 91.68,
    "duration": 5.1
  },
  {
    "text": "Transformer Network which kind of again",
    "start": 93.979,
    "duration": 5.621
  },
  {
    "text": "has an encoder decoder architecture but",
    "start": 96.78,
    "duration": 4.74
  },
  {
    "text": "there are some other blocks that I am",
    "start": 99.6,
    "duration": 3.839
  },
  {
    "text": "naming here as is something known as a",
    "start": 101.52,
    "duration": 3.72
  },
  {
    "text": "self-attention block",
    "start": 103.439,
    "duration": 3.661
  },
  {
    "text": "then something known as a feed forward",
    "start": 105.24,
    "duration": 4.32
  },
  {
    "text": "Network then in the decoder you have",
    "start": 107.1,
    "duration": 4.5
  },
  {
    "text": "self-attention encoder decoder attention",
    "start": 109.56,
    "duration": 4.14
  },
  {
    "text": "again feed forward Network so it's all",
    "start": 111.6,
    "duration": 3.9
  },
  {
    "text": "of this we need to understand the words",
    "start": 113.7,
    "duration": 3.54
  },
  {
    "text": "look familiar you have attention your",
    "start": 115.5,
    "duration": 3.119
  },
  {
    "text": "feed forward Network so that doesn't",
    "start": 117.24,
    "duration": 5.1
  },
  {
    "text": "look too problematic but uh how these",
    "start": 118.619,
    "duration": 5.401
  },
  {
    "text": "different blocks interact what exactly",
    "start": 122.34,
    "duration": 3.3
  },
  {
    "text": "is there in each of these blocks is",
    "start": 124.02,
    "duration": 4.079
  },
  {
    "text": "something that we'll have to study and",
    "start": 125.64,
    "duration": 4.259
  },
  {
    "text": "that's what the focus of the next uh",
    "start": 128.099,
    "duration": 4.14
  },
  {
    "text": "half an hour to one hour would be right",
    "start": 129.899,
    "duration": 5.161
  },
  {
    "text": "so before we focus on the differences",
    "start": 132.239,
    "duration": 4.621
  },
  {
    "text": "right this clearly looks at least in the",
    "start": 135.06,
    "duration": 3.179
  },
  {
    "text": "diagram a bit different from the",
    "start": 136.86,
    "duration": 3.12
  },
  {
    "text": "recurrent neural network architecture or",
    "start": 138.239,
    "duration": 3.72
  },
  {
    "text": "The Recoil neural network with attention",
    "start": 139.98,
    "duration": 4.16
  },
  {
    "text": "architecture that we were used to",
    "start": 141.959,
    "duration": 5.341
  },
  {
    "text": "uh there's still some similarities which",
    "start": 144.14,
    "duration": 4.42
  },
  {
    "text": "come out right and I'll tell you the few",
    "start": 147.3,
    "duration": 3.659
  },
  {
    "text": "easy ones right the input is still given",
    "start": 148.56,
    "duration": 5.22
  },
  {
    "text": "to me fully right and the output again",
    "start": 150.959,
    "duration": 4.741
  },
  {
    "text": "will be produced one word at a time",
    "start": 153.78,
    "duration": 5.52
  },
  {
    "text": "right the other uh similarity is that in",
    "start": 155.7,
    "duration": 5.58
  },
  {
    "text": "RN and also I had said that at the input",
    "start": 159.3,
    "duration": 5.46
  },
  {
    "text": "you could just feed in your uh favorite",
    "start": 161.28,
    "duration": 6.239
  },
  {
    "text": "uh word Vector so for word embedding so",
    "start": 164.76,
    "duration": 3.78
  },
  {
    "text": "you have again your feeding word",
    "start": 167.519,
    "duration": 2.281
  },
  {
    "text": "embeddings",
    "start": 168.54,
    "duration": 3.6
  },
  {
    "text": "and this block right if I were to look",
    "start": 169.8,
    "duration": 4.2
  },
  {
    "text": "at it as an encoder as same as what I",
    "start": 172.14,
    "duration": 4.14
  },
  {
    "text": "had in the rnns so the N output of the",
    "start": 174.0,
    "duration": 4.44
  },
  {
    "text": "encoder was these representations right",
    "start": 176.28,
    "duration": 3.84
  },
  {
    "text": "and don't worry about the change in",
    "start": 178.44,
    "duration": 3.78
  },
  {
    "text": "notation because there are more uh",
    "start": 180.12,
    "duration": 3.66
  },
  {
    "text": "intermediate outputs here so I need to",
    "start": 182.22,
    "duration": 4.14
  },
  {
    "text": "use no more variables but in the case of",
    "start": 183.78,
    "duration": 6.9
  },
  {
    "text": "rnns you had X1 X2 up to X Phi as input",
    "start": 186.36,
    "duration": 8.459
  },
  {
    "text": "and the output was H1 h2h5 right so in a",
    "start": 190.68,
    "duration": 5.699
  },
  {
    "text": "sense what you are doing is that you are",
    "start": 194.819,
    "duration": 3.78
  },
  {
    "text": "taking the inward embeddings as input",
    "start": 196.379,
    "duration": 5.28
  },
  {
    "text": "right and then you were Computing a",
    "start": 198.599,
    "duration": 4.741
  },
  {
    "text": "contextual representation how are you",
    "start": 201.659,
    "duration": 2.881
  },
  {
    "text": "Computing a conflict contextual",
    "start": 203.34,
    "duration": 2.64
  },
  {
    "text": "representation using the bi-directional",
    "start": 204.54,
    "duration": 4.5
  },
  {
    "text": "lstm right so H2 dependent on H1 so",
    "start": 205.98,
    "duration": 4.92
  },
  {
    "text": "hence you had all seen all the",
    "start": 209.04,
    "duration": 4.559
  },
  {
    "text": "information up to or rather HD dependent",
    "start": 210.9,
    "duration": 4.86
  },
  {
    "text": "on HT minus one so when you are",
    "start": 213.599,
    "duration": 3.961
  },
  {
    "text": "Computing HT you had information of all",
    "start": 215.76,
    "duration": 3.6
  },
  {
    "text": "the T minus one Birds before it that's",
    "start": 217.56,
    "duration": 3.599
  },
  {
    "text": "where the contextual representation was",
    "start": 219.36,
    "duration": 4.439
  },
  {
    "text": "coming out but at each step what you are",
    "start": 221.159,
    "duration": 4.321
  },
  {
    "text": "doing is this right so you are Computing",
    "start": 223.799,
    "duration": 4.08
  },
  {
    "text": "a new representation for that word which",
    "start": 225.48,
    "duration": 4.74
  },
  {
    "text": "was a context of a representation here",
    "start": 227.879,
    "duration": 3.841
  },
  {
    "text": "looks like more computations are",
    "start": 230.22,
    "duration": 3.36
  },
  {
    "text": "happening but at the output you just",
    "start": 231.72,
    "duration": 4.799
  },
  {
    "text": "have these Z1 Z2 Z5 so again you could",
    "start": 233.58,
    "duration": 5.28
  },
  {
    "text": "think of it as you have the excise as",
    "start": 236.519,
    "duration": 4.8
  },
  {
    "text": "input and you are producing these zis as",
    "start": 238.86,
    "duration": 4.68
  },
  {
    "text": "an output and these z-is are again some",
    "start": 241.319,
    "duration": 3.84
  },
  {
    "text": "kind of a contextual representation of",
    "start": 243.54,
    "duration": 3.24
  },
  {
    "text": "the input that you have given that means",
    "start": 245.159,
    "duration": 4.261
  },
  {
    "text": "zis are not only aware of X I's they are",
    "start": 246.78,
    "duration": 4.679
  },
  {
    "text": "also aware of all the other words in the",
    "start": 249.42,
    "duration": 4.14
  },
  {
    "text": "uh sentence right so that's that's what",
    "start": 251.459,
    "duration": 3.84
  },
  {
    "text": "is happening here so the similarity is",
    "start": 253.56,
    "duration": 3.899
  },
  {
    "text": "that RNN takes X1 to x y and gives you",
    "start": 255.299,
    "duration": 5.041
  },
  {
    "text": "H1 to H5 Transformers also takes X1 to",
    "start": 257.459,
    "duration": 5.161
  },
  {
    "text": "X5 and gives you Z1 to Z5 the naming of",
    "start": 260.34,
    "duration": 4.139
  },
  {
    "text": "the variables may be different but just",
    "start": 262.62,
    "duration": 3.48
  },
  {
    "text": "both have the same semantics in the",
    "start": 264.479,
    "duration": 2.881
  },
  {
    "text": "sense that they are some kind of a",
    "start": 266.1,
    "duration": 3.18
  },
  {
    "text": "contextual representation of the input",
    "start": 267.36,
    "duration": 3.839
  },
  {
    "text": "how this contextual representation is",
    "start": 269.28,
    "duration": 3.96
  },
  {
    "text": "computed is different and the main",
    "start": 271.199,
    "duration": 3.601
  },
  {
    "text": "difference is going to be that we are",
    "start": 273.24,
    "duration": 3.239
  },
  {
    "text": "going to do something which does not",
    "start": 274.8,
    "duration": 3.48
  },
  {
    "text": "require the current connections or",
    "start": 276.479,
    "duration": 3.841
  },
  {
    "text": "something which can be computed in",
    "start": 278.28,
    "duration": 4.32
  },
  {
    "text": "parallel so that's the main change how",
    "start": 280.32,
    "duration": 3.599
  },
  {
    "text": "do we effect that change is something",
    "start": 282.6,
    "duration": 5.46
  },
  {
    "text": "that we will discuss as we go along okay",
    "start": 283.919,
    "duration": 6.981
  },
  {
    "text": "so let's start",
    "start": 288.06,
    "duration": 2.84
  },
  {
    "text": "yeah so let's look at each of these",
    "start": 291.419,
    "duration": 3.78
  },
  {
    "text": "layers in detail let's say my goal would",
    "start": 293.58,
    "duration": 4.02
  },
  {
    "text": "be to kind of go over each of these",
    "start": 295.199,
    "duration": 4.5
  },
  {
    "text": "yellow boxes that you see here word",
    "start": 297.6,
    "duration": 3.36
  },
  {
    "text": "embedding of course you know so there's",
    "start": 299.699,
    "duration": 2.521
  },
  {
    "text": "nothing to say much over there but",
    "start": 300.96,
    "duration": 3.12
  },
  {
    "text": "self-attention feed forward networks",
    "start": 302.22,
    "duration": 3.9
  },
  {
    "text": "then again self-attention the case of",
    "start": 304.08,
    "duration": 3.96
  },
  {
    "text": "decoder encoder decoder attention and",
    "start": 306.12,
    "duration": 3.24
  },
  {
    "text": "feed forward networks again in the case",
    "start": 308.04,
    "duration": 3.36
  },
  {
    "text": "of decoder so what do each of these",
    "start": 309.36,
    "duration": 4.02
  },
  {
    "text": "blocks have right and of course there",
    "start": 311.4,
    "duration": 3.66
  },
  {
    "text": "will be some overlap in the discussion",
    "start": 313.38,
    "duration": 3.72
  },
  {
    "text": "so if you understand this this should be",
    "start": 315.06,
    "duration": 3.66
  },
  {
    "text": "fairly straightforward if you understand",
    "start": 317.1,
    "duration": 3.9
  },
  {
    "text": "this then I'll not even say anything",
    "start": 318.72,
    "duration": 4.44
  },
  {
    "text": "about this right so mainly three",
    "start": 321.0,
    "duration": 3.36
  },
  {
    "text": "components that we need to understand",
    "start": 323.16,
    "duration": 3.18
  },
  {
    "text": "self-attention and good or decoder",
    "start": 324.36,
    "duration": 3.779
  },
  {
    "text": "attention and the feed forward neural",
    "start": 326.34,
    "duration": 3.78
  },
  {
    "text": "networks so we'll look at these in the",
    "start": 328.139,
    "duration": 5.701
  },
  {
    "text": "coming slides okay starting with",
    "start": 330.12,
    "duration": 6.62
  },
  {
    "text": "self attention",
    "start": 333.84,
    "duration": 2.9
  },
  {
    "text": "right so this is the self-attention",
    "start": 337.08,
    "duration": 3.119
  },
  {
    "text": "block so this is what is happening I",
    "start": 338.46,
    "duration": 5.299
  },
  {
    "text": "have the word I okay",
    "start": 340.199,
    "duration": 3.56
  },
  {
    "text": "and I have the word embedding for I here",
    "start": 343.86,
    "duration": 4.38
  },
  {
    "text": "I have just used some random",
    "start": 346.38,
    "duration": 3.72
  },
  {
    "text": "initialization but you can either do",
    "start": 348.24,
    "duration": 3.12
  },
  {
    "text": "random initialization that's the",
    "start": 350.1,
    "duration": 3.18
  },
  {
    "text": "practice you don't really rely on any",
    "start": 351.36,
    "duration": 3.48
  },
  {
    "text": "word embeddings you just do a random",
    "start": 353.28,
    "duration": 3.66
  },
  {
    "text": "initialization and then these are",
    "start": 354.84,
    "duration": 3.78
  },
  {
    "text": "passing through something known as self",
    "start": 356.94,
    "duration": 3.96
  },
  {
    "text": "attention and then producing a new",
    "start": 358.62,
    "duration": 4.74
  },
  {
    "text": "representation for every word right",
    "start": 360.9,
    "duration": 3.66
  },
  {
    "text": "that's what is happening here so very",
    "start": 363.36,
    "duration": 2.52
  },
  {
    "text": "similar to what is happening in the case",
    "start": 364.56,
    "duration": 4.38
  },
  {
    "text": "of RNN you have the excise",
    "start": 365.88,
    "duration": 5.099
  },
  {
    "text": "and then you get some Edge Ice right so",
    "start": 368.94,
    "duration": 3.84
  },
  {
    "text": "the same thing is happening here of",
    "start": 370.979,
    "duration": 3.961
  },
  {
    "text": "course the advantage the only reason I",
    "start": 372.78,
    "duration": 3.359
  },
  {
    "text": "would be interested in discussing this",
    "start": 374.94,
    "duration": 3.06
  },
  {
    "text": "is that if this overcomes the",
    "start": 376.139,
    "duration": 4.201
  },
  {
    "text": "disadvantage of rnns which is",
    "start": 378.0,
    "duration": 4.56
  },
  {
    "text": "sequential computation can I do this in",
    "start": 380.34,
    "duration": 5.1
  },
  {
    "text": "part right so that's the idea uh that's",
    "start": 382.56,
    "duration": 5.52
  },
  {
    "text": "the main uh idea here right so the self",
    "start": 385.44,
    "duration": 4.02
  },
  {
    "text": "attention mechanism once you understand",
    "start": 388.08,
    "duration": 3.6
  },
  {
    "text": "it fully we'll realize that it can be",
    "start": 389.46,
    "duration": 4.98
  },
  {
    "text": "computed in parallel and still give the",
    "start": 391.68,
    "duration": 4.92
  },
  {
    "text": "same flavor that means it still computes",
    "start": 394.44,
    "duration": 3.72
  },
  {
    "text": "a contextual representation that means",
    "start": 396.6,
    "duration": 3.42
  },
  {
    "text": "it's aware of all the other words in the",
    "start": 398.16,
    "duration": 4.5
  },
  {
    "text": "input right so that's the main",
    "start": 400.02,
    "duration": 5.959
  },
  {
    "text": "thing that we'll be discussing",
    "start": 402.66,
    "duration": 3.319
  },
  {
    "text": "so now this is called self attention",
    "start": 409.74,
    "duration": 5.34
  },
  {
    "text": "and what does the attention how do you",
    "start": 412.5,
    "duration": 4.08
  },
  {
    "text": "compute attention right so the attention",
    "start": 415.08,
    "duration": 3.72
  },
  {
    "text": "just requires two inputs so in the",
    "start": 416.58,
    "duration": 4.559
  },
  {
    "text": "attention that we have seen so far we",
    "start": 418.8,
    "duration": 6.239
  },
  {
    "text": "had St minus 1 comma h i right which was",
    "start": 421.139,
    "duration": 6.12
  },
  {
    "text": "used to compute the attention at time",
    "start": 425.039,
    "duration": 5.521
  },
  {
    "text": "step t for the input I right but here",
    "start": 427.259,
    "duration": 4.681
  },
  {
    "text": "there is no decoder we're just talking",
    "start": 430.56,
    "duration": 3.9
  },
  {
    "text": "about encoder and now the word self",
    "start": 431.94,
    "duration": 4.199
  },
  {
    "text": "should tell you what is going to happen",
    "start": 434.46,
    "duration": 3.48
  },
  {
    "text": "here right what I'm going to be",
    "start": 436.139,
    "duration": 4.201
  },
  {
    "text": "interested in is that I have all the",
    "start": 437.94,
    "duration": 4.92
  },
  {
    "text": "word representations now if I want to",
    "start": 440.34,
    "duration": 4.38
  },
  {
    "text": "compute a new representation for the",
    "start": 442.86,
    "duration": 3.72
  },
  {
    "text": "word movie right I want to Output a new",
    "start": 444.72,
    "duration": 3.72
  },
  {
    "text": "representation of the word movie then",
    "start": 446.58,
    "duration": 5.1
  },
  {
    "text": "can I compute this representation as a",
    "start": 448.44,
    "duration": 5.759
  },
  {
    "text": "attention weighted sum of all the other",
    "start": 451.68,
    "duration": 5.16
  },
  {
    "text": "representations in my sentence right and",
    "start": 454.199,
    "duration": 4.681
  },
  {
    "text": "if I were to do that all I need to do is",
    "start": 456.84,
    "duration": 5.04
  },
  {
    "text": "I need attention function right and",
    "start": 458.88,
    "duration": 5.159
  },
  {
    "text": "which takes in",
    "start": 461.88,
    "duration": 5.039
  },
  {
    "text": "my current representations h i and H J",
    "start": 464.039,
    "duration": 4.201
  },
  {
    "text": "right so these are what I'm going to",
    "start": 466.919,
    "duration": 3.301
  },
  {
    "text": "call edges this change of notation is",
    "start": 468.24,
    "duration": 3.6
  },
  {
    "text": "unavoidable right because I have more",
    "start": 470.22,
    "duration": 3.06
  },
  {
    "text": "outputs being produced here or more",
    "start": 471.84,
    "duration": 2.82
  },
  {
    "text": "intermediate outputs being produced here",
    "start": 473.28,
    "duration": 2.819
  },
  {
    "text": "so I'll have to use more variables",
    "start": 474.66,
    "duration": 4.02
  },
  {
    "text": "earlier H was the output of the encoder",
    "start": 476.099,
    "duration": 5.461
  },
  {
    "text": "itself now that I am calling as Z so H",
    "start": 478.68,
    "duration": 5.1
  },
  {
    "text": "is now the word embeddings that you have",
    "start": 481.56,
    "duration": 5.82
  },
  {
    "text": "computed it so this is H1 H2 H3 up to H5",
    "start": 483.78,
    "duration": 5.34
  },
  {
    "text": "now I want to compute a new",
    "start": 487.38,
    "duration": 3.96
  },
  {
    "text": "representation for movie and I'm going",
    "start": 489.12,
    "duration": 5.4
  },
  {
    "text": "to call it as S4 so I want to compute S4",
    "start": 491.34,
    "duration": 6.96
  },
  {
    "text": "as an attention weighted sum of all the",
    "start": 494.52,
    "duration": 6.06
  },
  {
    "text": "Phi inputs that I have that means first",
    "start": 498.3,
    "duration": 4.019
  },
  {
    "text": "I need to compute the attention weights",
    "start": 500.58,
    "duration": 3.78
  },
  {
    "text": "and what how am I going to compute the",
    "start": 502.319,
    "duration": 4.141
  },
  {
    "text": "attention weights everything is only on",
    "start": 504.36,
    "duration": 3.899
  },
  {
    "text": "the encoder side so I want to compute",
    "start": 506.46,
    "duration": 3.78
  },
  {
    "text": "the attention weight between the words I",
    "start": 508.259,
    "duration": 4.561
  },
  {
    "text": "and J and that would be a function of",
    "start": 510.24,
    "duration": 6.0
  },
  {
    "text": "just h i and H J now what function do I",
    "start": 512.82,
    "duration": 5.04
  },
  {
    "text": "choose how do I use that all that we'll",
    "start": 516.24,
    "duration": 3.719
  },
  {
    "text": "see as we go along but this is the basic",
    "start": 517.86,
    "duration": 3.84
  },
  {
    "text": "idea right so once I have these Alpha",
    "start": 519.959,
    "duration": 5.161
  },
  {
    "text": "ijs I'm just going to compute S4 as the",
    "start": 521.7,
    "duration": 6.78
  },
  {
    "text": "attention weighted sum so Alpha 4 J J",
    "start": 525.12,
    "duration": 7.32
  },
  {
    "text": "equal to 1 to capital T into h j right",
    "start": 528.48,
    "duration": 6.78
  },
  {
    "text": "so now I am just taking a weighted",
    "start": 532.44,
    "duration": 5.579
  },
  {
    "text": "aggregate of all these representations",
    "start": 535.26,
    "duration": 5.639
  },
  {
    "text": "right and these are the alphas for the",
    "start": 538.019,
    "duration": 4.981
  },
  {
    "text": "time step 4 and using that I'm going to",
    "start": 540.899,
    "duration": 4.921
  },
  {
    "text": "compute S4 similarly now I should better",
    "start": 543.0,
    "duration": 5.1
  },
  {
    "text": "clear some things here similarly if I",
    "start": 545.82,
    "duration": 4.68
  },
  {
    "text": "want to compute S2 I am again going to",
    "start": 548.1,
    "duration": 4.739
  },
  {
    "text": "take an attention weighted sum and these",
    "start": 550.5,
    "duration": 4.08
  },
  {
    "text": "are going to be Alpha 2 so there will be",
    "start": 552.839,
    "duration": 4.321
  },
  {
    "text": "Alpha 2 1 Alpha 2 2 all the way up to",
    "start": 554.58,
    "duration": 4.68
  },
  {
    "text": "Alpha 2 Phi in this case and general",
    "start": 557.16,
    "duration": 5.22
  },
  {
    "text": "Alpha 2T and once I have the alphas I'm",
    "start": 559.26,
    "duration": 5.579
  },
  {
    "text": "going to compute S2 as the attention",
    "start": 562.38,
    "duration": 6.0
  },
  {
    "text": "weighted sum so Alpha 2 J",
    "start": 564.839,
    "duration": 8.461
  },
  {
    "text": "h j j equal to 1 to capital T or Phi in",
    "start": 568.38,
    "duration": 7.079
  },
  {
    "text": "this case right and these are just to",
    "start": 573.3,
    "duration": 3.9
  },
  {
    "text": "remind you my word embeddings are what",
    "start": 575.459,
    "duration": 4.44
  },
  {
    "text": "I'm calling as the edges right so it's",
    "start": 577.2,
    "duration": 4.02
  },
  {
    "text": "just going to take a weight at some of",
    "start": 579.899,
    "duration": 3.06
  },
  {
    "text": "the edges",
    "start": 581.22,
    "duration": 3.78
  },
  {
    "text": "now you already see the advantage I",
    "start": 582.959,
    "duration": 4.021
  },
  {
    "text": "already have the edges with me right I",
    "start": 585.0,
    "duration": 4.26
  },
  {
    "text": "just got the entire sequence of words so",
    "start": 586.98,
    "duration": 4.5
  },
  {
    "text": "I just looked up the word embeddings so",
    "start": 589.26,
    "duration": 4.5
  },
  {
    "text": "I already have all the edges with me",
    "start": 591.48,
    "duration": 4.56
  },
  {
    "text": "now there is no recurrence here right so",
    "start": 593.76,
    "duration": 4.98
  },
  {
    "text": "I can compute the alpha twos at the same",
    "start": 596.04,
    "duration": 4.979
  },
  {
    "text": "time as alpha 1 at the same time as",
    "start": 598.74,
    "duration": 5.099
  },
  {
    "text": "Alpha 3 because now I am not depending",
    "start": 601.019,
    "duration": 5.521
  },
  {
    "text": "on one step to another right I don't",
    "start": 603.839,
    "duration": 4.68
  },
  {
    "text": "need for computing Alpha once I don't",
    "start": 606.54,
    "duration": 4.2
  },
  {
    "text": "need to know rather for computing Alpha",
    "start": 608.519,
    "duration": 3.721
  },
  {
    "text": "threes I don't need to know what",
    "start": 610.74,
    "duration": 2.94
  },
  {
    "text": "happened at time step two right because",
    "start": 612.24,
    "duration": 3.0
  },
  {
    "text": "I am just looking at the contextual",
    "start": 613.68,
    "duration": 4.8
  },
  {
    "text": "representation for this word by taking",
    "start": 615.24,
    "duration": 5.94
  },
  {
    "text": "uh a vote from all the other words in",
    "start": 618.48,
    "duration": 4.38
  },
  {
    "text": "the sentence and I'm only looking at H's",
    "start": 621.18,
    "duration": 4.68
  },
  {
    "text": "I'm not relying on S2 here right so this",
    "start": 622.86,
    "duration": 4.919
  },
  {
    "text": "attention equation unlike the earlier",
    "start": 625.86,
    "duration": 5.46
  },
  {
    "text": "equation which was St minus 1 and H I",
    "start": 627.779,
    "duration": 6.961
  },
  {
    "text": "now this is just h i comma h j right so",
    "start": 631.32,
    "duration": 5.22
  },
  {
    "text": "there is no dependence here I already",
    "start": 634.74,
    "duration": 3.779
  },
  {
    "text": "know all the edges",
    "start": 636.54,
    "duration": 4.2
  },
  {
    "text": "so I can just compute this all at one go",
    "start": 638.519,
    "duration": 3.721
  },
  {
    "text": "right all of this will become a bit more",
    "start": 640.74,
    "duration": 3.42
  },
  {
    "text": "clear I'm just giving you a trailer of",
    "start": 642.24,
    "duration": 3.719
  },
  {
    "text": "what is coming right so two takeaways",
    "start": 644.16,
    "duration": 5.1
  },
  {
    "text": "from this slide one is that",
    "start": 645.959,
    "duration": 5.341
  },
  {
    "text": "I have H's as the input and I'm going to",
    "start": 649.26,
    "duration": 3.48
  },
  {
    "text": "compute this intermediate representation",
    "start": 651.3,
    "duration": 3.599
  },
  {
    "text": "s and the way I'm going to compute that",
    "start": 652.74,
    "duration": 4.38
  },
  {
    "text": "is by taking a weighted sum of all the",
    "start": 654.899,
    "duration": 5.641
  },
  {
    "text": "edges so this makes sure that my s's are",
    "start": 657.12,
    "duration": 4.8
  },
  {
    "text": "contextual right because they are",
    "start": 660.54,
    "duration": 2.46
  },
  {
    "text": "looking at all the words in the",
    "start": 661.92,
    "duration": 3.419
  },
  {
    "text": "neighborhood and while doing this I'm",
    "start": 663.0,
    "duration": 5.04
  },
  {
    "text": "going to rely on the attention equation",
    "start": 665.339,
    "duration": 4.56
  },
  {
    "text": "this is called self-attention because",
    "start": 668.04,
    "duration": 3.96
  },
  {
    "text": "I'm looking at self this is not earlier",
    "start": 669.899,
    "duration": 3.421
  },
  {
    "text": "when I had attention that was between",
    "start": 672.0,
    "duration": 3.66
  },
  {
    "text": "decoder and encoder now I have attention",
    "start": 673.32,
    "duration": 3.9
  },
  {
    "text": "within the encoder right I'm looking at",
    "start": 675.66,
    "duration": 3.299
  },
  {
    "text": "all the words in the input itself this",
    "start": 677.22,
    "duration": 4.44
  },
  {
    "text": "is not as opposed to earlier where I had",
    "start": 678.959,
    "duration": 4.981
  },
  {
    "text": "the attention between the encoder and",
    "start": 681.66,
    "duration": 3.72
  },
  {
    "text": "the decoder so I was looking at the",
    "start": 683.94,
    "duration": 3.42
  },
  {
    "text": "decoder to State at time step T and then",
    "start": 685.38,
    "duration": 4.26
  },
  {
    "text": "Computing the attention at attention of",
    "start": 687.36,
    "duration": 3.419
  },
  {
    "text": "all the inputs that's not what is",
    "start": 689.64,
    "duration": 2.4
  },
  {
    "text": "happening here this is self-attention",
    "start": 690.779,
    "duration": 3.541
  },
  {
    "text": "this is attention within the input",
    "start": 692.04,
    "duration": 4.979
  },
  {
    "text": "itself right and this can already you",
    "start": 694.32,
    "duration": 3.959
  },
  {
    "text": "have a feeling that this can be",
    "start": 697.019,
    "duration": 3.181
  },
  {
    "text": "paralyzed we'll concretize that feeling",
    "start": 698.279,
    "duration": 4.56
  },
  {
    "text": "further as we go along okay",
    "start": 700.2,
    "duration": 5.52
  },
  {
    "text": "yeah so this is what uh now now what I",
    "start": 702.839,
    "duration": 5.041
  },
  {
    "text": "want to do is try to First motivate",
    "start": 705.72,
    "duration": 4.08
  },
  {
    "text": "right that what we are doing here and",
    "start": 707.88,
    "duration": 3.66
  },
  {
    "text": "why do we need to do this right why do",
    "start": 709.8,
    "duration": 4.44
  },
  {
    "text": "we need to compute this attention or why",
    "start": 711.54,
    "duration": 4.2
  },
  {
    "text": "do we need to rely on all the other",
    "start": 714.24,
    "duration": 2.88
  },
  {
    "text": "inputs because that is not what we were",
    "start": 715.74,
    "duration": 3.42
  },
  {
    "text": "doing earlier we are of course computer",
    "start": 717.12,
    "duration": 4.32
  },
  {
    "text": "contextual representation but earlier",
    "start": 719.16,
    "duration": 3.78
  },
  {
    "text": "the attention was only between encoder",
    "start": 721.44,
    "duration": 2.76
  },
  {
    "text": "and decoder so why do you need this",
    "start": 722.94,
    "duration": 2.82
  },
  {
    "text": "self-attention why does it make sense",
    "start": 724.2,
    "duration": 3.6
  },
  {
    "text": "right so now if you have the sentence",
    "start": 725.76,
    "duration": 3.84
  },
  {
    "text": "the elephant the animal didn't cross the",
    "start": 727.8,
    "duration": 4.26
  },
  {
    "text": "street because it was too tired here the",
    "start": 729.6,
    "duration": 4.859
  },
  {
    "text": "word it is referring to animal and not",
    "start": 732.06,
    "duration": 5.219
  },
  {
    "text": "street right so when I'm Computing a",
    "start": 734.459,
    "duration": 5.401
  },
  {
    "text": "representation for it it should pay more",
    "start": 737.279,
    "duration": 4.441
  },
  {
    "text": "attention to animal it as opposed to",
    "start": 739.86,
    "duration": 3.36
  },
  {
    "text": "stream that's why I need this",
    "start": 741.72,
    "duration": 3.179
  },
  {
    "text": "self-attention because I need to capture",
    "start": 743.22,
    "duration": 4.02
  },
  {
    "text": "the importance of the words in context",
    "start": 744.899,
    "duration": 4.68
  },
  {
    "text": "as with respect to the current word",
    "start": 747.24,
    "duration": 3.96
  },
  {
    "text": "right and this would change if the",
    "start": 749.579,
    "duration": 3.241
  },
  {
    "text": "sentence was different if the sentence",
    "start": 751.2,
    "duration": 3.24
  },
  {
    "text": "was the animal didn't cross the street",
    "start": 752.82,
    "duration": 4.259
  },
  {
    "text": "because it was congested so now the word",
    "start": 754.44,
    "duration": 5.459
  },
  {
    "text": "it here actually represents refers to",
    "start": 757.079,
    "duration": 4.801
  },
  {
    "text": "street so now when I'm Computing a",
    "start": 759.899,
    "duration": 3.901
  },
  {
    "text": "contextual representation for it it",
    "start": 761.88,
    "duration": 3.42
  },
  {
    "text": "should pay more attention to stream",
    "start": 763.8,
    "duration": 4.14
  },
  {
    "text": "right so the same word but paying",
    "start": 765.3,
    "duration": 4.68
  },
  {
    "text": "attention to different uh input",
    "start": 767.94,
    "duration": 4.32
  },
  {
    "text": "different other contextual words right",
    "start": 769.98,
    "duration": 4.02
  },
  {
    "text": "that's why I cannot use like a static",
    "start": 772.26,
    "duration": 3.54
  },
  {
    "text": "embedding of it which is just taking the",
    "start": 774.0,
    "duration": 3.839
  },
  {
    "text": "word embedding I need this contextual",
    "start": 775.8,
    "duration": 4.44
  },
  {
    "text": "embedding and that's why I need to learn",
    "start": 777.839,
    "duration": 4.261
  },
  {
    "text": "these Alphas here the alphas should",
    "start": 780.24,
    "duration": 5.099
  },
  {
    "text": "learn to focus more on street here the",
    "start": 782.1,
    "duration": 5.1
  },
  {
    "text": "alpha should learn to focus more on",
    "start": 785.339,
    "duration": 3.62
  },
  {
    "text": "anyway that's why I need this",
    "start": 787.2,
    "duration": 4.139
  },
  {
    "text": "self-attention so that's what is being",
    "start": 788.959,
    "duration": 5.56
  },
  {
    "text": "said here and this is what we call it as",
    "start": 791.339,
    "duration": 5.101
  },
  {
    "text": "self attention and we'll distinguish",
    "start": 794.519,
    "duration": 3.601
  },
  {
    "text": "this from concentration which I have",
    "start": 796.44,
    "duration": 3.0
  },
  {
    "text": "already said that cross attention is",
    "start": 798.12,
    "duration": 3.18
  },
  {
    "text": "between the encoder and the decoder but",
    "start": 799.44,
    "duration": 3.839
  },
  {
    "text": "here the attention is within the encoder",
    "start": 801.3,
    "duration": 4.2
  },
  {
    "text": "inputs itself right so our goal would be",
    "start": 803.279,
    "duration": 4.381
  },
  {
    "text": "that for a given word I want to be able",
    "start": 805.5,
    "duration": 4.38
  },
  {
    "text": "to compute the similarity score or the",
    "start": 807.66,
    "duration": 4.2
  },
  {
    "text": "attention score with all the other words",
    "start": 809.88,
    "duration": 3.66
  },
  {
    "text": "in the sentence right so this Matrix is",
    "start": 811.86,
    "duration": 3.659
  },
  {
    "text": "what I want to fill that I want to",
    "start": 813.54,
    "duration": 4.44
  },
  {
    "text": "compute a representation for street and",
    "start": 815.519,
    "duration": 4.56
  },
  {
    "text": "I'm going to compute it as a weighted",
    "start": 817.98,
    "duration": 5.34
  },
  {
    "text": "sum of all these representations",
    "start": 820.079,
    "duration": 5.521
  },
  {
    "text": "so in my weighted sum what should the",
    "start": 823.32,
    "duration": 4.86
  },
  {
    "text": "alphas be right that's what I want to",
    "start": 825.6,
    "duration": 5.52
  },
  {
    "text": "compute okay that is what my goal is so",
    "start": 828.18,
    "duration": 5.339
  },
  {
    "text": "this entire Matrix which is a t cross T",
    "start": 831.12,
    "duration": 4.14
  },
  {
    "text": "Matrix is what I want to compute I want",
    "start": 833.519,
    "duration": 3.721
  },
  {
    "text": "to compute all these Alphas if I have",
    "start": 835.26,
    "duration": 4.379
  },
  {
    "text": "all these Alphas then I compute the SS",
    "start": 837.24,
    "duration": 4.74
  },
  {
    "text": "by just taking a weighted Alpha weighted",
    "start": 839.639,
    "duration": 4.621
  },
  {
    "text": "sum of the edges right so that's what my",
    "start": 841.98,
    "duration": 5.22
  },
  {
    "text": "goal is okay and I want to be able to do",
    "start": 844.26,
    "duration": 4.8
  },
  {
    "text": "this in parallel which you already have",
    "start": 847.2,
    "duration": 3.78
  },
  {
    "text": "a feeling that you can do this in",
    "start": 849.06,
    "duration": 3.959
  },
  {
    "text": "parallel because in particular right",
    "start": 850.98,
    "duration": 3.539
  },
  {
    "text": "when could you do that in parallel so",
    "start": 853.019,
    "duration": 6.721
  },
  {
    "text": "let me call this Alpha Street right",
    "start": 854.519,
    "duration": 7.32
  },
  {
    "text": "or I could have used the timestamp so",
    "start": 859.74,
    "duration": 5.82
  },
  {
    "text": "one two three four five six right so",
    "start": 861.839,
    "duration": 7.041
  },
  {
    "text": "this is Alpha six",
    "start": 865.56,
    "duration": 3.32
  },
  {
    "text": "Alpha six one alpha six two three all",
    "start": 870.839,
    "duration": 4.74
  },
  {
    "text": "the way up to Alpha six capital T and",
    "start": 873.48,
    "duration": 4.62
  },
  {
    "text": "say this is Alpha two one alpha two two",
    "start": 875.579,
    "duration": 5.461
  },
  {
    "text": "all the way up to Alpha to T right now",
    "start": 878.1,
    "duration": 4.739
  },
  {
    "text": "when can I compute these in parallel",
    "start": 881.04,
    "duration": 4.2
  },
  {
    "text": "earlier remember we had spoken that uh",
    "start": 882.839,
    "duration": 4.201
  },
  {
    "text": "when we are looking at encoder decoder I",
    "start": 885.24,
    "duration": 3.659
  },
  {
    "text": "could not compute two rows of alpha in",
    "start": 887.04,
    "duration": 3.84
  },
  {
    "text": "parallel because this row of alpha",
    "start": 888.899,
    "duration": 3.421
  },
  {
    "text": "depended on everything that happened",
    "start": 890.88,
    "duration": 3.72
  },
  {
    "text": "before right but now I don't have that",
    "start": 892.32,
    "duration": 5.459
  },
  {
    "text": "dependency to compute this Alpha I just",
    "start": 894.6,
    "duration": 7.08
  },
  {
    "text": "need H2 and H1 which I already have to",
    "start": 897.779,
    "duration": 6.781
  },
  {
    "text": "compute this I need H6 and H3 which I",
    "start": 901.68,
    "duration": 4.38
  },
  {
    "text": "already have I don't need to see what is",
    "start": 904.56,
    "duration": 3.48
  },
  {
    "text": "happening here to be able to compute",
    "start": 906.06,
    "duration": 4.5
  },
  {
    "text": "this function right so you already have",
    "start": 908.04,
    "duration": 4.08
  },
  {
    "text": "a feeling that all these rows can be",
    "start": 910.56,
    "duration": 3.48
  },
  {
    "text": "computed in parallel as compared to the",
    "start": 912.12,
    "duration": 4.079
  },
  {
    "text": "previous case where each row dependent",
    "start": 914.04,
    "duration": 3.9
  },
  {
    "text": "on the previous row right because of",
    "start": 916.199,
    "duration": 4.921
  },
  {
    "text": "this St minus 1 that you have right so",
    "start": 917.94,
    "duration": 4.8
  },
  {
    "text": "this Matrix is what I want to compute",
    "start": 921.12,
    "duration": 3.3
  },
  {
    "text": "and I should be able to compute this",
    "start": 922.74,
    "duration": 4.2
  },
  {
    "text": "Matrix in parallel is what my wish list",
    "start": 924.42,
    "duration": 4.38
  },
  {
    "text": "would be so essentially what we want is",
    "start": 926.94,
    "duration": 3.48
  },
  {
    "text": "a table like this which has some numbers",
    "start": 928.8,
    "duration": 4.8
  },
  {
    "text": "and those numbers are meaningful in the",
    "start": 930.42,
    "duration": 4.68
  },
  {
    "text": "sense that here it and animal are",
    "start": 933.6,
    "duration": 4.44
  },
  {
    "text": "related so the contextual similarity",
    "start": 935.1,
    "duration": 5.099
  },
  {
    "text": "between them should be higher right in",
    "start": 938.04,
    "duration": 3.18
  },
  {
    "text": "particular when I am Computing the",
    "start": 940.199,
    "duration": 3.0
  },
  {
    "text": "contextual representation for it I",
    "start": 941.22,
    "duration": 3.84
  },
  {
    "text": "should have more weight on animal as",
    "start": 943.199,
    "duration": 3.301
  },
  {
    "text": "opposed to the other words in the",
    "start": 945.06,
    "duration": 3.779
  },
  {
    "text": "sentence right so that's what we want so",
    "start": 946.5,
    "duration": 5.339
  },
  {
    "text": "we want to learn these Alphas in that",
    "start": 948.839,
    "duration": 6.24
  },
  {
    "text": "manner okay so how do we do that is the",
    "start": 951.839,
    "duration": 5.701
  },
  {
    "text": "question what kind of a architecture do",
    "start": 955.079,
    "duration": 3.541
  },
  {
    "text": "we use",
    "start": 957.54,
    "duration": 3.12
  },
  {
    "text": "so to start that discussion and just to",
    "start": 958.62,
    "duration": 3.899
  },
  {
    "text": "be able to relate it to what we have",
    "start": 960.66,
    "duration": 3.359
  },
  {
    "text": "already seen right so in the earlier",
    "start": 962.519,
    "duration": 3.901
  },
  {
    "text": "case we were",
    "start": 964.019,
    "duration": 5.641
  },
  {
    "text": "talking about the attention function as",
    "start": 966.42,
    "duration": 5.7
  },
  {
    "text": "s t minus 1",
    "start": 969.66,
    "duration": 3.72
  },
  {
    "text": "and",
    "start": 972.12,
    "duration": 4.92
  },
  {
    "text": "hi right so just to kind of keep the",
    "start": 973.38,
    "duration": 6.12
  },
  {
    "text": "convention or the variables similar for",
    "start": 977.04,
    "duration": 4.56
  },
  {
    "text": "now for for some time right and we will",
    "start": 979.5,
    "duration": 4.079
  },
  {
    "text": "get rid of it soon we'll just think of",
    "start": 981.6,
    "duration": 5.76
  },
  {
    "text": "the rows as H I's sorry",
    "start": 983.579,
    "duration": 6.721
  },
  {
    "text": "the rows as s I's and the columns as at",
    "start": 987.36,
    "duration": 5.46
  },
  {
    "text": "J and now what you are interested in is",
    "start": 990.3,
    "duration": 4.68
  },
  {
    "text": "to compute an attention function between",
    "start": 992.82,
    "duration": 4.92
  },
  {
    "text": "the s i and the AJ but unlike earlier",
    "start": 994.98,
    "duration": 5.219
  },
  {
    "text": "now as I said the Si's and the edges are",
    "start": 997.74,
    "duration": 4.68
  },
  {
    "text": "available to you at one go so you can",
    "start": 1000.199,
    "duration": 4.801
  },
  {
    "text": "compute all of this at one go as opposed",
    "start": 1002.42,
    "duration": 4.859
  },
  {
    "text": "to earlier when your sis which are",
    "start": 1005.0,
    "duration": 3.959
  },
  {
    "text": "essentially the decoder states were",
    "start": 1007.279,
    "duration": 3.36
  },
  {
    "text": "getting available only one step at a",
    "start": 1008.959,
    "duration": 3.3
  },
  {
    "text": "time right so just to draw a clear",
    "start": 1010.639,
    "duration": 4.5
  },
  {
    "text": "variable level analogy between what we",
    "start": 1012.259,
    "duration": 4.2
  },
  {
    "text": "have seen so far and what we are going",
    "start": 1015.139,
    "duration": 4.26
  },
  {
    "text": "to see earlier uh I should say that this",
    "start": 1016.459,
    "duration": 4.62
  },
  {
    "text": "is actually just h i right because these",
    "start": 1019.399,
    "duration": 3.841
  },
  {
    "text": "are both the same representations but I",
    "start": 1021.079,
    "duration": 3.48
  },
  {
    "text": "just wanted to connect it to the earlier",
    "start": 1023.24,
    "duration": 2.699
  },
  {
    "text": "discussion where we had the S and the H",
    "start": 1024.559,
    "duration": 3.541
  },
  {
    "text": "and the difference is that now the s's",
    "start": 1025.939,
    "duration": 4.26
  },
  {
    "text": "are also available in at one go as",
    "start": 1028.1,
    "duration": 4.079
  },
  {
    "text": "opposed to the earlier case right so now",
    "start": 1030.199,
    "duration": 4.441
  },
  {
    "text": "what is the attention function that we",
    "start": 1032.179,
    "duration": 5.28
  },
  {
    "text": "should choose so earlier we had this",
    "start": 1034.64,
    "duration": 4.319
  },
  {
    "text": "attention function right so when we're",
    "start": 1037.459,
    "duration": 3.84
  },
  {
    "text": "discussing about rnns this was the",
    "start": 1038.959,
    "duration": 3.84
  },
  {
    "text": "attention function where you had St",
    "start": 1041.299,
    "duration": 3.961
  },
  {
    "text": "minus 1 here at J here both were",
    "start": 1042.799,
    "duration": 4.441
  },
  {
    "text": "undergoing some linear transformation",
    "start": 1045.26,
    "duration": 3.659
  },
  {
    "text": "then the resulting output was going",
    "start": 1047.24,
    "duration": 4.2
  },
  {
    "text": "under a non-linearity so you have this",
    "start": 1048.919,
    "duration": 4.561
  },
  {
    "text": "tannage and then you had a vector which",
    "start": 1051.44,
    "duration": 4.02
  },
  {
    "text": "was the same size as this vector and",
    "start": 1053.48,
    "duration": 3.3
  },
  {
    "text": "then you take the dot product between",
    "start": 1055.46,
    "duration": 3.36
  },
  {
    "text": "these two vectors so you get just get a",
    "start": 1056.78,
    "duration": 4.08
  },
  {
    "text": "scalar quantity in fact we are calling",
    "start": 1058.82,
    "duration": 4.62
  },
  {
    "text": "it A's and then the A's get normalized",
    "start": 1060.86,
    "duration": 4.5
  },
  {
    "text": "to give you the alphas right so that's",
    "start": 1063.44,
    "duration": 4.02
  },
  {
    "text": "the function that we were choosing",
    "start": 1065.36,
    "duration": 6.24
  },
  {
    "text": "earlier now for the this work right the",
    "start": 1067.46,
    "duration": 6.24
  },
  {
    "text": "Transformers paper introduce a new",
    "start": 1071.6,
    "duration": 3.48
  },
  {
    "text": "attention function and that is what",
    "start": 1073.7,
    "duration": 5.82
  },
  {
    "text": "we'll uh try to arrive at now right",
    "start": 1075.08,
    "duration": 6.24
  },
  {
    "text": "so if you notice this equation right",
    "start": 1079.52,
    "duration": 3.6
  },
  {
    "text": "there are three vectors involved here",
    "start": 1081.32,
    "duration": 3.92
  },
  {
    "text": "there is s",
    "start": 1083.12,
    "duration": 5.22
  },
  {
    "text": "there is H and then there is B it",
    "start": 1085.24,
    "duration": 4.48
  },
  {
    "text": "remember this is also a vector right",
    "start": 1088.34,
    "duration": 3.42
  },
  {
    "text": "because the output of this is a vector",
    "start": 1089.72,
    "duration": 3.78
  },
  {
    "text": "and that gets multiplied by our DOT",
    "start": 1091.76,
    "duration": 3.06
  },
  {
    "text": "product with another Vector let's have",
    "start": 1093.5,
    "duration": 2.7
  },
  {
    "text": "three vectors",
    "start": 1094.82,
    "duration": 5.239
  },
  {
    "text": "and then you have uh",
    "start": 1096.2,
    "duration": 3.859
  },
  {
    "text": "two linear Transformations happening so",
    "start": 1100.16,
    "duration": 3.12
  },
  {
    "text": "this is the first linear transformation",
    "start": 1101.84,
    "duration": 4.5
  },
  {
    "text": "which is at here and then here's the",
    "start": 1103.28,
    "duration": 4.5
  },
  {
    "text": "second linear transformation that is",
    "start": 1106.34,
    "duration": 4.32
  },
  {
    "text": "happening right uh",
    "start": 1107.78,
    "duration": 4.86
  },
  {
    "text": "and then you have this one non-linearity",
    "start": 1110.66,
    "duration": 5.16
  },
  {
    "text": "in the form of the tannage right uh and",
    "start": 1112.64,
    "duration": 6.0
  },
  {
    "text": "then what you have is that that internal",
    "start": 1115.82,
    "duration": 5.16
  },
  {
    "text": "non-linearity multiplied by those two",
    "start": 1118.64,
    "duration": 4.62
  },
  {
    "text": "linear Transformations their entire",
    "start": 1120.98,
    "duration": 3.84
  },
  {
    "text": "equation gives you one vector and then",
    "start": 1123.26,
    "duration": 3.24
  },
  {
    "text": "you're taking a DOT product between that",
    "start": 1124.82,
    "duration": 4.32
  },
  {
    "text": "vector and your V right so just to",
    "start": 1126.5,
    "duration": 4.5
  },
  {
    "text": "summarize there are three vectors",
    "start": 1129.14,
    "duration": 3.6
  },
  {
    "text": "two linear Transformations one",
    "start": 1131.0,
    "duration": 3.419
  },
  {
    "text": "nonlinearity and then finally a DOT",
    "start": 1132.74,
    "duration": 4.02
  },
  {
    "text": "product right so the final answer that",
    "start": 1134.419,
    "duration": 3.301
  },
  {
    "text": "you get",
    "start": 1136.76,
    "duration": 4.38
  },
  {
    "text": "right if I were to simplify this",
    "start": 1137.72,
    "duration": 6.12
  },
  {
    "text": "what you get at the end is just the dot",
    "start": 1141.14,
    "duration": 4.919
  },
  {
    "text": "product between two vectors this Vector",
    "start": 1143.84,
    "duration": 4.44
  },
  {
    "text": "in turn was computed using two other",
    "start": 1146.059,
    "duration": 4.921
  },
  {
    "text": "vectors which in turn had gone through a",
    "start": 1148.28,
    "duration": 3.96
  },
  {
    "text": "linear transformation right so that is",
    "start": 1150.98,
    "duration": 2.88
  },
  {
    "text": "what is happening here that is what the",
    "start": 1152.24,
    "duration": 4.319
  },
  {
    "text": "equation is and now I could do this in",
    "start": 1153.86,
    "duration": 4.199
  },
  {
    "text": "other ways and that's where the",
    "start": 1156.559,
    "duration": 4.261
  },
  {
    "text": "Transformer set of equations for uh",
    "start": 1158.059,
    "duration": 4.5
  },
  {
    "text": "attention will drop out from right but",
    "start": 1160.82,
    "duration": 3.06
  },
  {
    "text": "this is what is happening here the final",
    "start": 1162.559,
    "duration": 4.021
  },
  {
    "text": "computation is a DOT product and within",
    "start": 1163.88,
    "duration": 4.74
  },
  {
    "text": "that green Vector you had these two",
    "start": 1166.58,
    "duration": 3.479
  },
  {
    "text": "linear Transformations which were",
    "start": 1168.62,
    "duration": 3.12
  },
  {
    "text": "happening followed by a non-linearity",
    "start": 1170.059,
    "duration": 4.521
  },
  {
    "text": "right so now let's look at",
    "start": 1171.74,
    "duration": 6.66
  },
  {
    "text": "but in the earlier case you had this St",
    "start": 1174.58,
    "duration": 6.219
  },
  {
    "text": "minus 1 which was being generated at",
    "start": 1178.4,
    "duration": 4.56
  },
  {
    "text": "every time step right but here now you",
    "start": 1180.799,
    "duration": 4.62
  },
  {
    "text": "have h i and h j",
    "start": 1182.96,
    "duration": 4.62
  },
  {
    "text": "which is they just have one word",
    "start": 1185.419,
    "duration": 4.14
  },
  {
    "text": "embedding for hi and one word embedding",
    "start": 1187.58,
    "duration": 4.32
  },
  {
    "text": "for a j whereas here you had these three",
    "start": 1189.559,
    "duration": 4.321
  },
  {
    "text": "vectors which were participating right",
    "start": 1191.9,
    "duration": 5.46
  },
  {
    "text": "so now from this h i n h j how do you",
    "start": 1193.88,
    "duration": 6.12
  },
  {
    "text": "get these three vectors to participate",
    "start": 1197.36,
    "duration": 4.14
  },
  {
    "text": "in this equation right so you have these",
    "start": 1200.0,
    "duration": 2.88
  },
  {
    "text": "three vectors participating in the",
    "start": 1201.5,
    "duration": 3.6
  },
  {
    "text": "equation but now you just have h i and H",
    "start": 1202.88,
    "duration": 4.02
  },
  {
    "text": "J so where do you get these three",
    "start": 1205.1,
    "duration": 3.78
  },
  {
    "text": "vectors from right how do you generate",
    "start": 1206.9,
    "duration": 4.019
  },
  {
    "text": "these three vectors that's the question",
    "start": 1208.88,
    "duration": 4.62
  },
  {
    "text": "and that's what uh one of the uh",
    "start": 1210.919,
    "duration": 5.481
  },
  {
    "text": "Innovations are one of the uh",
    "start": 1213.5,
    "duration": 5.039
  },
  {
    "text": "equations proposed in the case of",
    "start": 1216.4,
    "duration": 3.88
  },
  {
    "text": "Transformer based models how do you get",
    "start": 1218.539,
    "duration": 5.341
  },
  {
    "text": "these three vectors uh from these two uh",
    "start": 1220.28,
    "duration": 6.18
  },
  {
    "text": "word embeddings so that's the idea",
    "start": 1223.88,
    "duration": 4.86
  },
  {
    "text": "so what we'll do is we'll use this",
    "start": 1226.46,
    "duration": 3.839
  },
  {
    "text": "Matrix transformation and that's not",
    "start": 1228.74,
    "duration": 3.36
  },
  {
    "text": "surprising because anyways in the case",
    "start": 1230.299,
    "duration": 4.38
  },
  {
    "text": "of the original equation also you had",
    "start": 1232.1,
    "duration": 4.86
  },
  {
    "text": "this linear transformation right and now",
    "start": 1234.679,
    "duration": 4.561
  },
  {
    "text": "from each Vector I want to be able to",
    "start": 1236.96,
    "duration": 4.86
  },
  {
    "text": "generate these three uh vectors so I'll",
    "start": 1239.24,
    "duration": 4.74
  },
  {
    "text": "use three linear Transformations right",
    "start": 1241.82,
    "duration": 4.26
  },
  {
    "text": "and they call them specifically as key",
    "start": 1243.98,
    "duration": 5.16
  },
  {
    "text": "query and value vectors and the",
    "start": 1246.08,
    "duration": 5.099
  },
  {
    "text": "corresponding matrices as the key Matrix",
    "start": 1249.14,
    "duration": 4.98
  },
  {
    "text": "the query Matrix and the value Matrix",
    "start": 1251.179,
    "duration": 5.761
  },
  {
    "text": "right so that's what we'll do on the",
    "start": 1254.12,
    "duration": 5.6
  },
  {
    "text": "next slide",
    "start": 1256.94,
    "duration": 2.78
  }
]