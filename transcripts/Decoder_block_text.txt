foreign [Music] ER and we start looking at the decoder right so the decoder of course operates in the sequence here it will operate sequentially because it generates one output then that output feeds in as the input while generating the next output and so on right so now what does the decoder look like right so the decoder of course takes input from the encoder also so as I said the output of the encoder is E1 up to e t right this layer so this is of course fed as the decode right because the decoder needs the context of what the input was what is it translating right so that contextual representation you have captured at the last layer and that you feed to the decoder so the decoder is going to get inputs from the encoder it's also going to have his self inputs right which is the inputs of the words that have been decoded so far right so it'll have these two types of inputs and let's see how it deals with these two types of right so so what would be the output Dimension right so let's first look at that too so we know what the inputs are now we look at the output so the output and what do I need to generate at the output again this is not new to you we have already seen this in the context of rnns so each output here would tell you a probability distribution over the vocabulary right and you will take the arc Max from there right so you would want that if there are 37 000 words so that's what the number 37 000 is then you'll get the probability of each of these 37 000 tokens and hopefully uh at the first position uh this should have been go actually right so let's assume there is a go input here and this is the first position so you hope that at the first position Nan has the maximum uh probability because that's the correct translation then at the second position Transformer has the right probability because that's the correct translation and so on like you continue uh like that till you produce stop uh at the last position right so your output is uh probability distribution over the entire vocabulary hence the output Dimension is one cross 37 000 where 37 000 is the size of the vocabulary so in general I should just say it is of size V right that's what I should have said so now we understand what the inputs and outputs for the decoder are so now let's just zoom into what the decoder actually contains right so just like the encoder the decoder would also be a multi-layered network so it would have say typically 6 to 12 8 any I mean depending on the kind of uh training data you have the amount of training data you have the kind of problem that you're dealing with you would have any number of layers but the most basic ones have six eight or twelve layers right so there are 12 layers of processing happening and just as in the case of encoder each of these layers has an identical structure right so whatever is happening within a layer is the same is just that the output of one layer acts as the input to the next layer right so that's the same as what we had in the encoder but now what we need to look at is what is inside each of these layers right that's what we need to understand so let's zoom into one such layer so you will have the inputs so these are the uh small H number of time steps which have been decoded so far right all the inputs have not been available because whatever has been decoded that comes in as input so whatever how many steps have been decoded so far only that many inputs would exist right then you have something known as the Mast multi-head self attention right uh so there's this Mast word which has been introduced here okay then again at the output of one layer you have the feed power Network so this as I said is the same exactly the same as what we saw earlier right so again the same thing is going to happen here you would have these inputs H1 up to h capital T right which would give you some intermediate outputs S1 S2 up to St and maybe there's some space here maybe something else would come uh so maybe you'll get some other outputs uh let me call them M1 M2 up to NT and then finally all of that will pass to the feed forward Network and you will get the outputs Z1 Z2 up to set T right and then this process will repeat across layers and in the final layer you will have the output projection to a soft Max layer right you'll have a soft Max layer which would predict a probability distribution over the vocabulary which will hopefully Peak at the right word right so this is what is happening here so I need to explain what is this mask and what is happening in between here right so these are the two things that I want to explain which are different than what happens in the encoder right uh yeah so this is what happens so you have the encoder inputs coming right those were the E's right so in addition it's of its own inputs which is the self part it also gets inputs from the encoder and helps you have a multi-headed cross attention right cross because this is between the encoder and the decoder hence you have the cross attention layer then the output of the Cross attention layer goes to the feed forward Network right so again I will repeat this as many times as required at every stage you have I should not call it t but say suppose Capital T1 right because the number of words in the input may be different for the number of words in the output right in Source language you might have six words in target language you might have eight words to say the same thing so you'll have Capital T1 inputs here again at this point Capital T1 intermediate representations would be computed again here Capital T1 intermediate representations would be computed and then again here Capital T1 output representations would be computed this is one layer then these T1 outputs would feed to the next layer and again the same processing would app right and all of this this entire block is happening in parallel right all these computations are happening in parallel of course like what I mean by that is all these T1 outputs are getting computed in parallel then all these T1 outputs are getting computed in parallel then all these T1 outputs are getting computers right so now what is this Mast and what is the multi-head cross attention these are the two things that we need to understand okay so now we'll try to understand uh what this uh uh the Mast self-attention and the multi-headed cross attention looks like right so we have these inputs so we'll assume right in any case now one question of course is that in the case of the encoder you know that the sequences of length T because that is the input given to you right but in the case of the decoder how do you know what the sequence length is right because you don't know what the output is you're trying to generate the output so the length of the output is unknown to you because unless you generate the stop signal or the stop word you don't really know what the length of the output is right so you'll assume some Max sequence length and let me call that Max sequence length as uh T1 right so that's for the max sequence limb that you will assume but now when you start producing the first word right at that time you have T1 inputs right of which only one is valid because that's the go word and you know that okay go is the start signal that I get but all of these are junk right you don't know what these are right and now what are you going to uh do uh for each of these you are going to compute the q k and V vectors right so these are the capital T1 inputs given to you right and you're going to compute the q1 K1 V1 right the QQ Matrix the uh the query Matrix the key Matrix and the value Matrix right but while doing that you are aware that whatever value you're getting for these guys right that does not make sense whatever key you are getting for these guys does not make sense whatever query you are getting for these guys this does not make sense because these are some junk inputs you don't even know right this might just be a special symbol saying empty right you don't know what it is right now right so when you are Computing a refined representation for go right then let me call that as Z1 or rather sorry let me not make those mistakes again right so I'm going to call this as S1 right uh when I'm going to compute a refined representation for S1 I don't so S1 if you remember is going to be summation Alpha over all the VJs where J's go from 1 to T1 right but from V2 to VT I don't trust my inputs because there are some junk inputs right I don't even know what symbols should come there so it does not make sense for me to compute a refined representation using V2 to VT right so what I will do and that's why this is called masked self attention is I'll I'll make these Alphas zero right so whatever Alpha is greater than the current decoded word I will make those zero right so Alpha 2 to Alpha capital T will become 0 so these V's will not participate in the computation so S1 would only depend on the representation of go right and in the first case it makes sense right but now as you go deeper at the first case you might think this is just trivial right but now as you go deeper let's understand what will happen right so now suppose you have decoded this much right you have go none and Transformer right so you have these three inputs available to you now you are Computing a represent refined representation Z1 Z2 Z3 all the way up to Z T one right now what will you do when you're Computing Z1 your formula was Alpha into v j j is equal to 1 to Capital T1 but now since you have decoded up till time step 3 you will set you will let alpha 1 Alpha 2 Alpha 3 be whatever they were right and then you will set the remaining ones to zero right that's what you will do that's what masking does so the masking means that zeroing out the attention weights which do not matter right I'm just doing a slight uh abuse of explanation here in the sense that uh the alphas need to sum up to one right so while you mask out the zero you'll also do something so that these three now scale them up so that they become they sum up to one right but that is internal detail what you need to understand conceptually is that when you're Computing a refined representation of a Z1 you will set all the other Alphas to zero right similarly so this I should call as alpha 1 J so this is Alpha 1 1 Alpha 1 2 Alpha One three now similarly when you're Computing the defined representation for Z2 your equation would be Alpha to J into v j and again Alpha 2 1 Alpha 2 2 Alpha 2 3 is what you care about and all the other Alphas you will set to zero right so that all that's all that masking does right now the question would be that what happens to Z4 right so I will I'm going to compute Z4 right Z4 would also have a formula so in that formula what do I do right so you you will again have only these Alphas ready in that formula all the other Alphas would not be of use but actually it doesn't matter right because again when you are doing so the key thing to realize here is that these did not participate in the computation right these gave you the uh corresponding q k and V but when it was time to use those V's you just zeroed out the waves right so irrespective of what your input was this did not participate in the computation similarly whatever you compute here will not participate in the rest of the computation because the corresponding Alphas would always be zeroed out you'll always use this masking right here also you will have a masking so only that part of the input which is currently being decoded is going to participate in the computations the rest of the input will get masked out because you will set the corresponding Alphas to zero right so that's what masking means so we are done with the first masked self-attention block here which is it's just going to use a only those many entries right one to h means the number of time steps which have been decoded so far instead of using one to Capital T1 it will only use those many entries which have already been decoded right so that's what you will have here at this point you'll have Z1 Z2 Z3 it also have some other things but you don't care about them because they will not participate in the rest of the computation right so now going forward you have the encoder representation coming up so this was the Mast self attention now this is the Mast cross attention right this is the layer which is new I need to understand what is happening in that layer right so let's see what happens there yeah so you have this S1 to T right all the s representations are computed and now again the ease that you had right the uh you had these capital T representations coming out from the encoder you'll again pass them through your standard QE uh key and value matrices right and then you will have to words within the decoder or the representations coming from the decoder as the query right so whatever has been decoded so far that will become the query and then you will pass it through this normal attention Network right so now what happens is the following that let's understand this conceptually so This S1 to St so far right I should call it S1 to S small Edge right that is the number of words that have indicated so far those have been uh refined representations of your original words by looking at the context also right the context came in through that summation Alpha V right so these are refined representations already but taking care only of self attention that means within the decoder now you want to refine these representations further in the light of whatever you had seen in the encoder right so now you want to take S1 as the input you want to generate a q1 from that okay then using q1 and these K1 K2 up to K capital T where capital T was the number of tokens in the uh encoder right you will compute the corresponding Alphas right so you'll have alpha 1 1 Alpha 1 2 Alpha One T so these Alphas tell you how much attention should you give to the ith input in the encoder while Computing a refined representation for the jth decoder value right and now using this you will compute your Z1 as whatever Alphas you have computed Alpha 1j into the values which are coming out of here right so these are going to be some VJs and you're going to sum over J equal to 1 to T right the same computation happens again you have the key query and value it's just that now the attention is between the encoder the decoder representation and the encoder representation and you're Computing a new representation for the decoder using an attention weighted sum of the encoder values right so that's what is happening in the Mast uh cross attention you are doing a cross between the encoder and the decoder and at this level you will then perhaps get out I don't know what I was calling it earlier but maybe I'll just call it M1 up to M T1 and then these will pass through the feed forward Network to give you Z1 Z2 up to Z D1 right so this is what the entire decoder block looks in the new edition was this cross attention where now the key and the value come from the encoder and the decoder brings the query and this Mast self-attention where the only concept was of this masking that The Words which have not been decoded so far you don't let them participate by setting the corresponding Alphas to zero right so that's all there is to learn about the decoder for now right now the Last Detail that we need is the output from the decoder so you have multiple such layers within the decoder right and let's assume this is the last layer which again has the same identical structure it receives the inputs from the previous layer it has the Mast self-attention must cross attention then the feed forward Network and then whatever comes out right so suppose you get these five and two dimensional embeddings right uh and at each stage you want to predict what the next output is going to be so you take this 512 dimensional input embedding uh use a matrix of size 512 cross V to give you a v dimensional output and then you apply a soft Max on that to give you the distribution over the vocabulary and then pick the arc Max from there to feed it as the input for the next time step right so that's what happens at the output layer right so we have seen the full encoder decoder we have seen all the blocks within an encoder layer so the encoder has many layers each layer is again composed of sub layers so you have the multi-added self attention and the feed forward neural network the decoder again has many layers each layer is composed of sub layers and you have three sub layers here the self attention within the decoder the cross attention between the encoder and the decoder then the feed power Network and one key uh property or key difference in the decoder is that when you're looking at the inputs you only look at whatever has been decoded so far that's why you need to do this masking so that the inputs which are not available so far do not participate in the computation by setting the corresponding Alphas to zero right so that's all I have to say about Transformers I'll end this lecture here and this would probably be the last recording for the course I will see you in the next session if to clear any doubts that you might have but we are done with the syllabus for this part of the course thank you everyone