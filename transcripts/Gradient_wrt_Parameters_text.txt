foreign [Music] with respect to the parameters where are we we finished this part then we finished this part we don't care how many hidden layers are there we'll just keep applying the same formula again and again so we are done with this very nasty part which could have repetitions there could be many hidden layers and now we are finally ready to compute this okay so let's go there okay uh so let's look at that right so what do we want now we want the derivative of the loss function with respect to one of the weight matrices right w k and I want this to be again generic I don't it should not the formula I need not have to recompute the formula for W1 W2 W3 I should just be able to do it for a generic uh WK that's what I'm interested in and again my recipe is going to be the same instead of computing this uh gradient at one go right so this is the derivative of a scalar with respect to a matrix so this is again going to be a matrix but instead of computing all the elements the entire Matrix at one go I am first going to compute the derivative of one of the elements of this Matrix right so what I'm going to do is I just ignore this part for now I'm going to let it be I'm going to compute the derivative of the loss function with respect to one element of the Matrix so that could be say w k i j right so this Matrix has n cross n or N1 cross N2 entries so I'm going to focus on Computing the derivative with respect to one of these entries right and now let me just link it to the diagram that I have suppose this is the entry that I am looking for which in this case tragically happens to be W 2 2 right so it's the yeah it's the weight connecting the second neuron in this layer to the second neuron in this layer so it's uh the 2 comma 2 entry of the W2 mid so that's what that is and what have I done so far right so far I have done derivatives up to this point right I've already done the derivative of the loss function with respect to every element of this layer right and now I see that the weight is only connected to one of the guys here right so that guy may be a two two the weight is only connected to that so now I'm going to exploit this fact right so if I want to compute yeah so I want the derivative of the loss function with respect to one of the entries in The Matrix and now I can again split it into two parts right so let's see I already know the derivative of the loss function with respect to this guy right which is uh a k i right this is the ith neuron in the kth layer I already know the derivative of the loss function with respect to that and so I can find the derivative of the loss function with respect to a k i I already have some formula for that from the previous lectures and then the derivative of a k i with respect to w k i j right now this how do I compute this is the question and this is again going to be very uh straight forward so let's see what that is it just turns out to be h k i j right so let's see why that is the case okay so now uh let's see a k again I'll take a simple case of a 2 cross so Suppose there are only two neurons here or maybe let's take all the three neurons okay right and now this so this is the AK Vector I'm going to just ignore B K I'm just going to write it at the end somewhere now this is my w Matrix so this is w k 1 1 uh this is going to be w k 1 3 uh this is w k 2 1 I'm just going to write the second row completely w k 2 3 okay and this multiplied by h k minus 1 which is just going to be h uh K minus 1 in this case uh so I had taken certain values okay let me just write it as K in this case was 2 right K was equal to 2 uh so my h k minus 1 is going to be H 1 1 H 1 2 H 1 3 right and plus I have the B Vector which is going to be I I'll ignore the B Vector right because you'll see that it does not show up in the formula right now this a K2 how do I get a K2 I get it by multiplying this by this right so if I open that up it's going to be w k 2 1 into h11 plus w k 2 2 into H 1 2 plus w k 2 3 into h 2 3 and plus there would be this bias term right which would be B oh okay 2 in this case Okay so that's that's what it is going to be and now I'm taking the derivative of this quantity with respect to uh one of the weights right so what is the weight that I am looking at I'm looking at w k i j in which this case the wkij is w k 2 2. so all of these other terms are going to disappear right that's why I said the bias would matter in fact more terms don't matter and the derivative of this quantity with respect to w k 2 is just going to be H 1 2 right and now let's understand what is uh H what is this what are these indices one and two here so this was just the uh I and J right so that's what this one and two are so I was trying to take the derivative of uh a I was trying to take the derivative of a K2 with respect to w k this was the guy right so this was again 2 2 and my the answer which I got was h one two in this case right so this is actually I minus 1 and this is J so that's what our formula is going to be right so let me just delete all of this and show you the formula it's going to be uh K minus 1 sorry that was K minus 1 so I got H 1 2 so this one is for the previous layer so it's K minus 1 and the jth neuron in that layer so G right so that's how I got this answer so we just did that derivative I'm sorry I made some mistakes there but if you go back and look at it I think it should be fine so we got this as the answer so this is the derivative of the loss function with one element of the weight Matrix w k right so if I look at the derivative of the loss function with respect to the entire Matrix then it's going to be a collection of these entries and for any guy here I now have the formula I can just substitute those formula right so that's what I'm going to do now and I'm going to do that for this simple case when w k is 3 cross 3 Matrix right so the derivative of the loss function with respect to w k is going to be a part collection of the partial derivatives with respect to all elements of w k and there are nine such elements okay and the formula for any such guy was this so now I'm going to substitute this formula to get this Matrix right so what is this I'm going to just substitute the right values of K and I so this is going to be derivative of K1 and then this is going to be K minus 1 1 this is the formula that we had computed on the previous slide right this was h of K minus 1 comma J so I'm just going to substitute the values of I and J right so this is of K and J so this is J and you already know K so this is what I get right so this uh I've just substituted everything from the previous slide onto this slide okay now let's look at notice something about this Matrix right so all the guys in this column all the terms that are getting multiplied are the same same for the next column all of these guys are the same next for same for the next column the other interesting thing is that across the rows the entries are the same right these three guys are the same these three guys are the same and these three guys are the same right so if I were to take say if I were to take two vectors right a one A2 A3 multiplied by B1 B2 B3 right so this is known as the outer product of two vectors this is a three cross one vector multiplied by a one cross three Vector so the product is going to be a three cross three Matrix right so let's look at what do the entries in The Matrix look like the entries in The Matrix would look something like this it would be a one into B1 A1 into B2 A1 into B3 okay and then uh a 2 into B1 A2 into B2 A2 into B3 and then A3 into B1 A3 into B2 A3 into B3 right so you have the same situation that you have uh here right if you compare all these guys were the same again all of these are b1s all these guys are the same all of these are V2s all these guys are the same all of these are p3s right and similarly all these three guys are the same just as these three guys were the same and so on right so what I'm trying to say is that this Matrix here can actually be written as the outer product of two vectors and what are those two vectors this is one vector right so this will be your B vector and uh this will form your a vector right so that's how I'm going to write it now the outer product between these two vectors so this is what it's going to look like this is the a vector and this is the B Vector the transpose operated because that was the sleeping vector and the a vector was the standing Vector right so now you have a formula for the derivative of the loss function with respect to K and what do you need for that you need two quantities one is the derivative of the loss function with respect to a k that we have already done in the previous lecture so I can compute this the other is the activation at the K minus 1 layer and that you anyways compute in the forward pass right so you compute you start with the input you do the First Transformation to that which is W X plus b then you get the first activation layer preactivation layer then you apply the activation on that to get the activation layer so this is does not require any derivatives any computations right this is just a forward pass and you know how to compute every element in the network during the forward pass so this does not have a gradient Associated it is not some derivative it's just h k minus this one which is the activation at the K minus 1 at layer so that you already have so both these quantities you have so you now know how to compute the derivative of the loss function with respect to w k okay now going to the last part we just come find the biases so you want to compute the derivative of the loss function with respect to bias again the same idea so if I look at this bias okay and this is connected to this activation pre-activation here and I know what the formula for computing that pre-activation was so now if I want to take the derivative of the loss function with respect to one of the elements of the bias Vector then I can split it into these two parts this again I already know how to compute and what is the derivative of a k i with respect to bki you can just see from this formula that it's just equal to 1 8 so this part will disappear and you're just taking the derivative of b k i with respect to bki which is one so the only thing that you will get is this so this is now we can have the gradient Vector so uh the derivative of the loss function with respect to the BK Vector is just going to be a collection of all these partial derivatives which is just the derivative of the loss function with respect to a k right because each of these entries is just the partial derivative of the loss function with respect to one of the elements of a k so collection of all those is just going to be the gradient of the loss function with respect to a k right so we are done we have done the derivatives of the loss function with respect to the weights and the biases that is what our eventual goal was right now we are going to just recap all of this and try to put this into one algorithm in the next video thank you