foreign [Music] okay so now we are ready to wrap up this discussion on back propagation we'll take everything that we have done so far and put it together into a nice algorithm so why we have all the pieces of the puzzle so we have the derivative of the loss function with respect to the output layer we have the derivative of the loss function with respect to any hidden layer activation and pre-activation we have the derivative of the loss function with respect to the weights and the biases right now we can write all of this into a full learning algorithm so this is what it looks like I'm going to start with the gradient descent algorithm so you had start at time step 0 you run it for some thousand iterations you initialize all the weights in the network at every stage what will you do you will first compute all the activations and all the actually should have been the other way around all the pre-activations and the activations and the output using the forward pass right and you know the formula for this right you start with X you compute A1 as W1 X plus b and you compute H1 as G of A1 then you compute A2 as W 2 H1 plus v and so on right so this all is simple Matrix Vector multiplication there are no gradients involved this is just taking the input and passing it through a series of Transformations and all of this is coming from a formula that you can Implement right you know how to implement these functions right you know how to implement this you know how to compute the element wise uh logistic for example if G is equal to the logistic function okay so this is straightforward you'll just do a forward propagation on the input right this should have been comma X8 because you're taking the inputs now once you have done the forward propagation you do the backward propagation so once you have done the forward propagation you compute y hat you also know y right so using that you can compute the loss function okay loss function depends on y hat and Y and you will need all of these things right they were showing up in the back propagation formula that you had seen right so we will see that again so all of these quantities you will need right so everything that you have computed in the forward propagation you will need it in the backward propagation also and what is the output of the backward propagation it's the derivative of the loss function with respect to all the weights in the network and I'm just collectively calling it as a derivative of the loss function with respect to Theta whereas the Theta is a large collection of Weights once you have that you can just update the weights using the gradient descent update right so now let's zoom into the forward propagation and the backward propagation right so this is the forward propagation for k equal to 1 to L minus 1 this is what you will do you will compute a k as so A1 is equal to B1 plus w k w 1 into H 0 and H 0 as I had said is going to be equal to X right and then once you have that you can compute h k as the uh by applying the activation function on the AK Vector this is all you just need to run this Loop l minus 1 times and what happens to the lth layer there you will first compute uh Al okay I've just computed I've just put the output layer outside because for the output layer you need to use a spatial function you don't use the same G function right that's why I put it out so now we have computed everything you have computed the activations for all the layers including the output layer activations and free activations and then you have computed the output also and this is all you need to compute the loss so if you have y hat you can also compute the loss right so once you do the forward propagation you have all the edges all the A's and the Y hat now you start doing the backward propagation so first what will you do you will compute the gradient with respect to the output layer and this is what our formula was now this you already know because you have computed in the forward propagation this is just the one hot Vector where there will be a one in the correct class and this you know from the training data right you know for this example what is the correct class right so this entire algorithm is run for one example for now okay one input X so that input you know what the Y Vector is and that's why you can compute the one hot Vector okay to this you know now from k equal to this is actually wrong this should have been L minus 1 2 1 right because you always start from the last layer and keep going on to the first layer so you compute the gradients with respect to the parameters I want to compute the derivative of the loss function with respect to the parameter in the last layer so this is k equal to l minus 1 to 1 is what you are doing right so I want to compute the derivative of the loss function yeah so now you want to compute the derivative of the loss function with respect to the weights in the last rear which is W3 so this will be a from L uh going from L to 1 right so w 3 uh which will depend on the derivative of the loss function with respect to A3 and uh H2 right so this you have already computed in the forward pass this you have already computed this is just computed outside the loop so you have all the elements that you require to compute this right similarly you can compute the derivative of the loss function with respect to the weights in the layer three so this also you can do because you just need this quantity which you have already computed because K is equal to l right now we are running the loop from L to 1 okay so this is done now you compute the ingredients with respect to the layer below so now you can compute the derivative of the loss function with respect to K uh K minus 1 so you had started with k equal to l to 1 right so now at this point uh K is equal to l so K minus 1 would be L minus 1 so which would be H2 so you're Computing the derivative of the loss function with respect to H2 and for that you need the weights W3 which you already have and you need the derivative of the loss function with respect to A3 which again you already have right so this I already explained this when I was saying that you're just going step by step and then you compute the gradients with respect to the uh pre-activation layer below so this is what you want to compute and for that you just need this quantity which you had just computed and this quantity which you have already argued is easy to come right so you just this Loop just keeps going on and on till the first layer and you just keep Computing all the uh the the gradients with respect to all the weights all the activations all the pre-activations all the biases in the network so this entire Loop you could write in Python you first do the forward propagation then do the backward propagation so we have the formula for all the weights it does not matter it's w 1 W 2 W 3 the same formula applies similarly we have the formula for all the preactivations all the activations and all the preactivations so we just keep applying this formula inside a loop right so I don't have to do this painful computation where I'm trying to compute the derivative of the loss function with respect to every weight w k i j or W3 1 comma 2 W 3 2 comma two and so on right I just have a generic formula I'm just doing Matrix operations and I get the derivatives with respect to all the you can say so that is what is the entire back propagation algorithm is coded in just this these many steps at a very small Loop and all of these are Matrix Vector multiplications so we are almost done one last thing that was left was uh the derivatives of the uh the G primes right which I did not covers I I already told you it's easy to do so this is our gz so if it's a logistic function then this is what it is and this is how you can compute G Prime so this is what you will do right so you can again write a function to compute G prime it takes any value as input uh foreign H is equal to G of a right so you just pass that a and you substitute in this formula so you get G Dash right that's all that this says and in fact it can be written even more uh simply it's just G of Z into 1 minus G of Z you can derive this this is not and similarly for this you can derive and it's just 1 minus U of Z square right so those G primes are easy to compute so that's all I had so if I got this formula as saying right so if I had H already right and if I want to compute G then I already have G Prime then I already have everything that I wanted to compute right so this is all we are done with the entire back propagation algorithm uh we have seen it in quite gory details uh you have to watch these videos a few times to get a complete grasp on it but everything that you need to understand it is there in the videos and the slides so please look at it so I'll end here and the next class we'll go back to gradient descent and look at a few variants of your data so thank you