foreign [Music] trade-off and from there we went on to this equation which involved the regularization term so what we said is that so far we were discussing about minimizing the training error which was computed as the average error over the M training examples and this was empirically computed as say the difference between the true value and the predicted value right and then we realized that if you minimize this you are not actually minimizing the true error because this is not a good approximation for the true error it does not account for this additional term which comes from model complexity and instead of just minimizing this it's possible that you might minimize this to zero but still your true error would be high because your model complexity is high right and hence instead of minimizing just a training error you should minimize training error plus some regularization term and this regularization terms should serve as a proxy for the model complexity means which means that if the model complexity is high then this value should be high if the model complexity is low then this value should be low right so then you are aiming to not just minimize the training error the empirical training error but also minimize the model complexity and find some sweet spot in this trade-off right that's where we were and then I said that this kind of forms the basis for many regularization methods and today we are going to look at a a few regularization methods starting with L2 regularization then we look at data set augmentation parameter sharing adding noise to the inputs adding noise to the outputs early stopping Ensemble methods drop out right so quite a bit of a long list and we will maybe look at this formula in detail at least in the context of L2 regularization right so let's start there so in L2 regularization this is what we do right so we Define our the loss that we want to minimize or the effective loss that we want to minimize we Define it as a sum of the training loss which is this so which is again I will just write it down once more this is again summation I equal to 1 to m or rather average oops M of whatever your training losses and we have been dealing with a squared error loss right so in the LT case of L2 regularization instead of just minimizing this you also try to minimize this right that means you are trying to minimize the L2 Norm of your weight say that's what this quantity is and this is some multiplying factor which decide how much weightage should be given to uh this uh term in the total loss function right so this is the total loss function and this decides how much weight to be given to this component if you set Alpha to zero then essentially you are not doing any kind of regularization your total loss is just the empirical training loss and you are back to uh the non-regularization non-regularization loss function right but if you set Alpha to a certain value it tells you how much do you care about this regularization term now the question is ah how does this act as a regularization so this is indeed a function of w right there is no denying that so we wanted the loss function to be of this form so it is indeed in this form right but minimizing this how does this control for model complexity that's the question that we would like to understand right so what are we saying right we are saying that you find me weights such as that the training loss is minimized and also the weights values are very small right so what does that actually mean suppose we consider only two weights right say w one and W two now in the absence of this I'm just saying that you go ahead and pick up whatever weight values you want right and just minimize my training error that means I could pick up very large values of the weight of the weights rights all of the entire R2 plane is open to me right so let's just extend this so any value in the R2 plane is open to me right now I am saying that hey you know you can minimize the loss but I do not want these W values to blow up because if these W values blow up my regularization term will blow up and hence my effective loss would still be high right so I am telling you that here you cannot allow the weights to grow in other words I am just drawing some kind of a boundary here and saying that the more you go out of this boundary right the more uh bigger my W1 and W2 are going to be the more bigger my L2 Norm is going to be of the weights and more bigger this loss term is going to be and hence my effective loss will not be minimized right so now I am saying that you add as many parameters as you want you add more model complexity you have a million parameters a billion parameters but for any given parameter I will not allow you to grow those values a lot because the moment you grow those values a lot then this loss term will increase and that will increase my total loss right so in some sense I am restricting the model complexity by allowing you to have have many weights but I am not giving you a lot of freedom in choosing the values of those ways right so that's how this is acting as a proxy for controlling the model complexity okay so now if that is the loss function then we are interested in the derivative of the loss function with respect to W right because for all gradient descent methods and its variance this is what our in quantity of interest is so the derivative of the quantity on the left hand side is the derivative of this quantity plus the derivative of this quantity with respect to W and this quantity is actually Alpha by 2 W transpose W so if you take the derivative of that with respect to W you can think of this as W Square so one of the W's will disappear and you'll have a two term here which will cancel with this so w square is the derivative is 2W so you the twos will cancel and you'll just be left with Alpha W right so that's that's how you can think of how we arrived at this formula right now ah so now what happens to your update rule right so your update rule earlier was the new value of loss is equal to the old value minus ETA times the derivative and now this is what your derivative looks like right and I've just opened up the brackets so this is what your earlier update rule would have looked like if you did not have the alpha term now since you have the alpha term this term gets added right so L2 regularization is very easy to implement if you have already implemented gradient descent then all you need to do is change this update Rule and add this term or subtract this term and you have the L2 regularized update rule for green interesting right now this becomes more complicated as you look at other algorithms like atom and variance of it and so on but we will not discuss those but at least in the case of vanilla gradient descent where the update rule was just move in the direction opposite to the gradient it's now subtract that term and then further subtract this term that's all your update rule is so extremely easy to implement now let us see what is the geometric interpretation of this right so to understand the geometric interpretation we will do slightly uh long ish I would say a derivation right we'll just try to uh go a bit deeper uh into what the Taylor series says and from there what do we derive and so on right so let's assume that W star okay is the optimum yeah so let's assume that WSI is the optimum solution for LW so LW remember was our unregularized loss function right so our new loss function is uh L tilde W is equal to LW plus this Alpha by 2 into W transpose W so this LW as well as was our unregularized loss function so let's assume that the optimal solution for that was given by some W star the sum W star which exists which gave me the lowest value for l w right so that's all we are defining right and there's no problem with that definition foreign now consider a point which is ah say a point w which is in the neighborhood of w Star right so the way to write that would be W is equal W is equal to W Star Plus U so there's a point w which is in the neighborhood of w star the other way of saying that is that let's consider a jump such that you have W minus W Star right so uh you could think of it this way this is more natural that we are looking at a point w which is in the neighborhood of w star and from that equation you can derive this equation right so that's that's the small jump that you have made okay now let's let's we'll now do a bit of reasoning or a bit of derivation starting from there right and now let's see what the Taylor series says this is Taylor series second order approximation so far in this course we have been dealing only with the first error approximation but now I'm talking about second order approximation and what this says is that the loss function in this neighborhood right so w is the point in the neighborhood is the loss function at w Star Plus the first order term plus the second order right so you already know this from the Taylor series that we have seen earlier and now the interesting thing here is that this term is zero why is that zero because that's the optimal for the loss LW right and if it's the optimal for the loss LW then the derivative at that point is going to be zero right it's going to be some Minima at the Minima the derivative is going to be 0 right so that's why this quantity is going to be zero I know I'm just going to expand this right so now what I have done here is this quantity has disappeared and this U I have written as W minus W star so wherever I see do I have substituted as W minus W star nothing great again about that and this Delta WL this quantity has disappeared so I'm just left with the first term and this last term here okay that's what my LW looks like where LW uh yeah it's the unregularized loss now let's just look at a few other quantities yeah so now uh this is the uh if I compute the derivative of this right so this is what my LW was now if I compute the derivative of this that's the same as the derivative on the uh of the quantity on the left hand side so I can take the derivative of this quantity and I can just open up the brackets I have derivative of L W Star Plus H into W minus W Star right so this again you can think of this as uh uh x h x right so you are kind of taking the derivative of a quantity which has x square right and the derivative of that one of the X's would disappear and just left with X and in this case X is W minus W star and then we get 2 coming out which will get canceled with this right so that's how you can think of this uh derivation that I have written here so this is what I get right so the derivative of the loss function with respect to W turns out to be H into W minus W star and we had seen earlier that this H is actually the Hessian which is a matrix containing the second order uh partial derivative so we had already already seen this in a earlier lecture so I'll repeat that okay so that's that's nothing great here right I've just applied Taylor series and done some simplifications I mean just taken some derivatives so I have just a set set of steps which you can uh which one step follows from the previous and you could just go back and look at this if you have any doubts but there's nothing conceptually difficult that I've done here okay now uh from here let's uh now look at this loss right this is our regularized loss and the derivative of the regularized loss as we had derived on the previous slide was this plus Alpha W right this we are derived so remember LW is equal to L W plus Alpha by 2 W transpose W now if you take the derivative of this with respect to W so here this derivative all of these are derivatives with respect to W it's obvious from the context so I have not suffixed it explicitly but it's derivative with respect to w uh and so this would just be derivative of this quantity plus the derivative of this quantity and the derivative of this quantity as we saw on the previous slide was just Alpha W right so again nothing new that I have written here but now what I'm going to do is I'm going to try to use this value instead of this quantity here right that's the main change I am going to do as I go ahead okay now let there be another W which is the optimal solution for the regularized loss right so now if there's such a w tilde exist then the derivative of the loss function at that point would be zero right and now we have seen that the derivative of the loss function at any point was given by this formula we just saw this on the previous slide so I've just substituted that formula instead of this formula and instead of the generic W I have substituted W tilde right that's the only change that I have done here right yeah again I'm just doing a lot of steps but there's nothing much going on here right just like I'm defining some quantities and then using those quantities and at points I'm at some points I'm using the properties that the derivative is zero right so again uh there's nothing to uh really say much about here right at this point once we reach uh the final answer that we're looking for then I'll have significant things to say now this I can rewrite it as I've just opened up the brackets here so this is h w tilde minus h W Star Plus sorry W's plus Alpha W tilde equal to 0 so I have taken all the W tilde terms on one side right so it's h w tilde minus the plus sorry plus ah Alpha W tilde is equal to H W star right and this Alpha I can write it as Alpha I it does not make a difference it's just multiplying by the identity Vector so I into any Vector is the same as that Vector right so it does not make a difference so that's all I'm doing here and then just regrouping the terms this is how I arrived here now now I get a formula for w tilde and why do I know that this inverse exists because I've gone ahead and straight away taken the inverse how do I know that this inverse exists because H is the Hessian matrix so H is a positive positive semi division it Matrix positive semi definite Matrix and hence the inverse exists say so that's why I can write it as this quantity here okay ah so continuing in my journey now as Alpha tends to zero right if I don't use a regularization right if this quantity is 0 then this term disappears then I just get H inverse h w star so in that case my w tilde is actually equal to W star and that should indeed be the case right because if Alpha is 0 that means my second term was not this if the second term was not there then I'm just left with my original loss function and for the original loss function I had already defined that the optimal is W star so if I'm going to get rid of alpha if Alpha is equal to 0 then even for the regularized loss which is just equal to the original loss my w tilde equal to W Star right so simple uh observation but just trying to make the point clear okay now we are of course not interested in the case of alpha not equal to zero we are interested in the case when Alpha is not equal to zero right because we are using regularization and in that case we want to see what is the solution between W tilde and W star okay so let's go ahead so since H is a symmetric positive semi-definite Matrix we know that its eigenvalue decomposition will exist not just that we know that its eigenvalue decomposition the eigenvectors are going to be orthogonal so I can write it as Q Lambda Q transpose so uh I'm sure you have done this in a course on linear algebra if not there are these other videos that I have on linear algebra you can go ahead and look at that I do eigenvalue decomposition in quite a bit of detail there right so this I would assume you know that any Matrix you can write the eigen value decomposition and Q is orthogonal here so q q transpose equal to Q transpose Q is equal to I right we know that so now on the previous slide whatever I had I'm just going to bring that back okay this pen work is fairly cumbersome I need to get rid of okay and now instead of H so this is what we had on the previous slide right so this is what we had derived so far and I'm just bringing that back and wherever I see H I'm going to substitute the eigenvalue decomposition of H again all of this is nothing great happening here it is all just a series of steps that I am doing I am trying to reach a point after which I can make some conceptual comments where I can get some insights into what is happening into L2 regularization right so right now I'm just going through the drill now I am just going to do a few more simple things yeah so again instead of I here I have written it as q i q transpose so Q into I is of course equal to q and then q q transpose is again equal to I so there is no harm in writing this this is uh correct right and now once I have done that I can now regroup some terms and I can write it as the product of three matrices here right so I have taken the middle part as common which was Alpha the Triangular Matrix Lambda which is the Matrix of eigen values of H then Alpha I and then I have q here and Q transpose here right so I have just regrouped the terms now this becomes like the product of three matrices a b c and I am trying to find the inverse of this product then the inverse is going to be C inverse into B inverse into a inverse right so that's what it's going to be so I can just do that's where I'm headed now right so I'll just do that yeah so I can write it as Q transpose inverse then Lambda plus Alpha inverse and Q inverse and this Q inverse and Q will multiply out and give me I and Q transpose inverse is of course Q because Q is an orthogonal Matrix right so a few things have got simplified for me now so this is where I end up with right and here again I would notice that if Alpha is 0 right then this term disappears I have left with Lambda I inverse into Lambda so that will also disappear that will become I then I am q q transpose into W transpose q q transpose will again become I so I'll just be left with W star which means in the absence of regularization my w or when I said Alpha to 0 my w tilde is equal to W star which is the same as saying that if I do not ah use a regularizer my optimal solution is W Star right so I am just saying this again to make sure that in the steps that we have done we have not made any mistake uh things are still looking meaningful in whatever we have done okay ah yeah so let's just go ahead a bit now yeah so now I can write it as a W tilde is q d q transpose where D is this Matrix so this is this is a diagonal matrix this is also a diagonal matrix so the sum of two diagonal matrix is going to be a diagonal matrix and then you are again multiplying it by a diagonal matrix so that's also going to be a diagonal matrix hence I am calling it d d as the diagonal matrix so now W tilde is equal to this quantity right so now I have reached somewhere that I wanted to reach and from here on I can start hopefully start making some observations right so let's just work with this foreign so this is where we are we had of derivation for w tilde and we have now been able to express W tilde as a function of w star with these Matrix Q D and Q transpose right so what exactly is happening here right so uh W star was my optimal solution for the unregularized case now when I am using the regularization what is happening W star is first getting rotated by this Matrix Q transpose okay so this will give me some let's call that quantity as Z right so Q transpose W star is again going to be a vector so Q here would be an N cross n Matrix W is an N cross one vector so the result is again going to be n cross 1. right and now that is getting multiplied by again an N cross n diagonal matrix right so now multiply by multiplying by a diagonal matrix if you only have the diagonal terms and everything else is 0 and if multiply the vector by such a matrix then it's the same as taking every entry of the vector and scaling it by the corresponding diagonal element so there are n diagonal elements and N entries in the vector so this multiplication the result that you get is simply uh every entry of the product is actually this corresponding entry multiplied or scaled by the corresponding diagonal element right so that's what is going to happen here and so on right so that's what the diagonal multiplication looks like so first you rotated it then you scaled it by D and then again whatever you got so let's call that Z1 right so this was my z uh now I got a Z1 and again I'm taking Z and rotating it by Q again right so that that's what I'm doing here so this I've made this observation now what does this mean and so on it's still that's something that we need to understand but this is what is happening here from a linear algebra perspective right so w style first gets rotated by Q transpose to give Q transpose W star however if Alpha is equal to 0 right if Alpha was 0 then this just becomes this whole thing here just becomes I right if Alpha is equal to 0 and then Q transpose rotated W star then Q rotated it back in its original position and we got uh Q right because q and Q transpose are inverses of each other hence uh because Q is a orthogonal matrix so there was no effect of alpha is zero again just saying this for the sake of showing that whatever we are doing is sounding correct right ah but if Alpha is not equal to 0 then what is happening is the question right then what does D look like so first D has two components so first component is this so let us look at what that component looks like right ah by this I mean the whole of it right so first Lambda plus Alpha I so you have so you have a matrix which contains the eigen values on the diagonal everything else is 0 and you're adding the Alpha I Matrix so it has Alpha s all the way on the diagonal and there are n of these Alphas so adding these two matrices so we'll just get Lambda 1 plus Alpha Lambda 2 plus Alpha and so on on the diagonal right now the inverse of a diagonal matrix is just the address is just another diagonal matrix containing the reciprocals of the diagonal elements right so this would be what the inverse would look like every element on the diagonal would be the inverse of it right so this is what is happening here so each of these quantities is just going to be the inverse of that quantity now D is actually this further multiplied by Lambda right so this is what D is so now let me again try to multiply it by Lambda yeah so this is what I'll get right so I had this Matrix and now I am multiplying that Matrix but another diagonal matrix and it has lambdas on the diagonal so if you do this multiplication you will just get the product of these diagonal elements on the product Matrix right so that's all you will get okay so this is what this middle guy here looks like okay and if Alpha was 0 then the middle guy would just become I or this would all disappear so we'll just get I and there is no change happening W tilde is equal to W star but now if Alpha is not equal to 0 then what exactly is helping Happening Here Right foreign transpose W Star right so this is the first multiplication that is happening now each element of that Vector is getting scaled scaled by this quantity right and after it gets scaled by that quantity it is rotated back by q that is what is happening right the rotations performed by q r transpose and Q R inverses of each other but in between this scaling is happening right and now if the eigen values is very large if it's greater than Alpha right then this quantity that you have on the diagonal is actually going to be equal to 1 right so a lot that means for those weights there will be no scaling which will happen they will remain the same as their original value right so remember this W star is a vector of weights and we are talking about every element of that Vector getting scaled but we're just seeing that if the eigen value is large then those weights will not get Scaled let's suppose Lambda 1 was very large then nothing no change will happen to this guy similarly if Lambda 2 was very large then nothing would happen to this guy because Lambda by Lambda plus Alpha would almost be equal to 1 right now on the other hand if yeah if Lambda is very less than Alpha then this quantity almost becomes zero that means whatever elements were there if this is what your W star was and say this is the ith element if the corresponding ith eigen value was very small then it is almost like this weight will go to zero and this is actually what we wanted what the effect that we were hoping for was that some of the weights should shrink because if all the weights are large then your L2 regularization term which is W transpose W is going to be high and now what we are seeing is that if the eigen values are small right then those weights are going to shrink a lot and that's exactly what we were hoping for and the other weights will remain the same as what they were earlier right they will not grow but they would remain the same as what they would have been in the absence of regularization right so now you are seeing the effect that certain weights are shrinking here and that's a proxy for controlling the model complexity because now you are not allowing the model to do what it wants you're not giving it you're given it capacity but then you have not given it the freedom to fully utilize this capacity the full capacity would be I should be able to take the weight W1 and set any value between minus infinity to Infinity 80 but now I am not allowing you to do that I have given you as many weights you wanted but on each weight I've now drawn some boundary and if you go out that boundary then W transpose W is going to increase and this regularization term is going to ensure that some of those weights actually decrease actually actually shrink right now still we need to look at a few more things here right so effectively what will happen the the significant directions where there are larger eigen values those will be retained because those are not getting scaled but the directions where the eigen values are small those will shrink and those weights will be heavily penalized they might almost go to zero and now if you look at the effective number of parameters that your network has then this is the effective number of parameters right because your total number of parameters was n now many of those parameters are getting scaled down significantly by this Factor right now the depending on how many are getting scaled down if you take this summation this would be less than n so effectively you have you have ended up reducing the complexity of the model right because you started off with n parameters but now because of the scaling not all of your n parameters are fully effective and hence the effective number of parameters that you have is actually much less than M right so that's what is happening that's how it is controlling the model uh complexity okay now let's just look at a bit more of geometric interpretation also of this so now what I'm showing you here are two loss Contours so and we are using the same uh oops so this is my uh LW so this is what my unregularized loss looks like this is my Alpha W transpose sorry not Alpha Dot this is what my w transpose W looks like so this is the regularization loss and you can see that that loss has this uh if you look at this this Contour here this is the Contour of a bowel which is the Contour of a square function and indeed it should be the Contour of a square function because this is a square function right so this is the place where W transfer transpose W is equal to say one W transpose W equal to two three and so on right so these are the Contours that you have here and these are the Contours of w right now let's make some observation about these plots right and what is w tilde W tilde is the place where these two plots are intersecting right of course the I could have drawn some more uh this Contours will go on right and similarly I could have expanded this Contours also so there are many places where these two plots will interact intersect right it's not that this is the only uh space a place that they will intersect but this is where my final solution came out for this problem this is the W tilde that I ended up with so w tilde was the optimum point for L tilde where L tilde was L theta plus Omega Theta right and this is L Theta and this is my Omega R Theta okay so that is what is happening here now I want you to look at this plot right so I will now now we want to look at this plot and try to understand how this relates to the geometry and how this relates to the idea that certain weights are actually shrinking what exactly is happening here right I want us to pay attention to all of these so I'll clear these drawings because we understand what each of these plots corresponds to right now in the absence of regularization this is what your W was W star so that means it has taken this value along the W1 axis and a ticket taken this value along the W to axis okay so you can see that these W's are relatively large right now in the presence of regularization this is what my solution looks like let me just change the color so this is my value for W2 and this is my value for W1 so indeed my weights have shrunk right not just that my weights have not shrunk uniformly right it's not that W1 has become half its original value and W2 has also become half its original value you can see that W1 has decreased much more as compared to W1 right so first observation yes indeed my weights Shrunk the second observation that the weights have not shrunk uniformly they have shrunk by certain different factors and that is expected because those factors depend on the eigen values so those factors are Lambda 2 Lambda 2 by S Alpha right now the question is why did W1 shrink more than W2 right and to understand that you need to look at this loss surface here now that if you look at that you should be able to understand why if W2s shrunk more then that means W2 is not so important as compared to W1 so if you look at this loss surface the solid loss surface does it give you that impression that's the question I'm trying to ask and indeed it does right so I'll tell you why so if you look at this direction which is the W2 Direction then here you can see that the distance between the contour lines is very smaller as compared to the distance between the contour lines along this direction so this is the W1 Direction and this is the W2 Direction what does that tell us that along this w n axis even if I change the weight a lot I'm not losing much in terms of my loss right so I have more flexibility here if I want to shrink the weights then I have a better chance of shrinking this weight because whatever is the optimum value for this weight even if I shrink it my loss is going to shrink a bit more slowly because there's a higher gap between the contour lines here so the loss is not rapidly changing along this direction but along the W2 Direction you can see that the loss is rapidly changing so now if I ever to aggressively shrink W2 that even for small shrinks in W2 my loss will go increase right and that is not not acceptable to me because I want to shrink it but I also want to maintain this trade-off that this law should also be small I want my W's to be small but I also want this loss to be small hence I have ended up with this trade-off that the weights which are more significant right which is W1 in this case I am shrinking it less because if I shrink it a bit more if I had brought it down to this value then my loss would have been even higher right and that is not something that I want right whereas when I'm shrinking W1 I have more leeway because along this axis my loss is less sensitive to changes in W because by a small change in W my loss is also decreasing slowly that's what is you can indicated by the slope right so this Gap if you remember indicates the slope the slope here is smaller that means a small change in W does not cause a large change in uh the loss right whereas along this axis the slope is steeper because this Gap is small so small change in W causes a large change in the law side so this figure kind of summarizes everything that we wanted to understand about L2 loss it tells us what does the shape of L2 loss look like it tells us what the optimum is in the absence of L2 loss and then it tells us how there is a compromise being made where you are shrinking the weights but you are not shrinking all the weights uniformly you are shrinking them according to different proportions and that's exactly what is happening here and these proportions actually decide are decided based on which directions are important and which directions are not important because the eigen values these are remember the eigen values of the Hessian and the eigen values of the Haitian tell us which directions are more sensitive and which directions are less sensitive right so I am not proving that but you can go back and look at that the eigen values of the Hessian matrix these were the eigen values of uh yeah of the Haitian Matrix so these tell us or which directions are more important and which directions are less important right so all of this has now fallen in place and this picture kind of summarizes what happens with LD regulations so that's all I had about L2 regularization I'll end this video here