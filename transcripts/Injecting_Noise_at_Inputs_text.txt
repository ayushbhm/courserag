foreign [Music] technique that we're going to see is about adding noise to the inputs so earlier I used to have a different sequence in the course where I used to First teach Auto encoders and then do that but do this lecture but now I've changed that so I'll just make some changes on this slide accordingly so we will see this in the auto encoder but for now let's assume you have the following setup that you are given a certain input and you have say a one layer Network just ignore this for now right I'll come back to this soon so this is your inputs assume this layer is not there this layer is directly feeding into this and then you have the output right so this is what your normal neural network would look like now what weight uh adding weight uh noise to the weights what this method does is you take your inputs let's assume your inputs were say binary for the sake of simplicity right now what you do is you add a noise process such that for every input say with probability 80 percent it will keep the input as it is and with probability 20 percent it will flip the input right that is a very simple noise process so if you have like 100 uh digits as input or an input belongs to R 100 or in this case uh 0 comma 1 raised to 100 because there are only binary inputs then uh with probability eighty percent it will keep each of those hundred values as they were and with probability 20 it will flip it so in effect what you would expect is that twenty percent of your inputs has been corrupted right because twenty percent of it will get a change right so now you have added noise to the input and now once you have added noise to the input what is a deep neural network good at it is good at mapping x to the true y right that means it's very good at reducing the training error but now from this x you have given created a random X like a modified X or a corrupted X sorry not a random X but a corrupted X and now it is trying to map this corrupted x to the input right and now every time you see this training instance this corruption would be slightly different so across epochs as you are modifying the every time you are applying this noise process you are seeing a slightly different corrupted version of the original input and every time it has to map that to the same y so now again the job of the network has become harder because it's it's in a way similar to what you did with the augmentation of the data set right so you had an original two and then you passed it uh shifted to or rotated two and now both of these it needs to map to the label two right so now it has a more to learn and same thing you're doing here you had an original X you corrupted it and it has to map it to Y next Epoch you again corrupted it but this time the corruption would be different maybe in the first Epoch uh the say the first 20 bits were corrupted now maybe the bits from 20 to 40 are corrupted and so on it's every time it's seeing a different corrupted version of the input right and this this index here is the epoch or that I'm looking at and it has to map all of those two wire right so now memorizing the input becomes or this taking the input memorizing it and mapping to the Y becomes input because now your input is every time a bit corrupted so it has to deal with these Corruptions also and now what we can actually show right that for a simple input output Network right that means there is no uh hidden layer so you have a uh so I have a bunch of inputs and then you directly have the output right so there is no in no input output layer so your inputs are say X1 to x n and now if you add some noise to all of your inputs right and if that noise is a gaussian noise that means say that noise is coming from a gaussian distribution zero mean and some variance then you can show that this is actually equal to L2 regularization right and we will see that on the next slide so this is the setup that we are considering this is exactly what I drew on the previous slide that you have these n inputs and to each of these inputs you have added some gaussian noise okay and the noise added to each input is independent of the other inputs and then you're trying to predict a y okay and the noise is coming from zero mean and some variance so now your X tilde that the corrupted X is the original X plus this noise and now your original y hat right without the corruption your this is what your y hat should have looked like right there's no surprising here no surprising I have not used any non-linearity this is just like a linear transformation right so your model is simply Y is equal to W transpose X this is a linear model that you're using here right and so this is what your y hat should be but now instead of Y hat you are actually Computing y tilde and why do I say y tilde because now you don't have X's you have X tildas right you have the corrupted axis now for the X if I substitute the value of x tilde as X Plus Epsilon then I can just rewrite this summation as follows this is what will happen right so it is x i x i is equal to X Plus Epsilon so that summation now splits into two parts and the first part of course is the same as your y hat so I can write y tilde as y hat plus W I uh plus summation w i Epsilon I okay now what are we interested in we are interested in this expected error now the expected error what is it that I am Computing this is the output based on the corrupted input and this is the true output right this is not y hat remember this is not y hat this is the true output the true label that was given to me right and we are interested in this expected error this is what we have been doing throughout when we are looking at bias variance trade-off right so this is the quantity that I am interested in so let me try to find out what that quantity is so I'm just going to now the rest of it is just now going to be a set of steps so I'll just maybe write down everything first yeah so I'll just now go over it one by one this will be easier to do it that way so I had y tilde okay so now what I have done is I have written y tilde as y hat plus this quantity so no uh no issues there then I have three quantities here A B and C so I have grouped them so I have a minus B plus sorry A minus C plus this B quantity and of course there was the whole Square outside always okay so now this now Becomes of the form let me just say p plus Q the whole Square so it is going to be p square plus Q squared plus 2 p q so it is going to be the expectation of p square plus Q squared plus 2 PQ this is an expectation of a sum so I can write it as the sum of expectations so expectation of P Square expectation of 2pq and expectation of our Q square right now let's make a few observations so this quantity here there are n terms here right so you could think of it as a 1 plus a two plus plus all the way up to a n square right so now when you expand this what are the kind of terms that you are going to get you are going to get the squares of all of these guys A1 square plus a 2 square all the way up to a n square and then you are going to get these uh and choose two terms where you will have a 1 a 2 of course multiplied by something plus ah A2 A3 again multiplied by something and so on right and so this is going to be a very long sum and you are taking the expectation of that sum so it's going to be a sum of the expectations so this is what it would look like right and my a a one a two here is of course W 1 Epsilon 1 W 2 Epsilon 2 and so on right now wherever you encounter A1 A2 remember that a one a two here is Epsilon some Epsilon I Epsilon J and since the epsilons are independent then all these except all these could be written as expectation of Epsilon I into expectation of Epsilon J and of course some other quantities here but the main thing here is that Epsilon expectation of Epsilon is zero because Epsilon is a zero mean noise so all those terms which contain A1 A2 are going to be disappearing why will they disappear because you are taking the expectation of a product and the two random variables in the product are independent so that expectation can be written as the product of expectations and in the individual terms are there now become zero so all these terms are going to be disappearing and so you will only be left with the uh with the quantities which contain the squares so those are these quantities right so w i square plus Epsilon I into Epsilon I square and you'll have n such terms so that's why you have n terms here right so that's that's a pretty long ah um yeah I mean long set of explanation for what how I went from here to here right and again I have said that this quantity also becomes zero so why does that quantity become zero so this is y hat minus y okay so here you are a random variable is y hat and this is w i Epsilon I so Y is of course not a random variable w i is also not a random variable so what is the random variable y hat and Epsilon I right so this is again the product expectation of the product of two random variables and now again y hat and Epsilon I are independent because y hat had nothing to do with Epsilon that was the uncorrupted output whereas Epsilon is the noise that you have added and the noise that you have added had nothing to do with the uncorrupted output so these two are independent random variables so this expectation you can also show that will go down to 0 right so then the only thing that you are left with is this and this quantity uh so let's see from there where do we go so this becomes the expected value of this error right plus ah the the summation w i square is not the random variable so expectation of Epsilon Square into this sum right this sum here and the expectation of Epsilon square is just Sigma Square so what you get effectively is your loss term right which this was the training error right so if you estimate this expectation from the training error from the training data that is what you are going to do because you only have the training data at training time so this is your L Theta and this is your Omega Theta and this is actually the same as the L2 loss because you are minimizing the sum of the squares of the weights which is the same as the L2 Norm penalty right so in the simple input output Network without any non-linearity uh adding noise to the inputs is the same as using a weight DK right so this is the L2 Norm penalty is also called weight Decay because as we saw it decays the weights right by this Factor Lambda by Lambda Lambda plus Alpha okay so that's this is another regularization technique and we have seen its relation to uh weight DK in the simple input output Network without any non-linearity okay uh silent this here