foreign [Music] module I was telling you that the next topic that we'll talk about is the representation power of a multi-layer network of Sigma neurons so let's understand what I mean by that right so just a quick recap right so earlier uh when we are finishing perceptrons we had introduced this multi-layer network of perceptrons and we had spoken about the representation power of a multi-layer network of person drones uh analogously now we are going to talk about the representation power of a multi-year network of sigmoid neurons so it's just the same template being repeated right and there we had uh made an interesting statement right that a multi-layer network of perceptrons with a single hidden layer and I don't care about the number of neurons in this layer I know that it can be exponentially but exponential but I don't care about that a single hidden layer can be used to represent any Boolean function precisely and the meaning of that was that if you have an N variable input then you would have a Boolean function of n variables and it would have a certain truth table now no matter what the truth table is right and we had seen that for an N variable input you could have 2 raised to 2 raised to n functions so any of these functions you can come up with a network which can precisely represent this function that means when you give a certain input configuration the networking produce the same output as is mentioned in the truth table right and the way we were doing that is by having these exponential number of uh neurons in the hidden layer such that every neuron was like spatializing for or every perceptron was specializing for one specific input right so that's what the representation power of a multi-layer network of percept transfers now my goal is to make a similar statement okay and this of course I had we had proven by construction or we just constructed a network and we were convinced that it can actually represent any network irrespective of whether it's linearly separable or not right now my intention is to make a similar statement for multi-layer network of Sigma neurons right and the statement is a multi-layer network of neurons with a single hidden layer so far so good it's the same as what I see on the left hand side can be used to approximate okay now here are where the differences start so this was represent now I'm saying approximate right so it's that means it's not going to be exactly equal any continuous function this is good news right because here I was only talking about Boolean functions now any continuous function it any function f of x which is continuous I can approximate it to any desired precision now what does any desired Precision means right so the way you would have seen this in many mathematical proofs right I would design a desire I would Define a threshold Epsilon right and what I'm saying is that you choose whatever value for Epsilon right it could be 0.0001 it could be 0.100 zeros one right and I can come up with a network such that if my network output right so now this is the situation right I have a n dimensional input I'm calling it r x which belongs to r n okay and I'm going to design a network I don't know what the network looks like and the network is going to give me a certain output let me call that g of X so far I've been calling it at F hat of X and is calling in G of X right and I know that there is some true function also such that when I take this input and pass it through the two function I get the output as f of x what I am saying is that what this statement is saying that I can come up with any network I can come up with a network such that the difference between the true output okay this is how I'm quantifying the difference I'm just taking the difference between these two values if these are real numbers I'm just talking about the absolute uh difference uh between those two real numbers right would be less than Epsilon okay so I would be able to approximate it up to the Precision of Epsilon for any of the inputs that are possible I'll pass it through my network I'll get a g of X and I am confident that it will not deviate from f of x more by Epsilon and I can construct such a network you give me an Epsilon I'll give you a network which will be which will produce an output which is within an Epsilon from the true output right and of course as you can imagine just as we had the case there that we had this exponential number of neurons you could imagine and we'll see that in more detail that as the Epsilon becomes smaller and smaller you will need a more and more powerful Network that means you need more and more neurons in this hidden layer this may not be obvious right away but this is something this idea is something that we will develop during this lecture okay so now let me just delete this and let me just put it on the slides so in other words I'm just going to read it out as it is in other words there's a guarantee that for any function f of x that's what I was showing that is a function which takes an N variable input and produces an M variable output in my example uh sorry M dimensional output in my example M was just one right I was just producing a single dimensional output we can always find a neural network with one single hidden layer and many neurons right I don't know how many neurons it would have but I can find a network with a with a single hidden layer and a large number of neurons such that the output would always satisfy this condition right now this is a very very powerful theorem right and this is what uh makes deep learning so attractor right so what is it saying right we are take any complex reliable function that you have let's return back to our oil mining example where f of x was very complex we did not even know what f is and we know that it's a very complex function based on your input variables such as salinity pressure temperature Marine diversity and all that it's very hard to know what that function is but now I can come up with a neural network which when takes an input its output would be very close to the desired function right in theory I can do that of course the Practical difficulty is that this such a neural network would have a hidden layer with a very large number of neurons and if it becomes exponential then we know it's not practical right but at least theoretically I can do that and this is only talking about a single hidden layer right and one of the things that maybe you will see through an assignment or in a quiz or something is that you if you had one more layer then you could reduce the number of neurons in each layer right so from exponential it will fall down to something which is linear which is manageable right so that is a separate topic but right now remember we are talking about this constraint case where we are saying only a single hidden layer okay so that's what we want to be able to uh prove right and this is the famous uh Universal approximation theorem and the actual proof would be a bit difficult but what I'm going to show you is an illustrative proof which would be good enough just as we had seen approved by construction it was actually an illustrative proof I just constructed a network and convinced you that this statement is true I'm going to do something similar here right and whatever I'm going to do here is based on this excellent illustration right this link that you see in the bottom which is an online textbook one of the very earlier early online textbooks on deep learning and it has a very good uh Tech stand and accompanying illustration explaining whatever I'm going to do explain on the slide so all the material that I have presenting here the ideas are borrowed from there right so uh just wanted to acknowledge that as well as point you to that okay so so this is what an arbitrary function looks like right so I have my input X I'm dealing with so this is the x axis and this is my f of x r y axis right and you can see that I have a very arbitrary looking function right it's maybe I I it's not a sine X it's not a cosine X it's maybe a mixture of some of these functions right uh it's not x square X Cube or any of the standard functions that you might know now we are interested in knowing that if this is what my true relation is can I come up with a network of neurons that can represent this arbitrary function to a certain desired degree of precision okay that's what I want to do okay so let's see how I go about it so what we observe is that such an arbitrary function can be approximated by several Tower functions right so these are what I call as taus ok and I have constructed towers of certain widths and it should be obvious that so this this this sum of all these towers right or this the function that you see here outlined here okay and I can just go on is what my approximation of the original curve is and you can see that this is a slightly poor approximation right I am close at certain points but I'm also far from the curve at certain point so the case in point being this right so at this value of x the actual function tells you what the orange curve tells you what the actual function value should be but my approximation would predict it as this right let me just change the color right so this is what my approximation would predicted as but the actual value is this right so there's a gap right but now interestingly right if I I've used a certain number of towers here right and let me just delete this now if I increase the number of towers okay this as you can read here is a poor approximation as I increase the number of towers right my approximation becomes better and better right now you can see that the tower the sum of the towers that I have right I'm just calling it a sum of towers because I have many of these individual towers and I've just added them they're just all at displaced locations and when I add all of them together as you'll see on the next slide I get this function back right and now when I'm using a large number of towers where each Tower is very narrow then my approximation is becoming better right so this slide as such has nothing to do with neural networks so far right so don't try to connect it to what does this mean in respect to sigmoid neurons perceptrons nothing we're just trying to make a basic observation that any arbitrary function I could I could approximate it by a series of tower functions right and the reason I like this is that see all of these whether I look at this Tower or this Tower or this Tower they're all rectangles right only their bits are only their heights are varying and their position on the x-axis is way right but otherwise they all have the same form which means they are a rectangle right so they are like the same function family so if I know how to construct one Tower I can scale and displace and construct as many towers that I want right so that is something that I like that the building block is very simple it's just a rectangle if I can understand how to make one rectangle then it's just about making smaller bigger rectangles and placing them at different places on the x axis right so that is something that I like about this idea here right and we're going to take it further the other thing is that as I increase the number of towers which eventually will kind of connect back to increasing the number of neurons that you have I can get better and better approximation so that's where the idea that if you give me a particular Epsilon I'll be able to find a network we have not gone there yet right but if you give me a particular Epsilon you can see that at least in this in this pictorial illustration right I can get better and better right I can make the approximation as good and I've stopped the slider here but you can imagine there is still scope of reducing the width of the rectangles in which case my approximation will become even better and better right okay so I'll move ahead from here yeah more the number of such star functions better the approximation okay so now this is what has happened in the previous slide I had this I constructed a tower and I placed it at a certain position on the x-axis and that is where it is I constructed one more Tower it was at a different position one more Tower at a different position so I've just shown you some of the towers in those large number of towers that I had right now I could you could imagine that I would have one more uh right say one more Tower here which would look something like this and it should go up and come here like this right so I've not shown you all the towers that were there on the previous slide but this is what is happening right you are constructing many of these towers independently then just adding them all up to get the main function that you are trying to approximate right and all of these are similar except for their position on the x-axis and for their Heights or lengths as you may see right okay so this is what we have come to so now if I take an input and if I know I have been given an input X if I know if I have a magical box called the tower maker okay so I will make a tower I have as many such boxes as I want right which I'm making these towers okay and then I passed all of these towers so some there's a tower maker right it is taking an input okay and it's just making a tower all of these boxes are identical in what they do the only difference is the amplitude or the height and the position right but otherwise they are all Tower makers if I get all these Tower makers and if I add them up then I can construct my original function plan right that's the idea so what I'm looking for now is this fundamental unit called the tower maker if I have that then I have given you a network which can approximate from given function to any arbitrary degree of precision right this is exactly the cartoon version of the network now you need to quantitize it why is this the cartoon version of the network it takes an input X okay and it gives an output which is f of x which is an approximation of the original output the thinner my Towers the better the approximation would be so I could control that knob now the only thing missing here is what the tower makers are right so I give the input make the towers add them all up and that's what my network is looking like Okay so let's see okay so our job now is to figure out what this Tower maker black box is okay so we'll figure that out so let's start uh yeah let me just see what I want to do on this slide um yeah so I have an input X let me just explain what all is obvious here at the input X okay and I have two parameters W1 B1 W2 B2 so you have been looking at these parameters so these two neurons that I have drawn here are going to be sigmoid neurons so they're going to take an input X and they're going to just simply do 1 over 1 plus e raised to minus um W1 X Plus B1 that is what this neuron is going to output and the other neuron is going to Output 1 over 1 plus e raised to minus W 2x plus B2 right so these are both sigmoid neurons okay now having clarified that now let's see okay now what I want to do I wanted to make a few changes here let's see so first thing I'll do is I'll just make B1 and B 2 0 okay that will simplify some things for me nothing wrong in doing that okay it's a bit hard to do this with the mouse okay so B1 B2 have made it zero okay uh now you can see I can show h11 I can show h12 I have show h21 all of these are hidden right now okay so let me just say uh uh show h11 okay so I'll just show h11 so it's looking like a straight line as I said it's a sigmoid function but surprisingly it's looking like a straight line and it's not hard to see why because oops the formula for the sigmoid function is 1 over 1 plus e raised to minus W 1 X Plus B1 right and now if W1 is 0 right then the X does not matter irrespective of what x is it will give the same value and what is that value going to be 1 over 1 plus e raised to minus B and B is also B1 is also 0 in this case so it's just going to be 0.5 irrespective about the x is hence throughout the x axis the value of the Y function is a 0.5 or the value of H1 is 0.5 it makes sense right now to make it really a sigmoid meaningful sigmoid function I'll have to change W1 make it non-zero and as I make it non-zero you can just see right and as I move from 1 it's still a very uh gentle slope right it's not Steep and as I keep increasing W right the slope becomes steeper and steeper to the point of almost becoming like a step function right as I keep increasing it almost becomes like a step function right so this is how the sigmoid function looks like and if you have a very large the takeaway is that if you have a very large W then you actually recover the step function right you get the step function okay so this is what h11 is looking like now let me take a b two a bit higher so that I can show you the difference between one now let me show h12 again for now it looks like a flat line because my W2 is 0 so let me increase W2 okay and you can again see that it will become closer to the step function right and as I adjust B it will move to the left or the right right that's what the B function does it tells you the point of crossover right from at what point it will start going from 0 to 1 right so that's what the B controls okay and now I want to do um I've defined h21 as h11 minus h12 right that's how I've defined it so let me just do this here so I'll show you h21 oops uh maybe not this way yeah okay so now let me just hide the other ones I'll reset everything and just show you HD one so this is looking like a tar function now right so this is the kind of tower that I wanted it's not a perfect Tower but it's also because my two functions did not become completely step function if I make W's very high then you can see that it will become closer and closer to a tower function right it'll start looking like an Impulse right and I could increase B to increase the width of the track so now it's looking more and more like a tower function so I've got my tower maker what is my tower maker take one sigmoid neuron take another sigmoid neuron set the W is very high right if you set the W is very high then you recover the step function and then what you are essentially doing let me just show that also let me reset and show this one how did you get the tower because you are doing the red function uh minus the blue function right that's what you were doing red minus blue is what you are doing right so now at this point both are zero here both are zero so all of this will remain zero you are doing 0 minus 0 right at this point both are 1 right so again from this point onwards you will get 1 minus 1 which is 0 right and in this region the red curve is one but the blue curve is zero so in this region you will get 1 minus 0 which is one so hence you will get a tower like that right so that's the reason you are getting that topper function right I'll just redraw it right so this is the reason you got the tower function okay so that's why we have set it up it way so now I've got a network which can produce a tower function and not just any network it uses my sigmoid neurons I just have to set the W's to a high value and I can control the B's to control the uh width of this Tower function right so let me just show you that now if I make [Music] um okay now if any if I increase the gap between the bees okay then the width of the Tau function increases if I make it very small of course the length also decreases the height also decreases but the width also decreases I can use the bees to control the uh width of the tower function right so that's that's what is happening here okay so now I have got my tower maker that I was looking for so let's go ahead right so this was a basic function now let's look at the entire network so you have X you have two sigmoid neurons the first one will give you a step function because you set the W to very high the second one will also give you a step function which will give you a w very high then you subtract one from the other right so you have plus one as the weight and minus 1 as the weight so that's the same as h11 minus h12 and that would give you a Tau function right so that's I've come up with a neural network now right which will give you the Tau function and what is h21 what will you use for h21 nothing it's just a linear function right so it's just taking this input it's just taking the uh weighted sum of its input so I'll just call it as w i H 1 I is what it is doing where w i is equal to 1 to 2 and W 1 is equal to 1 and W2 is equal to -1 right so that's all that h21 is doing right just taking the weighted sum of its input and the weights have been hand coded right so now this network has been constructed which can give you a tar function which I can have a neural network which gives me a tar function and now now I'm happy right because that is the tower block is what I was looking for I have my tower Maker Now