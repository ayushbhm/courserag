foreign [Music] observations right so the case two was when you are going to estimate the so now we are looking at what happens if we look at the training observations right so again we are interested in this quantity we cannot estimate it because of this so we have approximated using these three terms and now the first term so now we look at the case when we try to estimate that expectation from the training data right so what do I mean by that again making things clear we are interested in this quantity I cannot estimate it because of this f of x here so I have expressed it as a sum of these three quantities and now the first quantity which was an expectation I am going to empirically estimate it using the training data so using the N training points okay that's what I have put here right so this is the empirical estimation of the error from the training data this is again a small constant and now we have this quantity here now earlier we showed that this quantity becomes 0 because we showed that these two are actually independent and Y were those two independent because Epsilon was y minus f of x right and the other quantity was F of f hat of x minus f of x and we argued that these y's which are actually coming from the test data did not participate in the estimation of hefat hex hence these two quantities were independent of each other right but now we cannot make that argument because why which is coming from the training data now has actually participated in the estimation of f hat X so now these two quantities are not independent hence Epsilon and this this quantity is not independent hence I cannot write this as a product of the expected value of Epsilon multiplied by the expected value of this quantity and then this will not become zero hence the whole term will not become zero right so now when you are trying to estimate it from the training data this term is not disappearing right so now y minus f of x is not independent of F at x minus f of x because the training data was used for estimating the parameters so now these two quantities this expectation cannot be written as this product and hence it will not be zero and hence the empirical train error so now what is the empirical train uh if I had assumed if I had just done this approximation right so if you had asked me hey what is the error of your model and I said okay this is the error I have computed it from the training error I have estimated it empirically from the training data then actually you would have been way off why because this quantity is not zero so if this quantity is high then you are training your estimate would actually have been way off your error would have been the empirical error computed from the training data plus this quantity but what you have reported is only the empirical error computed from the training data you did not report this quantity you don't even know how to compute this quantity right but there is this quantity sitting here which you cannot ignore now in the case of test data you are able to ignore know that hence if you had reported this as the test data then your true error your whatever estimate you got would have been very close to the true error as we showed at the previous slide but if you are going to estimate this from the training data then that is not the case right so that is what we have learned and we had this intuition that training error as computed from the training data this is the empirical training error is actually very over optimistic because it does not consider this other quantity sitting here right and test error is not optimistic because in that case this quantity disappears so whatever you get from the test error is actually very close to the true error that you might get from the expectation but now how is this related to ah model complexity right so I said that there is when you are trying to estimate it from the training error this quantity sits here but then I also said that as your training model complexity increases your training error is overly optimistic what does that mean that as your model complexity increases this quantity is actually very high and hence if you look only at the empirical estimation from the training error then you are being very optimistic because you are ignoring a large quantity but now why does this quantity depend on the model complexity right that is something that we have not seen so that is what we will see next okay so now we will ah try to wrap up the discussion on bias variance by talking about how the true error is actually uh what's the relation between the error and the model complexity right so this was the quantity that was bothering us right this was a quantity which did not go to zero in the case when we're trying to estimate these expectations from the training data and then it ah that meant that if you just compute the empirical training error then you are missing out on things right you are not really ah giving a true picture of the true error right so this was the quantity that was bothering us and now this quantity again I could think of it uh of about am of estimating it empirically right so what do I mean by that I could again think of this as summation I equal to 1 to ah M or n Epsilon i f hat of x i minus f of x i I can think of estimating this quantity empirically okay this is just a comment right just an observation now why this I'm making this point is that uh there's this Lemma called Steins Lemma we can show that uh yeah this should have been sorry n because we had n training samples and this should have been average so what we can show that this quantity which is the empirical estimate of the expectation is actually equal to this quantity ok now this is a joke that I have every year with my students you don't ask me what Stein's Lemma is and I will not ask you what Stein's Lemma is but we just for this discussion we just need to take it for granted that Stein Slimmer says that this expectation that you are interested in is actually equal to this quantity right now if you take this on face value if you just assume that time slamma is good is correct then from here on let's see what answers do we get right where do we reach if we assume that this is actually correct and this is actually correct because Stein's Lima has proven this right ah okay so now if this is indeed correct then what is happening here right so when will this quantity that you have here this quantity right which is the same as this quantity that you have when will that be high right this is a derivative so what does that mean that if you have a small change in the observation right if there is a small change ah in the observation then it causes a large change in the estimation right so if you are y i right so you are given this x i comma y i pairs right and now if there was a small suppose you are given the x i s ten and the Y I was 20 right and instead of that now if the X I was 10 and the Y high was 20.1 this is small change right what this says is that if there is a small change in y i there is a large change in the estimated value right so you are now your training data has changed instead of this you had this point and now ideally you would expect that whatever F hat X you got right that means whatever parameters you estimated that shouldn't change much because there is a very small change in the data right but when would this quantity be high if this quantity is high it means that if you have even a small change in y i you are seeing a very large change in your F hat which means you are changing a very large change in the W's that you have estimated right and this is not good right this is definitely not good because for this much small change you don't want the model to vary a lot right and for what kind of models we saw that this happens that a small change in the data could lead to a large change in the F hat x that you got this is what we saw in the case of when we were discussing the first example of simple and complex models so when we had the simple model of Y is equal to W and X plus W naught and then when we trained it we had a total of 500 training samples and then when we trained it using 30 different samples this line did not change much right that means my f hat X did not change much right but the same experiment then when we repeated it with a complex model right which was X raised to 25 all the way up to W naught we saw that when we take these 30 different samples which is the same as saying that I am changing the training data right and when I change the training data my estimation was changing a lot in the case of the complex models right so hence this quantity you can say is going to be high for complex models as compared to simple models right so that's that's what ah we are concluding from these two points right that the expected quantity that we were interested in in which does not go to 0 in the case of training data is actually equal to this quantity and now we see that this quantity has this sensitivity term which says that if the small change in y i what happens to the change in F hat and we are making this observation that this quantity would be high for complex models because a small change in y I was causing a lot of changes in the estimated value of f hat X or the estimated values of the parameters whereas for simple models it was not causing a lot of change right so connecting these observations now we can say that this quantity that you see here is actually going to be high for complex models as compared to simple models right ah in complex model would be more changes so just to ah so what does that eventually mean right so on the previous slide we had that true error is equal to empirical train error plus a small constant plus some term and now we are seeing that this term is actually proportional to model complexity or it is a function of model complexity so I am just going to write it as Omega Model complexity right so just to indicate that this is actually dependent on the model complexity so higher the model complexity larger this term would be that means your true error would be farther away from the empirical trainer so you would estimate the train error and tell me hey it's almost zero but I will not believe you because you have not told me anything about this quantity you have only estimated the train error which is given by summation I equal to 1 to n y i minus y hat I the whole Square on the training data right you have not given me anything about this so even if you tell me that hey your training error is 0 I will not believe you because this quantity has not been accounted for and I know that higher the model complexity this value would be high that means more complex your model more farther away would your empirical estimate of the training error be from the true error right and just to make sure that this indeed happens that complex models are actually more sensitive to changes in the training data as opposed to simple models let's just take a look at that right let's just try to see that yeah so now what I have taken this is some training data that was given to me and I estimated the true model I estimated the complex model which is the red colored model and the simple model which was the green color model now what I've done is uh I have changed one of these data points only one of the data points right I have instead of taking X comma y I have taken this x comma y right and now you can see that my simple model has not changed much right so my f hat of x so I changed y but my f hat of X did not change much that means this quantity is going to be small right but for the complex model my my estimation has changed quite a bit right so now when I changed a y by a small quantity my f hat X has also changed dramatically right in this region it's very different uh from what it was originally right so hence for complex models small changes in the training data could cause a large change in the estimated function right the F at X or the estimated values of the parameters right so that is just a empirical demonstration or an anecdotal demonstration of this idea that complex models are more sensitive to changes in the training data and that is the quantity that bothers us so thus quantity uh maybe I should go to the next slide I'll go to the next slide okay I can just do this slide first right okay so hence while training instead of minimizing the train error we should minimize this error right so now if you're going to just focus on minimizing the train error okay you might make it zero okay so now what is the what is your true error was the expected value on the right hand side is actually equal to the train error so this is what my train error is right my train error is I equal to 1 to n sorry I keep changing M and M but I assure you understand what I mean this is what L train is and what I was trying to do is I was just trying to minimize this but now I've realized that if I try to minimize this and make it to zero there's this another quantity which is dependent on the model complexity which keeps increasing so in effect my expected error is increasing right so I might have done a great job of reducing this to zero but the cost of increasing this significantly and then the net effect is as my model is still bad when I am going to pass it test instances which were not seen during training this expectation is going to be this error is going to be still very high right so that's why you should not try to just minimize the train error but train error plus the model complexity is what you should try to minimize now how do you encode this model complexity drive just Define a generic function which says that this captures the model complexity now how do you define model complexity something that we will see as we go along but the main idea here is that you should not just try to minimize this error but you should also try to minimize the model complexity and yeah this Omega Theta would be high for complex models and small for simple models so if you have complex models this quantity would be high and hence your the loss that you are trying to minimize is still going to be high so it will not allow you to make the models very complex right and this acts as an approximation for the quantity that we just saw right so this is this is the model complexity term that we had seen and now you have to come up with some functions which act as an approximation of this and when you are trying to reduce this it actually decreases the model complexity which means it will decrease this quantity which means it will decrease this sum right so you should come up with omegas which decrease the model complexity because if the model complexity decreases this term will decrease and hence this term will also decrease right so that's the way you are going to handle this complex term here because directly handling it make looks difficult but you could Define some omega thetas such that you are sure that if you minimize that Omega Theta your model complexity is going to be reduced and hence you will be sure that this term that was bothering you is going to be reduced right so this actually is the basis for all regularization methods right so this is what you do in regularization instead of just minimizing L train Theta which is the empirical training error you actually add some regularization term to that and then you see that I am going to minimize the empirical train error as well as the regularization error because that will make sure that my expected error which is what I was interested in is actually going to be small right and the pictorial view of this this is the same so this is what would happen if I were to just look at the training error right so this is L train which is computed just using those n training samples then my as I keep increasing the model complexity this will keep decreasing but what will happen is as I increase the model complexity this quantity is going to increase and hence my error is actually going to be high right so I need to find this sweet spot and this Omega Theta should ensure that this model model complexity is reasonable so that you have low training error but at the same time this this quantity is also not high and hence your expected error is also not high right that is why we use regularization now why do we care about regularization in the context of deep learning right why am I teaching you this in the context of deep learning so the answer to that is simple right so deep learning we said that regularization is basically trying to control for model complexity and in deep neural networks you know that these are highly complex models why are they highly complex because they have many layers many non-linearities and many parameters so they are actually they can completely overfit right because this we also know ah from the universal approximation theorem that you can completely overfit your training data right you can get arbitrary close to your true function if you keep adding layers or if you keep adding neurons right and you don't want to do that right you don't want to become very close to your training data because then your test error would be high and that why that is why you do not want to overfit because if you leave if you design a very deep neural network with many neurons many parameters then it will be able to drive the training error to zero but while doing so you have increased the model complexity by adding many parameters and many layers and that is going to give you poor generalization ah capability or poor or a higher test error that's why you add regularization so that you control for model complexity now what are these different regularization methods that you can use in the context of dipline learning is something that we'll see in the next lecture thank you so I'll end it here and I'll see you in the next lecture