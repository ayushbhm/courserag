[Music] so while all this is happening great we're getting very good good models big models trained on very large amounts of data very good performance on very complex tasks like generation but there are also calls for sanity right we also need to be sure that whatever we are doing is interpretable fair responsible green right and these terms which are not uh so popular like 10 15 years back i mean they were just maybe being discussed on the sidelines now have become quite mainstream right because of these rapid advancements that we are making people are realizing hey with all this happening we need to also be a bit conscious about the implications of what we are building right and that's what this section would be about right so i'll start with the calls for sanity right so so this is the paradox of deep learning right so why does deep learning work so well despite high capacity these are very large models and when you start do a basic course on machine learning you will the first thing that you learn is that hey if you have very complex models then they are susceptible to overfitting they'll completely learn the training data but they'll not perform well untested right but this is not what is happening for deep learning models right they are high capacity but they are still somehow being able to overcome this problem of overfitting right to a certain extent some cases they do overfit then there is this numerical instability i spoke about this vanishing exploding gradients and even gans and diffusion based models they are often hard to uh not diffusion-based models but at least gans are often very hard to train and get unstable during uh training right then they have this sharp minima right because this uh the uh the the loss function that you have in deep data networks is not a good convex loss function right so it has a many multiple local minima and some of these are very sharp and then it leads to certain kind of challenges during our training despite all this deep learning works quite well in practice right it's also not very robust right so this is an example where you have the image of a stop symbol and now you just put some stickers on it right and for a human it does not matter right it's still the stop signal right but now uh the machine is reading it as a speed limit of 45 right so now it's like completely different from what the intended meaning here was right so it's not very robust in practice right so we still don't know right why this ah this happens right because it looks like it should not work it is also not very robust but still in very many real world applications we are seeing it perform very well and give deliver results right so we still don't know why this happens uh and there are many such examples right so so this is again another example right so people have been able to come up with images which can fool deep neural networks right so i just i mean scientists have come up with these regular patterns right but now when you feed this to a deep learning based image classifier it thinks that the first image is of a king penguin right then you could just uh let your imagination run a build wit and realize why it could be thinking that right i mean there are some patterns here similarly the second image it classifies as starfish right so these these challenges do exist but still uh that has deep learning is the current popular paradigm and delivering good results in various fields right so now while there are no clear answers yet but slowly and steadily there is an increasing emphasis on explainability it's not just good that you tell me your model gives me a good accuracy you also need to tell me why i am getting this accuracy right so that then i have a better grasp of these kind of issues that why is it seeing a speed limit fortify just because i put some stickers here and there right so these these uh things are uh coming into focus and a lot of research is happening on uh the theoretical justifications for why deep learning works why the uh research on explainability of these models how do you explain the performance of a deep learning model and so on right and hopefully all of this will bring some sanity right so here are a few advances in that direction right so there's now a workshop on human interpretability in machine learning which started in 2016. uh there's this clever hands uh toolkit which was developed right so so clever hands was this horse right and i think in the uh last century where um so so the horse had an owner and then he used to do the street show where uh where someone in the audience would ask it uh arithmetic question right what is 5 plus 2 right and the way the horse would answer is i think by nodding or by tapping its feet and it would kind of stop right so 5 plus two is seven so peop i mean during the shows it would do it seven times right and then people thought hey this horse is able to actually compute the answer and so on right but that was not the case what people later on realize is that everyone in the audience was giving it cues naturally they're cheering for it right so when the when when it nodded seven times they would all like erupt in say in an applause or something and the horse would take that as a signal oh i should stop now and it would get the right answer right so it's this thing where the answer is correct but there is no reason for the answer to be correct right so that's what was happening in the case of clever hands and like based on that anecdote or that incident right this clever hands tool kit was designed to again look at similar problems uh why do you get the answers uh right uh instead of just saying that oh i have got the answer right right so this toolkit allows uh machine learning systems uh to be tested on adversarial examples right so i showed you a few examples of examples on the previous slide where some stickers were added to the sign or some random patterns were generated and now uh the models this this repository allows you to benchmark the models on such adversarial examples and if they don't do well then you know that they're not really learning to solve the problem but are somewhat like clever hands where they are relying on some other cues to arrive at the right answers without really understanding what is the question being asked right and then there are now several workshops on blackbox nlp where the focus is on analyzing and interpreting the neural networks which are used for nlp and there's this nice uh textbook interpretable machine learning a guide for making black box models explainable right so the reason i'm bringing up all this is that this was not so much in focus a few years back right but now this has become quite mainstream and a lot of you who are looking for interesting research problems to solve in this space this is a very active area that you could pursue the second is about fairness and responsibility right so again if you look at a brief history around 2011 there are hardly any papers talking about fairness or responsible ai and so on and by 2017 when deep learning had almost taken over completely taken over nlp speech and vision there was a massive increase in the number of papers talking about fairness and responsible ai and that has just grown in the last has grown further in the last three to four years right and i'll tell you what fairness and responsibility i why are we worrying about this problem right so look at this uh image right so this was passed through a facial recognition kind of a software right which looks at images and try to make certain judgment about the people right and the first image it has classified as low risk so this software was trying to predict whether uh the people in the images being shown could become criminals in the future or might commit some crime in the future right so the first person it's showing it's low risk on a scale of one to ten just three whereas for the second person it's showing high risk on a scale of one to ten it's eight right but now if you look at the past record of these people then the person in the first image has two armed robberies one attempted armed robbery and then after that again there was one grand theft right whereas the other person who has been marked as high risk had four uh juvenile offenses right or misdiminis and then after that there were no offenses right but what is happening here right is that since the second person is black and the model has a certain bias towards black people it is recommending or it is uh falsely judging right or in a very biased way uh judging that this person has a higher chance of becoming a criminal in the future right so it's biased against blacks it's not fair to black set and this is a repeating theme in various ai models where they are biased based on the biases that they see in the training data maybe this model saw a lot of images during training of black people who were criminals maybe that's the bias in the data that you had and most white people images that it show they were not criminals right and that bias then gets reflected in the outputs that the model generates uh when deployed in real-world situations right uh similar studies were done on facial recognition software and it was found people took the uh facial recognition systems from deep tech companies right microsoft ibm and so on and they found that if you are a white male then the facial recognition accuracy is very high right this corresponds to uh the second last column right so light no sorry the third last column which is lighter male right you are a light skinned or a white skinned male then all of these are giving you close to 100 accuracy if you are light skinned female then the accuracy decreases and the worst accuracy is for darker females which is the column in which you see the red bars and it's like atrociously low right so again this shows that the model has not really been trained for black women and hence it's not able to do a good job of recognizing faces of black right and the huge gap between the performance of the best category uh or the best performing category right versus the category for which the model has the worst performance and then because of these things right because this is not fair this is biased many civil rights and other research groups have been writing to prominent people this is a letter to jeff bezos demanding that it should start providing facial recognition technology governments because this technology is giving you biased outputs and then the government might take actions based on those biased outputs for example the criminal classification which example which i showed earlier right and now uh around 2020 ibm amazon microsoft all of them have stopped selling uh facial recognition technology they have taken a public stand that will no longer be supporting facial recognition technology for a police purpose for supplying it to police or even other government agencies right again like i spoke about daily 2 this is a recent example now if you ask it to generate images of success you can see that most of the images are of males if you ask it to generate images about sadness most of the females are about most of the images are about females right so this kind of gender bias or bias towards certain sections of the population is very common in these most deep learning models because of the bias and the training data that they have been trained with right and this again happens in various domains right so now if you use an ai model to decide whether you should give loan to someone or not again it would be biased towards certain sections of the society because it would be the case that in its training data there were certain sections which did not often pay their loans back right so it's biased for example against women migrants or people of color right which is not fair right i mean you have to judge each case individually and not based on similar uh things that you have seen in the past right so there's and definitely not like two people who have the same profile but only differ in their terms of their skin color you cannot have a different decision for them right so that's what is happening in many of these models hence this calls for being fair and responsible and there is again a challenge right so now again people are becoming aware of these and trying to push research in these areas so stanford has this ai audit challenge where the idea is to build models which are compliant and do not do any illegal discrimination okay so while we are talking about fairness and responsible ai there is also another axis around which we need to talk about being responsible right so i was talking about the human brain right so it has 10 is to 15 synapses but it only consumes 15 watts of power right whereas if you look at these current deep learning models i showed you one model which was trained on 2048 tpus which is a huge amount of power and compute right and in the period from 2012 to 2018 there has been a 300 000 folds increase in the amount of uh compute compute that is being used for training these models and this has been doubling every few months right and you can see uh in the in the shaded portion here right that in the past few years there's a rapid increase in the size of these models or the uh computes that you're using uh for training these models right and that that has to stop right because you are being irresponsible in some sense and this should give it make it clear so if you have like a take an air travel from new york city to san francisco then this is the carbon footprint for one passenger right which you see on the extreme left-hand side and average uh human in one year uh does the amount of co2 emission that it does is marked as 11 right whereas if you look at the amount of co2 emission that happens for training a transformer based ai model just unimaginable right i mean you have the the entire just the graph before it is the amount of co2 emission in the full lifetime of a car right the entire lifetime of a car this is the amount of co2 emission that it does and what you get by training one single transformer right it's just like at least five times that right and this is again irresponsible and that's it again we are talking about climate change we are talking about uh being responsible citizens of the earth and if you train such models there's clearly a problem there right so there's a lot of push for green ai now to make models smaller and smaller and also these issues mix right so this is also now uh not just a problem of environment it's also a problem of being responsible it's also a problem of being fair right i'll just read out this quote from this famous paper you can read google a bit about this paper so is it fair that the residence of maldives right which is likely to be underwater by 2100 or the 800 000 people in sudan are affected by drastic floods who are affected by drastic floods is it fair that they pay the environmental price of training and deploying even larger english language models right so all a lot of this is english centric work and a lot of computers being used for training english models so is it fair that people living in maldives or sudan are paying the environmental price for training these models when similar models are not being produced for their own languages right so then what is the price they are paying for it's a very valid question to think about and again this is a very active area of research not trying people are trying to rethink about hardware itself to make the compute uh much more efficient right uh so that was slightly on a somber note right it was saying as we started off well we spoke about all the successes of deep learning but then we had this somber note on hey we should be responsible fair talk about green ai talk about having sanity in what we are doing just not put out models but also try to explain why they work and so on so i didn't want to end on that somber note so i'll again talk about some exciting times right so the ai revolution is now uh driving scientific research also and let me tell you what i mean by that this is the uh protein folding uh uh problem right and deep mine in 2020 released a model for predicting the protein structure of sorry the the three structure of proteins right so what do i mean by that right so we uh every protein right is made up of a sequence of amino acids bonded together right and we know these amino acids this sequence for all the proteins that are there in the body right but now this is just like a i would say a illustrative representation or just a linear representation of what the protein looks like but that's not the reality right these amino acids they actually interact with each other and based on these interactions the protein starts taking certain shapes and it actually has this 3d structure right now knowing this 3d structure is important because if you know this 3d structure then it helps in drug design because if you develop a structure which is compatible with this 3d structure then it would lead to effective drug delivery for example and a lot of you could relate to this in terms of this covet pandemic right so we heard that this virus has this spike protein right which has a certain crown like shape and because of that it is able to attach on certain things well it's a similar analogy here that you have a certain structure a 3d structure it has a certain shape if you know what that shape is then you could design things which could attach or be compatible with that structure and that helps in drug design right so this structure 3d structure is not really known for all the proteins in the body right and now what uh deepmind has been able to do with its alpha fold model that it has been able to get a significant increase in the uh accuracy with which we are being able to predict these 3d structures right of course the problem is nowhere close to being solved but there is a remarkable increase in where we were like just in 2019 versus where we are in 2022 today right so that's exciting and maybe it would take a while for things to uh because as we have again realized during the pandemic that things move very different in at a very different pace in the scientific field of medicine and so on where there are a lot of regulations and a lot of other things need to fall in place but at least the science is getting pushed and that might show soon show effects in actual applications right yeah similarly in astronomy right so there is an interest that you have a certain very i'll just try to explain this at a very high level right because in fact even i understand at a very high level so suppose you have a certain galaxy right and you're interested in how it will age or how it will look when it ages now this problem people have tried for human faces right they have been able to generate these images of how you will look like say 5 years from now 10 years from now 100 years old now can you apply the same at the galaxy scale and try to predict about how the galaxy would look right and now if you can predict that then if you can generate that image right then you then scientists can make certain claims about what is going to happen there right is that star becoming older or what exactly are some of the physical phenomena that might be linked because of which this image is getting generated right so this is again a very different way of doing astronomy which is not being thought about or popular a few years back right okay uh similar uh stuff has happened uh for uh uh for finding uh fundamental variables right which are hidden in experimental data right so i'll not try to explain this i'll just i don't think i have time to play this video also but there's a very interesting video here i would recommend that you see the first five minutes of this video to understand uh what this is about right okay now the last section that i want to talk about is efficient deep learning so we are living in the era of mobile phones right and the number of smartphone subscriptions has increased at least in the context of a country like india there is increasing mobile penetration in the rural areas right what that means is that now you should start thinking about how do you serve this population which might not have computers right but they have access to mobile phones so can you build ai models which run for this particular form factor right and people have this uh good connectivity also so now there is increasing uh emphasis on designing models which can run on the edge right so if you have a drone can you which would have limited memory limited compute can you take your ai models and make them run on these devices right so that's another push that we have and a lot of things are being done to address these constraints of power storage you want real-time output so you cannot have like you submit a sentence and the result comes back after one or two seconds you want it to be in milliseconds right and what if you have drones which are not connected to internet right they're just stand alone can you do computations on the devices a lot of research is also happening in making these large models work for small devices so i do how do you take these multi-billion parameter models and make them smaller while not compromising on the accuracy right so this is all i had so i'll just stop here there are a few reference material here that you could look at but i'm done with the lecture i hope you enjoyed the history of course there are many other things which i might not have covered in particular i have not talked much about the advances in things like text to speech voice conversion even automatic speech recognition but the idea was to give you a very high view of how the field has evolved from early neural networks to recurrent networks to transform the base models gans diffusion-based models and so on so with that i'll end this lecture thank you