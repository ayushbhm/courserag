[
  {
    "text": "foreign",
    "start": 0.299,
    "duration": 3.0
  },
  {
    "text": "[Music]",
    "start": 6.62,
    "duration": 14.89
  },
  {
    "text": "observations right so the case two was",
    "start": 21.619,
    "duration": 5.98
  },
  {
    "text": "when you are going to estimate the",
    "start": 24.6,
    "duration": 5.339
  },
  {
    "text": "so now we are looking at what happens if",
    "start": 27.599,
    "duration": 3.901
  },
  {
    "text": "we look at the training observations",
    "start": 29.939,
    "duration": 3.78
  },
  {
    "text": "right so again we are interested in this",
    "start": 31.5,
    "duration": 4.44
  },
  {
    "text": "quantity we cannot estimate it because",
    "start": 33.719,
    "duration": 4.68
  },
  {
    "text": "of this so we have approximated using",
    "start": 35.94,
    "duration": 6.92
  },
  {
    "text": "these three terms and now the first term",
    "start": 38.399,
    "duration": 4.461
  },
  {
    "text": "so now we look at the case when we try",
    "start": 44.219,
    "duration": 4.081
  },
  {
    "text": "to estimate that expectation from the",
    "start": 46.02,
    "duration": 3.9
  },
  {
    "text": "training data right so what do I mean by",
    "start": 48.3,
    "duration": 4.32
  },
  {
    "text": "that again making things clear",
    "start": 49.92,
    "duration": 4.44
  },
  {
    "text": "we are interested in this quantity I",
    "start": 52.62,
    "duration": 3.419
  },
  {
    "text": "cannot estimate it because of this f of",
    "start": 54.36,
    "duration": 3.48
  },
  {
    "text": "x here so I have expressed it as a sum",
    "start": 56.039,
    "duration": 4.081
  },
  {
    "text": "of these three quantities and now the",
    "start": 57.84,
    "duration": 4.62
  },
  {
    "text": "first quantity which was an expectation",
    "start": 60.12,
    "duration": 4.14
  },
  {
    "text": "I am going to empirically estimate it",
    "start": 62.46,
    "duration": 3.6
  },
  {
    "text": "using the training data so using the N",
    "start": 64.26,
    "duration": 3.78
  },
  {
    "text": "training points okay that's what I have",
    "start": 66.06,
    "duration": 3.96
  },
  {
    "text": "put here right so this is the empirical",
    "start": 68.04,
    "duration": 3.54
  },
  {
    "text": "estimation of the error from the",
    "start": 70.02,
    "duration": 3.72
  },
  {
    "text": "training data this is again a small",
    "start": 71.58,
    "duration": 4.92
  },
  {
    "text": "constant and now we have this quantity",
    "start": 73.74,
    "duration": 3.72
  },
  {
    "text": "here",
    "start": 76.5,
    "duration": 4.08
  },
  {
    "text": "now earlier we showed that this quantity",
    "start": 77.46,
    "duration": 5.58
  },
  {
    "text": "becomes 0 because we showed that these",
    "start": 80.58,
    "duration": 4.26
  },
  {
    "text": "two are actually independent and Y were",
    "start": 83.04,
    "duration": 3.66
  },
  {
    "text": "those two independent because Epsilon",
    "start": 84.84,
    "duration": 5.76
  },
  {
    "text": "was y minus f of x right and the other",
    "start": 86.7,
    "duration": 6.72
  },
  {
    "text": "quantity was F of f hat of x minus f of",
    "start": 90.6,
    "duration": 6.3
  },
  {
    "text": "x and we argued that these y's which are",
    "start": 93.42,
    "duration": 5.46
  },
  {
    "text": "actually coming from the test data did",
    "start": 96.9,
    "duration": 3.66
  },
  {
    "text": "not participate in the estimation of",
    "start": 98.88,
    "duration": 3.9
  },
  {
    "text": "hefat hex hence these two quantities",
    "start": 100.56,
    "duration": 4.559
  },
  {
    "text": "were independent of each other right but",
    "start": 102.78,
    "duration": 5.22
  },
  {
    "text": "now we cannot make that argument because",
    "start": 105.119,
    "duration": 4.441
  },
  {
    "text": "why",
    "start": 108.0,
    "duration": 3.84
  },
  {
    "text": "which is coming from the training data",
    "start": 109.56,
    "duration": 4.32
  },
  {
    "text": "now has actually participated in the",
    "start": 111.84,
    "duration": 4.44
  },
  {
    "text": "estimation of f hat X so now these two",
    "start": 113.88,
    "duration": 4.26
  },
  {
    "text": "quantities are not independent hence",
    "start": 116.28,
    "duration": 3.78
  },
  {
    "text": "Epsilon and this this quantity is not",
    "start": 118.14,
    "duration": 5.1
  },
  {
    "text": "independent hence I cannot write this as",
    "start": 120.06,
    "duration": 6.3
  },
  {
    "text": "a product of the expected value of",
    "start": 123.24,
    "duration": 5.46
  },
  {
    "text": "Epsilon multiplied by the expected value",
    "start": 126.36,
    "duration": 5.22
  },
  {
    "text": "of this quantity and then this will not",
    "start": 128.7,
    "duration": 4.5
  },
  {
    "text": "become zero hence the whole term will",
    "start": 131.58,
    "duration": 3.6
  },
  {
    "text": "not become zero right so now when you",
    "start": 133.2,
    "duration": 3.96
  },
  {
    "text": "are trying to estimate it from the",
    "start": 135.18,
    "duration": 4.62
  },
  {
    "text": "training data this term is not",
    "start": 137.16,
    "duration": 5.48
  },
  {
    "text": "disappearing",
    "start": 139.8,
    "duration": 2.84
  },
  {
    "text": "right so now y minus f of x is not",
    "start": 143.16,
    "duration": 4.32
  },
  {
    "text": "independent of F at x minus f of x",
    "start": 145.62,
    "duration": 3.36
  },
  {
    "text": "because the training data was used for",
    "start": 147.48,
    "duration": 3.78
  },
  {
    "text": "estimating the parameters so now these",
    "start": 148.98,
    "duration": 4.74
  },
  {
    "text": "two quantities this expectation cannot",
    "start": 151.26,
    "duration": 4.14
  },
  {
    "text": "be written as this product and hence it",
    "start": 153.72,
    "duration": 4.14
  },
  {
    "text": "will not be zero and hence the empirical",
    "start": 155.4,
    "duration": 5.16
  },
  {
    "text": "train error so now what is the empirical",
    "start": 157.86,
    "duration": 5.159
  },
  {
    "text": "train uh if I had assumed if I had just",
    "start": 160.56,
    "duration": 4.44
  },
  {
    "text": "done this approximation right so if you",
    "start": 163.019,
    "duration": 4.621
  },
  {
    "text": "had asked me hey what is the error of",
    "start": 165.0,
    "duration": 4.68
  },
  {
    "text": "your model and I said okay this is the",
    "start": 167.64,
    "duration": 3.3
  },
  {
    "text": "error I have computed it from the",
    "start": 169.68,
    "duration": 2.88
  },
  {
    "text": "training error I have estimated it",
    "start": 170.94,
    "duration": 3.72
  },
  {
    "text": "empirically from the training data then",
    "start": 172.56,
    "duration": 4.2
  },
  {
    "text": "actually you would have been way off why",
    "start": 174.66,
    "duration": 4.98
  },
  {
    "text": "because this quantity is not zero so if",
    "start": 176.76,
    "duration": 5.22
  },
  {
    "text": "this quantity is high then you are",
    "start": 179.64,
    "duration": 4.26
  },
  {
    "text": "training your estimate would actually",
    "start": 181.98,
    "duration": 3.78
  },
  {
    "text": "have been way off your error would have",
    "start": 183.9,
    "duration": 3.839
  },
  {
    "text": "been the empirical error computed from",
    "start": 185.76,
    "duration": 4.44
  },
  {
    "text": "the training data plus this quantity but",
    "start": 187.739,
    "duration": 4.08
  },
  {
    "text": "what you have reported is only the",
    "start": 190.2,
    "duration": 3.42
  },
  {
    "text": "empirical error computed from the",
    "start": 191.819,
    "duration": 3.361
  },
  {
    "text": "training data you did not report this",
    "start": 193.62,
    "duration": 2.759
  },
  {
    "text": "quantity you don't even know how to",
    "start": 195.18,
    "duration": 3.0
  },
  {
    "text": "compute this quantity right but there is",
    "start": 196.379,
    "duration": 3.661
  },
  {
    "text": "this quantity sitting here which you",
    "start": 198.18,
    "duration": 3.839
  },
  {
    "text": "cannot ignore now in the case of test",
    "start": 200.04,
    "duration": 3.779
  },
  {
    "text": "data you are able to ignore know that",
    "start": 202.019,
    "duration": 5.22
  },
  {
    "text": "hence if you had reported this as the",
    "start": 203.819,
    "duration": 7.021
  },
  {
    "text": "test data then your true error your",
    "start": 207.239,
    "duration": 5.28
  },
  {
    "text": "whatever estimate you got would have",
    "start": 210.84,
    "duration": 3.3
  },
  {
    "text": "been very close to the true error as we",
    "start": 212.519,
    "duration": 3.36
  },
  {
    "text": "showed at the previous slide but if you",
    "start": 214.14,
    "duration": 2.879
  },
  {
    "text": "are going to estimate this from the",
    "start": 215.879,
    "duration": 3.541
  },
  {
    "text": "training data then that is not the case",
    "start": 217.019,
    "duration": 4.201
  },
  {
    "text": "right so that is what we have learned",
    "start": 219.42,
    "duration": 3.78
  },
  {
    "text": "and we had this intuition that training",
    "start": 221.22,
    "duration": 4.019
  },
  {
    "text": "error as computed from the training data",
    "start": 223.2,
    "duration": 4.38
  },
  {
    "text": "this is the empirical training error is",
    "start": 225.239,
    "duration": 4.681
  },
  {
    "text": "actually very over optimistic because it",
    "start": 227.58,
    "duration": 4.079
  },
  {
    "text": "does not consider this other quantity",
    "start": 229.92,
    "duration": 4.5
  },
  {
    "text": "sitting here right and test error is not",
    "start": 231.659,
    "duration": 4.8
  },
  {
    "text": "optimistic because in that case this",
    "start": 234.42,
    "duration": 4.26
  },
  {
    "text": "quantity disappears so whatever you get",
    "start": 236.459,
    "duration": 3.661
  },
  {
    "text": "from the test error is actually very",
    "start": 238.68,
    "duration": 3.479
  },
  {
    "text": "close to the true error that you might",
    "start": 240.12,
    "duration": 5.1
  },
  {
    "text": "get from the expectation",
    "start": 242.159,
    "duration": 5.761
  },
  {
    "text": "but now how is this related to ah model",
    "start": 245.22,
    "duration": 5.76
  },
  {
    "text": "complexity right so I said that there is",
    "start": 247.92,
    "duration": 4.319
  },
  {
    "text": "when you are trying to estimate it from",
    "start": 250.98,
    "duration": 3.06
  },
  {
    "text": "the training error this quantity sits",
    "start": 252.239,
    "duration": 4.441
  },
  {
    "text": "here but then I also said that as your",
    "start": 254.04,
    "duration": 5.759
  },
  {
    "text": "training model complexity increases your",
    "start": 256.68,
    "duration": 5.22
  },
  {
    "text": "training error is overly optimistic what",
    "start": 259.799,
    "duration": 4.021
  },
  {
    "text": "does that mean that as your model",
    "start": 261.9,
    "duration": 3.96
  },
  {
    "text": "complexity increases this quantity is",
    "start": 263.82,
    "duration": 4.2
  },
  {
    "text": "actually very high and hence if you look",
    "start": 265.86,
    "duration": 4.02
  },
  {
    "text": "only at the empirical estimation from",
    "start": 268.02,
    "duration": 3.48
  },
  {
    "text": "the training error then you are being",
    "start": 269.88,
    "duration": 3.3
  },
  {
    "text": "very optimistic because you are ignoring",
    "start": 271.5,
    "duration": 4.08
  },
  {
    "text": "a large quantity but now why does this",
    "start": 273.18,
    "duration": 4.62
  },
  {
    "text": "quantity depend on the model complexity",
    "start": 275.58,
    "duration": 3.54
  },
  {
    "text": "right that is something that we have not",
    "start": 277.8,
    "duration": 3.6
  },
  {
    "text": "seen so that is what we will see next",
    "start": 279.12,
    "duration": 6.299
  },
  {
    "text": "okay so now we will ah try to wrap up",
    "start": 281.4,
    "duration": 5.7
  },
  {
    "text": "the discussion on bias variance by",
    "start": 285.419,
    "duration": 3.84
  },
  {
    "text": "talking about how the true error is",
    "start": 287.1,
    "duration": 4.26
  },
  {
    "text": "actually uh what's the relation between",
    "start": 289.259,
    "duration": 4.261
  },
  {
    "text": "the error and the model complexity right",
    "start": 291.36,
    "duration": 4.32
  },
  {
    "text": "so this was the quantity that was",
    "start": 293.52,
    "duration": 4.02
  },
  {
    "text": "bothering us right this was a quantity",
    "start": 295.68,
    "duration": 4.56
  },
  {
    "text": "which did not go to zero in the case",
    "start": 297.54,
    "duration": 4.379
  },
  {
    "text": "when we're trying to estimate these",
    "start": 300.24,
    "duration": 4.56
  },
  {
    "text": "expectations from the training data and",
    "start": 301.919,
    "duration": 6.0
  },
  {
    "text": "then it ah that meant that if you just",
    "start": 304.8,
    "duration": 5.459
  },
  {
    "text": "compute the empirical training error",
    "start": 307.919,
    "duration": 4.381
  },
  {
    "text": "then you are missing out on things right",
    "start": 310.259,
    "duration": 5.16
  },
  {
    "text": "you are not really ah giving a true",
    "start": 312.3,
    "duration": 4.74
  },
  {
    "text": "picture of the true error right so this",
    "start": 315.419,
    "duration": 3.241
  },
  {
    "text": "was the quantity that was bothering us",
    "start": 317.04,
    "duration": 4.32
  },
  {
    "text": "and now this quantity again I could",
    "start": 318.66,
    "duration": 6.84
  },
  {
    "text": "think of it uh of about am of estimating",
    "start": 321.36,
    "duration": 5.82
  },
  {
    "text": "it empirically right so what do I mean",
    "start": 325.5,
    "duration": 3.979
  },
  {
    "text": "by that I could again think of this as",
    "start": 327.18,
    "duration": 7.22
  },
  {
    "text": "summation I equal to 1 to ah M or n",
    "start": 329.479,
    "duration": 8.981
  },
  {
    "text": "Epsilon i f hat of x i",
    "start": 334.4,
    "duration": 6.76
  },
  {
    "text": "minus f of x i",
    "start": 338.46,
    "duration": 4.92
  },
  {
    "text": "I can think of estimating this quantity",
    "start": 341.16,
    "duration": 4.2
  },
  {
    "text": "empirically okay this is just a comment",
    "start": 343.38,
    "duration": 4.5
  },
  {
    "text": "right just an observation now why this",
    "start": 345.36,
    "duration": 5.64
  },
  {
    "text": "I'm making this point is that uh",
    "start": 347.88,
    "duration": 6.3
  },
  {
    "text": "there's this Lemma called Steins Lemma",
    "start": 351.0,
    "duration": 5.699
  },
  {
    "text": "we can show that uh yeah this should",
    "start": 354.18,
    "duration": 5.1
  },
  {
    "text": "have been sorry",
    "start": 356.699,
    "duration": 4.981
  },
  {
    "text": "n because we had n training samples and",
    "start": 359.28,
    "duration": 4.38
  },
  {
    "text": "this should have been average so what we",
    "start": 361.68,
    "duration": 4.019
  },
  {
    "text": "can show that this quantity which is the",
    "start": 363.66,
    "duration": 5.039
  },
  {
    "text": "empirical estimate of the expectation is",
    "start": 365.699,
    "duration": 6.661
  },
  {
    "text": "actually equal to this quantity ok now",
    "start": 368.699,
    "duration": 5.461
  },
  {
    "text": "this is a joke that I have every year",
    "start": 372.36,
    "duration": 3.54
  },
  {
    "text": "with my students you don't ask me what",
    "start": 374.16,
    "duration": 3.479
  },
  {
    "text": "Stein's Lemma is and I will not ask you",
    "start": 375.9,
    "duration": 3.84
  },
  {
    "text": "what Stein's Lemma is but we just for",
    "start": 377.639,
    "duration": 3.601
  },
  {
    "text": "this discussion we just need to take it",
    "start": 379.74,
    "duration": 3.72
  },
  {
    "text": "for granted that Stein Slimmer says that",
    "start": 381.24,
    "duration": 4.38
  },
  {
    "text": "this expectation that you are interested",
    "start": 383.46,
    "duration": 5.22
  },
  {
    "text": "in is actually equal to this quantity",
    "start": 385.62,
    "duration": 5.34
  },
  {
    "text": "right now if you take this on face value",
    "start": 388.68,
    "duration": 4.019
  },
  {
    "text": "if you just assume that time slamma is",
    "start": 390.96,
    "duration": 4.739
  },
  {
    "text": "good is correct then from here on let's",
    "start": 392.699,
    "duration": 5.701
  },
  {
    "text": "see what answers do we get right where",
    "start": 395.699,
    "duration": 5.34
  },
  {
    "text": "do we reach if we assume that this is",
    "start": 398.4,
    "duration": 3.66
  },
  {
    "text": "actually correct and this is actually",
    "start": 401.039,
    "duration": 2.88
  },
  {
    "text": "correct because Stein's Lima has proven",
    "start": 402.06,
    "duration": 4.56
  },
  {
    "text": "this right ah",
    "start": 403.919,
    "duration": 5.581
  },
  {
    "text": "okay so now if this is indeed correct",
    "start": 406.62,
    "duration": 6.419
  },
  {
    "text": "then what is happening here right so",
    "start": 409.5,
    "duration": 6.479
  },
  {
    "text": "when will this quantity that you have",
    "start": 413.039,
    "duration": 5.301
  },
  {
    "text": "here",
    "start": 415.979,
    "duration": 2.361
  },
  {
    "text": "this quantity right which is the same as",
    "start": 419.639,
    "duration": 4.141
  },
  {
    "text": "this quantity that you have when will",
    "start": 421.86,
    "duration": 3.66
  },
  {
    "text": "that be high right this is a derivative",
    "start": 423.78,
    "duration": 4.74
  },
  {
    "text": "so what does that mean that if you have",
    "start": 425.52,
    "duration": 4.98
  },
  {
    "text": "a small change in the observation right",
    "start": 428.52,
    "duration": 6.54
  },
  {
    "text": "if there is a small change ah",
    "start": 430.5,
    "duration": 4.56
  },
  {
    "text": "in the observation then it causes a",
    "start": 435.3,
    "duration": 5.04
  },
  {
    "text": "large change in the estimation right so",
    "start": 438.6,
    "duration": 3.36
  },
  {
    "text": "if you are y i",
    "start": 440.34,
    "duration": 4.62
  },
  {
    "text": "right so you are given this x i comma y",
    "start": 441.96,
    "duration": 6.239
  },
  {
    "text": "i pairs right and now if there was a",
    "start": 444.96,
    "duration": 4.799
  },
  {
    "text": "small suppose you are given the x i s",
    "start": 448.199,
    "duration": 4.801
  },
  {
    "text": "ten and the Y I was 20 right and instead",
    "start": 449.759,
    "duration": 5.28
  },
  {
    "text": "of that now if the X I was 10 and the Y",
    "start": 453.0,
    "duration": 5.3
  },
  {
    "text": "high was 20.1 this is small change right",
    "start": 455.039,
    "duration": 5.821
  },
  {
    "text": "what this says is that if there is a",
    "start": 458.3,
    "duration": 5.98
  },
  {
    "text": "small change in y i there is a large",
    "start": 460.86,
    "duration": 6.66
  },
  {
    "text": "change in the estimated value right so",
    "start": 464.28,
    "duration": 5.16
  },
  {
    "text": "you are now your training data has",
    "start": 467.52,
    "duration": 4.079
  },
  {
    "text": "changed instead of this you had this",
    "start": 469.44,
    "duration": 4.379
  },
  {
    "text": "point and now ideally you would expect",
    "start": 471.599,
    "duration": 4.741
  },
  {
    "text": "that whatever F hat X you got right that",
    "start": 473.819,
    "duration": 4.261
  },
  {
    "text": "means whatever parameters you estimated",
    "start": 476.34,
    "duration": 3.84
  },
  {
    "text": "that shouldn't change much because there",
    "start": 478.08,
    "duration": 3.839
  },
  {
    "text": "is a very small change in the data right",
    "start": 480.18,
    "duration": 4.68
  },
  {
    "text": "but when would this quantity be high if",
    "start": 481.919,
    "duration": 4.741
  },
  {
    "text": "this quantity is high it means that if",
    "start": 484.86,
    "duration": 4.44
  },
  {
    "text": "you have even a small change in y i you",
    "start": 486.66,
    "duration": 4.439
  },
  {
    "text": "are seeing a very large change in your F",
    "start": 489.3,
    "duration": 3.48
  },
  {
    "text": "hat which means you are changing a very",
    "start": 491.099,
    "duration": 4.021
  },
  {
    "text": "large change in the W's that you have",
    "start": 492.78,
    "duration": 5.699
  },
  {
    "text": "estimated right and this is not good",
    "start": 495.12,
    "duration": 5.04
  },
  {
    "text": "right this is definitely not good",
    "start": 498.479,
    "duration": 3.301
  },
  {
    "text": "because for this much small change you",
    "start": 500.16,
    "duration": 3.96
  },
  {
    "text": "don't want the model to vary a lot right",
    "start": 501.78,
    "duration": 5.3
  },
  {
    "text": "and for what kind of models we saw that",
    "start": 504.12,
    "duration": 6.299
  },
  {
    "text": "this happens that a small change in the",
    "start": 507.08,
    "duration": 5.5
  },
  {
    "text": "data could",
    "start": 510.419,
    "duration": 4.5
  },
  {
    "text": "lead to a large change in the F hat x",
    "start": 512.58,
    "duration": 4.62
  },
  {
    "text": "that you got this is what we saw in the",
    "start": 514.919,
    "duration": 4.021
  },
  {
    "text": "case of when we were discussing the",
    "start": 517.2,
    "duration": 3.36
  },
  {
    "text": "first example of simple and complex",
    "start": 518.94,
    "duration": 4.2
  },
  {
    "text": "models so when we had the simple model",
    "start": 520.56,
    "duration": 5.04
  },
  {
    "text": "of Y is equal to W and X plus W naught",
    "start": 523.14,
    "duration": 4.92
  },
  {
    "text": "and then when we trained it we had a",
    "start": 525.6,
    "duration": 4.62
  },
  {
    "text": "total of 500 training samples and then",
    "start": 528.06,
    "duration": 3.899
  },
  {
    "text": "when we trained it using 30 different",
    "start": 530.22,
    "duration": 4.08
  },
  {
    "text": "samples this line did not change much",
    "start": 531.959,
    "duration": 4.201
  },
  {
    "text": "right that means my f hat X did not",
    "start": 534.3,
    "duration": 3.539
  },
  {
    "text": "change much right but the same",
    "start": 536.16,
    "duration": 3.54
  },
  {
    "text": "experiment then when we repeated it with",
    "start": 537.839,
    "duration": 4.321
  },
  {
    "text": "a complex model right which was X raised",
    "start": 539.7,
    "duration": 6.0
  },
  {
    "text": "to 25 all the way up to W naught we saw",
    "start": 542.16,
    "duration": 4.98
  },
  {
    "text": "that when we take these 30 different",
    "start": 545.7,
    "duration": 3.36
  },
  {
    "text": "samples which is the same as saying that",
    "start": 547.14,
    "duration": 3.84
  },
  {
    "text": "I am changing the training data right",
    "start": 549.06,
    "duration": 4.14
  },
  {
    "text": "and when I change the training data my",
    "start": 550.98,
    "duration": 5.22
  },
  {
    "text": "estimation was changing a lot in the",
    "start": 553.2,
    "duration": 4.68
  },
  {
    "text": "case of the complex models right so",
    "start": 556.2,
    "duration": 4.44
  },
  {
    "text": "hence this quantity you can say is going",
    "start": 557.88,
    "duration": 5.76
  },
  {
    "text": "to be high for complex models as",
    "start": 560.64,
    "duration": 4.86
  },
  {
    "text": "compared to simple models right so",
    "start": 563.64,
    "duration": 4.379
  },
  {
    "text": "that's that's what ah we are concluding",
    "start": 565.5,
    "duration": 5.1
  },
  {
    "text": "from these two points right that the",
    "start": 568.019,
    "duration": 4.141
  },
  {
    "text": "expected quantity that we were",
    "start": 570.6,
    "duration": 3.359
  },
  {
    "text": "interested in in which does not go to 0",
    "start": 572.16,
    "duration": 3.9
  },
  {
    "text": "in the case of training data is actually",
    "start": 573.959,
    "duration": 4.201
  },
  {
    "text": "equal to this quantity and now we see",
    "start": 576.06,
    "duration": 4.2
  },
  {
    "text": "that this quantity has this sensitivity",
    "start": 578.16,
    "duration": 4.859
  },
  {
    "text": "term which says that if the small change",
    "start": 580.26,
    "duration": 4.68
  },
  {
    "text": "in y i what happens to the change in F",
    "start": 583.019,
    "duration": 4.981
  },
  {
    "text": "hat and we are making this observation",
    "start": 584.94,
    "duration": 5.82
  },
  {
    "text": "that this quantity would be high for",
    "start": 588.0,
    "duration": 4.86
  },
  {
    "text": "complex models because a small change in",
    "start": 590.76,
    "duration": 4.44
  },
  {
    "text": "y I was causing a lot of changes in the",
    "start": 592.86,
    "duration": 4.08
  },
  {
    "text": "estimated value of f hat X or the",
    "start": 595.2,
    "duration": 3.18
  },
  {
    "text": "estimated values of the parameters",
    "start": 596.94,
    "duration": 3.42
  },
  {
    "text": "whereas for simple models it was not",
    "start": 598.38,
    "duration": 3.899
  },
  {
    "text": "causing a lot of change right so",
    "start": 600.36,
    "duration": 5.22
  },
  {
    "text": "connecting these observations now we can",
    "start": 602.279,
    "duration": 7.801
  },
  {
    "text": "say that this quantity that you see here",
    "start": 605.58,
    "duration": 7.439
  },
  {
    "text": "is actually going to be high for complex",
    "start": 610.08,
    "duration": 5.04
  },
  {
    "text": "models as compared to simple models",
    "start": 613.019,
    "duration": 4.141
  },
  {
    "text": "right ah in complex model would be more",
    "start": 615.12,
    "duration": 4.62
  },
  {
    "text": "changes so just to ah so what does that",
    "start": 617.16,
    "duration": 5.1
  },
  {
    "text": "eventually mean right so",
    "start": 619.74,
    "duration": 4.68
  },
  {
    "text": "on the previous slide we had that true",
    "start": 622.26,
    "duration": 3.18
  },
  {
    "text": "error",
    "start": 624.42,
    "duration": 3.72
  },
  {
    "text": "is equal to empirical train error plus a",
    "start": 625.44,
    "duration": 6.959
  },
  {
    "text": "small constant plus some term",
    "start": 628.14,
    "duration": 6.6
  },
  {
    "text": "and now we are seeing that this term is",
    "start": 632.399,
    "duration": 3.781
  },
  {
    "text": "actually proportional to model",
    "start": 634.74,
    "duration": 3.3
  },
  {
    "text": "complexity or it is a function of model",
    "start": 636.18,
    "duration": 3.719
  },
  {
    "text": "complexity so I am just going to write",
    "start": 638.04,
    "duration": 2.76
  },
  {
    "text": "it as",
    "start": 639.899,
    "duration": 2.94
  },
  {
    "text": "Omega Model complexity right so just to",
    "start": 640.8,
    "duration": 3.659
  },
  {
    "text": "indicate that this is actually dependent",
    "start": 642.839,
    "duration": 4.381
  },
  {
    "text": "on the model complexity so higher the",
    "start": 644.459,
    "duration": 5.401
  },
  {
    "text": "model complexity larger this term would",
    "start": 647.22,
    "duration": 5.34
  },
  {
    "text": "be that means your true error would be",
    "start": 649.86,
    "duration": 5.159
  },
  {
    "text": "farther away from the empirical trainer",
    "start": 652.56,
    "duration": 4.2
  },
  {
    "text": "so you would estimate the train error",
    "start": 655.019,
    "duration": 4.021
  },
  {
    "text": "and tell me hey it's almost zero but I",
    "start": 656.76,
    "duration": 3.78
  },
  {
    "text": "will not believe you because you have",
    "start": 659.04,
    "duration": 3.12
  },
  {
    "text": "not told me anything about this quantity",
    "start": 660.54,
    "duration": 3.539
  },
  {
    "text": "you have only estimated the train error",
    "start": 662.16,
    "duration": 4.38
  },
  {
    "text": "which is given by summation I equal to 1",
    "start": 664.079,
    "duration": 5.76
  },
  {
    "text": "to n y i minus",
    "start": 666.54,
    "duration": 5.82
  },
  {
    "text": "y hat I the whole Square",
    "start": 669.839,
    "duration": 6.18
  },
  {
    "text": "on the training data right you have not",
    "start": 672.36,
    "duration": 5.46
  },
  {
    "text": "given me anything about this so even if",
    "start": 676.019,
    "duration": 3.421
  },
  {
    "text": "you tell me that hey your training error",
    "start": 677.82,
    "duration": 4.139
  },
  {
    "text": "is 0 I will not believe you because this",
    "start": 679.44,
    "duration": 4.98
  },
  {
    "text": "quantity has not been accounted for and",
    "start": 681.959,
    "duration": 4.32
  },
  {
    "text": "I know that higher the model complexity",
    "start": 684.42,
    "duration": 4.08
  },
  {
    "text": "this value would be high that means more",
    "start": 686.279,
    "duration": 4.201
  },
  {
    "text": "complex your model more farther away",
    "start": 688.5,
    "duration": 3.66
  },
  {
    "text": "would your empirical estimate of the",
    "start": 690.48,
    "duration": 3.9
  },
  {
    "text": "training error be from the true error",
    "start": 692.16,
    "duration": 4.14
  },
  {
    "text": "right and just to make sure that this",
    "start": 694.38,
    "duration": 3.78
  },
  {
    "text": "indeed happens that complex models are",
    "start": 696.3,
    "duration": 3.96
  },
  {
    "text": "actually more sensitive to changes in",
    "start": 698.16,
    "duration": 3.72
  },
  {
    "text": "the training data as opposed to simple",
    "start": 700.26,
    "duration": 3.78
  },
  {
    "text": "models let's just take a look at that",
    "start": 701.88,
    "duration": 5.66
  },
  {
    "text": "right let's just try to see that",
    "start": 704.04,
    "duration": 3.5
  },
  {
    "text": "yeah so now what I have taken this is",
    "start": 708.54,
    "duration": 4.68
  },
  {
    "text": "some training data that was given to me",
    "start": 711.54,
    "duration": 5.4
  },
  {
    "text": "and I estimated the true model I",
    "start": 713.22,
    "duration": 5.76
  },
  {
    "text": "estimated the complex model which is the",
    "start": 716.94,
    "duration": 4.139
  },
  {
    "text": "red colored model and the simple model",
    "start": 718.98,
    "duration": 4.08
  },
  {
    "text": "which was the green color model now what",
    "start": 721.079,
    "duration": 5.101
  },
  {
    "text": "I've done is uh I have changed one of",
    "start": 723.06,
    "duration": 4.8
  },
  {
    "text": "these data points only one of the data",
    "start": 726.18,
    "duration": 6.32
  },
  {
    "text": "points right I have instead of taking",
    "start": 727.86,
    "duration": 4.64
  },
  {
    "text": "X comma y I have taken this x comma y",
    "start": 732.72,
    "duration": 5.4
  },
  {
    "text": "right and now you can see that my simple",
    "start": 735.54,
    "duration": 4.68
  },
  {
    "text": "model has not changed much right so my f",
    "start": 738.12,
    "duration": 4.82
  },
  {
    "text": "hat of x",
    "start": 740.22,
    "duration": 2.72
  },
  {
    "text": "so I changed y but my f hat of X did not",
    "start": 744.24,
    "duration": 4.68
  },
  {
    "text": "change much that means this quantity is",
    "start": 747.3,
    "duration": 3.599
  },
  {
    "text": "going to be small right but for the",
    "start": 748.92,
    "duration": 4.32
  },
  {
    "text": "complex model my my estimation has",
    "start": 750.899,
    "duration": 4.62
  },
  {
    "text": "changed quite a bit right so now when I",
    "start": 753.24,
    "duration": 5.099
  },
  {
    "text": "changed a y by a small quantity my f hat",
    "start": 755.519,
    "duration": 4.981
  },
  {
    "text": "X has also changed dramatically right in",
    "start": 758.339,
    "duration": 4.44
  },
  {
    "text": "this region it's very different uh from",
    "start": 760.5,
    "duration": 4.139
  },
  {
    "text": "what it was originally right so hence",
    "start": 762.779,
    "duration": 4.021
  },
  {
    "text": "for complex models small changes in the",
    "start": 764.639,
    "duration": 4.861
  },
  {
    "text": "training data could cause a large change",
    "start": 766.8,
    "duration": 5.64
  },
  {
    "text": "in the estimated function right the F at",
    "start": 769.5,
    "duration": 4.38
  },
  {
    "text": "X or the estimated values of the",
    "start": 772.44,
    "duration": 2.94
  },
  {
    "text": "parameters right so that is just a",
    "start": 773.88,
    "duration": 4.62
  },
  {
    "text": "empirical demonstration or an anecdotal",
    "start": 775.38,
    "duration": 5.699
  },
  {
    "text": "demonstration of this idea that complex",
    "start": 778.5,
    "duration": 4.92
  },
  {
    "text": "models are more sensitive to changes in",
    "start": 781.079,
    "duration": 4.26
  },
  {
    "text": "the training data and that is the",
    "start": 783.42,
    "duration": 4.14
  },
  {
    "text": "quantity that bothers us so thus",
    "start": 785.339,
    "duration": 4.701
  },
  {
    "text": "quantity",
    "start": 787.56,
    "duration": 2.48
  },
  {
    "text": "uh maybe I should go to the next slide",
    "start": 790.339,
    "duration": 6.221
  },
  {
    "text": "I'll go to the next slide okay I can",
    "start": 794.279,
    "duration": 4.56
  },
  {
    "text": "just do this slide first right okay so",
    "start": 796.56,
    "duration": 4.32
  },
  {
    "text": "hence while training instead of",
    "start": 798.839,
    "duration": 4.381
  },
  {
    "text": "minimizing the train error we should",
    "start": 800.88,
    "duration": 4.56
  },
  {
    "text": "minimize this error right so now if",
    "start": 803.22,
    "duration": 3.84
  },
  {
    "text": "you're going to just focus on minimizing",
    "start": 805.44,
    "duration": 4.38
  },
  {
    "text": "the train error okay you might make it",
    "start": 807.06,
    "duration": 5.64
  },
  {
    "text": "zero okay so now what is the what is",
    "start": 809.82,
    "duration": 5.639
  },
  {
    "text": "your true error was the expected value",
    "start": 812.7,
    "duration": 4.92
  },
  {
    "text": "on the right hand side is actually equal",
    "start": 815.459,
    "duration": 3.781
  },
  {
    "text": "to the train error so this is what my",
    "start": 817.62,
    "duration": 3.839
  },
  {
    "text": "train error is right my train error is I",
    "start": 819.24,
    "duration": 5.52
  },
  {
    "text": "equal to 1 to n sorry I keep changing M",
    "start": 821.459,
    "duration": 5.401
  },
  {
    "text": "and M but I assure you understand what I",
    "start": 824.76,
    "duration": 4.28
  },
  {
    "text": "mean",
    "start": 826.86,
    "duration": 2.18
  },
  {
    "text": "this is what L train is and what I was",
    "start": 830.579,
    "duration": 3.961
  },
  {
    "text": "trying to do is I was just trying to",
    "start": 833.459,
    "duration": 3.06
  },
  {
    "text": "minimize this but now I've realized that",
    "start": 834.54,
    "duration": 3.78
  },
  {
    "text": "if I try to minimize this and make it to",
    "start": 836.519,
    "duration": 4.201
  },
  {
    "text": "zero there's this another quantity which",
    "start": 838.32,
    "duration": 3.959
  },
  {
    "text": "is dependent on the model complexity",
    "start": 840.72,
    "duration": 5.52
  },
  {
    "text": "which keeps increasing so in effect my",
    "start": 842.279,
    "duration": 6.18
  },
  {
    "text": "expected error is increasing right so I",
    "start": 846.24,
    "duration": 3.899
  },
  {
    "text": "might have done a great job of reducing",
    "start": 848.459,
    "duration": 4.021
  },
  {
    "text": "this to zero but the cost of increasing",
    "start": 850.139,
    "duration": 4.081
  },
  {
    "text": "this significantly and then the net",
    "start": 852.48,
    "duration": 3.659
  },
  {
    "text": "effect is as my model is still bad when",
    "start": 854.22,
    "duration": 3.72
  },
  {
    "text": "I am going to pass it test instances",
    "start": 856.139,
    "duration": 3.901
  },
  {
    "text": "which were not seen during training this",
    "start": 857.94,
    "duration": 4.44
  },
  {
    "text": "expectation is going to be this error is",
    "start": 860.04,
    "duration": 4.26
  },
  {
    "text": "going to be still very high right so",
    "start": 862.38,
    "duration": 3.3
  },
  {
    "text": "that's why you should not try to just",
    "start": 864.3,
    "duration": 3.9
  },
  {
    "text": "minimize the train error but train error",
    "start": 865.68,
    "duration": 4.26
  },
  {
    "text": "plus the model complexity is what you",
    "start": 868.2,
    "duration": 4.62
  },
  {
    "text": "should try to minimize now how do you",
    "start": 869.94,
    "duration": 5.04
  },
  {
    "text": "encode this model complexity drive just",
    "start": 872.82,
    "duration": 3.72
  },
  {
    "text": "Define a generic function which says",
    "start": 874.98,
    "duration": 3.539
  },
  {
    "text": "that this captures the model complexity",
    "start": 876.54,
    "duration": 4.56
  },
  {
    "text": "now how do you define model complexity",
    "start": 878.519,
    "duration": 4.201
  },
  {
    "text": "something that we will see as we go",
    "start": 881.1,
    "duration": 3.72
  },
  {
    "text": "along but the main idea here is that you",
    "start": 882.72,
    "duration": 4.08
  },
  {
    "text": "should not just try to minimize this",
    "start": 884.82,
    "duration": 3.84
  },
  {
    "text": "error but you should also try to",
    "start": 886.8,
    "duration": 6.599
  },
  {
    "text": "minimize the model complexity and",
    "start": 888.66,
    "duration": 4.739
  },
  {
    "text": "yeah this Omega Theta would be high for",
    "start": 897.839,
    "duration": 3.601
  },
  {
    "text": "complex models and small for simple",
    "start": 899.82,
    "duration": 3.6
  },
  {
    "text": "models so if you have complex models",
    "start": 901.44,
    "duration": 4.139
  },
  {
    "text": "this quantity would be high and hence",
    "start": 903.42,
    "duration": 4.32
  },
  {
    "text": "your the loss that you are trying to",
    "start": 905.579,
    "duration": 4.141
  },
  {
    "text": "minimize is still going to be high so it",
    "start": 907.74,
    "duration": 3.48
  },
  {
    "text": "will not allow you to make the models",
    "start": 909.72,
    "duration": 4.82
  },
  {
    "text": "very complex right",
    "start": 911.22,
    "duration": 3.32
  },
  {
    "text": "and this acts as an approximation for",
    "start": 914.639,
    "duration": 4.2
  },
  {
    "text": "the quantity that we just saw right so",
    "start": 916.92,
    "duration": 2.82
  },
  {
    "text": "this is",
    "start": 918.839,
    "duration": 4.141
  },
  {
    "text": "this is the model complexity term that",
    "start": 919.74,
    "duration": 5.159
  },
  {
    "text": "we had seen and now you have to come up",
    "start": 922.98,
    "duration": 3.719
  },
  {
    "text": "with some functions which act as an",
    "start": 924.899,
    "duration": 3.721
  },
  {
    "text": "approximation of this and when you are",
    "start": 926.699,
    "duration": 3.961
  },
  {
    "text": "trying to reduce this it actually",
    "start": 928.62,
    "duration": 4.019
  },
  {
    "text": "decreases the model complexity which",
    "start": 930.66,
    "duration": 3.359
  },
  {
    "text": "means it will decrease this quantity",
    "start": 932.639,
    "duration": 3.06
  },
  {
    "text": "which means it will decrease this sum",
    "start": 934.019,
    "duration": 3.661
  },
  {
    "text": "right so you should come up with omegas",
    "start": 935.699,
    "duration": 3.601
  },
  {
    "text": "which decrease the model complexity",
    "start": 937.68,
    "duration": 3.24
  },
  {
    "text": "because if the model complexity",
    "start": 939.3,
    "duration": 3.599
  },
  {
    "text": "decreases this term will decrease and",
    "start": 940.92,
    "duration": 4.26
  },
  {
    "text": "hence this term will also decrease right",
    "start": 942.899,
    "duration": 4.081
  },
  {
    "text": "so that's the way you are going to",
    "start": 945.18,
    "duration": 3.779
  },
  {
    "text": "handle this complex term here because",
    "start": 946.98,
    "duration": 3.539
  },
  {
    "text": "directly handling it make looks",
    "start": 948.959,
    "duration": 3.481
  },
  {
    "text": "difficult but you could Define some",
    "start": 950.519,
    "duration": 4.68
  },
  {
    "text": "omega thetas such that you are sure that",
    "start": 952.44,
    "duration": 4.68
  },
  {
    "text": "if you minimize that Omega Theta your",
    "start": 955.199,
    "duration": 3.781
  },
  {
    "text": "model complexity is going to be reduced",
    "start": 957.12,
    "duration": 3.659
  },
  {
    "text": "and hence you will be sure that this",
    "start": 958.98,
    "duration": 3.479
  },
  {
    "text": "term that was bothering you is going to",
    "start": 960.779,
    "duration": 4.5
  },
  {
    "text": "be reduced right so this actually is the",
    "start": 962.459,
    "duration": 5.641
  },
  {
    "text": "basis for all regularization methods",
    "start": 965.279,
    "duration": 4.141
  },
  {
    "text": "right so this is what you do in",
    "start": 968.1,
    "duration": 3.179
  },
  {
    "text": "regularization instead of just",
    "start": 969.42,
    "duration": 4.38
  },
  {
    "text": "minimizing L train Theta which is the",
    "start": 971.279,
    "duration": 5.041
  },
  {
    "text": "empirical training error you actually",
    "start": 973.8,
    "duration": 5.52
  },
  {
    "text": "add some regularization term to that and",
    "start": 976.32,
    "duration": 4.8
  },
  {
    "text": "then you see that I am going to minimize",
    "start": 979.32,
    "duration": 4.44
  },
  {
    "text": "the empirical train error as well as the",
    "start": 981.12,
    "duration": 4.68
  },
  {
    "text": "regularization error because that will",
    "start": 983.76,
    "duration": 4.62
  },
  {
    "text": "make sure that my expected error which",
    "start": 985.8,
    "duration": 4.2
  },
  {
    "text": "is what I was interested in is actually",
    "start": 988.38,
    "duration": 3.48
  },
  {
    "text": "going to be small right and the",
    "start": 990.0,
    "duration": 3.6
  },
  {
    "text": "pictorial view of this this is the same",
    "start": 991.86,
    "duration": 5.599
  },
  {
    "text": "so this is what would happen",
    "start": 993.6,
    "duration": 3.859
  },
  {
    "text": "if I were to just look at the training",
    "start": 997.62,
    "duration": 5.04
  },
  {
    "text": "error right so this is L train",
    "start": 999.48,
    "duration": 5.099
  },
  {
    "text": "which is computed just using those n",
    "start": 1002.66,
    "duration": 4.44
  },
  {
    "text": "training samples then my as I keep",
    "start": 1004.579,
    "duration": 4.26
  },
  {
    "text": "increasing the model complexity this",
    "start": 1007.1,
    "duration": 3.179
  },
  {
    "text": "will keep decreasing but what will",
    "start": 1008.839,
    "duration": 3.601
  },
  {
    "text": "happen is as I increase the model",
    "start": 1010.279,
    "duration": 3.961
  },
  {
    "text": "complexity this quantity is going to",
    "start": 1012.44,
    "duration": 4.139
  },
  {
    "text": "increase and hence my error is actually",
    "start": 1014.24,
    "duration": 4.68
  },
  {
    "text": "going to be high right so I need to find",
    "start": 1016.579,
    "duration": 5.401
  },
  {
    "text": "this sweet spot and this Omega Theta",
    "start": 1018.92,
    "duration": 5.039
  },
  {
    "text": "should ensure that this model model",
    "start": 1021.98,
    "duration": 4.199
  },
  {
    "text": "complexity is reasonable so that you",
    "start": 1023.959,
    "duration": 4.38
  },
  {
    "text": "have low training error but at the same",
    "start": 1026.179,
    "duration": 5.52
  },
  {
    "text": "time this this quantity is also not high",
    "start": 1028.339,
    "duration": 5.34
  },
  {
    "text": "and hence your expected error is also",
    "start": 1031.699,
    "duration": 3.5
  },
  {
    "text": "not high right that is why we use",
    "start": 1033.679,
    "duration": 3.841
  },
  {
    "text": "regularization now why do we care about",
    "start": 1035.199,
    "duration": 4.181
  },
  {
    "text": "regularization in the context of deep",
    "start": 1037.52,
    "duration": 3.539
  },
  {
    "text": "learning right why am I teaching you",
    "start": 1039.38,
    "duration": 4.38
  },
  {
    "text": "this in the context of deep learning so",
    "start": 1041.059,
    "duration": 4.74
  },
  {
    "text": "the answer to that is simple right so",
    "start": 1043.76,
    "duration": 4.02
  },
  {
    "text": "deep learning we said that",
    "start": 1045.799,
    "duration": 3.781
  },
  {
    "text": "regularization is basically trying to",
    "start": 1047.78,
    "duration": 5.96
  },
  {
    "text": "control for model complexity",
    "start": 1049.58,
    "duration": 4.16
  },
  {
    "text": "and in deep neural networks you know",
    "start": 1054.08,
    "duration": 3.42
  },
  {
    "text": "that these are highly complex models why",
    "start": 1055.88,
    "duration": 2.76
  },
  {
    "text": "are they highly complex because they",
    "start": 1057.5,
    "duration": 2.88
  },
  {
    "text": "have many layers many non-linearities",
    "start": 1058.64,
    "duration": 4.08
  },
  {
    "text": "and many parameters so they are actually",
    "start": 1060.38,
    "duration": 4.32
  },
  {
    "text": "they can completely overfit right",
    "start": 1062.72,
    "duration": 4.199
  },
  {
    "text": "because this we also know ah from the",
    "start": 1064.7,
    "duration": 4.44
  },
  {
    "text": "universal approximation theorem that you",
    "start": 1066.919,
    "duration": 4.981
  },
  {
    "text": "can completely overfit your training",
    "start": 1069.14,
    "duration": 4.56
  },
  {
    "text": "data right you can get arbitrary close",
    "start": 1071.9,
    "duration": 4.38
  },
  {
    "text": "to your true function if you keep adding",
    "start": 1073.7,
    "duration": 4.68
  },
  {
    "text": "layers or if you keep adding neurons",
    "start": 1076.28,
    "duration": 4.019
  },
  {
    "text": "right and you don't want to do that",
    "start": 1078.38,
    "duration": 3.179
  },
  {
    "text": "right you don't want to become very",
    "start": 1080.299,
    "duration": 2.76
  },
  {
    "text": "close to your training data because then",
    "start": 1081.559,
    "duration": 3.661
  },
  {
    "text": "your test error would be high and that",
    "start": 1083.059,
    "duration": 3.48
  },
  {
    "text": "why that is why you do not want to",
    "start": 1085.22,
    "duration": 3.6
  },
  {
    "text": "overfit because if you leave if you",
    "start": 1086.539,
    "duration": 4.5
  },
  {
    "text": "design a very deep neural network with",
    "start": 1088.82,
    "duration": 4.38
  },
  {
    "text": "many neurons many parameters then it",
    "start": 1091.039,
    "duration": 3.721
  },
  {
    "text": "will be able to drive the training error",
    "start": 1093.2,
    "duration": 3.54
  },
  {
    "text": "to zero but while doing so you have",
    "start": 1094.76,
    "duration": 3.72
  },
  {
    "text": "increased the model complexity by adding",
    "start": 1096.74,
    "duration": 3.84
  },
  {
    "text": "many parameters and many layers and that",
    "start": 1098.48,
    "duration": 4.02
  },
  {
    "text": "is going to give you poor generalization",
    "start": 1100.58,
    "duration": 5.219
  },
  {
    "text": "ah capability or poor or a higher test",
    "start": 1102.5,
    "duration": 5.58
  },
  {
    "text": "error that's why you add regularization",
    "start": 1105.799,
    "duration": 4.681
  },
  {
    "text": "so that you control for model complexity",
    "start": 1108.08,
    "duration": 3.479
  },
  {
    "text": "now what are these different",
    "start": 1110.48,
    "duration": 2.699
  },
  {
    "text": "regularization methods that you can use",
    "start": 1111.559,
    "duration": 3.661
  },
  {
    "text": "in the context of dipline learning is",
    "start": 1113.179,
    "duration": 4.141
  },
  {
    "text": "something that we'll see in the next",
    "start": 1115.22,
    "duration": 4.86
  },
  {
    "text": "lecture thank you so I'll end it here",
    "start": 1117.32,
    "duration": 6.859
  },
  {
    "text": "and I'll see you in the next lecture",
    "start": 1120.08,
    "duration": 4.099
  }
]