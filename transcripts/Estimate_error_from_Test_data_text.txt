foreign [Music] data as D which contains M plus n points where you have M ah or other n training points and M test points right or the other way around as we defined on the previous slide now we also know that ah some true function exists such that Y is f of x plus Epsilon what does that mean that Y is is related to x i by some true function f but there is also some noise Epsilon in the relationship right just like in any in relation that you might have some noise so you ideally want Y is equal to f of x but there might be some minor noise and for Simplicity we assume that this noise is zero Center that means it has zero mean and a small variance Sigma square right so that's what we are going to assume and of course we do not know F right if you knew F then there's nothing to do but we don't know f so what we do is we approximate f using F hat and then we estimate the F hat the parameters of f hat using uh the training data which is going to be a subset of this capital D right so I am going to refer to the training data as T and it's going to be a subset of the D which I have defined earlier and ah Y is going to be F hat of X now right so that is the approximation that we have made instead of Y is equal to F X Plus Epsilon which was a true relation and we did not know F we have come up with some approximation F hat and we have freedom in choosing what F hat we want to choose from like a family of functions and we could have different parameters for that right so we are interested in knowing ah the expected value of F hat x minus f of x the whole square right so this is the mean square error ah indeed so here expectation stands for the mean this is the error this is the square of the error so this is the mean square error so the simply put the square of the difference between the predicted value from R approximation and the True Value and this expectation computed over a large number of samples right that is what we are interested in estimate but of course we cannot estimate this directly right because this requires us to know f of x and we do not know f of x right so how do we compute this because this has a quantity that we do not even know right but we will still be able to estimate this empirically because although we do not know F we do not know the true function f we have been given a lot of training points right where these y's are known so those y's we know are coming from some relation f of x plus Epsilon so even if you don't know what the relationship is we are given some of these x's and the corresponding y's so we know the X comma y pairs right so we have some idea of what f of x looks like for the given X's that we have we do not know the full function but at least for the X's that are there in our training set we know what the Y's look like right so we have some of these and similarly F at X is also we can compute because we can pass the X through the function that we have approximated or the approximate function that we approximation function that we have used and that could give us y hat I right and then we could compute this as a difference so f x you could replace by Y and ah F hat X you could replace by this y hat right this is y hat which I often call it as just Y and it's from the context it's clear whether I am referring to F at X X or the original function there but now I will make it explicit that one is all I do not need to actually use y hat here I can just say F hat X which you understand what is and f x is just Y which are the training points that are given to me right so now ah let us go further so this is the quantity that I am interested in then right so I have just replaced f of x by Y and F hat of X by y hat right ah so this is what the quantity is that I am interested in and now I know that Y is actually f of x minus Epsilon right so I can just substitute sorry f of x plus Epsilon so I can just substitute that and now I'm just going to like kind of open up this square and rearrange some terms right so I'm looking at this as a minus B the whole Square where a is equal to F hat x minus f x and B is equal to Epsilon right so that is how I have expanded this uh formula I just open the bracket rearrange some terms and so on so a expectation of a sum is the same as the sum of the expectations so I can write this expectation of the three terms that I had a square minus 2 a B plus b square as the sum of the expectations of these terms so expectation of a square minus expectation of 2 a b uh plus expectation of b square right with a and b as defined earlier and of course a constant I can take outside ah so now if you look at it this is the quantity that I was interested in and I did not know how to estimate it but now I have been able to rewrite that quantity in terms of things here and here I see some hope now because now I have this y hat and Y here which is just the training data that was given to me those are the Y's and the predictions that are my model made so this is something I can estimate expectation of Epsilon Square I can estimate it's just the I'd already assumed that Epsilon comes from the normal distribution right so expect expectation of Epsilon square is just going to be Sigma Square so that I can compute again here I have a slight problem but we will come back to this right so I have just now Rewritten the quantity of interest in terms of a set of quantities of which some quantities I can compute and there is one quantity which I can still not compute but we will see how to deal with that ok so now what we will do is now we will take a small detour to understand how to empirically estimate an expectation and then return to our derivation let's see what I mean by that so suppose we have observed the goals scored in K matches you are watching some football uh a lot of football games and you've watched some 100 200 games and you have observed the number of goals scored say 100 200 games between if you want the same pair of teams or uh all cities one team is constant there right so how many goals on average does say Brazil score or some other team scores right I never observed this and now the way you compute the expected value we are interested in what is the expected value of Z so our random variable here is z and it can take any value 0 1 2 3 4 5 6 7 maybe as many goals as you can score and you are interested in the expected value right now expectation as you define in probability of course is you have to have repeat this a large number of times but you only have like some K matches that you have seen the way you compute the estimate or you empirically estimate the expectation is by using this formula right this is something that we have been doing for ages right I mean in in our studies in high school engineering and so on where we just compute we are given a set of samples and we compute the average as using this formula and that average is actually the empirical estimate of the expectation right and the analogy to our our derivation where we were stuck would be the following that I am interested in the expected value of the square error between y hat and Y and that I can just approximate from the data that I have so I have seen M training points for each of those training points I know what my y hat is because I can pass it through my F at X and get the answer I know what the true Y is because that was given to me in the training data and I can compute the difference between them Square it and take the average so this would be my empirical estimate for the expectation and that's all I can do right I cannot really compute the true expectation because that would mean to expectation or all possible X's that I can get and that's not possible to do for all the possible X's I need to compute the Y hats also know the true y but if I already knew the true y for all possible X's then there's nothing to estimate right so I just have a small sample of reasonable size so I'm just going to empirically estimate the expectation using that sample right so wherever I see expected value of y hat minus y the whole Square I am going to replace it by this formula okay so that is the main idea and that's why on the previous slide I was saying that this is something that we can compute and now I have shown you how to compute that right so let us return back ah to our derivation now so this is the expectation this is the expected error right the true error that we were interested in Computing I call this a true error because this is the difference between the predicted value and the true function value right but we do not know the true function value so we cannot estimate this quantity and now we have written it as a sum of three terms of which I just showed you how to compute the first one empirically right so now you could compute it empirically from the data but there are two cases you can either use the test data to compute this or to estimate this quantity or you could use the training data right so let us look at both the cases so first we look at the case when we are looking at the test data okay so now suppose I look at the test data then this is the true error that I am interested in now the first quantity here I'm going to replace it by the empirical estimate that I just showed on the previous slide which is the average computed from the m test points that you had and the test points were the points from n plus 1 to n plus M I know what the predicted value is I know what the true value is because I was given the data as x i comma y i pairs right so I had these pairs I know the what the true value is and I just compute this difference and this is of course easy to compute this is expectation of Epsilon square and I just told you earlier that explain expectation Epsilon comes from 0 comma Sigma Square so expected value of Epsilon square is just the variance which is Sigma Square so Sigma square is going to be small so this is a small ah constant that will get added here and then you have this third quantity that you still don't know how to deal with right ah so let's just see how to deal with that quantity so what I've written here is that this is the covariance between uh these two uh values right ah and the yeah so what I've written here is the this this quantity here I mean keeping the two aside is actually the covariance between these two random variables Epsilon which is the noise and F at x minus f x these two random variables and I'll tell you why I am saying that right ah so you have the covariance between X comma Y is e of X the expected value of x minus the MU mean of X and Y minus the mean of Y ah r x is Epsilon so the mean of X is of course 0 ah so the mean of X will disappear and then you have y minus mu y now I can open up the bracket so I get e of X Y minus ah X into mu X and now again the sum of expectations is the expectation of sum so I can take the two terms out so expectation of X Y minus expectation of X into mu y now mu Y is of course a constant right because you compute the expectation and then that's a constant so that will come out so you have e x y minus mu Y into e of X but again your V of X because X is Sigma is going to be 0 so your ah what you remain with is e of X comma y right so in this case when Epsilon was a random variable with mean 0 this product between treating Epsilon as X and this quantity as Y is actually the covariance between these two random variables right so that's that's something that we are noting now how do we use this fact what do we do with this is something that we will see on the next line right so so far ah so good uh we were interested in this we cannot estimate this so we extract estimated this as a sum of these three quantities of which the first two we know how to deal with the third one is still not clear right because it again has f of x so we need to see if we can somehow get rid of this third quantity or make some comment on that right so that part is still remaining so let us look at the third part and make some observations about it so ah remember that Epsilon here what is Epsilon actually so Y is equal to f of x plus Epsilon so Epsilon is actually ah y minus f of x right ah so now you are trying to find the covariance right so this quantity as I said is the covariance so between Epsilon and this ah other random variable and I just told you that Epsilon is actually equal to Y minus f of x side we just saw that that simply comes from the fact that Y is equal to f of x plus Epsilon now the statement that I am going to make is that this y minus f of x is actually independent of f hat x minus f of x why am I saying that because when I am talking about the Y's here I am talking about the Y's coming from the training data because this expectation we are Computing from the training data right oh sorry ah test data right so this expectation we are Computing from the test data whereas F hat of X was estimated from the training data your test points did not participate in the X in the estimation of f hat of X and these y's are actually corresponding to the test data right hence these two random variables are actually independent because why the Y's that you are seeing here did not participate in F hat of X that they did not have any say in the estimation estimated parameters of f hat X because this belongs to test data and the test data did not participate in the estimation of f hat X right so now these two quantities are actually independent that means Epsilon and this quantity is independent right because y Epsilon is simply y minus f of x and that means the covariance between Epsilon and this other quantity is going to be 0. right because these two are independent random variables so there ah covariance is going to be zero right so that's why you can say that in this case when you are trying to estimate these expectations from the test data your the covariance term is actually going to be 0. yeah so I mean sorry I should have said it a bit differently so the expectation of e Epsilon comma this random variable can just be written as the product of the two expectations and ah so sorry I shouldn't have said that they are in sorry so this is correct so the expectation of this quantity which was what you had here you can now since Epsilon and the other random variable are independent you can write it as the product of their expectations and since the expected value of Epsilon is 0 you get the final answer as 0 right so in the case when you are trying to estimate this expectation from the test data this quantity disappears right so then the true error is actually equal to the empirical test error plus a small constant so what does this mean that if you are going to X going to estimate the true error empirically from the test data which is the first quantity that you see here if you are going to estimate it empirically from the test data then your true error is actually very close to the empirical test error so if this is what your estimate is going to be then you are not making a very ah large gap right because there is just plus a small constant right that is what we derived here because it was the sum of this quantity plus this quantity plus this quantity we proved that this quantity is 0 this is a small constant so if I were to expect if if I were to estimate the true error using the test data then my uh exp my up my estimation is actually very good right so if I compute the error from the test data then I'm actually very close to the true error right ah now let's see what happens if we had to estimate the error from the training data so that's why whenever you want to estimate the model so now you have trained the model and you want to see how good the model is it does not make sense as I'll show soon to estimate it from the training data because the training does is always going to be optimistic right as you keep increasing the model complexity it will go to zero but your test error might still be high and your test error actually gives you the true picture because if you empirically estimate the error from the test data then you are actually very close to the true error is what we have just derived