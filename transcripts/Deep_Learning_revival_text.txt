foreign [Music] functions and initialization methods and these are all geared towards making deep learning or deep neural networks train better right so we'll first do a quick recap set the context about why we are talking about activation functions and initialization methods and then introduce a bunch of activation functions as well as initialization methods okay so let's start with a quick recap as I said so uh when you train neural networks so we started with this very simple Network which had just one parameter W in fact we started with two parameters wnb but I've just kept one parameter and we already saw how to train this network the idea was to use gradient descent or any of its variants and the main ingredient there was to update the weight using some kind of an update rule which internally contained the derivative right so this is the quantity that was important and we saw various variants of the gradient descent algorithm but in all of these the gradient shows up in one way or the other right so this is a quantity which is important ah and uh we also saw how to this how to compute this quantity right so uh we saw we had derived this for the simple Network and the key observation that we had made there was that the derivative is actually proportional to the input X right that's the one important observation that we had made and that also had kind of aided our discussion on what happens when the input is passed because in most cases this x would be 0 and then we came up with these adaptive methods and so on right so this observation we have made a column of couple of times before about the derivative formula having this X as a factor and hence if x is large something can happen if x is small something can happen and so on right yeah so uh then from this very thin and very shallow Network we went to wider Network which had many inputs but it's still a shallow Network there's only one layer input and now in fact there's no layer input and output that's it and even in this case when the update rule Remains the Same it's just that the same update will applies to all the parameters and the derivative for any parameter again shows up this red term here which is the input connected to that way right so again this term was showing up and if this is high low 0 and so on uh we saw what are the ramifications of that right similarly now if you have a thin network but a deeper Network we still use the derivative it's just that we compute the derivative using a chain rule but nothing else changes right I mean the conceptually everything Remains the Same and again in this chain rule this H 0 has shown up here which is again the input to the network and this is for the weight W1 but in general for any weight you had some formula for the derivative we don't care what the actual formula was but all we care about is there was this term h of I minus 1 uh sorry this should have been suffix it's I minus 1 and not like h i minus 1 right so it's just the suffix is I minus one and for w one of course I minus 1 would be 0 so H 0 showed up here and at 0 was the same as X the input but for any layer if I'm looking at this layer then the derivative of the loss function with respect to this weight is going to be proportional to H2 that means the input that this weight was connected right so the H's are the inputs coming from the previous layer they are of course also the output of some layer but for this current layer they are the input right so the derivatives are always proportional to the inputs connected to the weight so that's the main observation that we had made and I'm just repeating that in this uh recap that we are doing right uh okay now uh if if there is a network which is deep and wide again we calculated the same thing we calculated the derivative of the loss function with respect to any weight by using this chain rule applied across multiple Parts not just one path but three different parts here and again we saw uh some uh formula for this and we derived this in quite detail when we studied the back propagation algorithm right so now the question is are the points to remember right now are that training neural networks is a game of gradients you have you compute gradients at every layer and then you use whatever variant or your favorite variant of the gradient based approach it could be momentum nag atom add a Max whatever you want to use but the derivators will get used inside them right and this gradient is the way of quantifying the responsibility of the parameter towards the loss the higher the gradient higher the responsibility lower the gradient lower the responsibility right and the gradient with respect to a parameter is proportional to the input connected to that parameter in the single or the input output Network this input was just X in a multi-layer network it's just the input from the previous layer which is h i minus 1 right so that's these are things to remember now things to uh wonder about are that we learned this back propagation algorithm right and we said that this is the basis for training all the Deep neural networks right and we saw feed forward neural networks already later on in the course we will see convolutional neural networks recurrent neural networks and then Transformers and for all of them training happens using the back propagation algorithm right so is it that this back propagation algorithm was something that was discovered in the last decade maybe around 2009 2010 and then deep learning became so popular because we have been uh we I mean kind of know that deep learning has been popular since 2009 2010 in NLP maybe around 2014 and so on but in the last decade right so is it that around that time this algorithm got discovered and then we all started switching to deep neural networks no actually right so the back propagation algorithm existed much before anything late 70s or even before that if for all I know but I definitely know that in 90 1986 there was this it was made popular in the context of neural networks by rumala heart and team right so it has existed for a long time so what was happening since from 1986 to 2009 2010 when deep learning became really popular right why was deep learning not so popular in the 90s or early 2000s given that the algorithm used for training it existed back then right so what was stopping it from becoming popular so the issue is that while this algorithm existed in theory you know that you can train a deep neural network by just chaining the gradients and Computing the gradients using the chain rule in practice when you're trying to train deep neural networks as deep as four or five layers it was not very successful right and what I mean by successful not successful is that the networks did not converge right did not converge reliably of course a lot of other things have changed now you have faster compute so earlier if you had to use a certain number of flops then you would need so many days of computation now maybe you need a few days of computation so that has changed but in general there were other things other more uh theoretical things because of which it was hard to train deep neural network it's not just a com issue of compute right so there are three things that have changed since the 1990s right one is of course we have much more data now so if you have many parameters to train as a deep neural network would have you need larger amount of data that we have you need faster compute we have that but there were some other things also which needed to fall in place and that's what we'll focus on in this lecture right so it's not that this suddenly got discovered in 2019 and then people started using deep learning right so until 2006 it was very hard to train them and in 2006 there was a seminal work that I'll talk about which allowed us to train deep neural networks and that suddenly sparked or revived the interest in deep neural networks and from then on we have seen the success story which has led us to where we are currently in 2022 right so we will talk about what happened in this initial years from 2006 to 2019 which kind of helped us train this deep neural networks and what was the effect of that and how is that connected to the lecture that we are looking at today right so that's going to be the focus so I'll end this video here and I'll come back and talk about unsupervised pre-training which is something that happened in 2006 and enabled the training of DPR Networks