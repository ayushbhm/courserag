foreign [Music] so one thing that people have shown right is and particular this paper which uses skip connections as a regularizer and I'll talk about that in a bit that the effect of regularization is to smoothen the Lost landscape right so typical uh and you can go on to this website right which allows you to explore typical lost Landscapes that you encounter in neural networks and you can see that this is very bumpy right it's not nowhere closer to the very nice loss functions that we have been dealing with which have like a single plateau and then a valley and so on it there are like multiple plateaus rallies here and there are some very sharp slopes here right and what happens is that if you have flat surfaces right and let me show you a plot it will become clear right so if you have a flat surface you will get better regularization why why is that the case because if you have a flat surface then for a large range you have the loss as the same right and now your if you cannot over fit like I mean in the sense that you have figured out a value of the loss you know that for a large neighborhood any of the W's would have given you a similar loss right but now in this plot where you have these bumps up and down if you had over fit and selected gone into one Valley around that Valley there are many other Hills right so with a small change you get a very large change in the loss so your model becomes very sensitive right so one effect of regularization that people have shown is that it smoothens the Lost surface right so they have analyzed the surface of lost surface of neural networks and seen that with regularization and this was in the context of skip connection being used as a regularization we have not done skipped connections yet we'll do that in the case of convolutional neural networks but they also act as some sort of regularizer and they show that the loss surface actually smooth inside so that leads to better optimization so now coming to the taxonomy of things right what are the regulation techniques that you have used so we had this loss function which was the empirical training error and we added different regularizations to it so one is based on data so we added uh we looked at data augmentation where we had different rotations of the same image that we had added similarly in speech and text also you can do data augmentation and you also did this noise injection where at the output where the input we were adding some noise to the inputs right so both at the input and output we had manipulated the data a bit so that you add some noise and now the model cannot overfit because your data itself has some noise right the other is you could change the architecture right so there we saw Dropout is one such change then skip connections which are used in convolutional neural networks is another such thing and weight sharing right which we just briefly touched upon we again looked at it in the case of Dropout but weight sharing is again something which is used in convolutional neural networks and it acts as a good regularizer right and then again pooling which is used in the context of convolutional neural networks all of these three I am mentioning here for the sake of completeness but we'll see them in the context of cnns and that that time it will become clear that these act as regularizers because they help in reducing the number of parameters they help in reducing the model complexity right at least these two here okay uh then uh it has also been shown that the optimization process itself can act as a regularizer right so some papers have shown that gradient descent has prefers less complex Solutions as compared to more complex Solutions and that is a bit more nuanced statement and it's a discussion in itself so I'll not go into the details of it but there's also some kind of implicit regularization happens because of these gradient descent based methods which prefer simpler Solutions as opposed to more complex Solutions then early stopping is again a kind of Regulation that we have discussed and then we have looked at penalizing the law cost itself right which is adding this Omega Theta term that we have seen L well regulation is something you can use we have studied L2 but similarly you could use L1 regularization right so this is one way of grouping the regularization techniques the other way of grouping them is into explicit regularization and implicit regularization so we have looked at mainly looked at explicit regularization like L2 regulation data augmentation all of these we have looked at and these two we will look at when we look at convolutional neural networks in the case of implicit regularization we have mainly looked at early stopping right but but the gradient descent based methods that we have looked at also have an implicit regularization in terms of their preference for like less complex Solutions and then even the initial learning rates that you set up in these methods in the gradient descent the based methods they also kind of act as some kind of a regularizer right because they also control how your training is going to proceed right so we have not looked at this in detail we will not cover that also but just wanted to give you a picture of this explicit and implicit regulation that happens right so this is all that I had to say about regularization I'll end this lecture here and in the next lecture we'll talk about activation functions and a few other things thank you