[Music] okay so now continuing on in our journey right so we have seen quite a few things uh we have seen about the revival we have seen about more advances like optimization activation functions etc which made this revival pop possible and then generated in interest in solving real-world problems and winning challenges in image tasks uh sequence labeling tasks and uh even games right now just again kind of re-looking at this period right from 2010 onwards and that has been absolute madness in terms of about the euphoria around ai right and the interest in ai so this is a report published by stanford ai index report and you can see right the number of publications in ai has dramatically increased and this has a lot of repercussions in terms of the speed which with the field is moving the pressure on young graduate students nowadays and the quality of conference reviews and so on right those are discussions for a separate day perhaps but you can see how rapidly it is increasing and you can see almost close to exponential growth in the past few years not quite exponential but very rapid growth in the past few years right ah this is also no matter how you slice indices whether you look at a number of publications in journals or conferences or book chapters all of this is rapidly increasing in the last few years right and similarly if you look at it across different fields whether it be machine learning computer vision nlp of course all of these fields in their own small ways the number of research the amount of research output that is coming out has significantly increased not such just research right there's also a lot of startups and interest in big tech companies uh in the area of ai and this has resulted in exponential growth in terms of the number of patents that are getting filed in ai right so this i call this a period of absolute madness where ai has kind of taken over everyone's imagination and now it is the field that you want to work in it is the field that you want to start have a startup in or have a phd in and so on right so they're living in that uh era which is very different from the ai winter which i had said right it's quite on the opposite side now a lot of funding for research in ai a lot of funding for startups in ai and so on right so just kind of uh recapping or seeing the same period from a different lens of how the research and development output has increased in ai in the past few years okay now again coming back to uh the technology side of things right so we talked about uh convolutional neural networks recurrent neural networks and so on becoming very popular and solve becoming these state of the art de facto models for speech and nlp right around this time while this was happening right ah in a very short span of time while we were seeing all this success uh there came what is known as transformers and which is again revolutionize the field of ai most models that you see today are transformer-based models so that's what we are going to talk about now and you'll again go back in time and talk about machine translation so transformers are first introduced in the context of nlp and machine translation so we'll go back to machine translation where the first effort started way back in 1954 and touched upon this earlier there was interest in translating between some politically important languages such as english russian due to the political environment at that time right a lot of promises were made right that within a few years we'll be able to have perfect translation machine translation between these languages right but then research went on and nothing much really happened right in fact there was this very uh critical report which came out in 1966 which said that hey the initial hype was just too much and people have not lived up to those expectations and we're not really seeing much progress in this field and this was around the same time as the ai winter right so this also contributed to that saying that hey all this hype is not really meaningful and we should look at other ways of solving this problem or maybe the time is not even right for looking at these problems right but research in machine translation continued this era was largely the yellow era that i have shown here on the timeline was largely the rule-based era of rule-based systems then around 1990s came what are known as the ibm models for machine translation which were probabilistic models right so they cast this entire problem of translation as the probability of finding the french sequence given an english sequence and the way i'm saying it you can you can understand that this is like a conditional distribution that you're trying to model and they came up with a model of how to how to do this right and also there were a lot of inference challenges in the process and they proposed various techniques for doing that and this model right which is quite a revolution at that time uh dominated nlp for the next 20 years right 1993 to almost 2012 13 till deep learning uh eventually took over right and so the first uh i mean of course deep learning in small ways was there before there were these word vectors and other things ah but the sequence to sequence model right which was introduced sequence to sequence with attention right and if i may i think attention was perhaps the idea of the decay right and it led to a paradigm shift in nlp uh which i showed in a era of like very big models very data hungry models and also pushed the boundary in terms of the performance of these models so within a couple of years of being introduced in 2014 people almost gave up on this phase-based statistical machine translation work which had taken 20 years which is 20 years in the making right in 20 years it dominated nlp and within a couple of years of these neural machine translation being introduced people even stopped comparing with those models and right now i don't think any paper actually even talks about these uh phase-based statistical machine translation that's the kind of impact uh that this has had right i mean just imagine like completely wiping away 20 years of i won't say wiping i think that's a bit too harsh but kind of moving like making a significant paradigm shift right in terms of how things are done right and not just that right i mean this is again i find it remarkable right so i said like one major revolution happened in 2014 and within three years right in 2017 this another model which is a transformer model came out which is very different from the uh recurrent neural network based model which was proposed in 2014 and it also takes the idea of kind of attention and puts it on steroids that means there's multi-headed attention and so on and that led to the second revolution after 2017 within a couple of years now people are no longer talking about the recurrent neural network based models that were so popular for machine translation right now most modern machine translation models are transformer based model two very big revolutions in a very short span of time and a key characteristic again was that we are moving in the era of bigger and bigger models and trained on really large obscene amounts of data right that's that's the story or that's the uh that's the story of today all right that's the current era in which we are living uh big models data hungry models right and uh then this journey continued so 2017 was when it got introduced in the context of machine translation and in 2018 came out this burt model right which the key idea here was that uh just as humans learn language and then specialize on certain tasks right so when when you're when you're growing up you learn about languages and then when you're at a certain age if i ask you to do sentiment analysis i can just show you a few examples and you will quickly get the essence of the task and then you'll be able to mark sentences as positive sentence or negative sentiment because you already know so much about language right so lovely going by the same analogy the idea in birth is that you could pre-train models using unlabeled data just as we learn in a very unstructured manner when we are growing up about languages you just train on unlabeled corpora and then when you are dealing with specific tasks maybe small amounts of label data would be enough right and this was like kind of reintroducing some sort of transfer learning again right and that idea has really again been the dominant paradigm today where you do this unsupervised pre-training and then train uh fine-tune on smaller amounts of data and the advantage of this is that with because you have access to a large amount of unlabeled corporate so if you think of a language like english you have the entire wikipedia you have so many news articles you have tons of data available online which you can escape from the web even for languages indian languages some of the indian languages like hindi marathi and so on you could get a lot of data on the web and you could just train using this data right this is not labeled data these are just sentences so this allows you to train really large uh models and then fine-tune them using small amounts of data right and then came gpt which is a lot all of you might be aware of this this is like a massive 175 parameter billion parameter model and it just takes a prompt and generates interesting things so there are exam there are entire websites uh dedicated to this i'm forgetting the name thinking just google for uh examples from gp33 or something and like you could just type a prompt right you could type a prompt of a famous personality i want to have a chat with you about the first computer right and it will generate a very coherent conversation based on this prompt or you just give it a newspaper headline and it will generate a very coherent one or two paragraphs of articles which is of an article which is in line with the headlines right so we've come a long way in terms of our generation capabilities if you look at where we were in 1954 where we had a simple system which had a few grammar rules and could translate a few sentences between english and russian now we have these really large uh billion parameter models and we have even moved past the billion parameter era right so this is the billion parameter club where as you can see from uh 2018 onwards 2018 to 2020 right i mean just look at elmo at one end which is the number of parameters in billions is what i'm showing on the y-axis uh to what we came to tnl uh sorry the tnlg model right uh the yellow bar that you see like a really huge number of parameters and then you have gpt3 which is not shown here this is only gpt 2 which is 175 billion parameters so that wouldn't even fit in the scale that i've used on the y axis right so that's that's how far we have come and that's how rapidly these models are increasing in terms of one the amount of training data they need second the amount of parameters that they have and third is the performance that you get with these models right all of these on all of these axis we are really pushing the frontier uh quite a bit and then came the trillion parameter club so this is an interesting animation so you have these number of synapses right so uh i remember at some point uh or maybe i'll be talking about that later or i don't know whether i already said that so human brain i think has 10 is to 10 or maybe 10 billion neurons right so 10 billion neurons and or maybe i'm like off by one order and then there are connections between these neurons those connections are called synapses and there are like 10 raise to 15 synapses in a human brain right so for a fruit fly it's greater than 10 raised to six for a honey bee it's around 10 raised to nine for mouse it is around 10 to 12 for cats 10 raised to 13 and for humans brains it has 10 to 15 synapses right on these synapses i would roughly equate them to the number of weights that are there in a neural network right if i used to if i were to take the brain analogy of neurons and how the neurons are connected and then apply it to the neural network analogy of artificial neurons and weights between neurons or weights connecting neurons then then synapses would correspond to the number of parameters that a model has right so we had this transformer model the first model which came out 2017 it had roughly 400 mil 400 million parameters so that's where it would fall on the axis just to the left of honeybee then came the 1.5 billion parameter model gpt 2 then you had the 10 billion megatron uh model then you have the 175 billion gpd3 model and you have the 1.1 trillion g shard model right which is for translation and then you have this recent 1.6 trillion parameter model uh which is the switch model right which is like slightly the to the to the right of ah mouse now so in terms of of course these are just for the sake of formulative it doesn't i mean the analogy only holds from the point of view of drawing a diagram of brains of living organisms are quite more complex and capable of doing much more things but if i was just to uh have fun with this then this is where we are in terms of evolution of neural networks right and this model was strained on 2048 tpus it is an insane amount of compute that is being used and insane amount of model size that has been used okay so this this was to kind of tell you about the era of transformers that we have if we have entered since 2017 where models are getting bigger and bigger and bigger and of course it's also reflecting in the performance of these models is not just like a race of building the biggest model but also show performance improvements by doing that right and then while these were happening in language transformers have also entered vision and here on the dark yellow axis i'm going to show you the evolution of image classification so you had this 2012 you had lx net which was first participated in the image net and won that competition although it was still around 16 percent or so error rate and then we had multiple convolutional neural network based models around the same time in object detection there was this move from the traditional object detection methods to a cnn based method rcnn and then there was an entire family of models of our cnn which came out and in parallel and image classification we had rest network so 2015 all of this is convolutional neural network based models and we also had convolutional neural network based models for object detection right and it continued for a while then in 2017 transformers came in nlp right so that's it's just like slightly you have to look at different timelines here but in 2017 17 is when transformers came and then by 2019 we had transformers enter uh the image classification scene and then they have entered the object detection scene also so now even the image classification object detection uh models uh the state-of-the-art models are based on uh transformers now right and this is kind of again replaced the convolutional neural network based models largely right i mean of course it's not saying that they're completely out but since which have been around since 1980s right so we are seeing like uh massive paradigm shifts right in the way things are happening in these fields and now transformers are the talk of the uh town as far as the state-of-the-art models for uh image speech and nlp are concerned today right so yeah okay so now ah we were talking about uh the rise of the transformers now this perhaps should have been a separate section but i just have it here so in parallel while we were talking about image classification and object detection which are largely regression and classification problems there's also interested in generation right you should be able to generate images automatically right you are able to synthesize an image and there was a lot of interest in that over the past few years the first models to do this were variation auto encoders and quite astonishingly right i mean if i tell you this at this point if you do not know much about variational autoencoders or any of these generation models you'd be astonished that they just take a noise vector that means they take a random vector as input and then generate real world images right so the images that you see here are generated by these models and none of these faces or none of these people actually exist in the real world right and you can see as time is passing by we're generating very very high quality real looking images right and this is all uh this these are innovations which are happening in what are known as generative models gans being the most popular of those i can see over the years starting in 2014 when gans were first introduced which is generative adversarial networks there have been a lot of innovations and improvements on that for the past uh for the six seven years after that right and now uh again here uh there has been like a paradigm shift right so in terms of what has now come out are what are known as diffusion based models right which try to overcome several drawbacks of gan based models and you some of you might have been so actually if you look at my twitter feed it's full of examples of hey i tried this generating this with dali too and this is just fabulous hey i tried generating this with the stable diffusion model and it's just fabulous right a lot of this kind of animations that you see on the right hand side of the screen which is about generating images and daily what it actually does right it just takes a prompt and generates very realistic images right so the example here is that it was given a prompt which says an arm chair in the shape of an avocado right and it generates such beautiful images this is not a retrieval mind you this is not that it has gone on the internet and retrieved images which were of avocado chairs these are images that it has generated starting with a noise vector and based on this prompt so think of it that you have told an artist that i want the image of a chair in the shape of an avocado and this is the artist's imagination right so dally is the artist here and an illustration of a baby daikon radish in a tutu walking a dog right a brilliant rendering in terms of faithfulness to the prompt that it was given right it was trained on many such text image pairs so that at test time when you're giving it a text it is able to generate an appropriate image corresponding to that text right and dali was a transformer based model it was not a diffusion based model and now dalit 2 has come which is a diffusion based model and it has exceeded i would say expectations as i said it is very recent 2022 and improvements on top of that which is the stable diffusion model and a few other computing models are are quite the rage now right people are coming up with these prompts and feeding them these to these models and trying to generate very realistic images there's an astronaut riding a horse in a photorealistic style right i mean this of course this image perhaps wouldn't exist anywhere right but the but the model is able to imagine and generate these kind of image so it's quite exciting times for what are known as generative models and we won't cover uh in this first part of the course right i may do a subsequent course but in the first course unfortunately we won't have the time to cover any of these generative models but i just thought it would be good to give some perspective on the advances happening in the generative field right i mean the field of generative models okay