foreign [Music] which looks like a obvious extension that you would do on Adam right so Adam already introduced the momentum term and then we know that nag is better than momentum based gradient descent right so Adam which uses momentum can we make it better by adding the nestrob effect to it which is the look ahead effect that we need to do right so that's we could that is something that we could do and that's exactly what Madame is which is nastrov is the nestro version of Adam so let's see how that works right so this is the update tool for Adam that we had now from here we'll now revisit uh nag The nestro Accelerated gradient descent and then see how to modify the update tools for Adam to get in this natural Factor here okay so recall the equation so now one thing which I have done here is that I have so remember that you could have beta U T minus 1 plus 1 minus beta times uh Delta WT right so both these forms are popular right so where you have beta and 1 minus beta where of course they sum up to one or you could just have beta and then no weight multiplied by the derivative of WT right so when we had originally introduced momentum we had used this equation but from when you used it in atom we were using this equation so both are popular so when I start off I will stick to this but at some point I will switch to this equation so just remember that both of these can be used interchangeably right so when I start off I'll just use this because it's easier to convey what I want to convey but at a later point I'll switch back to the this version of momentum okay um now you recall the equation for Mac the main idea in nag was that instead of looking at the gradient at time step T you first did the look ahead and then computed the gradient right that was the main idea in nag and then the rest of it of course Remains the Same right now what we're going to do is we're going to rewrite nag in a more convenient way and then import it into atom right or that rather than Port it to atom right so this is what we were doing in nag we were Computing a quantity called GT which was the gradient okay let me just write down the equations all of it first and then that becomes easier for me too explain okay okay so I'll just now walk you through the slide so we were Computing the gradient at the lookahead value right and once you compute the gradient at the look ahead value then you are Computing uh empty as the history plus the gradient at the look ahead value and then you are updating the new value as uh uh as this updated history right that's what you were doing now one thing I want you to notice here is the time steps that you have you have t you have T minus 1 and you have P plus 1 also right and it looks a bit cumbersome and we're going to try to rewrite that a bit more compact it that's one challenge the other challenge is what is written here but I'll come come to that right so let's now see what happens uh maybe I'll just go back a bit yeah so let's say you started at w0 okay and you computed a derivative at this point so which would be somewhere here it'll just screw it on the plot so that's where you have computed the derivative then at m0 uh since you have remember that we have kept the learning rate as one I have initialized it here in the top right corner here right uh so ETA is equal to 1 and of course n minus 1 is 0 at time step minus 1 you don't have any history so here m0 is going to be beta times 0 plus G 0 so it's just going to be G 0 so far so good and then W1 is going to be W 0 minus uh let me just choose the pen so I'll just do this once so there should have been an ETA here but ETA is set to 1 that's why I'm not using ETA right so remember that ETA is always there in all all the stuff that I'll do in the next two slides is just that the value of ETA is equal to 1 right and then you w 0 minus the derivative computed at uh yep uh yeah so instead of calling it GT GT in this case of course turns out to be Delta is of 0 itself but actually it would have been as if I follow this equation it would have been Delta W 0 minus ETA into beta into M minus 1 and this quantity just turned out to be 0 so that's how I am writing it here okay so that's what your W1 is now let's go ahead so if so this is where I have reached now now at G1 I am doing this look ahead right so when I'm Computing G1 I am actually looking at the derivative at a slightly further Point I've already moved by the history and then computed the derivative so that's where the lookup is happening right so that this is the point so now I'm looking ahead and Computing the derivative and now that is where look ahead happens in the original nag equation right and we are going to slightly modify that so now what is happening is that this yeah so now I have after doing the look ahead I have reached somewhere I have made that updates uh and then I've read somewhere and then I have computed my history Vector again this is my history Vector that I have computed again and then I've used that to make my final update right and this is what my final update looks like it's w 1 minus M1 right because I'm just substituting uh values in this equation now let me just use the pen so I'm just substituting values in this equation my ETA is equal to 1 and time step is 1 so W1 minus M1 is what I get and now if I substitute the value of M1 it turns out to be this quantity right it's beta M naught plus Delta W 1 minus beta M naught okay so now what is happening here is that if you look at this m naught right which is M of T minus 1 it is used twice right first it is being used to make this temporary update to the value okay you make that temporary update and then switch back to the original value again right we because you are not using the temporary value again right so you're switching back to the original value so using the B time not here and then again you are using beta I am not here and then again you are switching back from this temporary computed value that you had to the original value that you have right so you're making some extra computations here which could be our uh kind of done away with that's the goal that we want to achieve while giving you the same effect right so now what is happening here is that you look at G1 and it's getting some input which was W1 and then it is subtracting some quantity from it instead if I already had passed it a value which was already equal to this right because this I already had the previous time Step at the previous time step I already had M naught so if at the previous time previous time step itself I could have computed an updated value and send it here then I wouldn't have had to do this here right so that's the idea and you can see that what is happening here is that you have t t minus 1 and t plus 1 uh these are the three time steps that you see here right and instead can we write everything in terms of t and t plus 1 and push the look ahead uh such that I don't have to do these computations again right so that's what we want to do so let's see what we are going to do about it so the key thing here is that the vector M naught or M minus 1 or M of T minus 1 is being used twice then you're doing this temporary computation which is W1 minus beta M naught and you compute the gradient there and then you forget about this temporary computation you never use that again and then again you is in the last equation you Resort back to the value of W1 and then make an update from there right so that's something which is we want to get rid of and simplify this right so let's see if we can rewrite the equations so this is what W1 was right so W1 was W naught okay so W1 was W naught minus this and this is the calculation that I don't like right because I could have done this earlier and sent it a value which would have already taken care of this look ahead right and then I don't need to do one more look ahead here so that's the thing because we already had beta M naught so this quantity that you have here is beta M naught and I already had this at the previous iteration right so why am I doing this in the next iteration why am I coming up and Computing this in the next iteration here why am I doing it that's the question that we are trying to answer right so can we uh sorry I should have let me just go back sorry I Circle it the wrong so I'll start my expression again so when I'm Computing G1 I am doing this temporary computation which I don't want to do right and the what my argument is that at the previous time step I already had this beta m not right because that was already computed at the previous time step could I have done something at the previous time step and got an effective value of w which had already encompassed this beta I am not movement and then whatever w i get here I just compute the gradient of that right so I'm doing the liquid in advance and then sending it to the next time step that's the idea that we want to get to is it making sense so this is what we will do right so now we had W1 if you look at W1 minus beta M naught right so this is W1 minus beta M naught which is effectively this so you have what was W1 the way you computed uh W1 was you had started with W naught minus ETA times this lookahead gradient that you had computed and then you did this minus beta M naught right this is what you had done to get the value of uh uh uh so this is what was happening let's say if I do W 1 minus beta M naught this is what you will get and this already has this look ahead incorporated into this right which look ahead I was doing here what if I just do the look ahead in the previous iteration itself and when I'm Computing uh W1 what if I just put in this quantity also a over there foreign so now I am rewriting the equations and this is how I'm going to do when I compute GT plus 1 I'm just going to compute the derivative at the current value of w there is no look ahead happening here then I'm going to just compute the Mt plus 1 using the equations that I have okay and then I'm going to compute WT plus 1 as WT minus ETA so I have taken the look ahead value here and then the GT plus one right so now what the effectively what I'm doing is the following foreign I computed the look I had now look at is computed only at the momentum step which is Mt plus 1 and then I look at W1 and I'm going to compute the look ahead of the gradient in the next time step right so I've just changed the time steps a bit now notice in my new equations uh there is no WT minus one right everything in terms of t and t plus 1 so an easier way to look at this is that if you have a while loop if you put one step outside the while loop right then all your computations go like one step behind that right so that's that's what you are doing here you're just doing the look ahead before and then coming into the while loop so then you don't have to do the look ahead here and that is simplifying all your computations because now T minus 1 is nowhere here at this point you are not Computing a temporary value of w you are just Computing a new value of w at one go in the previous case you had to compute a new value of w twice once for the temporary look ahead and then one more time when you are updating right in this equation now all this you have income passed into one single equation right so that's why you have Rewritten nag and the reason we're doing this is now based on this set of equations now these are the set of equations that we want to bring to Adam right and that then the update rule for Adam will look quite simpler so this is what the update rule for nadum is so you have the same thing as Adam this is also same as Adam this is also same as Adam this is also same as atom so nothing has changed so far the only thing now you have done is at this step you're doing the look ahead the we are updating basis on the history as well as the current gradient so for a minute let's just ignore this these quantities right so if you ignore these quantities your update rule is simply of WT minus ETA square root of VT plus 1 the bias corrected Value Plus Epsilon right into M hat t plus 1 plus Delta WT so here you have you're just taking the gradient at the current time step and the history now all the other multiplying factors that you see here those are coming so this is actually uh the uh yeah let me just make it Mt Plus 1. now if this if I ever to bias correct it then I would divide it by 1 minus beta raised to t plus 1 and I'll have to do a similar bias correction here also well again divided by 1 minus beta raised to t plus 1 and then this beta and 1 minus beta are simply the contributions that I have for the history and the current gradient right so that's how I get it so it's a bit messy uh I I couldn't find a better way of explaining this if you have suggestions I'm open to using them but the main idea here is this the takeaway is this that you add the nestrov factor into the atom equations which means you do a look ahead and to make this look ahead a bit less Messier in terms of writing and making sure that most of your equations are same as atom and you just push your head into the last look ahead into the last equation uh we did that rewriting of Nag and we just explained that instead of doing something at T minus 1 then t and t plus 1 we had everything in terms of t and t plus one so the look ahead was already done before I went to the next iteration hence I did not need to do a look ahead again in the next iteration right and once you understand that then this equation is just the just a manifestation of that and the all the one minus beta beta terms that you see here now let me just clear the all the annotations these are simply because when we are doing nag or when we are discussing whatever we discussed before coming to this slide we did not have this one minus beta terms and this one minus beta term shows up here and the beta term shows up here and the denominator is simply corresponding to the bias correction that we do right so so that's that's all about madame and now let's see how it works in practice so look at an example uh yeah so I'll just run it and you can see the same nestro effect here right so the blue curve corresponds to an atom and I'll just run it again for your benefit you can see that the blue curve has smaller oscillations as compared to the black curve and that's the natural effect that you were hoping for right so that's that's all about madame uh so now which Optimizer to use I think uh uh the most common I guess the default one that I have seen in most papers is Adam if you don't know anything maybe just go with Adam however there are recent Works which are not so recent also there also three to four years old now which do point out some issues with Adam still by far at least for all the Practical work that I've seen Adam seems to be a very common choice and if not Adam one of the variants of Adam right maybe an Adam or uh yeah I think Madame is another good choice right so uh if you don't if you're a new learner just use off-the-shelf optimization optimizers and or Adam is as good a choice as any and some of them you could then practice a bit more so people have reported that just using gradient descent and then using or maybe momentum based gradient descent and then really knowing how to set the learning rate initial learning rate how to set the momentum value how to set the learning rate schedule if they know all of these well then they're able to get as good results as Adam right but it takes a while to get that kind of an expertise in that absence of that uh just using Adam simply off the shelf might be a good choice to use okay so that's all we had on the different uh optimizers to use now I'll just spend some more time on talking about learning rate schedules okay