foreign [Music] things were not working but now let's see if things start working so there this important contribution by Jeff hinton's group around 2016. now what you see here is a deep neural network of course by today's standards maybe it's a shallow network but at that time it was a deep neural network and even such networks were not being able to train by back propagation but they propose a simple idea right and we'll see this in the course where if you do something to initialize the weights of this network properly because earlier you are initializing the weights randomly and expecting back propagation to learn them but if you do something to initialize the weights properly then the training with back propagation becomes stable and you're able to train a very deep neural network of course this idea was again not completely new there were very deep Learners proposed by schmidur in 191 1991 1993. again many things did not fall in place at that time right around that time internet was not what it was in 2006 and there was not like large tons of data available to you uh even storages at that time were much smaller and there's not enough compute available right so while this idea was there it did not fructify because the other conditions were not conducive right you do not have enough Computing of data to really train very deep models whereas in 2006 the compute was better The gpus had still not entered the scene at least in deep learning but still the compute was better and you had more data to train so that's why it worked well in that era right of course some other Innovations also but there was the basic idea was there earlier and then after this what happened right from the period from 2007 to 2009 once this spark came in hey we always knew that deep learning is good because of the universal approximation theorem the only thing that was lacking was the ability to be able to train it now we have that so let's investigate this further right and the next three years a lot of ideas came in a lot of Investigations were done into why unsupervised pre-training works and that in turn led to insights hey maybe it works better because the optimization problem becomes simpler or hey maybe it works better because it allows it to generalize better so it acts as a regularizer in some sense right and these things led to developments in better optimization algorithms better regularization algorithms which then helped further in improving the training of these networks and some of these ideas about what these better initializations were better regularizations were better optimizations work all of this is things that we'll cover in the course right and then now at this point deep learning started becoming useful where people started winning uh a fairly competitive competitions right on handwriting recognition on mnist data set right then on a speech recognition and then uh visual pattern recognition this was on the traffic sign data so all of this now started materializing that now you just don't have it in theory that okay I'm able to train a deep network but using that I'm able to compete with the best models at that time and outperformed right so that's what happened around that time and then the image net famous image net challenge which came around which is I think around 2008 or 10 when it was first there and then in 2012 onwards the winner in the image net challenge which is roughly like you have 1000 different classes of Images cats dogs airplanes trucks and so on and you have a million uh training points and then you have a test set which contains images from this set and you have to classify those images accurately right so in 2012 we were able to do this with lxnet with a 16 error rate right so 84 percent sometimes the model was correct but then after right so then came zfnet which was again an eight layer network but we were able to do 11.2 percent reduce the error till 1.2 percent and then within a few years right which is around 2016 or 17. it went down to 3.6 percent at which point it became better than humans right so what does that mean is that if I show these test images Suppose there are thousand images in the test set even a human makes error on four or five percent of them right because some of those images may not be clear you may not the face of the dog may not be very clearly visible so you may not know be able to distinguish between two subclasses within the doc class right two different breeds of toxin and so similar errors whereas the model was now able to do it at 3.6 percent and you can see that the number of layers has also increased significantly starting with 8 in LX net going all the way up to 152 uh in the resnet model right and this is when it was I mean almost uh clear or universally accepted that now deep learning has arrived and for all image problems we have to migrate to uh what are known as convolutional neural networks or the Deep learning way of doing things right and similar uh inroads also started happening in the field of NLP and speech right slowly deep neural networks started penetrating and overtaking or rather replacing all the older methods right so this is like the golden period where everyone of course maybe you could say that the golden period is now where even just talks about deep learning and nothing else but this is the time where the real boom or the shift happened and you could talk of this as a transition uh period 2012 to 2016 right so while we are talking about the success on image Network right what image net data set the key deep Learning Network there was what are known as convolutional neural networks and this section is interestingly titled as from cats to convolutional neural networks so let's see if I use that title so again going back 1959 right uh uh Hubble and uh weasel did this experiment right what the experiment did was they had a cat which had different electrodes connected to different parts of its brain and now there's a screen in front of it where you see that stick there right so just imagine some stick pattern appearing on different portions of the image maybe on the top right corner top left corner Center maybe in the center on the left right and so on right and what they observed is depending on where the stick is different parts of the brain of the cat were getting activated that means the cat has certain receptive field and only if things get triggered and those receptive field to certain sections fire if drinks get triggered in a triggered in a different receptor field that means the stick is in a different position say the top right corner instead of the top left corner then a different portion of the brain fires right that means as we're talking about this distributed processing so different parts of the brain are actually looking at different things in the uh scene in front of you right and this is essentially the motivation behind the convolutional neural networks right which is uh okay I think this slide statement is wrong this is uh from a different place we'll fix it in the slides later on uh this was the neocognitron model right which is uh uh which essentially if you look at it right here again the same idea inspired by the cat experiment that different parts in the image are being handled by different parts of the network right so the receptive field as a concept emerged here that for different parts you could use different portions of the network very Loosely speaking of course we'll see this in detail so this title is wrong please note this and we'll change this later right so the slide number 36 yeah so this new cognitive model was proposed in 1980 right so quite uh and quite old and this idea of like using uh you know creating to different parts and having like a water is known as a shallow uh processing right in terms of not having a fully connected Network all of these are ideas that we'll uh see later in the course right and then of course in 1989 uh Jan likuno is considered as one of the founding fathers of deep learning uh was using convolutional neural networks and there's an interesting video on YouTube office demo from that time uh which was being used for recognizing handwritten digits right and the motivation at that time was this uh Postal Services where PIN codes and other digits get written and you want to automatically be able to pass them to sort the letters according to the PIN codes right so that's the application is trying to use convolutional neural networks to take an image of the postcard and like kind of try to extract the handwritten digits from there and just uh and kind of do a recognition of what that five digit number or seven digit number is and in 1998 from his initial model in 1989 which was the laneet model uh he proposed several improvements and we had the lane at Phi model in 1998 and this is also around the time when he introduced the now famous amnest data set right so almost every newbie today uh in the first delve into deep learning that I think the among the first few data sets that he experiment with is Ms so this data set was proposed in 1998 and this is what he had tested convolution neural networks on at that time right so the idea of cnns which became popular in image net in around 2012 was much earlier in existence it's just that it was not applied to very large scale problems like image net and then in 2012 when things were more conducive you had better optimization algorithms better initialization methods better activation functions better understanding of how to train deep neural networks it started showing success in those real world or other large-scale problems like image pilot had been in existence for quite a few years right