[Music] so in this module now we'll focus a bit more on this uh boolean functions and when are they linearly separable and not linear separate the reason that we want to delve into this is that we just saw that the perceptron model or the even the perceptron learning algorithm only has guarantees when the data is linearly separable so now what are these linearly separable boolean functions or rather what are boolean functions which are not linearly separable so that's something that we want to understand right so let us see one such simple example and this is a famous xor function that all of you know about so this is what the xor function looks like and now let's see what happens right so if i want to implement this using a perceptron then these are the four inequalities that my weights should satisfy right so for 0 0 inputs when i plug in this formula the output should be less than 0 for 0 1 0 it should be greater than equal to 0 for 0 1 greater than equal to 0 and for 1 1 again less than 0 right so let's just expand this so this is what the first condition says right i'll just expand the first one and not do it for the others w naught plus w 1 into 0 because x 1 is 0 plus w two into zero because x two is zero should be less than zero and that leaves us with the condition that w naught should be less than zero similarly i'll expand the other condition and leaves me with the condition that w two should be greater than equal to minus w naught w one should be greater than equal to minus w naught and then the last condition is w one plus w two should be less than minus w naught right and you can see that you cannot satisfy these four inequalities right there is one quantity which is greater than minus w naught another quantity which is greater than minus w two naught and i have added these two quantities and i want it to be less than w naught that cannot happen right so if i just add the two eq inequalities i will get w one plus w two is greater than equal to minus w naught right so i cannot simultaneously satisfy all these four inequalities right so that means i cannot find a set of w i's such that all my positive points will be on one side and my negative points will be on the other side that means this is not a linearly separable function right okay and we can see this from the diagram right so here is these four points right so you have this two negative points sitting at 0 1 and 1 0 and 2 positive points sitting at 0 0 and 1 1 right and now you can go home and practice this you can try to adjust these weights w1 w2 as much as you want right but you will not be able to draw a line such that the two positive points are on one side and so many of you will guess from the figure it's not possible that you cannot have a line such that only the positive points are on one side and only the negative points are on the other side you cannot draw such a line so it's clear from the figure it's actually clear from the inequalities the set of inequalities that you see there right so this is not going to be possible so this is an example of a function a simple boolean function which is not linearly separable right now the reason we're talking about uh not linearly separable functions because most real world data is going to be not linearly separable right so here suppose you have two classes of people right people who like machine learning and people who don't like machine learning right now it's going to be the case that there are some people here maybe the blue guys here right okay which satisfy all the characteristics of what the guys who like machine learning satisfy but they still don't like it right and same way the other way around also right so you could think of it a more easier example would be that if you have a certain locality right and you might assume that all people on that locality speak a particular language right but there might be a few people there who don't speak that language or speak another language right and so there would always be this outliers right so its data will never be linearly separable right and it need not just be outliers you could have other things also now this is a very good example there is no outlier here the people in the inner circle maybe behave a certain way people in the outer circle behave a certain way there's no outlier here but i can't draw a linear decision boundary to separate the yellow points from the purple points right so this is very much common in many real world situations right so this constraint that i can only deal with data which is linearly separable is something which is not acceptable in the long run right so we'll have to move beyond this perceptron model because the perceptron model can clearly handle only linearly separable data right okay now this can handle is again something which is to be defined what do i mean by can handle we'll come back to this in a later lecture but for now i guess you have a rough idea of what i mean right it cannot find a line which separates the positive points from the negative points okay so now with a single perceptron we cannot do this right now what i am going to show is that with a network of perceptrons you can do that with the network of perceptrons whatever a network means i have not defined what a network means we will define that with that you can separate positive points from negative points even if the data is not linearly separable to begin with right but before we see that so you start with this question right so how many boolean functions can you design from two inputs so let me start with some easy ones and then we'll try to calculate the number of functions and i once i show you a few patterns you should be able to guess the number of functions so these are two inputs given to the u and here's one function that you know which is the always off function similarly the always on function and you can see that from the numbering that they're going to be 16 such functions possible right and it simply follows that you have these four outputs right and each output could take 0 or 1 value so you'll have 2 raised to 4 which is 16 which is actually 2 raised to 2 uh raised to 2 right so this is the or function and this is the and function and then similarly you can construct other functions right so in uh of these how many are linearly separable so you can go and work this out on your own and turns out that all except xor and the not xor function are linearly separable you can go back and verify this so it's out of 16 there are two functions which are not linearly separable so in general how many boolean functions can you have from n inputs it is 2 raised to 2 raised to n right so that's what happened here it was n was 2 so you had 2 raised to 2 raised to 2 functions you can go back and convince yourself that if n is equal i mean if you have a general n number of inputs then the number of boolean functions would be 2 raised to 2 raised to m okay now of these how many are not linearly separable so when you had 16 functions two are not linearly separable so if you have two raised to two raised to n functions how many would be not linearly separable so many of you would try to say that the answer is n that's not the correct answer in fact this is an unsolved problem so you can go back and try looking for an answer for this right but that's that's not the point the point is that there are going to be some functions which are not linearly separable right and if that is the case with the simple boolean functions and i said in real world we have more complex cases where the data will not be linearly separable right so we need to have a way of dealing with not linearly separable functions we need to have a way of dealing with them in the generic case when your inputs and outputs are not boolean at all but for now in this lecture at least try to see if you can have a solution for the case where your inputs and outputs are a boolean so that's what we'll do for now okay so i'll end this module here and in the next module we'll try to come up with a network of perceptrons which can handle boolean functions which are not linearly separable even though we know that a single perceptron cannot handle boolean functions which are not linearly