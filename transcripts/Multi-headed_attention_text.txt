foreign [Music] block within the Transformer architecture and we saw that the entire self-attention can be done in parallel for all the capital T tokens and it all happens to these Matrix multiplications right so in effect what is happening is the following right so this is what is known as a scaled dot product self-attention and this is called one head and soon we'll see multiple such heads but we'll get there when we get there but for now just remember this is called cell scaled dot product based self attention so what exactly is happening here uh so this so this purple box here right this is a scale dot product unit right so this is what is lying inside this purple block Here and Now what is the input to the Box let's see uh so you get the key uh query and the value matrices how are these constructed originally remember at the input all you had were these H1 to HT right so you had these word representations for the keywords and we were calling them as H1 H2 and so on right now from the H1 H2 what happens uh you pass them through the linear projection right through the WQ W K and W V matrices to get the Q K and V matrices right so this is just multiplying WQ by these vectors and you just stack up these vectors into a matrix so that you can use this 2W key multiplied by that H Matrix right let's just call it h then you get the Q Matrix similarly w k multiplied by this gives you the K Matrix and WV multiplied by this gives you the V Matrix right all of this happening in parallel as uh three Matrix operations then you get the qkv and then this is what happens inside the self-attention head and at the output what do you get you get a Z1 you get Z1 Z2 all the way up to ZT right so this is what we had seen uh when we ended the last lecture now this is called one head so what what I mean by a head here so this is a one such unit which takes the inputs H1 to H2 and gives you the elements Z1 to Z2 right or the refined representation Z1 to ZT which also take care of the contextual information because they depend on the key they depend on the value and they depend on the query right so they take care of the contextual information now you could have one more such block right so you could have the same block repeated so let me just call this z11z21 and z uh one t right so this is the output of the first attention head similarly you could have the same block repeated where you have another set of parameters right so let me just call these uh W1 right these are the first set of eight matrices you have similarly you could have another's repeated block right and let me just show it on the next slide we just repeat these two blocks right and now you have head one head two and head one is giving you representations Z1 and this is giving you representations Z2 Now by several questions here right why would you have two heads and then if you are getting these two representations which one do you consider right so we'll answer those two questions but first let us motivate right why would you need multiple heads and we have already seen this in a different context before right so we want to motivate multi multiple heads in attention right so one head is uh one uh one one scale dot product unit which gives you Z one to Z T right the representation Z1 to Z T and I'm making a case for many such heads and so why why is why am I doing this right so we've already seen this in the context of convolutional neural networks right so where we had multiple filters right so these are three different filters uh operating on the same image right and each filter essentially does the same thing it has parameters say W1 W2 W3 it just goes over the image and gives you an output feature map right and the reason we wanted to capture have multiple filters is that we were hoping that each filter May capture a different characteristic from the image right some may detect edges some may detect blurs and so on right so that's why you had multiple filters right and more the filters the more abstract representations you could compute right the same argument holds here if you have one self attention then it will capture uh we'll learn one way of capturing the contextual information right but there might be more than one way of capturing this contextual information right so you might also want to have a situation where it focuses on animal with a very high attention and you might also want it to focus on some verb with a very high attention right and both of these might be important because one is indicating that it is a pronoun for animal and the other might be indicating that it is the object of this verb right and so in both both of these you might want to capture so one head could learn to give more attention to animal the other I had could learn to give more attention to this war right so just as you had multiple free filters to capture multiple characteristics from the image you could have this multiple attention headset and let's let's look at it a bit more carefully right so here's an example so the animal didn't cross the street because it was too tired right and I have the same copy of the sentence here and now I'm trying to learn a contextual representation for it and now I want to focus more on walks right because I want to know was what is the subject of it right so it is the subject here so I want to know that so I want to capture that information by paying more attention to Vos right but I also earlier made a case so what does that mean if I'm learning the alphas right so if I'm learning say let's this is one two three four five six seven eight nine ten this is the tenth word so if I'm learning Alpha 10 then this is Alpha 10 1 Alpha 10 2 right and this is Alpha 10 11 right so this is the uh attention that should be paid on the 11th word when you are Computing a refined representation for the tenth word right and I want this to be high right but I had also earlier made the case that I want the attention on animal to be high because animal is the uh I mean it is referring animal animal is the word for which it is the pronoun in this case right so that means if I were to compute again the weights Alpha 10 and so on then I want this to be high so I also have a case for this to be high I also have a case for this to be high so one way of dealing with this would we have two separate attention heads right and compute two Alphas one from one block of self attention another from the other block of self attention and this block could learn to give more importance to this Alpha and the other block could learn to give more importance to this Alpha right now of course this is slightly make believe right we understand that because we have already visited this in the case of recurrent neural networks now there is no signal right we are not telling the model that hey you need to pay more attention to was or hey you need to pay more attention to animal we just hope that when we are looking at the final loss function and if it indeed is beneficial to focus more on Wars right that means have a higher weight for Alpha which in turn will contribute uh accordingly when we try to compute the refined representation for say the tenth word right and that effectively reduces the loss so it would then learn to have a higher weight for wash right similarly if it helps so this is z 110 similarly if you had Z coming out from the other attention head and this will also participate in the fuel computation and then participate in the loss so if it helps that now this set of Alphas should be such that it should focus more on the word animal and if the loss dictates that then the machine would learn that or the model would learn that right so that we're not giving it an explicit signal we're just hoping that by Trying to minimize the loss it has more freedom now in some cases it can learn to put a high alpha here in some cases it can put a learn to put a high alpha here right so you're making a better design Choice by allowing it more uh choices right or allowing it more parameters in terms of the W's the projection matrices which in turn result in different Alphas right so just making it a more flexible model which has more choices or more options to how to adjust the alpha in different cases so it might choose to have Alpha was high for one case Alpha animal high in the other case and then do something different in the third head and so on right so that's the motivation for having multiple heads so I just already explained this I'll just skip over this the same thing whatever I explained that in one case it would want to focus on walls the other case it might want to focus question animal and this is actually from a actual train transformer right so we looked at the attention weights there were two heads and we looked at what the attention weights were and we found that in one case it is giving higher in one of the heads that is giving higher attention to animal and the other head it is giving higher attention to Wars right so it does learn to do such things right so this is what a two-headed attention would look like you would have the Q uh query uh sorry you'd have the query key and value vectors right which are coming from your projections say this should have been this is the H Matrix right which is H1 H2 all the way up to h t and you get out here is q k v okay so this is again the H Matrix and what you get out here is q k v right so you just have these two copies now this as I said would release Z1 Z2 all the way up to Z T and I'll just call it Z 1 1 and so on and this would give you Z One Z two although we have to Z T and I'll call it z2s right now what do I do with this I have two representations now computed for this word H1 to h t right so now what do I do with these two Z's simple I just concatenate them so that's all I'm going to do so you concatenate it so you get a larger representation and then you'd pass that to a linear transformation right so we'll see that uh soon and then what you get is the final output right so what let's just look at this carefully right what is happening here so again let me just look at some it's important that I get rid of this okay so this is the H here okay and let's just focus on H1 right now H1 suppose H1 was uh five and two dimensional vector right so now what I could do is uh I will choose W to be 512 cross 256. okay I'm just giving you some example so that means the projection which comes out right my q k v would be 256 dimensional right because it's 5 and 2 multiplied by a 5 and 2 cross 256 Matrix right or rather actually this would be um yeah so you get it it's I'll get I'll just project it down so this will be 256 dimensional right so at the output again I'll get 256 dimensional Z's in both the cases now when they concatenate I again get a five and two dimensional output right and this I could again pass it through whatever uh transformation I want right for example I could choose a 512 cross 1024 I could choose a 512 plus 256 or I could choose a 512 cross 512 right and depending on that I will know what my output Dimension would be if I choose this then my Z final Z is coming out of here would be 5 and 2 dimensional right so let's just understand it correctly so this H1 gave me a 256 dimensional Z1 this through this network or through this self-attention it I got another 256 dimensional H1 and then I got 512 dimensional uh output here which then again I pass it through a linear transformation right so it's because you're going to concatenate it makes sense that you the output of each of these is small right because if each of these is five and two dimensional then you concatenate you will keep going larger right and typically you use eight heads so now if each of this is five and two dimensional and then you concat it in them then you'll get a four zero nine six dimensional output here which is two larger because it increases the size of the parameters that will go through a linear transformation and so on right so typically what you do is if you want five and two dimensional size here right then you make sure that your each of your eight heads gives you a 64 dimensional output so when you concatenate then you get a five and two dimensional output so you start with the five and two dimensional output you adjust these Dimensions such that you get a 64 dimensional output at each of these heads you have eight such heads so when you concatenate them you'll again get a five and two dimensional output then you do an appropriate linear transformation you could choose this one so that your final z's are also five and two Dimension right so that's what you could do right so this is what a two-headed attention would look like and I've already told you how to extend it to multi heads you will just have the same block repeated as many times as you want and then finally you would just adjust all these Transformations right so as I said I could uh let's just look at it again if my edges are five and two dimensional these are H's then I pass them through I multiply them by a 64 cross 5 and 2 Matrix so I get 64 dimensional outputs here 64 dimensional outputs here same happens in all the eight heads so when I concatenate them I get a five and two dimensional output right so whatever I started with I get the same so I can just adjust the parameters accordingly and then I do a linear transformation to get my final Z1 to ZT right so remember this concatenation is happening per word right that means uh the Z1 representations coming out of each of these are getting concatenated here then the Z2 representations coming out of each of these are getting concatenated here right uh so it's per word so the input is a set of words you have capital t word embeddings and the output at this layer or at every layer right here here here at all these layers the output is again capital T embeddings right so that's you should remember that so we are done so we have the multi-headed attention okay so we are back to the basic block that we had so this is what we had we had these uh inputs coming in here right and then now we have seen this self attention in detail which could be a multi-headed self attention and I gave it inputs H1 H2 h t and then through all the processing that happens inside I get outputs I think I was calling these as S1 S2 all the way up to s t right so once I have got this now I need to understand what happens in the feed forward neural network right so let's uh focus on that now right and this encoder is typically a stacked encoder so you'll have six such blocks here that's why I'm calling this a basic building block this is one layer right so you passed in H1 to h t you got out Z1 to ZT now this Z1 to ZT becomes input to another such layer and again you get a new set of representations out from your capital T representations out right this way I've seen that the output of one layer acts as the input to the next layer right so all of this this looks identical in all these blocks and there could be 6 8 12 such blocks depending on the transform architecture that you are looking at now let's see what happens in the feed forward Network so now you had uh you so these are what the final output is of the feed forward network is z this intermediate output coming out of the self attention I should have called it s and this is the input H1 right so now what exactly happens in the feed forward neural network right nothing it's quite simple so remember that each of these guys here is a five and two dimensional representation or some D dimensional representation it is going to pass through a feed forward neutral Network and again give you a d dimensional representation at the output right that's all that is happening here so feed forward neural network is only acting as a projection layer here right so uh so this is the input S1 okay there's some intermediate being computed let me just call it uh say m okay right and then you get Z1 at the output right so this could again be five and two dimensional input one zero two four dimensional projection and then again five into dimensional output and of course there would be a non-linearity here you could use any non-linearity that you want right and typically it is one of the relu based either gelu or one of those normal non-linearities okay and the same set of parameters right so this here would have some parameters right so you'll have some W's here and then some another set of W's here right so let me just call them W Feed forward Network and let me call this W1 because it's layer 1 and W2 Layer Two right so the same set of parameters will be used everywhere right so each of these s1s or sis will pass to the same transition and give you the corresponding zi right so that's what I'm going to show with the animation that the same network is essentially being used everywhere right so you get this same output everywhere right so this uh yeah so you have the same network for each position and uh you use this uh as the this is the non-linearity that you're going to uh use right uh so nothing great happening within the feed forward neural network whatever output the multi-headed attention gives it just projects it and then gives you back a final output right so that's all we have done uh with the uh so that's all we are done with the uh encoder layer right so this is one layer of the encoder and now I could stack many such layers but each layer the internal working would remain the same just the output of the previous layer will be the input to this layer so nothing else changes right so we are done with the encoder part of the Transformer so the encoder is composed of n such identical layers and each layer is composed of these two sub layers one is the multi-headed attention and the other is the feed forward neural network uh and so the computer is computation is paralyzed in the horizontal direction right so what do I mean by that is that you of course so if you have these n layers right of course you cannot compute all the layers in parallel right because layer 2 will take the output of layer 1 as input right so unless you have done the layer 1 computation you cannot do the layer to computation right so when I say it's parallelized it's only within each layer right so for a given uh set of input samples right within that layer all the self attentions all the alphas all these Z they'll all get computed in parallel right unless I mean earlier when uh again I'll just repeat this because this is important in the case of an RNN when you are given H1 to h t and you had to compute Z1 to ZT right you first had to compute Z1 z2's and ZT and so on right and here we saw that using this large Matrix multiplications we get Z1 to ZT in parallel right you don't have to wait for the previous time step for the next time step to be computed right so this parallelism you see in every layer but of course across layers the computation is still sequential right because you need the previous layers output to do the next layers computation you have the input then the layer one outputs get computed then it feeds to Layer Two and so on till the end and each of these T cross 5 and 2 outputs get computed in parallel right and now this final output of the uh encoder right which is the output from the last layer we are going to denote it as e right so we'll refer to it as E1 to E capital T because there are t such tokens in the input and for each token you get this final refine representation which is contextual as well as gone through several layers of abstraction or several deep layers right