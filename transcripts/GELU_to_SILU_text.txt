foreign [Music] function that I want to talk about is gelu which is gaussian error linear unit it was first introduced in the context of Transformer model the first Transformer model around 2017 used this gelu activation function so let's motivate that so first we'll start with uh noting some similarity between relu and this hard thresholding right so this is what our hard thresholding looks like it is 1 if x is greater than 0 and it is 0 if x is less than or equal to zero right that's what the hard thresholding looks like and now let's look at what the value function looks like the value function can also be written in a similar form as f of x is equal to 1 into x if x is greater than 0 and maybe 0 into X if x is less than or equal to 0 this is how I'm going to rewrite it on the next slide so main thing to note here is that both the activation functions the output depends on the sign of the input if the sign is positive then you get a certain output if the sign is negative then you get a certain output right and now I'm just going to rewrite the radioactivation function as I said on the previous slide I could think of it as 1 into x if x greater than 0 or 0 into x if x is less than equal to 0 right now uh the Dropout function also does the same right I mean the Dropout idea of Dropout is that you have a certain number of neurons and if you apply the mask then you could think of this mask as the activation function which either zeros out the neuron or lets it pass through right and then apply some activation function on top of that right so it also does the same the only difference is that instead of one and zero it has this stochastic uh uh nature right so it's not always one not always zero it's decided based on a coin toss right so it could be uh the output is 1 into X if with probability p and 0 into x with probability 1 minus P right so all of these uh you just I want you to notice the similarities between this right so this function form it's looking similar but the conditions are a bit different it is X greater than 0 x less than or equal to 0 here it is with probability P it is 1 into x with probability uh uh 1 minus P it is 0 into X right so then it does make sense right it makes a case for combining these ideas where in the relu you have this uh similarity to the hard activation function and Dropout brings in this stochasticity so can you combine these two ideas and try to come up with a different activation function and that's where we are headed so how about writing the following right that I have an activation function which is M into X right and now m is a Bernoulli random variable so that means uh M takes on the value 0 or 1 with probability Phi X right so when I say I think you're used to this Bernoulli um of P which means that with probability P the output would be heads or 1 and with probability 1 minus P the output would be Tails or zero right so now instead of P the parameter is Phi X and Phi X is itself a function of uh the input right so that's how you are setting up the activation function and now what could this Phi X be right so let's make some observations this Phi X is supposed to be a probability right because it has to be the probability of heads or probability of one so it should lie between 0 to 1. right so that's the idea the Phi of x should lie between uh zero to one right that's the uh range that it can have right now what is the function that you know which gives you output between zero to one so the obvious choice which comes to minus the logistic function right so you could think of Phi of X as a logistic function of the input that you have now what is the input here let's try to understand that clearly right I am saying X but let's not confuse uh say this is your deep neural network and this is some hidden layer right let's call this the H2 hidden layer okay and now you have this preactivation let me call it a21 and this is the activation h21 right so this is the function that I am defining right h21 is equal to F of a to 1 and I have defined it as M into a to 1 where M itself is a function of a 2 sorry M uh m is a random variable which is Bernoulli distributed with the parameter Phi E21 right so that's the what I'm trying to that's how this whole picture is right so you have a neural network within that you have multiple hidden layers I'm looking at one of those hidden layers I'm looking at one of those neurons and I want to define the activation function which is this and this is how I have defined the activation function and now you can see that X actually is a21 which is just the preactivation of that neuron and now I want to uh get a value between 0 to 1 based on a21 so one of the simplest choice would be to just do one over one plus e raised to minus a218 this is the logistic function that we had seen and we know that it gives values between 0 to 1 right so that is one choice the other choice since we are in the domain of probability is to use the cumulative distribution function of the normal distribution right so the probability density function of the normal distribution looks like this right this is a bell shaped and if you look at the cumulative density uh function then it again has this s shape right because it again uh at minus infinity it's around zero and around 1 when you have integrated over the entire uh possibility of values you will get a probability of one right because a cumulative density function tells you P of X taking on values less than equal to X so P of x less than equal to Infinity is going to be 1 right so as this approaches Infinity uh your CDF would hit one right so that's another function that you can use so you could make the this function Phi of X as uh take it as the cumulative density function uh and parameterize of of the normal uh uh distribution and then parameterized by this input that you have right so these are the possibilities so overall I mean I said a lot of things let me quickly summarize so we first do this analogy between relu and hard activation and then rewrote a value accordingly then we try to see that relu and Dropout seem to be doing something with this stochastic uh difference between them right Dropout does this stochastically withdrawal DP and minus P so why not bring that stochasticity into the function itself and how I brought it in I have defined a random variable M right so now this random variable will take on certain value 1 or 0 with a certain probability and hence the activation function would differ would depend on X as well as have this stochastic feature because of this random variable and now this random variable needs to follow a certain distribution so it's a Bernoulli distribution because this random variable can take on value 0 to 1 so it's a binary random variable so it will follow a Bernoulli distribution and the bernalia distribution has a parameter p and now I am saying in this case the parameter P would just be a function of the input right which is as I and I also explained what this input is this input is the pre-activation at that neuron right so that's The quick summary of whatever I have said so far ah so it can be the logistic function or it can be the cumulative distribution of the standard normal distribution right so that's what I already uh explained uh earlier okay so now let's uh go further yeah so what is the expected value of f of x let's try to find that so expected value of f of x is the same as the expected value of M into X and uh that since it follows the Bernoulli distribution I can just write it with probability uh Phi X it would take on the value 1 then multiply it by X right and then with probability 1 minus 5x it will take on the value 0 and then multiply it by X and this effectively would just be Phi of x into X right and what was Phi of X it was the cumulative density function of the normal distribution so Phi of X just basically means the probability of X being less than or equal to X right that's how what that's what Phi of X means right so if I were to draw the cumulative density function then if I were to this is my x axis so if I take any X I'll get a value here and this value is essentially the P of X taking on values less than or equal to x z because it has been obtained by the integration of this curve and then till this point whatever was the area under the density curve that is what gets plotted here right so this is just probability of X less than or equal to X right and now this I could uh since this this is the probability density function of the uh of the normal distribution I know how to compute this there are certain approximations available for that and I can use those approximations right so what my FX actually is I can write F of uh I can write this quantity okay let me just draw the other one so so I can write this quantity okay you ignore the X here because this x corresponds to this x right so this quantity P of X less than or equal to is given by this nasty formula without this x right and then I can just multiply it by X to get the formula that I have here another approximation of it is this right so it can also be given as X into Sigma of 1.702 X and I've drawn both these formula here and you can see that they are very close to each other and look like the CDF of the normal distribution right so that's why effectively uh the gelu activation function turns out to be f of x is equal to X into Sigma of 1.702 X quite a weird function but well motivated right so this formula if I just give you you don't you won't understand where it came from but now you from this derivation you understand that it's multiplying the CDF value with the X itself and the CD F value there are multiple ways of approximating it and we have chosen one particular way of approximating it which is this right so that's that's how the Galu activation function is arrived at and again The quick summary was that you took relu you wrote it as 1X into 0x you took Dropout you understood that it's 1 and 0 with probability p n minus p and then you combine these two ideas brought it this random variable M then wanted to define the function X so you said okay what's the expected value of this function so this is what the expected value is you know how to deal with this quantity you can just write it as the formula for the CDF and there are multiple choices you chose one of the simpler choices and you came up with this neat uh slightly neat looking formula for the Galu activation function right so that's how you arrive at gelu uh and this is how it would look if you plot it so you have seen a relu we had seen the exponential uh linear unit and now we have seen this gelu which is this red colored uh right uh similarly you could have uh another a few other activation functions which are again kind of variants of relu itself so you have the scale exponential linear unit so remember we had seen the exponential linear unit and now how how was scaled exponential linear unit motivated from there we observed that these leaky relu and those variants they are not completely uh zero center right and we will see that when we draw the plot so we want this to be zero centered so one way to do that is using uh something known as normalization techniques which we'll see later on the course the other way is to kind of change the activation function itself so it has some normalization in it and that's exactly what cellu does right so let's let's try to understand what I was saying right so if you look at relu none of them are zero centered right and we saw what zero centered means in the case of the tan H function that there is equal distribution around a negative and positive clearly relu is not zero centered because along the positive axis it can take very large values along the negative side it only takes zero value so it's not zero centered exponentially uh Lu is slightly better because it has some values towards uh the negative so it has some balance right I mean it's still tilted towards a positive but at least some weight along the negative same with gelu but now if you scale this exponential linear unit by an appropriate Lambda then you could have some of equal distribution along these two paths and then you get almost zero centering and it depends on how you choose the value of Lambda and a few values of Lambda that people have suggested uh empirically uh derived are these Lambda values are Lambda equal to 1.05 or Lambda equal to 1.67 if you do this then you get some sort of zero centering and that's exactly what cellu tries to do right uh now upper now after these activation functions uh came up right then of course this question arises I mean till when can I keep discovering these functions right is there a better way of searching these activation functions can I automatically search for them right and there was this work which tried to search for Activation functions so think that there are many functions right this is a space of all functions possible you have Max you have I mean you have the relu here you have the logistic sigmoid tan H everything here right and there are many functions here now what they observed is that any such function is composed of two operations or unary operation this is what unary operation is so negation of x which you see here which operates only on a single input is a unary operator absolute value of x which again operates on a single input is a unary function a into X is again a unary function or exponent of all of these are unary functions so what are binary functions binary functions are functions which take two inputs right so this Max of X1 comma X2 is a binary function X1 into sigmoid or some X2 is again a binary function right so either these have unary so these have a combination of unary and binary uh operators so what if I tried many possible combinations of these unary and binary operators right these are easier to Define there's a limited space of unity operators that I consider a limited space of binary operators that I consider of course there are many possibilities here but if I restrict that search space and then try out all these combinations and then try to see which one gives me the best performance then can I discover better activation functions and this requires some sort of reinforcement learning which is beyond on the scope of this but what you can think right at a Layman level this is how how you could understand it there's a large space of functions right and you uh You observe that these functions are composed of certain operations so can you compose functions in different ways right and then try to see which composition gives you a better way and do this search in an Optimum way using some ideas from RL and then do you arrive at better activation functions right so they did this and the many activation functions popped out right so you can see for reference here this one looks somewhat like the elu gelu family right but there are other activation functions which also popped out and what they uh arrived it as that any function of this form which is X into sigmoid of a beta into X is a good activation function and this is actually uh the swish activation function and if you set the constant to 1.702 then you get uh gelu or if you make it a learnable parameter then you have uh the switch activation function right so they again when they try to search this they again arrived at this form which was similar to the form that gelu had and the generic form which is parametric is called Swish and if you set it to a specific value then you get the gelu activation function right and so so we call this one as swish we call the specific instance as gelu now there's what would we just call X Sigma x i so this is a specific case where beta is equal to 1 and this is called the silu which is sigmoid weighted linear unit right so you have the uh linear unit here and then you have the sigmoid weighted so that's what silu is uh so many activation functions right so we have seen quite a few now uh let me just three calls sigmoid tan H relu then we had leaky relu then we had parametric relu then we had elu then we had max out then we had gelu then we came to swish which was a generalization of gelu then we again saw silu which is yet another specialization of glue so this kind of covers all the popular activation functions that are out there now of course the main question is given so many activation functions which one should I actually use right so that's one question of interest and uh this is this plot right from the website papers with code shows how these uh how the use of these activation functions has evolved over time so this was around the time uh the Transformer idea became popular and that is the context in which the Galu activation function was proposed and now as the as Transformers are kind of popular this is one of the most popular activation functions today but relu over a large I mean almost greater than a decade now it has maintained its popularity so relu and Galu are two important activation functions which are uh quite uh popular even today sigmoid is again used it has because of its nice property of zero to one which makes it like a uh something that can look like a probability function it still has use in some activation functions inside in some attention functions and so on uh so it's still uh in some use so sigmoid and tanh are again used despite their multiple disadvantage advantages that we spoke about but they are still popular but relu and gelu are the most popular activation functions out there today right so with that a quick summary so sigmoids are generally bad we saw them but whenever you need something to be between 0 to 1 then we'll see a few cases where we want that uh and that time sigmoids are useful uh relu is more or less a standard unit for convolutional neural networks you can try some of these variants of value but they have not been so popular uh tanh is still used in lstms and rnns which we'll see later and gelu is commonly used in Transformer right so I would say tannage because it's required in lstm and rnns it's still popular and then gelu for Transformers and then relu for convolutional neural networks right so these are the three top activation functions which are currently still popular right so with that I'll end this discussion on activation functions and that's the end of the lectures for this week and next week we'll come and talk about weight initialization so that was the fourth pillar along which people made uh progress after this 2006 to 2009 period when people realized that there is a way to make deep learning work or deep neural networks train better and to to these are the four axes along which we should investigate which is optimization regularization activation function and weight initialization so the first three we are done with now we look at the fourth in the next lecture thank you