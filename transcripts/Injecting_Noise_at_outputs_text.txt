foreign [Music] technique that we are going to look at is adding noise to the outputs so we saw adding noise to the inputs but you could also add noise to the output so let's see how what that means so this is what your output is right so you are given a certain image so let's look at the classification problem and this is what your true Y is going to look like all the probability masses on the correct level which is 2 in this case so 0 1 2 all the way up to 9 and everything else is 0 right and now this is what your true Y is and you are trying to predict uh y hat or f x ah F hat of X which is very close to this 2 y right and that could lead to overfitting because if you have you if you have a lot of parameters you know that this error you can drive down to zero but now what if I do the following right so I instead of using this output the true output that was given to me uh and this is what my loss this is what I'm going to minimize right this is my cross entropy loss that I'm going to minimize so P here is the true label and Q is the predicted label so instead of using the true distribution what if I say that I don't trust the true labels they may be noisy or I don't want to trust the true labels because then I am trying to map my input exactly to the output and that is what overfitting is right so instead I am going to add some noise to the outputs what I'm going to do is that in my original distribution all my probability Mass was on the correct label right so what I'm going to do is I'm going to take away a small probability Mass from there and distribute it to the other labels right so now instead of all the mass being on one and everything else being 0 I'm instead of using those hard targets now I'm using the soft targets where there's a small non-zero probability for all the other outputs also right so this I have corrupted the output in some sense but not corrupted it by a lot right because still if Epsilon is small and Epsilon will be small my majority of the probability mass is still on the correct level but now when I apply my formula the cross entropy formula earlier in my Pi log UI if you remember only one term remained right the one which correspond to p i equal to 1 so only this the label the second term would remain but now because all the other values were 0 but now all the other values are not zero right so my computation is changing so the loss that I am actually trying to minimize is uh changing and that will act as a regularizer right because now you are not trying to minimize the true loss but a slightly corrupted version of the loss because you have made some change to the outputs right so that's what uh that's what you do in uh adding noise to the outputs and now since you are not what what you're doing done is again you have added some kind of a corruption to the loss function right so earlier you are looking at L theta plus Omega Theta right and now again if you open this up right and now again what has happened is earlier you had only uh so what you have now is Epsilon I Epsilon into log of P1 plus Epsilon into log of uh sorry P0 P1 Plus Epsilon into log of P3 log of P 9 and then you also have this 1 minus Epsilon into log of P2 which was the correct label right for the correct label this is the weight so now you can again think of this this was actually your true loss earlier without regularization right so you can think of this as L Theta so now you have some weighted L Theta so let me just call it as alpha 1 L Theta where alpha 1 is 1 minus Epsilon and then you have Plus or let me just call it 1 minus Epsilon only right so you have 1 minus Epsilon times your earlier loss plus Epsilon times some other loss which I can call as Omega Theta now so again you can see that you are doing some kind of a regularization here and that will help avoid overfitting right so that's the idea behind adding noise to the output levels so we'll end this here