[Music] okay so we are now at the last uh module for lecture two where we will be introducing a network of perceptrons right so now the word network has come in and will be seeing what a network looks like and then we will talk about the representation power of such a network of perceptrons right so what we are going to do is we are going to see how do you implement any boolean function irrespective of whether it is linearly separable or not to begin with using a network of perceptrons although we know that we're using a single perceptron we cannot build it all right we cannot implement okay so now for this discussion uh this is more like an illustrative proof right so we are going to not have a proof which has like a set of mathematical steps it will just be like we'll prove something by constructing a network itself right so let's for this discussion right we'll assume that uh true is equal to plus 1 and false equal to minus 1 as opposed to 0 and 1 that we have been using so far right and we'll consider the case of two inputs okay and we'll have four perceptrons so since i'm going to have a network of perceptrons it is clear that i'm going to have more than one perceptron so now i'm going to start with four perceptrons right and each input is connected to all the four perceptrons okay with specific weights and the red weights are minus one and the blue weights are plus one okay why so all that we are not considering right now it's just that you have two inputs four perceptrons every input is connected to every perceptron by specific weights some weights are minus one some weights are plus one right and then the bias or w naught is equal to minus two okay so w naught is minus two so what is the perceptron rule w naught plus the weighted sum of the inputs should be greater than zero for it to be one so what does that mean if w naught is minus 2 then the perceptron will fire only if the weighted sum of the inputs is greater than 2 because then you will have minus 2 plus a quantity which is greater than 2 so that quantity would be greater than 0 greater than equal to 0 and then the perceptron will fire right so it means that the weighted sum of the inputs should be greater than equal to two so this is all i have done this match fixing right there is i've like hard coded everything so far okay then i have one more perceptron which is my output perceptron what does that mean that the output of this perceptron will be the output of the network and if i want to implement a function if i say i want to implement the xor function using this network that means that for a given input 0 0 0 1 1 0 1 1 the output of this green perceptron should match the truth table of the xor function then i would say that i have implemented the xor function that is what i want and now each of these perceptrons the red perceptrons is connected to the green perceptron using a weight and i'm going to call those weights as w1 w2 w3 w4 okay and then the output is y this is all the setup that i have now why this setup where am i headed all that is not clear but you cannot stop me from having this setup all this is straight forward i have two inputs four perceptrons and then one output perceptron for the first set of red perceptrons i have hard coded the widths okay now i'm going to introduce some terminology and this terminology is going to stay with us for the rest of the course right so this network contains three layers the layer containing the inputs is called the input layer so x1 x2 is the input layer then the layer containing the four perceptrons is called the hidden layer right because this is between the input and the output so it is not exposed to the outer world i have something happening here which the outer world does not care about i don't care about what is the output of these red perceptrons i care about the output of the y person or the green perceptron this is a hidden layer and the output neuron is what is forming the output layer right so i have three layers input layer hidden layer and output layer right and the outputs of the four perceptrons in the hidden layer i am denoting them as h1 h2 h3h4 the output of the final layer i am denoting it as y and the red and blue edges are called the layer 1 weights and w1 w2 w3 w4 are called layer 2 weights okay so there are two layers one hidden layer one output layer and i'm not counting the input layer because it's there right so this layer one weights are called the red and blue edges are the layer 1 weights and w 1 w 2 w 3 4 are the layer 2 panes okay this is all definition all terminology now is the interesting part i'm going to claim that this network can be used to implement any boolean function whether that function is linearly separable or not what does that mean it means what i'm trying to say is that i can i have this generic template which has now four weights w1 w2 w3 w4 everything else in the network has been hard coded right the only four variables are w1 w2 w34 using this template i am claiming that i can learn implement any of those 16 boolean functions which i had shown including the xor and the naught of xor function right which uh are not linearly separable to billing below begin with right so what does that mean is that i can learn specific values of w1 w2 w3 w4 for each of these 16 boolean functions and these might be different values for the different functions such that when i plug in those values and now i start feeding 0 0 0 1 1 0 1 1 then my output would be the desired output as per the truth value of that function that's the claim i am making and this looks like an astonishing claim right because i know that some of these functions are not linearly separable that's one reason why this is an astonishing claim because i'm claiming that even non-linearly separable boolean functions i can start the other reason why this is an astonishing claim is that this is like a single template i'm not having like a separate network or separate configuration for different boolean functions i have the same configuration the only thing that is changing is the weights of w 1 w 2 w 3 w 4 if i change those values then i can implement any boolean function right this may look like an astonishing claim but it's not really if you really understand what is going on so what is going on here let us try to understand that right so each perceptron in the middle layer fires only for a specific input let's see why that happens right so as i said the red weights are minus 1 minus 1 okay now if the input is minus 1 minus 1 remember that 0 is minus 1 here so these are the 4 inputs that are possible minus 1 1 1 minus 1 and 1 1. these are the four inputs that fire now if the input is minus 1 minus 1 what will happen minus 1 into minus 1 will be 1 minus 1 into minus 1 will be 1. so the sum will be two so the weighted sum would be greater than equal to zero minus two plus two hence this perceptron will fire okay and now i will show that this perceptron will not fire for any of the other three inputs right if this is the input then you have minus 1 into minus 1 which is 1 1 into minus 1 which is minus 1 so 1 minus 1 will become 0 and plus the bias will become minus 2 which is less than 0 so this perceptron will not fire similarly you can see that for these two inputs this perceptron will not fire so this specific perceptron is only going to fire for this input because that's how i have confided the weights that's why i had this two red edges there minus one minus 1 because with that set of weights the only way this perceptron can fire is for the input minus 1 minus is that okay right similarly you can now go back and check right that the second perceptron the first perceptron will only fire from minus one minus one with the same argument the second perceptron will only fire for minus one one i'll show you how so this is minus one this is one so if your input is minus one minus 1 so we'll get minus 1 into 1 is 1 1 into 1 is 1 so 1 plus 1 is 2 plus the bias is minus 2 so this quantity would be greater than equal to 0 hence the perceptron will fire and you can check for any of the other 3 inputs it will not fire similarly the third perceptron will only fire for one minus one the fourth perceptron will only fire for one comma one okay so each perceptron is like kind of specializing for a specific input and it will only fire for that input and the question is so what right i mean what what how does this show that it can handle any function right so we need to go back to the uh the table that we had and the algebra the calculations that were happening there right so let's see what is happening here so if you have the xor function now xor function is the troublesome function because it's a not it's not a linearly separable function so let's see what will happen in this network for the xor function so when you have the input as 0 0 now when i say 0 0 assume minus 1 minus 1 the xor truth table says that the output should be 0 right now we know that h1 h2 s3 h4 the h1 will take on the value 1 everything else would be 0. so if i look at summation w i hi it will just be w 1 correct right it will be w 1 into h 1 plus w 2 into h 2 and so on so h 2 h 3 h 4 are 0 so the last 3 terms will be 0 and the first term which is w 1 will remain now if the input is 1 0 then for the xor truth table i know that the output should be 1. now my h1 h2 s3 h4 will take on these values and my output would be only w2 because my output is the summation of wi-hi now 3 terms will go to zero there only the w2 into h2 term will remain which will just be w2 because h2 is one that okay now for the next input w3 the next input w4 right now if i go back to my conditions what are the conditions that i want so if i want this to behave like the xor function then w one should be less than w naught right in that case it will not fire and hence it will be the same as the truth table of the xor function let me just clear some things so w 1 should be less than w naught if that is the case then it will not fire and i will get the output the desired output is that ok so this is minus 1 right uh ah okay okay oh this is the truth table yeah the two table has a problem so this should have been 0 1 and this should have been 1 0 okay is always fine now ok so now for this to behave so this is the aggregated that i get right now what is my y if the aggregation is less than w naught then it will be 0 if it is greater than w naught it will be 1 right so now for this to behave like the xor function w 1 should be less than w naught w 2 should be greater than equal to w naught w 3 should be greater than equal to w naught w 4 should be less than w 1. there is nothing new i have done here we did the same exercise earlier on right but now what is the difference do i have any contradictions here i do not have any contradictions here earlier when i had the xor function with a single perceptron i had contradictions in these inequalities now i don't have any contradictions because i have four weights w one w two w three w four i have four independent conditions that they need to satisfy there is no condition that is contradicting right and why is this happening because this is how i have designed it every neuron here every perceptron here is firing for a specific input and hence that is leading to a condition on a specific weight and two weights are not interacting with each other in any condition hence there are no contradictions that are happening here right so the network has been designed in a way such that you have four possible inputs and you have now these four possible weights so you have these four degrees of freedom so each weight can adjust to satisfy a given input does that make sense right so that's what is happening here i have come up with conditions which are not overlapping or contradicting with each other so hence irrespective of what the boolean function is now you can go and try the same thing for any other boolean function you can go and figure out the conditions and the same thing will happen you will have only w one in one condition w two in another condition w three in another condition w four in another condition right and hence you can always satisfy this by adjusting the w one to w four in a particular manner is that okay fine so that is why since there are no conditions each input is being handled by a specific perceptron hence you can implement any boolean function in this case right and not just the xr function as i said you can implement any of the 16 boolean functions so for each boolean function you will get a different set of conditions for w 1 w 2 w 3 w 4 and you can find some values of w 1 w 2 w 3 w 4 which will satisfy that particular boolean function right so you can go back and try this out okay now what if we have more than three inputs can you think of the same template now and adapt it for the case of more than three inputs the same idea will repeat you will have x1 x2 x3 now instead of four perceptrons in the middle layer you will have eight why because there are eight possible inputs now zero zero zero up to one one one so each of these perceptrons will adjust these red and blue weights in a way that for each perceptron only fires for a specific input right the first perceptron will fire when it's zero zero zero second will fire when it's zero zero one and so on right so now each perceptron again fires for a specific input each perceptron interacts with the green perceptron by one of the weights w1 to w8 so again when you write down the conditions in the truth table you'll have conditions like w1 has a specific condition w 2 has a specific condition none of these conditions will contradict none of the weights will interact with each other in any of these conditions so you can find the values of w 1 to w 8 such that all your 8 inequalities are satisfied for any of the boolean functions right so again you will be able to implement all the boolean functions which are possible okay right so that's the same template that you have used so what if we have uh n inputs right more than three inputs which is n inputs okay so then how many perceptrons will you need in the middle layer 2 to the power n right so when you had two inputs you needed four when you had three inputs you had eight so when you have n inputs there are two raised to n possible values that the inputs can take right so each perceptron will handle one of these specific inputs and then you will have two raised to n weights in the output layer and you could adjust them right yeah yeah bias will stay the same minus n right you could hand code the bias right so now what does this theorem say now any boolean function of n inputs can be represented exactly by a network of perceptrons containing one hidden layer with two raised to n perceptrons and one output layer containing one percent how do you prove this we've already proved it right we proved it by construction i gave you the network i gave you the template if you have n inputs you'll have two raised to n perceptrons each perceptron will cater to a specific input then you'll have these two raised to n weights in the output layer each weight will have a specific condition which you can satisfy because there'll be no contradicting conditions right so note that this network which has 2 raised to n plus 1 perceptrons 2 raised to n in the middle layer and 1 in the output layer is not necessary but sufficient all this is saying that you have you if you have 2 raised to n plus 1 you can implement it but we have already seen that the and function can be implemented by a single perceptron so you don't really need 2 raise to n plus 1 perceptrons right and the catch here is of course while this theorem looks interesting is that as n increases even if it becomes 10 20 100 also it becomes unmanageable right because you'll have 2 raised to 100 neurons which is obviously going to be very large right so this increases exponentially so we'll have to see our way of coming back from here right coming back from this exponential situation but for now we are happy that we can even though even if there are boolean functions which are not linearly separable which a single perceptron cannot handle we can have a network of perceptrons which can handle that how to make this network a bit more compact is a question for later but now we know that we can handle with a network of perceptrons so again while we have proved this why do we care about boolean functions right so let's try to relate this to a real-world example the same example of predicting movies right so this is how the example plays out that you have certain features as you call it so these were is actor matt damon is the genre thriller is the director christopher nolan and so on okay all these factors and these are the labels that you are given and these are all your past movie examples there are some examples which i'm yeah there are some examples which i am calling the positive examples because their label was one and some examples which i am calling the negative examples which for my labels are zero and as we have argued earlier for most real world examples the data will not be linearly separable that means you cannot draw a line such that all your positive examples lie on one side all your negative examples lie on the other side right so whatever i have shown you here is a boolean function right these are all boolean outputs and i have taken all the boolean all inputs as boolean right so this is a boolean function and in reality in real world examples this will not be linearly separable and this is a very real example right you have some n boolean decision factors based on which you are taking a boolean decision or a binary decision and now you want to be able to learn that what this theorem is saying is that you can come up with a network of perceptrons which will exactly implement this function what does that mean is that whatever your truth table looks like and this is what your truth table is looking like right you can come up with a perceptron network of perceptron so that it will exactly be able to implement this boolean function so right this theorem says that you can apply this in real world examples the catch of course is that you will need an exponential number of neurons uh perceptrons and we'll have to get back from that situation right but for now we are happy in knowing that it can be done right now how to do it more efficiently is a question for later right so that's what this is uh saying same thing which has been written on the slides you can read it later on okay so what is the story so far the networks of the form that we just saw are called multi-layer perceptrons or mlps in short you would have heard about this in several contexts the more appropriate terminology is actually multi-layered network of perceptrons why because you have perceptrons there's a network of perceptrons and there are many layers so it's a multi-layered network of perceptrons multi-layer perceptron does not make sense and there's a single perceptron it is not multi-layer it's a multi-layered network of perceptrons but more commonly mlp is the common terminology that is used right the theorem we just saw gives us the representation power of a mlp what does that mean what kind of functions can represent it says that it can represent any boolean function so that's the representation power with a single hidden layer right yeah so that's all that we have for today so we have been talking about the perceptrons we went a bit deeper uh into the perceptron learning algorithm we saw the saw that it actually converges then we saw that a perceptron has a limitation it cannot deal with functions which are not linearly separable then we made a case that in many real-world examples functions would be not would not be linearly separable so we need to deal with that situation and we dealt with that situation with a network of perceptrons we are still not completely happy because we have an exponential number of perceptrons but we are able to map this again to a real-world problem where you are given some data and that data may not be linearly separable but now given this template you can come up with a network of perceptrons and learn the waves w is in your network such that it will give you perfect separation on the input data right now from here we need to go a bit ahead and see how can we get rid of this exponential number of neurons and then also go a bit ahead and see if we can do something about this linear decision boundary so these are things that we'll deal with in the next lecture thank you