[Music] hi everyone uh so let's start with uh lecture one of this course uh where we'll be talking about a brief and maybe a bit selective partial history of uh deep learning right so you talk about uh deep learning right so most of this material uh the early material that is that at least there in these slides uh is taken by from this article on deep learning in neural networks and overview by schmidtober there might be some errors in my accounting of the history and if they are then i apologize for them and also feel free to contact me if you think certain portions need to be corrected or there are more things which have happened and you would like to uh me to add them right so i first did this history lecture almost like six years back uh and it was more easier to manage but in the past four five years i think there's been a much rapid explosion than what it was earlier even at that time there was quite a bit of work happening but it has exponentially grown since then and it's often hard to keep track of uh different things so maybe i've missed quite a few things i understand for example uh speech progress in speech is not very appropriately captured in these slides but the idea is just to give you an overall flavor from where we are and where we have reached and maybe a few things here and there would have been missed but it should still give you a fairly reasonable idea of what are the latest developments and how they have evolved over the years okay so with that uh primer let me just start so as i was trying to say that when we talk about deep learning uh we're talking about neural networks and at least we hear that keep hearing that a lot of inspiration comes from the biological neurons or at least we are also currently uh still striving right to try to understand how the brain does things and maybe come up with models of that right so let's start that history uh from biology actually and we go back uh quite a bit of quite a few years 150 years in time uh to around 1871 right and this was a time when people are trying to understand what does our nervous system look like right and joseph von gerlach who was one of the researchers or scientists in this field he came up with this idea that our nervous system is a continuous network right as opposed to being composed of various discrete cells which are connected to each other right and this view of the brain or the nervous system was called the reticular theory and this will be important in our discussions on deep learning as we uh keep seeing through these uh slides right so and again one of the things i want to emphasize through this history is that there are various discoveries happen not necessarily in deep learning or in computer science but in different fields right which have over the years influenced where we are today so one of these was again very early in the 1870s the staining technique was developed and it allowed it was a basically a chemical reaction which allowed you to examine nervous tissues better right and camillio golgi came up with this and using that chemical reaction he analyzed a slice of the nervous tissue and he came up with the same conclusion that actually this nervous system is like one continuous network and it's not composed of discrete units so he was also a proponent of reticular theory right and he his evidence for that was through the staining technique but quite interestingly around the same time uh santiago i won't pronounce the full name because i can't use the same technique the same straining technique which uh golgi had come up with right and by making similar observations using the same technique but maybe seeing things a bit differently he came up with the conclusion around 88 and 1888 1891 time frame that no actually the nervous system is made up of discrete individual cells forming a network right so two different uh prominent theories at that time one says that it's a single uh network the other says that it's a single continuous network the other says it's a network of discrete elements right that means there are many individual components which are connected together right and one is the neural neuron doctrine and the other is the reticular theory and then around 1891 this gentleman i'll again not pronounce the full name he coined the term neuron right this term was coined by him and today when we talk about artificial neurons the word neuron originally is attributed to this gentleman and he further consolidated the neuron doctrine that means he found further evidence and consolidated different views and said that indeed it seems that the neuron doctrine is the right way of explaining what the nervous system is as opposed to the reticular theory right and just uh trivia here he was also the person who coined the term chromosome it's the two very important terms that we hear about today both are coined by this gentleman here right now here's a question right so there are these computing theories one proposed by um golgi and the one by the other gentleman uh reticular theory and neuron doctrine right so now around 1905 when the nobel prize in medicine was given who do you think it went to right the person who propagated the reticular theory or the neuron doctrine what's your take on that so i hear various answers but it turns out that both of them got it right so by that time again i mean we have been like 1871 to 1906 quite a few years that in terms of the way uh research progresses today at least in deep learning like several generations right 35 years but still there was not uh any conclusion of these and both these schools of thoughts uh were in existence with their own champions right and again given that the nobel prize was given to both of them this resulted in lasting conflicting ideas and between the two groups of scientists who believed in this these two different theories right and this took quite a bit of time to resolve and not by developments in biology but developments in a very different field so around 1950s when electron microscopy was became useful at that time using the electron microscopy technology it was finally confirmed that the neuron doctrine is the right one that means that the brain or the nervous system has many small cells called neurons and they are all interconnected to each other through synapses right and this structure where you have neurons and they have connections between them is what kind of forms the foundation of modern deep neural networks also it took over quite a bit of time and progress in different fields to come to this conclusion which then motivated work uh in deep neural networks or neural networks at that time which has now evolved to where it has right so quite a bit of history and quite a bit of developments from different fields which have led to this place right so now moving on or in fact there is one more before we move on right there's one more thing about the brains which there was a great debate about which was uh whether the processing in brain is localized or is it distributed so what it means by means that there was one group which felt that if you're talking about speech or vision or any specific uh activities that the brain is responsible for they are localized there are certain portions of the brain which are responsible for speech certain for vision and so on and there was this another group which thought it's distributed that means different parts of the brain connect in different ways to do different activities it's not that one part of speech and the other part is so right and you can go back and look at this video i'll not play it which is about this great brain debate about localized versus distributed processing and that again took a while to settle right and now this is again important because again in modern deep networks we believe that it's more distributed different parts interact with each other to do different things whereas there's also push for having what is known as localized processing especially in modern multilingual models we want certain portions of the network to only adhere to computations related to a certain language or even now certain inputs right and the rest of the network may be doing other things and so on right so you can just look at this video also which is uh again another kind of a debate about brains the first one was whether it's single or distribu or disconnected now this is more about whether it's localized or distributed right and both of these are relevant even today in terms of modern deep neural networks right so from these uh biological neurons where the inspiration for neural networks came now let's move to the actual artificial neuron set and that's where this period of time that we'll cover which is called the spring to the winter of ai and i'll tell you why these two seasons have cropped up in us in our discussion right so around 1943 while we were still trying to understand uh what uh the brain is and we are in fact i mean still today trying to understand it uh uh mcculloch and pits right to neuroscientists and logistics right and again people coming from different fields who were contributing to this area right of course at that time computer science was not so evolved it was still a field which was in formation right so uh or just getting formed so so these two uh uh one neuroscientist and logician they proposed a simplified model of the neuron and this is something that we'll do in the course in uh detail in the next lecture itself where they just said that a neuron a model of the brain would be that there are multiple inputs coming to it right and these could be inputs from a sensory organs and based on that it takes a decision and a very simplified model is where all these inputs are binary and the decision is also binary so i take inputs like uh is it raining outside do i have money do i have time and if so maybe i'll go out to watch a movie right so that's how the decision uh making processes modeled with a very simplified model right and then this again got modified and this perceptron model was proposed by frank rosenblatt and this is what he had to say about it when he proposed it and if you look at the diagram of course the perceptron model is again something that we'll do in detail in the course uh you'll see that it's very similar to the earlier diagram except you see some weights here right so now these different decision factors have some weight i would give more emphasis to the input about me having money or not as opposed to whether i'm in the mood to go out or not because if i don't have money maybe i can't go out and really spend anything or do anything right so that's the basic idea here and this is what he had to say that the perceptron may eventually be able to learn make decisions and translate languages right now when i read this i i find something very strange about this statement right learn make decisions and translate languages so what is the oddity here right what what seems a bit odd here [Music] yeah so the phrase translate language i can understand about learning and making decisions because that's generally what we associate the brain with but why something so specific which is about translate languages right so this as you can see this is a period 1957 to 58 which is after the war and even during the war right i mean there was a lot of emphasis on being able to translate messages from enemies which spoke different languages german russian english and so on right and so translation was an important problem to be solved in that era and hence this was said that this will also be able to learn how to translate languages and today are almost like 70 years later right we are still trying to solve that problem very recently facebook has released a paper on which can do translation between 40 000 pairs involving 200 languages but there's still a very long tale of languages that we still need to enable translation for and of course this is just initial flag planting in the sense that by no means are these 40 000 directions uh the translations adequate they're still at various levels of quality and much more is desired to make them really useful so we are still trying to solve that problem after so many years right while this statement was made many years back and the reason i'm saying that is that as has been a characteristic of machine learning deep learning a lot of tall claims get made but it takes time for those claims to uh realize right and that often leads to certain dissatisfaction in terms of what we expect these systems to do versus what they can do of late of course we are making uh rapid progress where we're trying to meet some of these expectations but still i would say a lot needs a lot still a lot of still desire in terms of really understanding the way humans do and this statement right again very interesting the am this is the embryo of an electronic computer that the navy expects will be able to walk talk see write reproduce itself and be conscious of its existence right even today 70 years later this is the stuff of sci-fi movies right there's a stuff of futuristic movies where we see that okay ai can now become conscious it can reproduce itself it can take over the world and so on if you're still it's still the subject of sci-fi movies it's still not something that has happened in reality right so and this is an article from way back 1957-58 even today we see these such similar articles right so much not much has changed in terms of the hype that is generally there around ai machine learning deep learning then this all was for a single perceptron and what we know today as deep learnings which is like a multi-layer network of neurons right so this idea is also not new right this was there way back in 1965-68 by by scientist called evac nenco and his group who proposed what is looking like a very modern uh deep neural network right so this idea also existed quite bad a lot of these ideas have had their generations right they were proposed initially maybe the conditions at that time were not so conducive for these ideas maybe they were a bit ahead of their times and then 30 years back again people picked up those ideas and then maybe the conditions were right so there's again like a repeating theme in this area but around the same time what happened right in 1969 so 1957 when you saw those statements being made about perceptron and then the new york times articles and many such similar articles the next 12 to 13 years was what i would call as the spring time of ei that was a lot of curiosity around this area a lot of government funding available for this right at least in the u.s a lot of the funding was on science and technology was driven by government bodies at that time and there was a lot of interest in making really ai work given the promise made of what it could do if allowed to flourish right but then around 1969 in their now famous book uh minsky and pepper uh outlined the limits of what perceptrons could do right and what they said in very simple terms is that uh while we are thinking that uh perceptron can model any real world phenomenon what that means is that suppose i have a complex decision to make right which depends on say various inputs right and i'll just start using some terminology say rn x belongs to rn which oh sorry right which means there are n inputs right and you have a function and you want to take a decision and this function is of course complex that you don't know what it is right so what claims were being made is that if you have this relation where you have an x you have some function which gives you out y right and if you give me enough instances of this which means okay yeah if you give me x 1 y 1 x 2 y 2 x 3 y 3 enough input output pairs of this function right so you have decisions that you have taken in the past for certain inputs then i could train a model right which could take again an input x and give out an output y which would be very close to the true y that should have been there right that is the claim that was being made right and what uh and that's why right i mean translation languages could be one such example you have an input which is a sentence and you produce an output and what was being claimed is i can make a model which can take an english sentence and give you a russian sentence as output which would be very close to the true russian sentence that you would expect so those are the claims being made right any you could take a bunch of inputs and give the output which would be very close to the human output that you would expect now what uh minsky and pepper show that even for very simple functions right where this is not a very complex function but a very simple function like the xor function for two variables right just take one say a and b as input and you know what the xor function should give an output the perceptron is not capable of learning that also that means i cannot come up with a perceptron model which takes a and b as input and if i change a and b from 0 0 0 1 1 0 1 1 the output would be the same as the truth table of the xor function right so that's what they showed right and that kind of led to significant disappointment because now you have like a very simple function and you're claiming that a perceptron cannot even solve that then how do you expect you to solve much more complex real-world examples like the translation example or complex decision making like giving a bunch of parameters about a patient right blood sugar level cholesterol and so on and trying to predict the life expectancy or the possibility of a person getting a cardiac ailment in the future and so on and those are much more complex decisions than just a simple xor function right so this led to a lot of disappointment but unfortunately right so while this statement is true right and it's a classic example that we also use today i'll also be doing this in the course uh minsky and pepper said this only in the context of a single percept right they said they still said that if you have a network of perceptrons you could solve uh complex function right but somehow this second part that you cannot do this with a single neuron but with a layer of multiple multiple or multi-layer network of perceptrons but you can do it with a multi-layer network of perceptrons the second part often got lost and the first part got emphasized that hey you cannot do this and that led to a lot of disappointment and following this and of course i can't say that this was the primary reason but this and similar uh evidences which started emerging there's a slight decline in the interest in funding ai for almost the next two decades and this is what we call the winter of ai where the interest in funding large scale projects in ai started declining and people started i would say rationalizing their expectations from ai a bit more right but that does not mean that work completely stopped no one was doing uh yeah there was still work happening and small progress was being made in different uh areas right in particular in 1986 uh yeah so as i said this was the ai winter of connectionism and for some of you are initiated in this there's connexious ti and there's a symbolic ai so people were still interested in symbolic ai but lost interest in what is known as connectionist ai whereas today's ai is largely dominated by connectionist ai and we want to again try to see if you can come up with hybrid models because again today people are realizing that this way of doing ai the deep learning view of ai has its limitations and we need to start looking at hybrid methods or complete different alternatives to really push the frontier a bit more right and this during this period which i'm calling the ai winter not i it's generally called the ai winter there's also this abandonment of these ideas which were similar to deep learning at that time but some workforce of course continued and in particular there was this back propagation algorithm which all of you would have read about in blogs or various articles that you have read on deep learning this was again not something which has been discovered in the last 10 15 years in fact it was discovered and rediscovered several times throughout 1960s and 70s and of course there's not the benefit of having internet where if something gets discovered in one part of the world it immediately propagates so you might not be aware that someone else has discovered it and independently discovered this algorithm that's what was happening at that time and around uh 1986 uh rumel heart and others that including jeffrey hinton uh popularized in the this in the context of neural networks and they showed that this back propaganda propagation algorithm can be used for training uh neural neural networks and this was an important breakthrough because even to this day uh most modern deep neural networks are trained using the back propagation algorithm right there was another important and uh an interesting fact is this back propagation uh within it right or at the heart of it there's also gradient descent right which was much older right like almost one century one and a half century before back propagation and this was discovered or proposed by koshi and for a very different purpose right and he was trying to use gradient descent to compute the orbit of heavenly body so there's a lot of interest in astronomy at that time and he was trying to look at what is the orbit of heavenly bodies and in that context he had discovered gradient distance right so again whatever benefits we are enjoying today they come through the work of many great scientists over centuries in different fields and that's where that has helped us reach where we are today right in very unexpected uh ways so while we were still in this winter period again this 1989 what came up is this universal approximation theorem which is again something that will come up in the course right and what this said again to repeat what i said earlier that if you have a function which takes certain x as input and gives you y and in real world you don't know what this f is right so in real world if i want to say okay what is my decision in real examples suppose f is the function that i'm using to decide how to hire people or how to predict the life expectancy of a person i don't know what this f is that is not really known i just know it depends on certain factors right but and what i have is several examples of these inputs and outputs right i know a person who had a certain blood pressure level cholesterol level and so on sugar level and then i know how long that person lived and so on right so i have many such examples now what this theorem said is that you could come up with a multi-layered network of neurons right not unlike what papad and minsky had said that a single neuron cannot do it they said that if you have a multi-layered network of neurons right then you could take an x given many such examples you could learn a model such that now if you feed an x to it the y that it predicts would be very close to f of x which is the true value of the y that you would have got if you knew what that function is right so what it means is that what i'm trying to show in this diagram is this orange curve that you see here right okay somehow the pen is behaving a bit strange yeah the orange curve that you see here suppose this is what my actual f is and i don't know that right and now i'm trying to approximate it and those i'm doing that approximation with the help of these uh rectangular bars that you see right and what this was uh this theorem said is that if you have a very deep neural network or a multi-layer in fact it said even if you have a one layer network with a large number of neurons then you can approximate this very very well right and that's what we want to do we had the true function which we did not know all we knew was several instances of x1 and y1 x2 y2 what this theorem says is that if you have many such instances you could come up with a neural network right and that those rectangular bars are in some sense the output of the neural network such that that output would be very close to the real output right and the more neurons you add the better your approximation would be if you had fear of neurons your approximation would be poor like you see in this case but the more neurons you add it will be better right this theorem an illustrative proof of it is something that we will see but this was a real breakthrough right because now it has almost the reverse effect of what we had for the proof by pepper and minsky which said that even for simple functions a single neuron does not work now what this is saying is that for any arbitrary complex function not even boolean functions any arbitrary function that you have no longer what the function looks like you can always come up the neural network which will be able to approximate that function to any desired degree of precision right and that desired degree of precision is controlled by the number of neurons that you have if you're on fewer neurons my approximation would be very bad but if i keep adding neurons my approximation would become better and better right so this is a real breakthrough that was there and after this of course nothing changed right but for nothing changed much in the sense that people realize okay there is a lot of power in deep neural networks but for the next uh 20 years or so or maybe 17 18 years on the back of these two discoveries one is back propagation which allows you to train deep neural networks and the other is this universal approximation theorem which says that there is value in training deep neural networks because then you can approximate arbitrary real world functions uh people try to apply these ideas right to real world problems but what they notice is that training a multi-layer network of neurons using back propagation is not very stable and it often does not lead to convergence so while in theory you can use back propagation while in theory you can approximate any function but in practice when you're trying to train this really deep neural networks it's not working right so uh in the next few years from 1989 to not few almost two decades uh much practical progress did not happen in terms of deep learning right people knew these two things but they were not really able to make them work in most cases there were of course still a lot of progress happened on convolutional neural networks which you'll see soon right so that's that's where we were so we started with the spring where there was a lot of hype and enthusiasm around ai then things collapsed and then things kind of stabilized okay let's not completely ignore it let's keep making some progress and see if you can actually use back propagation to train the steep neural networks