foreign [Music] so that brings us to the next body which is on this some manual work on attention is all you need or the Transformers right so the Transformer architecture which was introduced around 2017 maybe yeah uh so that's that's what you're going to focus on while keeping in mind the limitations that we saw about in the case of a recurrent neural networks right so this is how we'll transition to Transformers right so we have the basic encoder decoder RNN based model where at first you have recurrent connections and that's causes a problem then you also have the attention-based encoder decoder model where we said we'll do the uh encoder computations then get rid of them and then we have the uh attention we just take the outputs of the encoder right um we just take the outputs of the encoder and then we have the attention function on top of that at every step we compute a new contextual Vector right so that's the attention-based model but here again uh we were able to compute the attention weights for one time Step In Parallel but across time steps T1 T2 and so on we had to wait for the previous computations to finish right that's where we were so from here we'll transition to the Transformer Network which kind of again has an encoder decoder architecture but there are some other blocks that I am naming here as is something known as a self-attention block then something known as a feed forward Network then in the decoder you have self-attention encoder decoder attention again feed forward Network so it's all of this we need to understand the words look familiar you have attention your feed forward Network so that doesn't look too problematic but uh how these different blocks interact what exactly is there in each of these blocks is something that we'll have to study and that's what the focus of the next uh half an hour to one hour would be right so before we focus on the differences right this clearly looks at least in the diagram a bit different from the recurrent neural network architecture or The Recoil neural network with attention architecture that we were used to uh there's still some similarities which come out right and I'll tell you the few easy ones right the input is still given to me fully right and the output again will be produced one word at a time right the other uh similarity is that in RN and also I had said that at the input you could just feed in your uh favorite uh word Vector so for word embedding so you have again your feeding word embeddings and this block right if I were to look at it as an encoder as same as what I had in the rnns so the N output of the encoder was these representations right and don't worry about the change in notation because there are more uh intermediate outputs here so I need to use no more variables but in the case of rnns you had X1 X2 up to X Phi as input and the output was H1 h2h5 right so in a sense what you are doing is that you are taking the inward embeddings as input right and then you were Computing a contextual representation how are you Computing a conflict contextual representation using the bi-directional lstm right so H2 dependent on H1 so hence you had all seen all the information up to or rather HD dependent on HT minus one so when you are Computing HT you had information of all the T minus one Birds before it that's where the contextual representation was coming out but at each step what you are doing is this right so you are Computing a new representation for that word which was a context of a representation here looks like more computations are happening but at the output you just have these Z1 Z2 Z5 so again you could think of it as you have the excise as input and you are producing these zis as an output and these z-is are again some kind of a contextual representation of the input that you have given that means zis are not only aware of X I's they are also aware of all the other words in the uh sentence right so that's that's what is happening here so the similarity is that RNN takes X1 to x y and gives you H1 to H5 Transformers also takes X1 to X5 and gives you Z1 to Z5 the naming of the variables may be different but just both have the same semantics in the sense that they are some kind of a contextual representation of the input how this contextual representation is computed is different and the main difference is going to be that we are going to do something which does not require the current connections or something which can be computed in parallel so that's the main change how do we effect that change is something that we will discuss as we go along okay so let's start yeah so let's look at each of these layers in detail let's say my goal would be to kind of go over each of these yellow boxes that you see here word embedding of course you know so there's nothing to say much over there but self-attention feed forward networks then again self-attention the case of decoder encoder decoder attention and feed forward networks again in the case of decoder so what do each of these blocks have right and of course there will be some overlap in the discussion so if you understand this this should be fairly straightforward if you understand this then I'll not even say anything about this right so mainly three components that we need to understand self-attention and good or decoder attention and the feed forward neural networks so we'll look at these in the coming slides okay starting with self attention right so this is the self-attention block so this is what is happening I have the word I okay and I have the word embedding for I here I have just used some random initialization but you can either do random initialization that's the practice you don't really rely on any word embeddings you just do a random initialization and then these are passing through something known as self attention and then producing a new representation for every word right that's what is happening here so very similar to what is happening in the case of RNN you have the excise and then you get some Edge Ice right so the same thing is happening here of course the advantage the only reason I would be interested in discussing this is that if this overcomes the disadvantage of rnns which is sequential computation can I do this in part right so that's the idea uh that's the main uh idea here right so the self attention mechanism once you understand it fully we'll realize that it can be computed in parallel and still give the same flavor that means it still computes a contextual representation that means it's aware of all the other words in the input right so that's the main thing that we'll be discussing so now this is called self attention and what does the attention how do you compute attention right so the attention just requires two inputs so in the attention that we have seen so far we had St minus 1 comma h i right which was used to compute the attention at time step t for the input I right but here there is no decoder we're just talking about encoder and now the word self should tell you what is going to happen here right what I'm going to be interested in is that I have all the word representations now if I want to compute a new representation for the word movie right I want to Output a new representation of the word movie then can I compute this representation as a attention weighted sum of all the other representations in my sentence right and if I were to do that all I need to do is I need attention function right and which takes in my current representations h i and H J right so these are what I'm going to call edges this change of notation is unavoidable right because I have more outputs being produced here or more intermediate outputs being produced here so I'll have to use more variables earlier H was the output of the encoder itself now that I am calling as Z so H is now the word embeddings that you have computed it so this is H1 H2 H3 up to H5 now I want to compute a new representation for movie and I'm going to call it as S4 so I want to compute S4 as an attention weighted sum of all the Phi inputs that I have that means first I need to compute the attention weights and what how am I going to compute the attention weights everything is only on the encoder side so I want to compute the attention weight between the words I and J and that would be a function of just h i and H J now what function do I choose how do I use that all that we'll see as we go along but this is the basic idea right so once I have these Alpha ijs I'm just going to compute S4 as the attention weighted sum so Alpha 4 J J equal to 1 to capital T into h j right so now I am just taking a weighted aggregate of all these representations right and these are the alphas for the time step 4 and using that I'm going to compute S4 similarly now I should better clear some things here similarly if I want to compute S2 I am again going to take an attention weighted sum and these are going to be Alpha 2 so there will be Alpha 2 1 Alpha 2 2 all the way up to Alpha 2 Phi in this case and general Alpha 2T and once I have the alphas I'm going to compute S2 as the attention weighted sum so Alpha 2 J h j j equal to 1 to capital T or Phi in this case right and these are just to remind you my word embeddings are what I'm calling as the edges right so it's just going to take a weight at some of the edges now you already see the advantage I already have the edges with me right I just got the entire sequence of words so I just looked up the word embeddings so I already have all the edges with me now there is no recurrence here right so I can compute the alpha twos at the same time as alpha 1 at the same time as Alpha 3 because now I am not depending on one step to another right I don't need for computing Alpha once I don't need to know rather for computing Alpha threes I don't need to know what happened at time step two right because I am just looking at the contextual representation for this word by taking uh a vote from all the other words in the sentence and I'm only looking at H's I'm not relying on S2 here right so this attention equation unlike the earlier equation which was St minus 1 and H I now this is just h i comma h j right so there is no dependence here I already know all the edges so I can just compute this all at one go right all of this will become a bit more clear I'm just giving you a trailer of what is coming right so two takeaways from this slide one is that I have H's as the input and I'm going to compute this intermediate representation s and the way I'm going to compute that is by taking a weighted sum of all the edges so this makes sure that my s's are contextual right because they are looking at all the words in the neighborhood and while doing this I'm going to rely on the attention equation this is called self-attention because I'm looking at self this is not earlier when I had attention that was between decoder and encoder now I have attention within the encoder right I'm looking at all the words in the input itself this is not as opposed to earlier where I had the attention between the encoder and the decoder so I was looking at the decoder to State at time step T and then Computing the attention at attention of all the inputs that's not what is happening here this is self-attention this is attention within the input itself right and this can already you have a feeling that this can be paralyzed we'll concretize that feeling further as we go along okay yeah so this is what uh now now what I want to do is try to First motivate right that what we are doing here and why do we need to do this right why do we need to compute this attention or why do we need to rely on all the other inputs because that is not what we were doing earlier we are of course computer contextual representation but earlier the attention was only between encoder and decoder so why do you need this self-attention why does it make sense right so now if you have the sentence the elephant the animal didn't cross the street because it was too tired here the word it is referring to animal and not street right so when I'm Computing a representation for it it should pay more attention to animal it as opposed to stream that's why I need this self-attention because I need to capture the importance of the words in context as with respect to the current word right and this would change if the sentence was different if the sentence was the animal didn't cross the street because it was congested so now the word it here actually represents refers to street so now when I'm Computing a contextual representation for it it should pay more attention to stream right so the same word but paying attention to different uh input different other contextual words right that's why I cannot use like a static embedding of it which is just taking the word embedding I need this contextual embedding and that's why I need to learn these Alphas here the alphas should learn to focus more on street here the alpha should learn to focus more on anyway that's why I need this self-attention so that's what is being said here and this is what we call it as self attention and we'll distinguish this from concentration which I have already said that cross attention is between the encoder and the decoder but here the attention is within the encoder inputs itself right so our goal would be that for a given word I want to be able to compute the similarity score or the attention score with all the other words in the sentence right so this Matrix is what I want to fill that I want to compute a representation for street and I'm going to compute it as a weighted sum of all these representations so in my weighted sum what should the alphas be right that's what I want to compute okay that is what my goal is so this entire Matrix which is a t cross T Matrix is what I want to compute I want to compute all these Alphas if I have all these Alphas then I compute the SS by just taking a weighted Alpha weighted sum of the edges right so that's what my goal is okay and I want to be able to do this in parallel which you already have a feeling that you can do this in parallel because in particular right when could you do that in parallel so let me call this Alpha Street right or I could have used the timestamp so one two three four five six right so this is Alpha six Alpha six one alpha six two three all the way up to Alpha six capital T and say this is Alpha two one alpha two two all the way up to Alpha to T right now when can I compute these in parallel earlier remember we had spoken that uh when we are looking at encoder decoder I could not compute two rows of alpha in parallel because this row of alpha depended on everything that happened before right but now I don't have that dependency to compute this Alpha I just need H2 and H1 which I already have to compute this I need H6 and H3 which I already have I don't need to see what is happening here to be able to compute this function right so you already have a feeling that all these rows can be computed in parallel as compared to the previous case where each row dependent on the previous row right because of this St minus 1 that you have right so this Matrix is what I want to compute and I should be able to compute this Matrix in parallel is what my wish list would be so essentially what we want is a table like this which has some numbers and those numbers are meaningful in the sense that here it and animal are related so the contextual similarity between them should be higher right in particular when I am Computing the contextual representation for it I should have more weight on animal as opposed to the other words in the sentence right so that's what we want so we want to learn these Alphas in that manner okay so how do we do that is the question what kind of a architecture do we use so to start that discussion and just to be able to relate it to what we have already seen right so in the earlier case we were talking about the attention function as s t minus 1 and hi right so just to kind of keep the convention or the variables similar for now for for some time right and we will get rid of it soon we'll just think of the rows as H I's sorry the rows as s I's and the columns as at J and now what you are interested in is to compute an attention function between the s i and the AJ but unlike earlier now as I said the Si's and the edges are available to you at one go so you can compute all of this at one go as opposed to earlier when your sis which are essentially the decoder states were getting available only one step at a time right so just to draw a clear variable level analogy between what we have seen so far and what we are going to see earlier uh I should say that this is actually just h i right because these are both the same representations but I just wanted to connect it to the earlier discussion where we had the S and the H and the difference is that now the s's are also available in at one go as opposed to the earlier case right so now what is the attention function that we should choose so earlier we had this attention function right so when we're discussing about rnns this was the attention function where you had St minus 1 here at J here both were undergoing some linear transformation then the resulting output was going under a non-linearity so you have this tannage and then you had a vector which was the same size as this vector and then you take the dot product between these two vectors so you get just get a scalar quantity in fact we are calling it A's and then the A's get normalized to give you the alphas right so that's the function that we were choosing earlier now for the this work right the Transformers paper introduce a new attention function and that is what we'll uh try to arrive at now right so if you notice this equation right there are three vectors involved here there is s there is H and then there is B it remember this is also a vector right because the output of this is a vector and that gets multiplied by our DOT product with another Vector let's have three vectors and then you have uh two linear Transformations happening so this is the first linear transformation which is at here and then here's the second linear transformation that is happening right uh and then you have this one non-linearity in the form of the tannage right uh and then what you have is that that internal non-linearity multiplied by those two linear Transformations their entire equation gives you one vector and then you're taking a DOT product between that vector and your V right so just to summarize there are three vectors two linear Transformations one nonlinearity and then finally a DOT product right so the final answer that you get right if I were to simplify this what you get at the end is just the dot product between two vectors this Vector in turn was computed using two other vectors which in turn had gone through a linear transformation right so that is what is happening here that is what the equation is and now I could do this in other ways and that's where the Transformer set of equations for uh attention will drop out from right but this is what is happening here the final computation is a DOT product and within that green Vector you had these two linear Transformations which were happening followed by a non-linearity right so now let's look at but in the earlier case you had this St minus 1 which was being generated at every time step right but here now you have h i and h j which is they just have one word embedding for hi and one word embedding for a j whereas here you had these three vectors which were participating right so now from this h i n h j how do you get these three vectors to participate in this equation right so you have these three vectors participating in the equation but now you just have h i and H J so where do you get these three vectors from right how do you generate these three vectors that's the question and that's what uh one of the uh Innovations are one of the uh equations proposed in the case of Transformer based models how do you get these three vectors uh from these two uh word embeddings so that's the idea so what we'll do is we'll use this Matrix transformation and that's not surprising because anyways in the case of the original equation also you had this linear transformation right and now from each Vector I want to be able to generate these three uh vectors so I'll use three linear Transformations right and they call them specifically as key query and value vectors and the corresponding matrices as the key Matrix the query Matrix and the value Matrix right so that's what we'll do on the next slide