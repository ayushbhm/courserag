foreign [Music] so let's see how do we go about that so the intuition is that we want to Decay The Learning rate in proportion to the update history right so for the features which are getting a lot of updates so I have a very dense feature so the derivatives are large and I updated it in the first step by a large value Second Step by a large value third step by a large value so I am doing reasonable updates for this so maybe now I need to be a bit conservative and Decay The Learning rate but if there was Speech feature which is very sparse got a very small update in the first iteration because the total gradient that I computed was very small again a small update in the second pass again a small update in the third pass then I need to let the learning rate be high right because I'm getting small updates so let me at least move a bit aggressively in those update directions right so that's what uh kind of is the intuition behind the update rule for Ada grad what I'm going to do is I'm going to maintain the history of the updates that I am making right okay so now let me try to explain what is happening here so now I am taking a running some of the history of the update so it's this Delta WT tells me uh what was the update that I've made or what was my gradients right and this is a running sum at every time step I am adding the current uh derivative and the square of it right so I'm just taking the uh squared derivative and keeping it as the history so it tells me the magnitude of the updates that I have done so far and then what I'm doing is I am taking the learning rate and I'm dividing it by this history so what will happen is if they have a feature which is very dense and which has got a lot of updates then this history keeps increasing right because you are making large updates at every time Step at time step 0 you made an update time step one you made an update time step two you made an update and all of this is getting added here and all of these were large quantities because this this was say a spa a dense feature right then your VT after a few iterations will grow and the learning rate would appropriately shrink right but if this was a sparse feature then across iterations maybe even when I reach t equal to 100 nothing much has been accumulated here right because all these updates were very very small because this was a sparse feature so hence my VT has not grown much and hence my learning rate will not decrease aggressively right so now the learning rate has become uh proportional or rather inversely proportional to my update history if I made large updates then I degrees the learning rate aggressively if I made only a few updates I don't decrease it aggressive right so that's what is happening here so just as have you have the update equations for w you can also have the update equations for uh B right okay so this is the algorithm from now on I'll not spend too much time on the code right I'll just uh uh modification of the codes that you have seen so far so in this place here this is where I'm accumulating the history right I'm just taking the previous update plus the current update square right that's what the equation said and then my learning rate here is going to be divided by this update right so now I think these are small code Snippets which you are comfortable with and you can take the equation and write the code for that right so to now to see this in action right we need to First create some data where one of the features is sparse right and I also remember that I can only show you a 2d plot that means I can have one feature on the air one axis and another feature on the other axis so I'm just going to be able to work with two features one of them is going to be W and the other one is going to be B right and remember the feature b or the feature corresponding to B is dense because it's always on right the corresponding input is always one so I can't do much about that so what I'm going to do is that I am going to create a data where my X which corresponds to W is going to be very sparse right so that's how I'm going to create this data so let's see this is what I'm going to do so this is what I'm saying right the third bullet point here says that we just have two parameters W and B of these the input feature correspond to B is always on so we can't really make its path so the only option is to make X pass so the way I've created the data for this is that I created some random 500 random X comma y Pairs and then for roughly 80 of these pairs I set X to 0 right and thereby making the feature uh X very sparse because 80 percent of the times it is 0 and that's the definition of a sparse feature that a majority of times are significantly more than a majority of times it's zero so that's how I created this training data and then I uh so I took that data and I ran the gradient descent momentum based gradient descent and nestro accelerated gradient descent algorithm on that using that data and uh the law surface was something like this and I am showing the 2D loss surface but let's try to understand uh let's try to make a commentary on it and try to see what it actually is right so as you know this red means high so these are the regions here where the loss is a bit high and also in those regions you see that there's a slightly larger difference between consecutive contour lines that means they are a bit flat region right and this region is blue here okay so that means in that region the loss is very low and and also from going from here to here as we go down let me just change the color going from here to here as we go down the slope is very Steep and that is obvious because the difference between two consecutive contour lines is now very small right so then this is essentially a surface like this which is peaking at the two red portions and then it's going into a valley which is the Blue Valley and it's going steeply into the valley right that's why you see this steep slopes here where the difference between two consecutive contour lines is very small right so that's what the loss surface looks like now I ran gradient descent I started from some point I started all the three algorithms from the same point right and I started gradient descent ah momentum based gradient is a menstrual right and the as I would like to say let me just go back to the slides so there's something interesting that is these three algorithms are doing right and can you spot it is the question right so what exactly is happening here and remember that uh I should have mentioned that my horizontal axis is the W axis and my vertical axis is the B axis as has been appropriately labeled on the plot right now with that information can you tell me what peculiar behavior is happening here okay good so what what you're saying is this right that uh I mean as I say it it looks like these algorithms went to a school where no one taught them greed in descent or no one taught them Pythagoras Theorem right so what what I mean by that is uh if you look here uh initially all the three algorithms are moving in the direction of B right they are not they have moved only till this point right till this year they've moved only little bit in the direction of w right so till this point is where they have reached so they have not much move from the initial value of w right but where they have moved a lot in the direction of B right and that is expected because B was the dense feature hence its gradients were larger and hence you were making larger movements in the direction of B whereas W was the sparse feature intentionally made sparse as explained on the previous slide so the gradients for w are smaller and hence you are making smaller movements in the direction of w right and after you reach the valley then the algorithms realize that now there is no more uh value in moving in the direction of B right anyways the gradients that are becoming smaller now because you have reached the Steep region and then it slowly starts moving very very slow steps in the direction of w because there again the gradients are small what can you do but then it still moves in that direction and then it reaches the minimum all right so it takes a right angled path right whereas ideally you would have wanted something which just directly travels like this which would mean that it is making proportionate movements in both the direction even though the derivatives in the direction of w are small you are still able to somehow jack up the learning rate so that you are able to make proportionate movements there and then reach the Minima faster or at least like a more appropriate root right so that's that's what is going wrong with these algorithms the algorithms that we've seen in the last lecture foreign are moving in the vertical Lexus and then they start moving along the W axis and we know the reason for this we already explained I just explained what the reason was and such sparsity right it is very common in large neural networks right you have thousands of input features and uh you many of them might be sparse right so there's not something like a toy example that I've created this is going to happen in many neural networks and hence you need to address this issue right that if you have sparse features can you still make faster movements in those directions right so now let's see what Ada grad does for this case Okay uh okay let me just play this so as you can see right aragrad is moving in the W direction also right why is that happening because the learning rate for w because our denominator is going to be small our square root of VT is going to be small right and if it's a quantity Which is less than 1 then the denominator is uh you're dividing ETA by a quantity Which is less than 1 so hence ETA increases and hence your updates in that direction increase and hence unlike the other algorithms you're proportionately moving in the direction of w Oz right of course here still the momentum wage algorithm was faster because momentum of course has its Advantage right it makes mistakes but it still reaches the Minima faster so let me just play that again you can see that the momentum based algorithms both nestrum and all this reached faster right but compared to gradient descent adagrad was able to move faster right and there is still scope to improve Ada guide because it does not have a momentum term if I had added a momentum term term then I could combine the advantages of momentum as well as this adaptive learning rate and we'll probably get in that direction at some point right we'll get in that direction meaning we'll get towards such an algorithm at some point right but for now you see the advantage that I'm able to move uh proportionately even though my derivatives are small I have adaptedly changed the learning rate ETA divided by the history of the updates and that is allowing me to do that right so remember my effective learning rate oh is ETA divided by square root of VT plus Epsilon and let me just ignore Epsilon for now so it's divided by the square root of VT and for w this VT is going to be very small because I am not getting large derivatives there hence ETA is getting divided by a very small quantity and if that quantity is less than 1 then effectively this learning rate is increasing right and that's what is pushing uh the derivative there right and I'll just explain this in more detail on the next slide foreign but one disadvantage that not disadvantaged but one thing I want you to notice and I'll just play this again if required is that as Ada grid starts reaching the Minima just observe from now on yeah so it's slowing down quite a bit right and what is happening is that by this time despite the uh smaller derivatives of in the direction of w by this time you have accumulated some history and now this effective learning rate is starting to slow down because it is getting divided by this accumulated history and by that time that history has increased right so as it comes close to the convergence not necessarily goes to conversions after a certain number of iterations when the history is becoming large then adagrad is slowing down it so that's one observation I'm making on this slide and we'll come back to see if we can somehow solve that problem okay so now let's examine this a bit more uh closely yeah so you remember that we are accumulating this history now V 0 is Delta W 0 squared V1 is Delta W 0 squared plus Delta W1 square and so on right so this is constantly growing VT is constantly growing uninhibitedly because you are adding those uh derivatives right and uh our derivative is proportional to X that means for the uh feature which is pass these derivatives would be small so then our history will accumulate slowly hence this effective learning rate will Decay slowly and it's also possible that initially it increases because us denominator might be less than one so the effective learning rate increases and even when it decreases it is slow to decrease right because your accumulations may happen only after a large number of iterations such that the denominator becomes large and then the effect of slowing down kicks in right but as opposed to this uh for a dense feature what would happen is you had the B feature which was a dense feature now this history is accumulating faster right because now the derivatives are going to be non-zero uh quite always right because this is like an always on feature so your history is accumulating fast uh it will not be zero for most of the time steps and therefore what will happen is uh VT will start growing rapidly and hence this will start decaying rapidly right so now effectively what will happen is this advantage that you had that you quickly move in the direction of B and came to where you want it to be that will not happen right because now this directions the in the increments in the vertical direction will happen a bit slowly as opposed to earlier where it was quickly moving in the vertical Direction now you are moving a bit slowly in the vertical Direction because of this effective learning rate decaying as you keep making updates right so that's what is happening in this algorithm okay and we can now see that uh visually uh so here uh d w let me just annotate the plot a bit so here DW is essentially this Delta W that I have uh and BT is the history that is getting accumulated for the weight w and ETA T is the learning rate for the weight w right so now what is happening let me just uh show you things here right so as I had said right the VT will keep increasing right because it's accumulating the gradients of course at some point when you reach close to the Minima and your Delta W becomes zero then there's no more additions happening right so that's why this will saturate here in this region right and that you can see here if you look at the derivative right in that region the derivatives have now almost become zero and hence no additions are happening to the history right no large additions are happening to the history it is very close to zero so some small small additions are happening but it's not reflecting in the uh plot right and uh looking at this also so initially remember that the way your plot was and this this itself is interesting and we should try to look at it in the context of the uh now the last plot that we had right how is the derivative uh changing so I'll make some comments I'll go back to the loss plot and I'll make some comments there and then come back here yes yes and I'm on the access x-axis you have the number of iterations right yeah so if I look at the loss plot right and let me just clear this and then make some comments here yeah so remember we had started from here okay so initially the uh you are in a slightly it's t steep right from the beginning it's steep right but the steepness is increasing that means as I keep going down my derivatives will become larger and larger right in magnitude the direction may be positive or negative but the magnitude will keep increasing then when I enter this Valley region my derivatives will start becoming smaller again right until they reach a point where the derivatives will become close to zero right so remember initially they'll increase because I'm going down the slope and the slope is increasing so the derivatives will increase and I'm showing it this way because they are increasing in the negative direction right so the slope here is negative uh so this is what the curve is like so here the slope at this point it's negative right because you're going down as you are going down or as you are moving in the X Direction the value of y is decreasing right or as so that's why the slope is going to be negative so it's going to increase in the negative Direction then again it's going to start decreasing and then it will come to zero right and that's exactly what is happening in the derivative plot that we just saw now let me go back to the plot yeah so this is what is happening right so initially uh here your derivatives as you keep going down it keeps increasing in magnitude right I mean remember that you are increasing on the negative side which is fine its magnitude is increasing and then at some point because here I'm going to take the square that's why I'm talking about the magnitude right and then when it comes close to the Minima it again starts decreasing in magnitude and then it comes to zero right and that's because the loss surface was like that right so this is how the history is getting accumulated this is how the derivatives are changing and this is what is happening the red curve tells you what is happening to the effective learning rate as your history is accumulating the learning rate keeps decreasing exponentially right so if uh either to just plot let's just see this is what the learning rate curve looks like right I'm the same plot I've plotted separately so the red curve I plotted in a new plot so that I can have more range on the y-axis so the y-axis tells you the learning rate and the x axis is the number of iterations as the number of iterations keeps increasing the learning rate keeps decreasing exponentially right so here it's continuously decreasing okay and here for example in the first step the learning rate was 0.72 because I had initialized ETA to 0.1 and then the denominator turned out to be 0.01 because V 0 turned out to be uh something which led to this point zero one nine and then my learning rate initially was 0.72 but as I kept going down it decreased exponentially right so the main thing here to show is that my I had kept my learning rate initially learning rate is 0.1 but it became 0.72 right because initially my history was not large and hence the learning rate increased and then it started decreasing again and that's exactly what we wanted initially it could be high and then it starts decreasing okay and the same argument for B also uh so here for yeah so the same argument for uh B uh so the derivatives so here the scale is a bit different right so derivatives are on the x axis you have two thousand four thousand sorry twenty thousand forty thousand sixty thousand that's why you're not able to see the DWS here carefully uh so that again the same behavior happens your gradients change uh uh as you go along and the history keeps getting accumulated the only thing I need you to focus on in this plot is that your history is constantly increasing and as your history is increasing here your learning rate is decaying even more faster right so here notice that the learning rate starts off itself with a very small value which is point zero one zero right and again I'd initialize the learning rate to the same value right so here again so here see remember the learning rate was 0.1 but your initial gradients were high because B is a very Spa dense feature so V 0 itself was 84.45 right and since that was high your initial learning rate itself was small and hence you are moving less aggressive in the direction of b as compared to gradient descent and uh nestro and moment right and that's why right these gradients are also getting accumulated very quickly it's already 84 this history right the blue curve that you see already starts off at 84 and it keeps growing from there on right that's why it reaches very high digits right it treats reaches five digit numbers and that's why your learning rate gets killed off aggressively right because your denominator has now become five digit number right the square root of a five digit number okay and that's again in line with what the intuition that we had right so these plots are again an explanation of everything that we saw right we're just seeing how the learning rate is changing how the gradients are accumulating aggressively in the case of B and hence the learning rate is decreasing aggressively right and the main main point is that in B the learning rate started off itself with 0.010 and then kept decreasing from there whereas for w the learning rate started off with 0.72 which was higher than the ETA which I had said and then started out decreasing from there and that's exactly what we wanted because W is a sparse feature so let the learning rate be higher and then let it decay as the gradients accumulate B is a dense feature so let the learning rate be small because anyways which are going to get large gradients there right so that adaptive Behavior has been seen here right okay yeah now one more thing to note is that even though as you come close to the Minima right as you Class come close to the Minima your derivatives have become zero you here you can't see them also because the scale here is very uh large uh so the derivatives have become zero that means these these quantities right that means these quantities have become zero right but your history does not become zero and that makes sense right because it's a running sum you're continuously summing it up so even if the new terms that are getting added are zero you still have the previous terms which you had accumulated in the sum so your history will going to be remain large only right so that's what is happening here ah and that could cause problems right and that exactly what is causing problems near the Minima when your gradients are becoming small right but your history is still large and hence your effective learning rate is small so now the gradients are small the effective learning rate is small and hence when you're close to the Minima I'll close in these flat regions you're not able to move fast right because you are not getting rid of this accumulative history that you have and we need to see if we can do something about that right so let's just uh look at that is it clear what I just said okay so now by using a parameter specific learning rate uh we have ensured that despite W being sparse we are still being able to move in that direction and also we have ensured that uh B is going undergoing a lot of updates but its effective learning rate is decreasing uh and there is this was something which is to ponder about which is not theoretically Justified now you could imagine that in the denominator I could have had ETA divided by VT as opposed to square root of VT right I would have got the same effect that my learning rate is inversely proportional the effective learning rate is inversely proportional to the accumulated history right but if you replace square root by just V T then this algorithm does not work so well and at least my guess is one of the reasons for that is that if you don't take the square root then these quantities become very large very quickly right and maybe that kills of the learning rate in an unfavorable way right but it's there's no clear theoretical justification that I have seen what's the flip side while we got an Adaptive learning rate for W and B the flip side is that over time the effective learning rate for B became so small that it was having a difficulty in moving in the W Direction in the B direction right so you see that now I was able to reach the right value of w right I was able to reach the right value of w that's the right value of w because my Minima is also here right so I'm in line with the right value of w but I took longer time to find the right value of b as opposed to the other algorithms which found the right value of B first and then took a longer time to find the right value of w right and this is happening because the history for B has accumulated and the updates in B are now very very small and we saw that in the plots where on the x axis on the sorry on the y-axis you had very large numbers which indicated a large amount of History has been accumulated so the question now is can we avoid this