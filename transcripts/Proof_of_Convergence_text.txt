[Music] so welcome back uh we're now going to talk about the proof of uh convergence right so the idea is that you have some intuition about how this algorithm works so let's now formalize it right whether it will actually converge or not right so i'm going to first state the theorem and before that i'll define something so two sets p and n of points in n-dimensional space are called absolutely linearly separable and i'm just going to read out the definition if n plus 1 real numbers w naught to w 9 exist and you recognize these numbers or these variables as the weights that i want right so that's what i want to find so of course the definition will contain these ways right so if such numbers exist such that for every point right and remember the points are themselves vectors right so this is oops a vector x 1 to x n belonging to p so every vector belonging to p satisfies this condition and similarly every vector belonging to n satisfies the other condition right and this could be greater than equal to zero right okay i have okay i fixed the uh inequalities so uh every point belonging to p should satisfy the greater than equal to w naught condition and every point belonging to n should satisfy the less than w naught condition right so if that happens then the points are said to be linearly separable that's just a definition now um so now here's the statement of the theorem right so if the sets p and n are finite and linearly separable that just means your data is finite and is linearly separable the perceptron nulling algorithm updates the weight vector a finite number of times right so what does it mean so you remember that in the algorithm while convergence while not convergence we are updating the weight vector w is equal to w plus x in some cases w is equal to w minus x in some cases right and the question was what if this keeps toggling and then we keep updating the vector infinitely or we never exit the loop like the loop becomes an infinite loop so this theorem is saying that will not happen right it will update it a finite number of times right so what is it saying that if the vectors in p and n are tested cyclically one after the other and that's what we are doing we're picking up vectors randomly for p and n our weight vector w is found after a finite number of steps t such that it can separate the two sets p and n right so that's what this is saying that this will not go on infinitely if your data is linearly separable the perceptron learning algorithm will find a weight vector w after a finite number of steps right now how do you prove this so let's look at that so let's look at the proof of this okay so i'll start with the proof uh first i'll talk about the setup so if x belongs to n right it's an a it's a point for which the output is 0 then minus of x belongs to p this is trivial because if x belongs to n then it means that w transpose x is going to be less than 0 which implies that w transpose minus x is going to be greater than equal to 0 right so now what this observation allows me to do is that we can just consider a single set p prime which is the union of p and n minus what is n minus n minus is the negation of all the points in n right and now for every point belonging to p prime right which is the union of p and your n minus i want to ensure that w transpose p should be greater than equal to zero right so this is just sim just changing the notation so earlier if i wanted w transpose p should be greater than equal to 0 right and w transpose n which is an n is a point which belongs to the negative side should be less than 0 right but now what i've done is from every n i have constructed another p right which is just the negative of n so now every point in my data set has the output to be plus one right it has the output to be p right one sorry uh because i've just take the negative other point so for whatever point for the original point the output was zero so for the negative of that point the output should be one right so i've just constructed the single set which is a collection of all the points for which the only condition that i need to satisfy now is w transpose p should be greater than equal to zero for all the points in p prime where p prime is as defined on this slide right so just a simple change nothing uh great here right this will just uh simplify some of the proof that some of the steps in the proof right okay so i'll start with the algorithm so p is all the inputs with label one n is all the inputs with label zero n is this negative of and minus is the set which contains the negations of all points in n and now i'm defining p prime as a union of p and n minus right and now i'm going to initialize w randomly again i'll have this convergence condition and what i'm going to do i'm going to pick up a random p a random point belonging to p prime where p prime is the union defined above so what am i going to do till it does not converge i'm going to pick up a random point p belonging to p prime and for that point i'm going to check if the condition is not favorable right this the condition should have been that the summation w i p i should have been greater than equal to 0 but if that is not the case then i'm going to make an update and my update is just going to be w is equal to w eclipse p right so notice that i don't need the other if condition because now i have converted all points to points such that the label has to be 1 that means the only condition that i care about is that for all points the dot product between w and p should be greater than equal to zero if that condition is not satisfied which is what this if loop is saying then i need to make a correction as i just simplified my algorithm nothing has changed conceptually it's just that i've conveniently changed all the ends to n minus and then that's why all my points in the data become positive points that means for all of them the label is one now and then in that case i only need to worry about one condition right that's what i have done further for convenience what i'm going to do is that i'm going to normalize all my inputs i can do that right whatever inputs i have i'll just normalize them and that does not change anything because my original point was p and i have normalized it so it has become p divided by the norm of p that's what normalization mean and if w transpose p is greater than equal to zero then this is also greater than equal to zero so it does not change anything with respect to the conditions so it's a simple step which does not change anything conceptually for me so i'm just going to normalize all the inputs and that ensures that the norm of p is equal to 1 right and this will help me in simplifying some steps in the proof okay so and now i'll define one more quantity which is w star so that let w star be the normalized solution vector what do i mean by that i've started with the assumption the proof says that if the data is linearly separable so if the data is linearly separable that means there exists some w right and i'm going to call that w as w star such that that is the value which will linearly separate my positive points for my negative point so some w exists i'm sure about that and i'm going to call that w star it's just that i don't know what that w star is but i can assume the existence of such a w star right so that's the w star that i'm going to use right we don't know what it is but it exists right now suppose at some time step t uh we inspected the point p i right what does that mean i have points p1 p2 all the way up to such some pm which are belonging to my set p prime right these are all the points and at every point inside that loop i'm going to randomly pick up one point right so that lets that point be pi so i'm at the t iteration of the loop and the point which i have picked up is p i okay and we found that the condition is violated we wanted w transpose p to be greater than equal to 0 and suppose it's less than 0 right suppose the condition has been violated okay so now what will i do we make the correction that my new weight vector is going to be the weight vector the current time step plus pi right that's what the correction that we are going to do is that correct okay now if i am going to make that correction so now let beta be the angle between w and w t plus 1. okay now what does cos of beta going to be it's going to be this dot product okay and why don't i have a w star here the norm of w star in the denominator because i had already assumed that w star is the normalized wake twitter right that means the norm is equal to 1 so i don't need to write that in the denominator okay so this is what cost beta is now let's just try to expand this right so i'll now look at the numerator alone uh let me just get rid of some stuff here so the numerator is uh w star the dot product between w star and w t plus one remember that w star and w t plus one are weight vectors right these are vectors so i can talk about the dot product and now i can just substitute the value of w t plus 1 which is nothing but w t plus p i which is again of course a vector it's the addition of two vectors now i'm going to open up these brackets so i get w star the dot product between w star and w t and the dot product between w star and p i okay now i'm going to define a new quantity so let me just explain something before i define that quantity so i have the points p1 p2 all the way up to some pm right these are all the points which belong to my p prime okay now i can compute the dot product between each of these points and w star okay i don't know what w star is but i can define the dot product i can say that this is the dot product okay and now what is dot product dot product is some scalar value right so for each of these i'll get some value belonging to r right some scalar value i will get now i can define the minimum of all these quantities okay right so let me call that minimum as delta so what is delta i have computed the dot product between w star and each of the points in p prime and whatever is the minimum value i am calling that as delta okay so what does that mean that any of these quantities here are going to be sorry [Music] yeah are going to be greater than equal to delta right because delta is the minimum value so all these values are going to be greater than equal to delta so that's how i'm going to define delta so now with that definition i can say that this quantity here is going to be greater than equal to this quantity this directly follows for my argument that all of these quantities are going to be greater than equal to delta so irrespective of which of these pi's i have picked up p1 to pm the wstar.pi is going to be greater than equal to delta hence this sum here is going to be greater than equal to the sum here right follows simply from the definition right where delta is the minimum dot product between w star and all the points that i have okay is this definition clear okay so now let's proceed so now i can again for w t again replace it by the definition right so wt was again wt minus 1 plus some pj so at the previous time step i would have taken some pj right and wt minus 1 i would have adjusted by adding some pj so pj is again one of these p points that you see here so again i'll do that expansion so i'll have i let me just yeah so this is what the expansion will look like and now again this quantity i know that this quantity is going to be greater than equal to delta the same argument so this quantity is the dot product between w star and one of these points and if delta is the minimum of those dot products so this quantity is going to be greater than equal to delta so i can replace that by delta and now i will have 2 delta is that ok and i can keep going like this unless a till from t i'll have t t minus 1 t minus 2 till i reach w naught right and i'll have some multiplier k here right so now the question is why is this multiplier k and why is it not t t is the number of steps i have taken right but at every step remember i may not have made a correction because for some pis that i would have picked the right condition would have been satisfied and i wouldn't have made a correction so my k the number of updates which i make is going to be less than equal to t that's why i have k here and not t delta so k is a quantity which is less than equal to t because every time i make an update this bracket opens up and then i replace some quantity there by a delta so this bracket will open up that means this uh this in many cases this pj would not have been there right i would have not changed my weight vector so this bracket will open up only that many times as many times the condition was violated and the maximum number of times the condition can be violated is t because i have made t update so far so i would have made some k less than t updates is that okay less than equal to t updates right ah let's see yeah so we do not make a correction at every time step so we make a correction only if this condition is satisfied and so at time step p we would have made k less than equal to t corrections this is exactly what i had said and every time we make a correction we get a delta term here right in this equation right because here i made a correction so because of that i got a delta here here i made a correction so because of that i got a delta again here right every time i make a correction i get delta so i will get k such deltas okay so this is what the numerator looks like okay now so so far we have that w transpose pi is less than equal to zero hence we made the correction right for the current point this condition was the unfavorable condition was satisfied hence i had made a correction and cos beta was given by this definition of that the numerator is greater than equal to this condition right so the numerator is equal to this and then i derived a series of things that the numerator is greater than equal to zero okay now i'm going to focus on the denominator okay so the denominator is the norm of w t plus 1 so the square of the denominator would be the square of the norm okay now again w t plus 1 the norm i can first i'm going to expand wt plus 1 which is wt plus pi right so wt plus pi norm square right that is what this is and the norm is just the dot product of the vector with itself right so wt plus uh the square root of the dot product so now this would be wt plus pi and dot product with wt plus pi okay now again i'll just open up the brackets okay so i get the square of the norm of wt plus 2 times wt dot pi okay plus pi square and i'd assume that the pi's are normalized so the norm of pi is going to be 1 so i'm just going to replace that by 1 okay and i'm going to do something else also so how did i get from here to here because of this condition i know that w t p i is less than equal to 0 that's why i am here right and if that is the case then 2 times w t p is also going to be less than equal to 0 that means this sum is going to be less than or equal to this sum let me just get rid of all the marks so this sum okay has a quantity which is negative okay this sum does not have that quantity so it follows that this sum is going to be less than equal to this sum that okay because it's just this sum plus a negative quantity so it's going to be less than equal to okay now the next step of course is to replace p i squared by 1 because p i's are normalized this was the simple assumption i had made now by induction again the same thing will happen i'm going to now i started with wt plus 1 and i ended up with this now i'm at wt i'll again do the same step there itself right and what will i get this is going to be wt minus 1 plus 1 right this will again happen this one which i had come here again the same thing will come here and i'll get 2 and how many such ones will get added the same argument as many times i make an update a 1 will be added right where is the one coming from the one is coming from this quantity if i made an update then i had added a pi if i had added a pi then i'll get this p i squared term and i'll get a 1 there so again here i'll have k right i'll keep simplifying this and i'll get to w naught square plus k by the same observation that we made about delta while dealing with the numerator okay so now we have the numerator is greater than equal to a certain quantity and the denominator square is less than or equal to certain quantity so now these two conditions combined with the definition of cos beta will give us the condition this cos beta is greater than equal to this quantity so i have simply substituted the numerator and i have substituted the denominator the simplified denominator and this would be greater than equal to because the numerator is greater than equal to this quantity and the denominator is less than equal to this quantity right so the numerator is greater than something the denominator is less than something so if i put the numerator over the denominator then the end result would be that cos beta is greater than equal to this quantity that you see here right so now here you have a k in the numerator and a square root of k in the denominator so which means i can roughly say this cos beta grows proportional to square root of k right and what is k k was the number of times i have updated my weight vector and this is the quantity that i wanted to handle on this is the quantity that i wanted to show was finite i wanted to show that k cannot tend to infinity okay now cos beta is going to be greater than or cos beta is going to grow proportional to square root of k so now as k increases cos beta will increase right now if k becomes arbitrarily large if it goes to infinite that means my algorithm is not converging it's it's keeping on updating then what will happen to cos beta cos beta will keep on increasing but can cos beta keep on increasing no cos beta is bounded between uh minus one and one right so cos beta has to be less than equal to one so cos beta cannot grow infinitely that means k cannot go infinitely k has to be bounded by a maximum number hence we have shown that k is a finite number so if the data is linearly separable right that means after a finite number of iterations the perceptron rolling algorithm will converge so although in our toy example it does not seem intuitive right that's easy to build this intuition that okay for one point i changed it for the second point again i changed it why were these updates not counterproductive so now we have a proof which says that the number of updates you will make will be finite if the data is linearly separable okay so that's what the perceptron uh this that's what the proof for the perceptron's learning algorithm say that it will converge okay so now coming back to our questions which we had started out so what about non-boolean inputs yes so real valued inputs are allowed in perceptron do we always need to hand code the threshold no now we can learn all the parameters not just the threshold w naught but also w1 to wn are all inputs equal no now a perceptron allows weights what about functions which are not linearly separable so this is still not possible with a single perceptron because we have seen that the boundary is linearly separable the previous proof was also that it will be able to find the weights only when the data is linearly separable right but we'll see how to handle this and that's what this course is about that what if data is not linearly separable okay so i'll end this module here and we'll talk a bit more about linearly separable functions