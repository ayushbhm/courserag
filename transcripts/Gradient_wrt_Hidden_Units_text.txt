foreign [Music] okay so now let's come to the next station in our journey which is we wanted to compute the gradients with respect to the hidden unit so it's again following this outline so we are done with this part right and now we want to talk to the hidden units and remember there could be many hidden layers right but I want to come up with a generic formula so that irrespective of whether I am in layer hidden layer 1 2 3 4 it doesn't matter I should have a similar formula or a similar set of computation and I don't want to derive this formula independently for every layer right the output layer of a spatials I am okay with deriving a formula for that but I don't want to do it for every hidden layer right so that's what we'll focus on okay so let's start so before that right so let's try to see what we are where we are headed right so this is our hidden layer okay this is the activation at the hidden layer so in particular this is say h to 2 because is the second hidden layer and the second neuron in the second hidden layer and I want to take the derivative of the loss function with respect to H22 and what this diagram is saying that there are multiple paths from the loss function to this H2 2 right that's what it is saying and there is some significance of that so what we'll try to do right um suppose I have variable Z okay and I compute say one function of Z right and let me call the function as q1 and suppose Q 1 of Z is just Z squared okay so I compute that so I've computed uh q1 of Z from Z similarly I say have another function right say Q2 of Z let that be Z Cube and I have computed that also from Z right and then let me have say Q 3 of Z which is equal to e raised to Z it's all of these are functions of Z and then say using these three as inputs I am Computing some p okay and suppose that P is just say q1 divided by Q2 I'm just not writing the off Z in the bracket right just to avoid I mean this is cumbersome to write and now is p a function of Z yes it is a function of z y because q1 is a function of Z Q2 is a function of Z Q3 is a function of Z so this p is also a function of Z because it has been computed using quantities which in turn were computed using right so I have this kind of a situation so now I can can I ask the question do can I ask you to compute the derivative of P of Z with respect to Z what do I mean by that would this derivative exist like or would it just be 0 the derivative would exist right because I just said that P of Z is a function of Z right because it is computed using quantities which in turn depend on zerlet so this function this question is a valid question now how will you compute this derivative this derivative would simply be the derivative of P right I'm just going to use shortcuts now with respect to q1 into the derivative of q1 with respect to Z plus the derivative of p with respect to Q2 into the derivative of Q2 with respect to Z Plus the derivative of p with respect to Q3 into the derivative of Q3 with respect to Z right so what am I telling you I am going to sum the derivatives along these three parts right I'm just going to apply the chain rule along these three parts and then sum the derivatives right so that's what you do when you have this kind of a situation this is just from basic calculus I've just revised that and now I'll just write this as a more compact formula right so this is the formula that I would write that if I want to compute the derivative of P of Z with Z such that there are multiple paths right there could be as many rights as such m paths such that I have Z I am Computing some intermediate quantities q1 Q2 up to q m using this Z then if I want to compute the derivative of p with respect to Z then it would be p with respect to q1 into q1 with respect to Z plus p with respect to Q2 into derivative of Q2 with respect to Z and I'm just going to sum all these parts that means there are M such parts and I'm going to sum the chain rule across each of these parts set and in any One path the chain rule this is what the chain rule looks like it's a product of these two quantities which is derivative of p with respect to q and then derivative of Q with respect to Z right so that's what it looks like straight forward this is what you know already from calculus and just quickly revise that right and that's exactly the kind of situation that we have here you have this quantity H22 you are interested in the derivative of the loss with respect to H22 you know that the loss depends on H to 2 because it is it depends on quantities which you are computed using H22 right which dependent on H2 so the loss depends on H2 and there are multiple paths how many parts are there there are K Parts here right because from s to 2 you go to a31 okay and from a31 you can go to the loss a32 you go to the loss because the loss depends on a31 a32 all the way up to A3 K all of this the loss depends on and all of these in turn depend on H22 so there are K parts from H22 to the loss function so if I want to compute the derivative of the loss function with respect to H22 I'll have to sum across these K paths right so in my case the P of Z is the loss function my Z is equal to h i j I took a specific h 2 2 but it's in general h i j and my intermediate outputs are these alms right so al1 al2 Al 3 and so on those were my intermediate outputs right okay so with that let's try to see how to find the derivative so again we are interested actually we are interested in the derivative of the loss function or the gradient of the loss function with respect to H2 remember H2 is a vector but I won't go to the vector directly I'll focus on one element of this maybe H22 and in general I'm just going to call it as i j right where I is the layer number and J is the neuron number right neuron number in that layer okay let's remember that so that is the setup and based on the formula that I had there are K paths which take me from an H I J to the loss function okay so I'm going to sum over all those K paths and each path I'll have this chain rule which I am going to compute and sum over those chain groups and what is the chain rule saying derivative of the loss function with the a unit in the next layer right so I want I so in the next layer the the a neuron in the next layer depends on the uh H neuron in this layer so that's a i plus 1 okay and there are K of those so I'll sum over M to 1 to K and then the derivative of that with respect to h i j right and I'm confidently doing this because just on the previous lecture or the previous video I have actually shown you how to compute the derivative of the loss function with respect to A3 so I already have this quantity right so I am happy that this quantity is in the path because that quantity I have already computed okay so let's go ahead so as I just said uh this quantity I have already computed in the previous video right you can see that we had computed the partial derivative of the loss function with respect to every neuron in the uh output layer right a31 a32 all the way up to A3 K so this quantity I have already computed right here 3 is equal to I plus 1 and these are the m so this I have already computed right what I didn't know was this right so what is this saying let's see I want to compute the derivative of one of these guys okay with respect to one of these guys that's what I want to do and I am saying that that derivative is this quantity most of you would get that if you don't get it let's try to see how we get it so give me a couple of minutes while I write a few things on the slide foreign how I went from here to here right so let's first understand how did we compute A3 this is what our formula for A3 was it was the weighted sum of the inputs plus the bias Vector right so it was just I took this as the input this red guys as the input which is the entire H2 Vector then multiplied them by the weights which was W3 and then added the biases which was B3 right so that's what this formula is capture now I've just expanded that formula I have taken the case where I have only two units two neurons in the output layer that's why I have only a31 and a32 and I have three neurons in the previous hidden layer that's why I have it's 2 1 h 2 2 h 2 3 so this weight Matrix would then be or 2 cross 3 Matrix multiplied by a three cross one vector and the bias is I would have just two biases corresponding to the two output neurons so I have this two biases right so that's how this computation is now let's write down one of of these guys right so a 3 1 is what is that equal to w three one one I'm just going to do the multiplication into H to 1 plus W 3 1 2 into h 2 2 plus W3 1 3 into h 2 3 plus the bias term which was B 3 1 okay now what are the indices here this is I plus 1 right so I plus 1 is equal to 3 this is my M right because I am trying to take the derivative of uh a i plus 1 comma M with respect to h i j right so my I plus 1 is equal to 3 and my m is 1 in this case and of course my I would be 2 in that case okay so my I is 2 Now what is my J I had taken the specific case of h 2 2 so my J is equal to 2 right so now suppose I want to take the derivative of this quantity which I'll write here derivative of a 3 1 with respect to H to 2 okay where this is I this is J and this is M okay now what would that be I have written the expanded formula of a31 and I am taking the derivative with respect to h 2 2 so this will disappear because this does not depend on its 2y H22 this will also disappear this will also disappear so what will remain is w312 into H22 and the derivative of that with respect to H to 2 would just be w312 and what is w312 what are these indices 3 is actually equal to I plus 1 okay one is coming from the m and this 2 is coming from the J right so that's why what I get w i plus 1 M J right so this is the mgth entry of the w i plus 1 made by tricks in this case it is the 2 comma 2 entry of W3 okay so that's how you should read this so I hope this is clear now we'll move ahead so this we had already computed in the previous lecture and this we just uh proved why this is equal to this way so am I done here not quite yet so let's let's uh spend some more time on this so now suppose I have this Vector right what is this Vector this is the derivative of the loss function with respect to the layer I Plus 1. this is how I will write it it will be a collection of the partial derivatives so in this case since I plus 1 is equal to l i have K elements here so I have the derivative of the loss function with respect to the first neuron in the output layer with respect to the second neuron in the output layer third neuron all the way up to kth neuron right and now what is this w i plus 1 so I had the w three Matrix okay and this dot here means here I should have had the row index instead of the row index I have a DOT here that means I don't care about the row I'm going to take all the rows and all the rows from which column the jth column right so that is what this notation means I have the W3 Matrix I'm going to take all the rows of that Matrix because I have not specified an index for the row but I haven't specified an index for the column so that means I'm taking all the rows for the jth column which is the same as taking the entire jth column right so how would I write that Matrix this is how I'll write this this is w 3 the first row jth column second row jth column all the way up to care through jfcon right so these are two uh matrices that I have now if sorry these are two vectors that I have I have still not shown why I came from here to here right that will become clear in a while why did I start discussing about these two just assume that these two are the vectors given to you right now what is the uh dot product of these two vectors going to be I'm going to take the dot product of these two vectors so it's going to be this multiplied by this this multiplied by this all the way up to this multiplied by this and the summation of those right so they are going to be K terms in the summation and every term in the summation is going to look like the product of these two quantities right so this quantity that you have here is essentially the dot product between the jth column of the W of the weight vector and this gradient Vector which you had already computed right so that's all it says this is the same formula that I have here and again this is a not very difficult right so even if you didn't get it now you could just go back and take a look at the slides all the notations have been explained clearly if you understand the notations this is a straightforward leap from there right so whatever we had computed what did I what am I trying to say what's my summary I have computed the derivative of the loss function with respect to one of the Hidden units right I wanted to in fact not even this right just one the dark red guy right I actually wanted to compute the derivative with respect to entire H2 but I've just computed with one element of H2 right and that is this quantity it's the dot product between two vectors one of the vectors is the jth column right and J comes from here the jth column of the weight Matrix and the gradient Vector right the gradient Vector is the uh what we had already computed earlier okay so where do I go from here I have the formula for one of these guys I want the formula for all of these guys right I want now to compute the derivative of the loss function with respect to any hidden layer h i okay so what will it be it will just be the collection of the partial derivatives with respect to all elements of those that hidden layer which is h i y and h i 2 all the way up to h i n and I just know the formula for one of these guys at h i j so I can just substitute the value of h i of I and J accordingly so I'll keep the I as it is because I have I here and wherever I see a j I am going to substitute 1 2 3 all the way up to n right so this is what it's going to look like it's a DOT product between the First Column of the weight Matrix with the gradient Vector the dot product of the second column of the weight Matrix with respect to the gradient Vector the dot product of the last column of the weight Matrix with respect to the with the gradient Vector right so what is that looking like this is the so let me just try to write the Matrix W transpose right w i plus 1 transpose so this would be the first column of w i plus 1 this would be the second column of w i plus 1 all the way up to the nth column of w i plus one remember I am writing the transpose hence I am drawing rows but saying Columns of w i plus 1 and all of this is getting multiplied by this gradient Vector right this gradient so essentially what I'm doing is that every uh uh every uh element of this Vector is essentially the product of one row of the Matrix with this gradient Vector right so now I can write it even more compactly I can just write it as the product between the Matrix W transpose and the gradient Vector right that's what I have shown here because the first element of the resulting product is going to be the dot dot product between the first row of this Matrix and the gradient vector the second row of this Matrix and the gradient Vector the third row of this Matrix and the gradient vector and I can write this entire thing as just very compactly as a matrix Vector computation and this is what I have done here right again if you are confident with your linear algebra if not there's a separate set of lectures that I put out on linear algebra you can go and look at them but this I'm sure most of you would be aware of it so I have taken the Matrix and I could write the entire gradient now this is the gradient of the loss function with respect to the hidden Unit H I which I can just compute as this and why am I confident of computing this this I already know it whatever are my current weights what is the current W3 Matrix looking like whatever it looks like I just have to take that and this I already computed in the previous lecture this was the gradient of the loss function with respect to the last layer right because here I is equal to 2 so I plus 1 is equal to 3. but now I have a problem right I have a slight problem here what is my problem now suppose this was derivative of the loss function with respect to H1 right so then what would this have been so I is equal to 1 now so I'll just substitute this this would just be W2 transpose this is not a problem for me I know the weight Vector whatever is the weight Vector I'll use that but this would have been derivative of a 2 with respect to loss function right that means this guy and this I have not computed yet right so if I compute this if I have a generic formula for computing this I already have a jumbled formula for computing the derivative of the loss function with respect to h i right if I have a generic formula for computing the derivative of the loss function with respect to any AI then I am done right because if I have any AI that I could be 1 I could be 2 I could be 3 I can just substitute it here depending on which h i I am dealing with right so I have not done that yet I've only done for the spatial case when uh I is equal to l right the last layer so this part I need to do now so that is what I am going to work on so this is what I said right I don't know how to compute this quantity when I is less than L minus 1 that means I have only done this I have not done these two guys here right so let's see how to compute it and that's going to be again easy so this is what this looks like so I already know how to compute the derivative with respect to each of these guys right now I want the derivative of the loss function with respect to this so again I'm going to do the same thing I'm trying to compute the derivative with respect to one of the elements here here okay and what would that be that would be the derivative of the loss function I've already computed till here so then I can just use that part as it is and then the derivative of H I J with respect to a i j right and remember that uh how did we compute the edges so this is suppose I look at H2 I had h21 H22 H2 3 and how did I compute that I had computed that from the a vector and there was a one-to-one correspondence right so H22 was just the G function applied to a22 h21 was just the G function applied to a to 1 and h23 was just the G function applied to a to 3. so if I am trying to compute the derivative of H I J with respect to a i j it's just the derivative of this G function that I had used right so I'm just trying to compute the derivative of this with respect to a i j I'm just going to call that as G Dash right because it could be any function but I am just taking the derivative so this is what it looks like this guy I have already computed this is just G Dash a i j right so now again from this one element let me go to the entire Vector so now I want the derivative of the loss function with respect to a i the entire Vector so it is again going to be a collection of the partial derivatives and every element I know how to compute so I have a formula for a i j so now I will go from AI 1 all the way up to a i n I'll just substitute J equal to 1 to n everywhere in the formula and this is what I get right so this is one vector that I have I could think of this as another Vector this is not the dot product between two vectors right because the dot product between two vectors gives me a scalar this is the element wise multiplication between two vectors right this first Vector is simply the the gradient of the loss function with respect to H and this second Vector is just a collection of the derivatives of the uh activations with respect to preactivations right so this element wise matrix multiplication I can show it as this this is called the hadamard product so I can just multiply every element of the first Vector with every element of the second vector and I'll get this quantity right so now I have computed a i Theta and let's see what it depends on right so it depends on the derivative of the loss function with respect to h i right so now let's see if I wanted to compute the derivative of the loss function with respect to A2 then I should know the derivative of the loss function with respect to H2 and then I should know these G's so I already know that that is not a problem right now suppose I wanted to compute the derivative of the loss function with respect to a 1 then I need the derivative of the loss function with respect to H 1 do I know that I know that because I had computed the previous formula and the derivative of H the loss function with respect to H1 depending on the derivative with respect to A2 and I just computed that right so I can just do it one by one at a time and keep going down the chain so now I have the formula for computing the derivative of the loss function with respect to the preactivation at any layer with respect to the activation at any layer and I just keep doing this one by one and whenever I want to compute something everything that I need for that is already compute right I'll just explain that intuition again so I started by Computing the derivative of the loss function with respect to a 3 right that is the first thing that I did and there was nothing special about A3 it was the activation at the output layer now when I want to compute this okay let me just do this on the previous slide Maybe yeah so I started by Computing the derivative of the loss function with respect to A3 so this I had already done right then I wanted to compute the derivative of the loss function with respect to h 2 and I was all set because I need W3 for that and I need the derivative of the loss function with respect to A3 which I had already computed right so I am ready to compute derivative of H2 so this part is done I have now been able to compute this then I wanted to compute the derivative of a 2 with respect to the loss function and the formula on the next slide I'll just go to the next slide now the formula on the next slide say is that the derivative of loss function with respect to A2 just depends on H2 and some quantity right which is easy to compute right this G Prime is easy to compute and we'll compute it at the end right for now just convince yourself because it's a simple function it suppose my uh my my activation function was the sigmoid function then I have 1 over 1 plus e raised to minus a 2 2 right H22 is equal to 1 over e raised to 1 over 1 plus e raised to minus a22 and if I want to compute the derivative of H22 with respect to a22 I can do that right I can apply the UV Rule and keep going and I'll be able to get it right so this quantity is easy to compute okay so that's why I'm not worrying about that yeah but apart from that it only depends on derivative of h 2 and that we just computed on the previous slide right so I'm I have everything that I want till this point then again I want to compute the derivative of the loss function with respect to H1 and I know for that I just need the derivative of the loss function with respect to H2 and I need W2 these are the two quantities involved and I have that because I just come sorry not H2 uh A2 and W2 right so these are the two quantities that I need for computing derivative of the loss function with respect to H1 and I just computed now I want to compute the derivative of the loss function with respect to A1 so for that the two con the quantity that I need is the derivative of the loss function with respect to H1 right and apart from that I need this simple quantity which I just showed is easy to compute right so that's if I just keep going one by one at every stage I have all the quantities that I wanted to uh want or want at that stage for computing the quantity of Interest right so now at this point you should be convinced that we have a generic formula for computing the derivative of the loss function with respect to the hidden layer no matter where the hidden layer is the first hidden layer second third fourth everyone we have this formula so we are almost done and now we'll reach the final part in our uh chain rule so let's look at that in the next module