[Music] um welcome back so we are in the next module now where we'll be talking about the perceptron learning algorithm right so far we introduced the perceptron we saw that the decision boundary that the perceptron learns is linear and then we introduce this concept of error where if the weights are not proper or if w1 w2 values are not proper then you will make some errors in dividing the points into the right half spaces right so that's that's what we have learned so far but we did not talk about how did we get in this w1 wt we were mainly trying to adjust it by hand so now we look at the perceptron learning algorithm which allows us to learn these ways right so now before we do that let us try to understand that apart from implementing boolean functions right i mean which does not look very interesting right why would you care about boolean functions so what is it that the perceptron can be used for and what is this connection to the body and functions right so let's uh let's revisit our example right so now we are again talking about the case of a binary classifier and we want to decide whether to watch a movie or not right and suppose we are given a list of m movies all the past movies that we had seen along with the class label whether we liked this movie or did not like this ok this is based on our past data so this is just a binary label on all the movies right and each movie could be represented by n features so these features could simply be the inputs which are contributing to the decision so one input is actor daemon the other input is the genre thriller and so on and these are all boolean inputs that we have and you could also have some real valued inputs like the imdb rating the critics rating of the movie and so on right so all of these inputs are feeding into our perceptron and there are also weights associated with them it's just that we don't know what these weights are and we want to learn these weights such that once we learn these weights then for any given new input it should allow us to make that decision right and we should be able to learn these weights in such a way that at least for the given data and we'll assume that the data is linearly separable so whatever m data points that are given to us we will assume that they are linearly separable that means you can draw a line or a plane or a hyper plane such that all the points for which the output should be positive are in the one half space and all the points for which the output should be negative or in the other half space right so we will assume that the data is linearly separable and then in this real classification problem right this is not a cooked up problem this is a real classification problem we are then interested in learning these weights right so that's what we want to use a perceptron for for handling these kind of classification problems right ah now let's try to look at the perceptron learning algorithm so i'll define some notation so let p be all the inputs for which the label is one right so these are all the positive points that we have been calling uh in all our discussion earlier right and n is all the inputs which have a label 0 so these are all the negative points right in our discussion earlier so now i do not know what the weights are so what will i do i'll just initialize the weights randomly right and while convergence or while not convergence till i reach convergence i'll do something right now what is the meaning of convergence here so the meaning of convergence here is that if i reach a point right such that my values for w are such that for all the positive points in the data which is given to me my output is 1 and for all the negative points in the data my output is 0 right so in other words if for all the positive points summation w i x i is greater than zero and for all the negative points summation w i x is less than zero that means if i have magically reached a configuration where i have learned the w is because excise i don't have a control over it these are the data that have been given to me so for if all the positive points this condition is satisfied and for all the negative points this condition is satisfied that means i have to been able to clearly separate my training data right and that's what i will define as conversions okay so while not convergence what will i do let's see so i'll pick a random point from the given set of points so the given set of points are the union of the positive and the negative points so i'll just mix up all this and pick a random point from there i'll call it x which is the notation we have been using and now if x belongs to p so remember that x in itself is uh x naught x 1 all the way up to x n right so in our previous example these are the end decision features that i had is actor matt damon is the genre thriller and so on and x0 was this constant input which had been set to 1 right so that is what x the bold x looks like it's a vector and similarly you have these w naughts to w n right which is the vector uh w okay sorry about my handwriting but i think it's kind of clear from the context what i am saying so this these are the two vectors so i am just going to take the weighted sum of w i x i and if the point was positive and if this weighted sum is less than 0 then you and i understand something bad has happened right because if the point was positive i wanted this weighted sum to be greater than zero that is what i have said here right so something is going wrong my weights are not the way they should be so i need to do some correction ok and this is the correction i am going to do i am just going to do w is equal to w plus x okay i am not going to say anything more about this at this point if the point was negative and if my summation was greater than equal to 0 then again i know that something wrong has happened because for such a point i wanted the summation to be less than 0 right but that has not happened so again i need to do some correction and my correction is going to be w equal to w minus x again i am not explaining why this magic formula right why am i doing plus x in one case and minus x in one case and how does this lead me to where i want to go and where do i want to go i want to go to convergence where my weights are such that that these two conditions are satisfied by all the positive and negative points right so how does this ensure that that is what is going to happen sorry okay yeah so why would this work right that's the question that i'm trying to ask right it's not very clear to me that why should this work so to understand this we'll have to look at a bit of linear algebra and a bit of geometry right so let's let's go there so we have two vectors w and x and these are what those vectors look like w naught up to w n just as i had written on the previous slide and x s one up to x one x two x n right and now if i consider the dot product between these two vectors then this is exactly what i get and this is what the expression that you are seeing on the previous slide i was taking some decisions based on whether this value was greater than z equal to 0 or less than less than 0 right so this is the quantity that we are dealing with and now i'm just telling you which is obvious that this quantity is nothing but the dot product between the weight vector and the data vector right the data point that i have so we can just rewrite the perceptron rule as the following so y is equal to 1 if the dot product is greater than equal to zero and y is equal to zero if the dot product is less than zero nothing magical here the dot product is just equal to this so i have replaced that cumbersome notation of summation and w i x i by this more compact dot product right now this is where now we need to start digging on if you're looking at the dot product right so now i've gotten hold of some quantity that i can try to explore from a linear algebra perspective right what does a dot product mean and then try to come up with some explanation for why should the algorithm work or what does it make why is it making sense right so we are interested in finding this line w transpose x equal to zero which divides the input into two halves right that we have already understood we want to learn these w right so now any a point so which lies on this line it will satisfy this equation i mean that's a trivial statement to make right so if you have the line y is equal to mx plus c right then any point x comma y which lies on this slide will satisfy this equation that's the same thing i am saying is just that now my way of writing equations is slightly different i am calling this as x 1 i am calling this as x 2 and maybe i am calling this as w naught right so that is the only notation change that i have so i have w 1 x 1 right plus w 2 x 2 plus w naught equal to zero so this is the equation of a line which is analogous to this equation that i have and all i am saying is that any point which lies on this line satisfies this equation and that's a trivial statement to me now what can you tell us about the angle between any such point right and the vector w so first of all we have to again realize that both x and w are vectors and that's exact precisely what is defined here so now i can ask this question and i am saying that any point which lies on the slice satisfies this equation so then if it satisfies this equation what can you say about the angle between the point and the vector w if it satisfies the equation w transpose equal to zero that means these two vectors are orthogonal we know that from linear algebra so the angle is going to be 90 degrees right and because we know that cos of the angle is equal to this formula and if w transpose x is 0 so the cos of the angle would be 0 and the angle would be 90 degrees right so this is slowly we have now started moving into the territory of geometry and linear algebra right so now we know that if the point lies on the line then the angle is 90 degree with the weight vector okay now let's uh go ahead right so if the point if the vector w right if it's perpendicular to every point on the line then it just simply means that it's perpendicular right so if this is the line w transpose x equal to 0 then my weight vector is going to be perpendicular to that so nothing great again right now let's see some points which lie in the positive half space of this line right so this is my line and i'm going to look at the points which lie in the positive half space what is the positive half space it means that w transpose x should be greater than 0 right so here are these points now my question is what would be the angle between these points and the weight vector w without knowing anything just that by looking at the picture you should be able to tell me that this angle would be less than 90 degrees right it's obvious from the picture right and why is oh let's let's see if i want to answer this now and now let's look at some points which lie in the negative half space right which are these red points what would be and by definition if it's in the negative half space and w transpose x is less than zero so what would be the angle between these points and the weight vector again just by looking at the picture it is clear that the angle should be greater than 90 degrees right so if i consider this point here this angle is clearly greater than 90 degrees because i mean you know it is greater than 90 degrees okay so that also is clear now right so now let us try to relate uh the expression that we had with this geometry right this simply follows from the formula so you had the cos alpha is the formula for that is w transpose x divided by the norm of w and norm of x right so if w transpose x is greater than zero right that means cos alpha is greater than zero if cos alpha is greater than 0 that means alpha is going to be less than 90 degrees right so that is what it follows from because the points lying in the positive half space satisfy this expression this inequality that's why the angle is going to be less than 90 degrees and because the points lying in the negative half space satisfy this inequality that's why the angle is going to be less than 90 degrees so you just don't know this pictorially now you also know that it follows from this inequality right so all positive points will have a acute angle all positive negative points will have an angle which is greater than 90 degrees right so now with this explanation in mind let's revisit the algorithm and let's try to see what is happening right so what was i doing if i had a positive point on which i was making an error then i was changing the weight vector by adding the point to the weight vector and that's what is happening if i had a positive point on which i was making an error then i was changing the weight vector by adding the point to it right so let's see what happens by that right so if x is belonging to p and if w transpose x is less than 0 that's what this expression means right then it means that the angle between this x and the current w is greater than 90 degrees right but that is not what i wanted i wanted this angle to be less than 90 degrees so now what is it what should my goal be if i am in this situation that my w and x are such that the angle is greater than 90 degrees and i want it to be less than 90 degrees so i should try to adjust the w at least such that whatever is the current angle that angle should decrease it may not become less than 90 degrees at one shot but at least it should decrease from where it is currently because right now i am in a bad situation and that's what i should do so let's see if what happens if we do this update so this is the update that we do our goal was to want it to be less than 90 degrees we somehow ignored that goal and went ahead and said okay let me do this so what i need to convince you is that whether this is in line with my goal right so if my new w is w plus x so let me now check what my new angle would be right my new angle would be proportional to this right now let me just substitute the value of w new it's w plus x okay let me just open up the bracket w transpose x plus x transpose x w transpose x is cos alpha right so cos alpha nu is actually greater than cos alpha that means alpha nu would be less than alpha that's what i wanted whatever was my current alpha i wanted my new alpha to be less than that eventually i want to get it to less than 90 degrees but i may not be able to do it at one shot so i will slowly try to make it less than right so that's what has happened here okay there is some hidden stuff here right so i've conveniently used this proportional sign and ignored the denominator all which simplifies the calculation for me and it makes it easier for me to show the intuition if you add the denominator and all you will kind of realize that it's a bit more trickier calculation but the idea is just to give you the intuition right so you understand now that with this update the alpha actually becomes less than what it was currently and that's exactly what i wanted right let's look at the other case if x was a negative point and if i was making an error right so if w transpose x was greater than equal to 0 that means the current angle is less than 90 degrees but what do i want i want it to be greater than 90 degrees right so if i look at it at one point one step at a time whatever is the current angle i want the new angle to be greater than this angle right so let's see if that happens if i do w nu equal to w minus x the same explanation cos alpha nu is going to be this i'll substitute the value i'll replace open up the bracket and now you can see that cos alpha nu is actually less than cos alpha right if cos alpha nu is less than cos alpha that means alpha nu is greater than alpha right and that's what i wanted my current angle was less than 90 degrees which was not acceptable to me i wanted it to be more than 90 degrees so this step has at least ensured that my angle and new angle is greater than my current angle and i'll maybe slowly keep doing this and move towards convergence slowly slowly i will keep changing this angle iteratively right because i am going to do this iteratively its not that once i pick up a point and i adjust my w i will never revisit that point right i'll keep doing this till i read convergence that will i keep picking these points again and again cyclically right so at some point i'll keep changing it and then eventually it will cross 90 degree right so that's the intuition that we have here right and that's why that update makes sense still a bit more to it right i mean some of you would start should start thinking oh wait when i see a positive point i make an error i change the w so that this angle becomes something now if the next step if i take a negative point i make an error i again change the w now how does this ensure that i don't keep toggling like this right for a positive point i change the w so that the angle became less than what it was earlier for a negative point i changed the w so that the angle became greater than what it was earlier but if i keep doing this then what if i just keep toggling and never reach convergence right so what's the proof of convergence that we have not done that i have just explained you the intuition behind these steps why do these steps make sense i still need to explain whether this algorithm will converge or not we will get to that later right so now let's try to see this algorithm on a toy data set right so to convince ourselves that the explanation was indeed correct right so we initialize w to a random value uh and so this is what my w is okay i will just initialize it randomly and you will observe that currently my w is exactly opposite of what it should be right it has a angle less than 90 degrees with all the negative points and an angle greater than 90 degrees with all the positive points so this is exactly opposite of what i wanted right yes w n suppose x w transpose x equal to 0 is the equation of the yeah yeah so i'm not happy with the situation because my w is clearly not what i want it to be so let's just run the perceptron algorithm right so what will i do what does the algorithm do it picks up a point randomly from the union of p comma n right so let's say i randomly picked up a point and the point was p1 okay and my condition is not satisfied so w x w transpose x or the dot product between w and x is less than 0 right whereas i want it to be greater than equal to 0 so i'll have to apply a correction right so now i'm going to apply the correction w is equal to w plus x where x is nothing but the point p 1 okay so if i apply this correction let's see what happens so you can see that my w has changed right and you can see the as i was expecting the new angle between w and p1 is now less than the older angle right so this greedy step has worked in doing whatever it was said to do right so it has reduced the angle now let me do it again right so now again randomly pick a point and say this time the point that i pick is p2 okay so i'll compute the angle between sorry i'll just compute the dot product w transpose x and again i am seeing that the uh [Music] sorry yeah this is where i was so i picked the point p 2 and again you can see with this w the angle is still greater than 0 right sorry greater than 90 degrees so i don't want that so i'll have to apply a correction here also what will the correction be i'll add p2 to w and i'll get a new w so this is what my new w looks like and you can again see that the angle has decreased in fact here it has already become less than 90 but i don't care about it all i care about is the angle is decreased now let me pick up another point and this time maybe i picked up n1 right and now with n1 i again need to apply correction because you can see that the angle is actually quite small right it's forget about greater than 90 is actually quite small so i need to again change it and my adjustment would be w into w is equal to w minus x and when i get do that i get this new w right and i can again see that the angle has increased okay now let me do again pick a random point and say it was n3 no correction is needed because you can see that n3 is already making an obtuse angle with the weight vector right so no correction is needed because the condition is already satisfied again i pick up a random point it's n2 so again n2 is already safe because the angle seems to be greater than 90 degrees so i don't need to do any correction now i pick up the point p3 and i need to make a correction because with p3 i can see that the angle is greater than 90 degrees so i will do the same correction and this is what my new w looks like okay now let me pick up the point another point randomly and say it is p 2 no correction is needed because you can see visually that the angle is less than 9 2 again i pick up n1 no correction is needed the angle is greater than 90 degrees again i pick up n 3 no correction is needed i keep doing this i keep cycling through the data and i now know that i have reached conversion i cycle through the entire data once and i will see that no correction is needed that means i have reached conversions and that is what convergence means i will just cycle through the entire data and if i do not have to change my w that means all my positive points have the right angle with w a right angle meaning the correct angle with w and all my negative points also have the correct angle with w so i don't need to change anything right so that's how it has changed now the question whether i could have just kept toggling so i picked up n2 i had to make a change if i pick p1 i had to make a change at the beginning you saw you were making changes so what if these changes were counterproductive right it just moved somewhere then again it moved somewhere and it ended up changing the angle with the original point which i had corrected in the last step right so that is not yet clear right whether the algorithm will keep toggling or whether it will converge that is not clear that something that we need to see