20 years of i won't say wiping i think that's a bit too harsh but kind of moving like making.
a significant paradigm shift right in terms of how things are done right and not just that right i mean.
this is again i find it remarkable right so i said like one major revolution happened in 2014 and within.
three years right in 2017 this another model which is a transformer model came out which is very different from.
the uh recurrent neural network based model which was proposed in 2014 and it also takes the idea of kind.
of attention and puts it on steroids that means there's multi-headed attention and so on and that led to the.
second revolution after 2017 within a couple of years now people are no longer talking about the recurrent neural network.
based models that were so popular for machine translation right now most modern machine translation models are transformer based model.
two very big revolutions in a very short span of time and a key characteristic again was that we are.