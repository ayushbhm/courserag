some output functions which allow me to cover those range right so we'll do that in more detail in this.
lecture but for now just remember that all the hidden activation functions i'm calling as g and the output activation.
function i'm calling as o okay and one example of the output activation function is softmax the other is a.
simple linear function both of these we are going to see soon right now to simplify notation what i'm going.
to do is i'm going to get rid of this right so it's understood that all of these are actually.
functions of the input right i don't need to write it again and again that i have an input and.
i'm computing a function of that input right it's understood that all of these are functions so i'm just going.
to remove that instead of a i of x that converts something i'm just going to write it as a.
i h i and so on right so that off x is something that i have deleted but it's understood.