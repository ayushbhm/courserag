pass so this does not have a gradient Associated it is not some derivative it's just h k minus this.
one which is the activation at the K minus 1 at layer so that you already have so both these.
quantities you have so you now know how to compute the derivative of the loss function with respect to w.
k okay now going to the last part we just come find the biases so you want to compute the.
derivative of the loss function with respect to bias again the same idea so if I look at this bias.
okay and this is connected to this activation pre-activation here and I know what the formula for computing that pre-activation.
was so now if I want to take the derivative of the loss function with respect to one of the.
elements of the bias Vector then I can split it into these two parts this again I already know how.
to compute and what is the derivative of a k i with respect to bki you can just see from.