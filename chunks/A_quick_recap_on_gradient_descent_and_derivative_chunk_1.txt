manner right and we saw that we just move one layer at a time in the backward Direction in so.
that at every point we already have whatever we need to compute the derivative of the loss function with respect.
to that element in the chain right so first of all we had said that the derivative of the loss.
function with respect to any parameter can be expressed as a chain Rule and then you just keep Computing every.
element in that chain and the idea was that any single element is this change is easy to compute and.
you just keep doing that right and we came up with a very small algorithm right which like four or.
five steps and you just compute the gradients of the entire weight Vector entire bias Vector at one go for.
all the weights and biases in the network right now while doing that we had used the gradient descent algorithm.
right now today what we are going to do is we are going to look at various variants of the.